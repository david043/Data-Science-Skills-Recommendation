job_title,company_name,summary
"Data Engineer, Analytics",Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Do you like working with big data? Do you want to use data to influence product decisions for products being used by over half a billion people every day? If yes, we want to talk to you. Our data warehouse team works very closely with Product Managers, Product Analysts and Internet Marketers to figure out ways to acquire new users, retain existing users and optimize user experience - all of this using massive amounts of data. In this role, you will see a direct link between your work, company growth, and user satisfaction. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

This is a full time position based in our office in Menlo Park. Responsibilities
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
Minimum Qualifications
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a MapReduce or an MPP system.
2+ years experience with object-oriented programming languages.
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Ability in managing and communicating data warehouse plans to internal clients
Preferred Qualifications
BS/BA in Technical Field, Computer Science or Mathematics.
Knowledge in Python or Java"
Data Engineer,The Kraft Group,"KAGR (Kraft Analytics Group)

The Data Engineer will drive the planning, design, and development of integration processes to build and improve the data warehouse. The Data Engineer will be responsible for performing ETL and ELT from many disparate systems into the data warehouse and will have the opportunity to complete projects from beginning to end.

Responsibilities & Accountabilities:
Data Integration
Using the enterprise ETL tool, create modify, and improve integration pipelines
Translate business requirements into data warehouse pipelines using ETL/ELT methodologies
Extract and load many disparate systems into a centralized data warehouse
Assist in gathering requirements for new pipelines
Documentation and Data Auditing
Implement data auditing strategies and processes to ensure data integrity
Document complex integration pipelines into easy-to-understand technical specifications
Perform data modeling to document existing and new tables in the data warehouse
BIDW Continuous Development
Monitor and troubleshoot data problems
Identify ways to improve existing processes
Handle multiple projects and meet deadlines
Additional projects and assignments as directed
Bachelor's Degree in Computer Science, Information Systems, or related field.
3-5 years of experience working with data using SQL or similar technology
3+ years of experience using a data integration platform, such as Snaplogic, SSIS, or Informatica
Strong understanding of data warehousing principles and methodologies
Ability to manage multiple projects in a fast-paced environment
Strong communication skills to all levels of technical expertise
Very high attention to detail
Familiarity with BI Visualization tools
Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"
Data Engineer,GEICO,"Do you want to be part of the transformation that's driving forward a truly data-driven company? As a Data Engineer you will work closely with Data Scientists and Machine Learning Developers to build the Big Data foundation that enables advanced AI and machine learning capabilities for strategically important initiatives.

You will be part of an innovative, highly collaborative team involved in data munging and integration, developing machine learning models, building simulation and forecasting tools, and operationalizing solutions on top of the Big Data platform. This position requires excellent technical skills, a strong desire to learn, good communication skills, attention to details, and the ability to self-manage. You will get great exposure as you work directly with a collaborative team to tackle tough business challenges.

Responsibilities:

As part of the Data Science team, you will help build the infrastructure required for optimal extraction, transformation, and loading of data from a wide variety of data sources in our data lake using Spark and Big Data technologies. You will help design and build data stores that integrate various data sources, suitable for development and deployment of important data science models.

You will also create and maintain optimal data pipelines for feeding machine learning services in production, which are providing real-time data-driven decisions for key business functions in the company.

You will identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, and usability. You will recommend technical solutions, and work with appropriate teams to implement them.

You will have an opportunity to work with cutting-edge researchers in advanced analytics and help build an extensive graph database scaled for millions of entities and relationships. You will then optimize the graph database for real-time scoring of machine learning models that will support a mission-critical application.

Required Skills:
3+ years of experience in IT with at least one year of experience in ETL/ELT methods, Java or Scala programming, and Spark and SparkSQL
Experience in data modeling (both transactional and dimensional), using tools such as ERWin
Exposure to Spark streaming and SparkML
Knowledge of NoSQL databases (such as Hbase or Cassandra), and Graph/Network analysis
Most importantly, you have the curiosity, passion, and ability to quickly learn, adapt and implement Open Source technologies
About GEICO

For more than 75 years, GEICO has stood out from the rest of the insurance industry! We are one of the nation's largest and fastest-growing auto insurers thanks to our low rates, outstanding service and clever marketing. We're an industry leader employing thousands of dedicated and hard-working associates. As a wholly owned subsidiary of Berkshire Hathaway, we offer associates training and career advancement in a financially stable and rewarding workplace.

Our associates' quality of life is important to us. Full-time GEICO associates are offered a comprehensive Total Rewards Program*, including:
401(k) and profit-sharing plans
Medical, dental, vision and life insurance
Paid vacation, holidays and leave programs
Tuition reimbursement
Associate assistance program
Flexible spending accounts
Business casual dress
Fitness and dining facilities (at most locations)
Associate clubs and sports teams
Volunteer opportunities
GEICO Federal Credit Union

* Benefit offerings for positions other than full-time may vary.

GEICO is an equal opportunity employer. GEICO conducts drug screens and background checks on applicants who accept employment offers."
Data Engineer,Bank of America,"Job Description:
The Global Information Security Identity and Access Management (IAM) organization defines policy and delivers capabilities to empower the bank to better control access to its data, systems, and infrastructure.

Data is a critical component of the Identity and Access Management (IAM) strategy. The organization's analytics team is building out data pipelines and database infrastructure to support reporting and analytics activities. The Data Engineer will help head efforts to collect, process, transfer and store diverse sets of data.

Key Responsibilities:
Determine the best solution(s) for integrating data from a variety of sources into a common data warehouse
Implement, maintain and monitor batch data pipelines with best practice quality controls
Own the abstract and technical design of databases (logical and physical data models)
Participate in the definition of a IAM Canonical Warehouse Model
Create business and technical metadata for the Warehouse
Define data retention and failover policies

Required Skills:
3-6+ years experience in a data engineering role
Experience with integration of data from multiple data sources and varying technologies
Ability to code, test, and document end-to-end processes
Experience with logical/abstract database design with data at scale using an ERD tool such as erwin or PowerDesigner
Command of at least one common scripting language (e.g. Python) and tools for data integration (e.g. Informatica)
In-depth knowledge of SQL, data modeling and enterprise RDBMS (e.g. SQL Server and Oracle)
Strong familiarity with UNIX and Linux
Experience adhering to robust audit standards
Ability to work in an agile environment and drive projects forward independently
Understanding of disaster data recovery procedures and testing necessary for business continuity
Bachelor's degree in computer science or another quantitative field

Desired Skills:
Knowledge of emerging data integration technologies
Ability to work with analytics team members to optimize application of different algorithms; interest in statistical analysis and machine learning
Familiarity with Identity and Access Management and/or information security-related data
Knowledge of Exadata, Hadoop, Hbase, Hive

Shift: 1st shift (United States of America)

Hours Per Week: 40"
Data Engineer,McKinsey & Company,"Qualifications
1-3 years of experience in the field of business intelligence, application development, database development and ETL and/or data analysis domains with strong Hadoop, Hive and/or Spark knowledge
Bachelors degree/Masters Degree/Diploma in IT, Computer Science, Information Systems, Math, Engineering or related discipline
1-3 years hands-on experience with programming languages like Python, Java or equivalent
Coding & application development abilities
Ability to translate business requirements to functionality, with estimates and implementation plans
Experience in the healthcare industry either pay or provider focused is a plus
Good communication skills
Excellent time management skills (manage estimated workload according to scope)

Who You'll Work With You'll work with our McKinsey Healthcare Analytics team in New York. Healthcare Analytics is a part of McKinsey Solutions and McKinsey's New Ventures.

Our Healthcare Analytics group leverages a big-data analytics platform that includes 100 terabytes of integrated claim, encounter, clinical, consumer, and other data. The team consists of 100 dedicated experts and 250 affiliated professionals with industry, advanced analytics, statistics, clinical, and software expertise, all of whom work to design, deliver, and operate advanced analytic tools to help healthcare clients around the world.

New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for  and expands our investment in  our entrepreneurial culture. Through innovative software as a service solution, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

What You'll Do You will be responsible for coding and testing tools and assets to deliver analytics to our customers which will be defined by high quality.

You will collaborate with the product and delivery team on the definition and requirements of an analytics product, within an agile framework. Additionally, you'll perform the transformation, filtering, and aggregation of raw data into concise, accurate, and focused data marts.

You'll be an integral part of our team with ample opportunities for coaching and mentorship from your senior colleagues."
Data Engineer,Apple,"The Fraud, Engineering, Algorithms and Risk group is responsible for combating fraud and abuse for Internet Software and Services at Apple. The FEAR engineering team is tasked with building mission-critical, robust and scalable distributed systems that can keep pace with data across a number of high-profile and large-volume Apple cloud properties. We are building the next-generation libraries, platforms and data pipelines to empower our data scientists to rapidly build and deploy complex models to production.

Key Qualifications
MS or BS in Computer Science or related field
3 or more years experience building large-scale distributed systems
Skilled in Scala or Java
Working knowledge with at least two of: Spark, MapReduce, HDFS, Cassandra, Kafka

Description
We engineer high-quality, scalable and resilient distributed systems that power data exploration, model building and production models. Our core systems need to work seamlessly across different execution contexts (real-time, near real-time and batch) and support diverse big data stacks such as Spark, Hadoop, Kafka, Cassandra and beyond.

We work at a unique intersection of huge data volumes and adversaries that are continuously adapting, which means we are operating at and beyond the limits of conventional big data systems. On our team you can be sure that every commit you make will come with the satisfaction that you are helping protect and improve the user experience of hundreds of millions of users.

This role requires in-depth knowledge with cutting-edge big data technologies. Tuning, troubleshooting and scaling these big data technologies are a key part of our work, where having a curiosity with the internal workings of these systems is key to being successful. This is a hard-core software engineering role, where a large part of an engineer's time is spent writing code with the remainder being spent on designing and architecting systems, tuning and debugging big data systems, supporting production systems and supporting our data scientists.

Education
BS in Math, Computer Science, or equivalent experience"
Data Engineer,"ABC Financial Services, Inc.","ABC's Software Development team is growing and we are looking for a Data Engineer with strong production database operation support experience for a Greenfield Development project. As a Data Engineer, you'll be working on challenging data problems, managing databases, helping in architectures design, conducting proof of concepts, and supporting new feature development that is highly scalable, maintainable, secure, reportable, sustainable, reliable, and fits in our continuous integration/deployment infrastructure. The Database Engineer should be open minded and curious in learning and adopting new solutions to solve the right problems. Our team enjoys participating in design sessions in front of a white board, implementing a highly performant and scalable infrastructure, and solving problems for customers. We have a collaborative, inclusive, passionate, high energy work environment where everyone's opinion and ideas are valued.

Data Engineer at ABC:
Understand the needs of external clients, ABC's strategic business objectives and vision, and data, and reporting needs from all internal and external clients to define data architectures that satisfy those needs.
Collaborate with data architects, system architects, software architects, design analysts, and others to understand business or industry requirements
Develop highly reliable, performant and scalable database architecture design in micro service ecosystem ensuring there will be no data corruption and race condition issues.
Develop tested and proven monitoring and backup and recovery plan.
Develop ETL data pipeline architecture (Stream and Batch) to ingest data from external sources
Managing, monitoring and ensuring the databases is up in optimal state and in compliance with customer SLA agreement.
Provide on call support, help troubleshoot and resolve any database issues in a timely manner.
Conduct post mortem studies and help prevent the same issues from occurring
Help Architectural team to research, mitigate risks and provide quick POC turnaround
Develop and communicate new technology standards and best practices as appropriate.
Solve complex database software/hardware issues.
Develop and evaluate database infrastructure performance criteria and measurement methods.
Study and analyze the capacity needs for database infrastructure including CPU, RAM, and storage.
Automate life-cycle management of database schemas, clusters, queuing, and much more.
Adhere to PCI/SOC and other security requirements in accordance with standards, policies and procedures.
Contribute as part of a team of software engineers building the next generation software solutions.
Design, build and manage databases for our high performance infrastructure.

Position Requirements
What we look for in a Data Engineer:
Flexibility- ABC is a client centric organization requiring a great deal of flexibility
Professional Development is offered and expected for all Software Development positions. Initiative to keep your manager informed of your desired development goals is key.
While we have multiple teams within Software Development we are one team and all communication and actions must represent that.
Create and maintain goals that fuel you to complete 2 hours of client time per month. Maintaining our client centric focus and user experience is vital in this role.
At ABC, our Software Development Organization has a multi-disciplined team that you will be interacting with on a daily basis. This includes Product Owners, Business Analysts, Quality Assurance Analysts, as well as your Manager.
Participate in Daily Status update
Peer and stakeholder reviews are held annually and require all to participate in a timely manner
To support ABCs client centric environment, you may be required to travel on site to visit a club for up to 5 days on an annual or bi-annual basis.
All testers are required to enter all test plans into ABCs TCM for all new development and enhancements
Perform other duties as assigned by manager or management
Regular and reliable attendance required
REQUIREMENTS:
Degree in Computer Engineering or Computer Science or 3-5 years equivalent experience
7+ years experience as a Production Database Administrator with strong engineering mindset.
7+ years experience with a relational database such as Oracle, MySQL or PostgreSQL
7+ years experience with architecting and implementing complex ETL solutions
5+ years experience in cloud computing (i.e. Amazon Web Services, OpenStack)
3+ years experience in Kafka and streaming technologies
3+ years experience in NoSQL, MPP or Big Data Database solutions
Proficient any of the two of the languages (Java, Ruby, Python, Unix scripting)
Strong understanding and proficiency in Linux
Strong understanding of Architecture design process, methodology, and high level of understanding of business environment, strategy and needs.
High-level understanding of the theories, methodologies and principals underlying technical analysis, design and implementation of software applications, systems, and/or databases.
Proven expert analytical and design skills at multi-product/multi-environment levels as well as an in-depth experience in research and analysis, project planning and implementation.
Familiarity with programmatic access to the database (e.g. JDBC, Hibernate)
Experience scaling the capacity of a database using techniques like caching or sharding
Works well in a fast pace agile environment
Excellent communication skills
Ability to learn and adapt in a rapidly changing environment
In-depth knowledge of the entire development process, including specification, documentation and quality assurance.
NICE TO HAVE:
Knowledge and experience in Microservice Architecture
PCI and HIPPAA compliance knowledge
Understanding of Linux containers (LXC) and similar technologies (Docker, Kubernetes, etc.)
Understanding in modern configuration management tools (Chef, Ansible, Fabric, etc.)
A public github account so we can better understand your thought process and problem solving approach

Relocation Assistance
N/A - Remote

Travel Requirements

EOE Statement
We are an equal employment opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, national origin, disability status, protected veteran status or any other characteristic protected by law.

This position is currently accepting applications."
DATA ENGINEER,NIKE INC,"If youre ready to innovate and become part of our Enterprise Data organization, come join us now! You will be part of an organization that is revolutionizing Nike technology platforms and architecting a data and analytics landscape that is simplified, modern, flexible and will ultimately enable Nike on its journey to 2020 and beyond. Your Role on the Team As a Data Engineer within the North America Market Place Analytics team, you will be a key member of a growing and passionate group focused on collaborating across business and technology resources to drive forward key programs and projects building enterprise data & analytics capabilities across Nike. Primary Responsibilities:  Good understanding and application of modern data processing technology stacks. For example, Spark, Hadoop ecosystem technologies, and others  Design and build product features in collaboration with business and IT  Design reusable components, frameworks, libraries like User Defined Functions  Build continuous integration and test-driven development environment  Performance/scalability tuning, algorithms and computational complexity  Develop architecture and design patterns to process and store high volume data sets  Participate in an Agile / Scrum methodology to deliver high - quality software releases every 2 weeks through Sprints  Troubleshoot production support issues post - deployment and come up with solutions as required
A Bachelor's degree in Business, Information Technology or related field  2+ years experience in a professional organization collaborating across multiple functions  Familiarity with Agile project delivery methods  Experience with AWS components and services (E.G. EMR, S3, and Lambda)  Experience with Jenkins, Bitbucket/GitHub and scheduling tools like Airflow  Strong Java programming, Python, shell scripting and SQL  Good understanding of file formats including JSON, Parquet, Avro, and others  Experience with RDBMS like Oracle  Experience with data warehousing, dimensional modeling and ETL development  Demonstrable ability to quickly learn new tools and technologies  Machine learning frameworks & statistical analysis with Python, R or similar  Exceptional interpersonal and communication skills (written and verbal)  Passion for data with demonstrated ability to use data to tell a story and influence decision making  Detail oriented with strong information seeking skills Competencies:  Effective Communicator  Broad Business Process and Systems Understanding  Analytical and Focused  Strategic Thinking and Tactical Execution  Continuous Learner"
Data Engineer,"Tensyr, Inc.","The Data Engineering team is responsible for data quality, analysis and creation of intelligent models for autonomous systems.

As a Data Engineer you will:
Work with large, complex real time data sets from autonomous vehicles. Solve difficult, non-routine analysis problems, applying advanced analytical methods as needed. Conduct end-to-end analysis that includes data gathering and requirements specification, processing, analysis, from various data acquisition systems on the car.
Prototype and build data analysis pipelines iteratively to provide insights on data from various sensors
Build and backtest models for autonomous systems data
Experience with the following is highly valuable but not required. Interest and eagerness to figure out how things really work (or not!) is more important.

Masters/PhD in Computer Science, or related hard science (physics, biology, statistics) field preferred but not required.
Experience with statistical software (e.g., R, Julia, MATLAB, pandas), database languages (e.g., SQL) and data science environments like Tensorflow or R/Spark.
4 years of relevant work experience (e.g., as a statistician / computational biologist / bioinformatician / data scientist), including deep expertise and experience with statistical data analysis such as linear models, multivariate analysis, stochastic models, sampling methods. Analytical engagements outside class work while at school can be included.
Applied experience with machine learning on large datasets.
Experience with a scripting language, such as Python, Ruby, etc"
Data Engineer,Amazon.com,"Amazon delights millions of customers around the world. Meet the behind the scenes team that enables our Human Resource and Operations Leaders to make informed decisions. The Amazon PeopleInsight team builds reporting and analytics tools for our teams that fulfill customer promise every day. Whether it is Fulfillment Center team that delivers your Prime order in two days, our Amazon Locker team that lets you pick up your package anytime that is convenient for you, our Prime Now team getting you lunch in under an hour, or one of many more, the PeopleInsight group is there providing people metrics along the employee lifecycle for our global operations businesses. In addition to standard reporting, we leverage predictive analytics to help our leaders focus their efforts in ways that will engage, retain and grow their associates.
We are now recruiting for an exceptional Data Engineer, Global Ops HR

The ideal candidate will be:
 A Well-Rounded Athlete Like a true athlete, you understand that we succeed or fail as a team. You are always ready to step up beyond your core responsibilities and go the extra mile for the project and your team. You nimbly overcome barriers to deliver the best products more quickly than expected.
 A Perpetual Student  You seek knowledge and insight. You challenge yourself to turn moments into masters classes. Whether closing a gap, developing a new skill, or staying ahead of your industry, you revel in the joy of learning and growing.
 A Skilled Communicator  You excel when interacting with business and technical partners whether you are chatting, sending a written message, or conducting a presentation.
 A Trusted Advisor  You work closely with stakeholders to define key business needs and deliver on commitments. You enable effective decision making by retrieving and aggregating data from multiple sources and compiling it into a digestible and actionable format.
 An Inventor at Heart  You innovate on behalf of your customer by proactively implementing improvements, enhancements, and customizations. Your customers marvel at your creative solutions to challenges they had not yet identified.
 A Fearless Explorer  You are drawn to take on the hardest problems, navigate ambiguity, and battle skepticism. You never settle, even in the face of overwhelming obstacles.
Roles and Responsibilities
In this role, you will lead the design of one or more product components serving the historical reporting, real-time reporting and predictive analytics needs of the Worldwide Operations HR org. You are the ideal candidate for this role if you are well-versed in multiple DW, reporting and visualization platforms. You will have hands on experience with fast-growing and evolving datasets of petabyte scale. You will understand the challenges of scaling analytical platforms globally, keeping the data security and regulations in mind. You build scalable systems and solutions. You are a team player, skilled at driving consensus through a data-driven approach working with peer engineers and business partners. You are eager to learn the latest technologies and know when to apply them.

Basic Qualifications Basic Qualifications
  Bachelors degree in Computer Science, Engineering, Mathematics, or a related field or 5+ years industry experience
 Demonstrated strength in data modeling, ETL development, and data warehousing.
 Comfort with advanced level SQL
 Hands on experience with a variety of DW technologies such as Oracle, Redshift, PostgreSQL
 Excellent SQL query performance tuning skills
 Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)
 Proven ability to contribute to data engineering best practices in a team of engineers
 Ability to understand business requirements, convert them into technical solutions and deliver software incrementally
 Proven ability to build and support components of a large-scale data solutions serving a global customer base

Preferred Qualifications Preferred Qualifications
 Experience building data products incrementally and integrating and managing datasets from multiple sources
 Hands on experience with AWS technologies such as Redshift, S3, EC2, etc.
 Hands-on experience with Linux/UNIX based systems
 Proven track record of building real-time reporting and analytics solutions

Amazon is an Equal Opportunity-Affirmative Action Employer  Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation."
Data Engineer,Sportradar,"Sportradar, the global leader in understanding and leveraging the power of sports data and digital content is seeking a Data Engineer for its Analytics and Data Storytelling group in Menlo Park.

As a Data Engineer at Sportradar in Menlo Park, you will build, train and deploy machine learning and AI algorithms for data analytics and storytelling content.

As the official data provider for the NFL, NBA, NFL, NASCAR, Sportradar has exclusive distribution rights to all in game data, that includes both live play-by-play stats and sensor data from chips in every player's shoulder pads for the NFL and optical tracking data for the NBA.

Our team is focused on going beyond the raw data to drive insights and engaging fan experiences. We are looking for someone who is passionate about sports and has a great sense of ownership and pride in the products they build.

WHAT YOU'LL DO Build production-quality machine learning pipelines:
Work on feature engineering using sports context
Build and train supervised and unsupervised ML models
Perform failure analysis
Optimize ML models to meet desired accuracy and performance
Test and productionalize models and integrate them with product pipelines
Be or become familiar with AWS products, NoSQL data store (MongoDB) Collaborate with product, design an d engineering teams

WHAT WE'RE LOOKING FOR IN YOU
Degree in computer science, mathematics, physics or related major Source control with Git
Coding experience in Python
Experience with Scikit-learn, Numpy, Scipy
Understanding of Theano, Lasagne, Keras, Pytorch or any other neural network libraries
1-3 years of relevant work experience in data engineering or junior data scientist role
Working experience in an agile environment
Be able to work both independently and as part of a team
Excellent verbal and written communication skills

PREFERRED QUALIFICATIONS
Strong sports knowledge, especially basketball and American football
Experience processing large amounts of data
NoSQL database experience (MongoDB)
Experience building models using Theano, Lasagne, Keras, Pytorch or any other neural network libraries
Ability to work with imperfect/missing/corrupted data
Willingness to explore raw data and extract meaningful features from it
Creativity in solving ML problems based on specifics of the data

WHAT YOU'LL GET FROM US
Talk sports every day
The opportunity to join an exciting team and make an impact on the product and our customer products
Camaraderie of a talented and passionate team, working hard to deliver innovative products
Competitive salary and benefits
Excellent medical and dental plans. 401K and PTO

ABOUT SPORTRADAR Sportradar is the leading provider of sports data and information globally, covering over 350,000 live events across 48 sports. Sportradar has developed the industry's most proficient software, distributing content and data that is easy to consume while setting new standards for speed and accuracy. Turner Media, Twitter and Samsung are among the leading companies in sports and digital media relying on Sportradar for innovative content, comprehensive applications and product development. Exclusive agreements with the NFL, NBA, NHL, International Tennis Federation and NASCAR are among its 65 worldwide league and federation partnerships. The firm's U.S. Advisory Board includes Ted Leonsis, Mark Cuban and Michael Jordan. Sportradar was recently named a finalist by Sports Business Journal in the category of Sports Breakthrough of the Year for its annual Sports Business Awards. Learn more at www.sportradar.com."
Data Engineer,Sportradar AG,"Sportradar, the global leader in understanding and leveraging the power of sports data and digital content is seeking a Data Engineer for its Analytics and Data Storytelling group in Menlo Park.

As a Data Engineer at Sportradar in Menlo Park, you will build, train and deploy machine learning and AI algorithms for data analytics and storytelling content.

As the official data provider for the NFL, NBA, NFL, NASCAR, Sportradar has exclusive distribution rights to all in game data, that includes both live play-by-play stats and sensor data from chips in every player's shoulder pads for the NFL and optical tracking data for the NBA.

Our team is focused on going beyond the raw data to drive insights and engaging fan experiences. We are looking for someone who is passionate about sports and has a great sense of ownership and pride in the products they build.

WHAT YOU'LL DO Build production-quality machine learning pipelines:
Work on feature engineering using sports context
Build and train supervised and unsupervised ML models
Perform failure analysis
Optimize ML models to meet desired accuracy and performance
Test and productionalize models and integrate them with product pipelines
Be or become familiar with AWS products, NoSQL data store (MongoDB) Collaborate with product, design an d engineering teams

WHAT WE'RE LOOKING FOR IN YOU
Degree in computer science, mathematics, physics or related major Source control with Git
Coding experience in Python
Experience with Scikit-learn, Numpy, Scipy
Understanding of Theano, Lasagne, Keras, Pytorch or any other neural network libraries
1-3 years of relevant work experience in data engineering or junior data scientist role
Working experience in an agile environment
Be able to work both independently and as part of a team
Excellent verbal and written communication skills

PREFERRED QUALIFICATIONS
Strong sports knowledge, especially basketball and American football
Experience processing large amounts of data
NoSQL database experience (MongoDB)
Experience building models using Theano, Lasagne, Keras, Pytorch or any other neural network libraries
Ability to work with imperfect/missing/corrupted data
Willingness to explore raw data and extract meaningful features from it
Creativity in solving ML problems based on specifics of the data

WHAT YOU'LL GET FROM US
Talk sports every day
The opportunity to join an exciting team and make an impact on the product and our customer products
Camaraderie of a talented and passionate team, working hard to deliver innovative products
Competitive salary and benefits
Excellent medical and dental plans. 401K and PTO

ABOUT SPORTRADAR Sportradar is the leading provider of sports data and information globally, covering over 350,000 live events across 48 sports. Sportradar has developed the industry's most proficient software, distributing content and data that is easy to consume while setting new standards for speed and accuracy. Turner Media, Twitter and Samsung are among the leading companies in sports and digital media relying on Sportradar for innovative content, comprehensive applications and product development. Exclusive agreements with the NFL, NBA, NHL, International Tennis Federation and NASCAR are among its 65 worldwide league and federation partnerships. The firm's U.S. Advisory Board includes Ted Leonsis, Mark Cuban and Michael Jordan. Sportradar was recently named a finalist by Sports Business Journal in the category of Sports Breakthrough of the Year for its annual Sports Business Awards. Learn more at www.sportradar.com."
Data Engineer,"BigBear, Inc.","Position requires US citizenship and candidate must have an active DoD Top Secret SCI Security Clearance.

BigBear, Inc. is a leading provider of big data computing and analytic solutions. We help people make sense of their data using our cloud-based platform and big data processing algorithms. Each day, we crunch massive volumes of structured and unstructured data into usable and actionable information for our customers.

We currently have an immediate position for a talented and passionate Data Engineer to join our top-notch team of forward thinking engineers, data scientists, analysts, and innovators. The successful candidate will be a self-starter that demonstrates excellent communication and problem solving skills with a strong drive for innovation.

Job Requirements

US Citizenship required.
BS/MS in Computer Science or related field.
3+ years professional software engineering experience.
Applied experience with high volume data flows, messaging pipelines, and ETL.
Solid experience with Python or Java.
Solid experience with Amazon Web Services EC2, S3, SQS, RDS.
Solid experience with Elastic technology (Elasticsearch, Kibana, Beats).
Experience working in the Linux environment (CentOS).
Preferred

Experience with Spring Cloud Data Flow, Apache Nifi, or Apache Spark.
Experience with Django Framework.
Experience with geospatial and location based data.
Experience working in an Agile/Scrum development process.
CISSP or Security+ certification.
If you are interested in this immediate opportunity, please submit your professional resume.

BigBear is an Equal Opportunity Employer"
Data Engineer,Upstart,"About Us:
Upstart is the first lending platform to leverage artificial intelligence and machine learning to price credit and automate the borrowing process. ~80% of U.S. borrowers pay back their loans, yet only ~45% of these people have access to prime credit. Our engineering team uses non-conventional variables at scale in an underwriting model that improves constantly to solve this problem. Upstart was founded by three diverse leaders that bring their individual approach to Upstart creating a culture where we get to the truth, move fast, and bring everyone in. Based in San Carlos, CA, Upstart is backed by Google Ventures, KPCB, Khosla Ventures, Founders Fund, First Round Capital, Eric Schmidt, and Mark Cuban.

Job Description

Do you love building infrastructure to move and analyze large volumes of data? Do you want to work for a company whose success depends on its ability to efficiently make sense of data? As a data engineer at Upstart, you'll be responsible for architecting systems to move, store, transform, and analyze large amounts of data, including financial and business data. You'll have an outsized impact on the productivity of the entire engineering and data science team and directly contribute to Upstart's core competitive advantage.

Requirements
------------

Experience building large-scale data pipelines and warehousing solutions
Experience with technologies like S3, Redshift, Spark, and Hadoop
Ability to evaluate the scale of data problems and determine the optimal solutions for those problems"
Data Engineer,Khan Academy,"Khan Academy is looking for passionate software developers to help change education  join us on our mission to provide a free, world-class education for anyone, anywhere.

Were fun, quirky people that come from a variety of backgrounds. Our team includes people from Google, Apple, Facebook, Mozilla, Microsoft, Pixar, Fog Creek, and tiny startups. Khan Academy attracts talent from some of the top organizations in the Bay Area; our team includes individuals who attended private universities and community colleges, as well as folks who didn't graduate from college. Together, were a small but strong team, deeply invested in your future.

Our Data Engineering team will be growing in the latter half of 2017. If you're looking for a faster timeline, please consider our immediate openings for Software Engineer or Marketing Analyst (Data Scientist) .

What youll build:

As part of the broader software engineering team, youll have the opportunity to work on backend projects as well as user-facing product features alongside frontend and mobile developers. Youll build functionality that unlocks users access to their learning histories and accelerates their learning. You will empower the team with data we need to understand our users needs. You will design infrastructure and run A/B tests to experiment with different presentations and pedagogies, enabling a richer learning experience for learners of all ages. Our team has the unique flexibility to work across the stack to bring new functionality directly to the learners who need it.

Examples of things weve worked on include:
Building a machine learning pipeline to drive individualized content recommendations
Leveraging model predictions to speed up the progression for a student in a grade-level mission
Building leaderboards for competition between hundreds of thousands of students in LearnStorm 2016
Integrating with the College Board to provide personalized SAT practice based on PSAT scores
Our tools:
Our team uses a diverse set of tools, including Jupyter notebooks running Python, R Studio, Google BigQuery / Cloud DataFlow, Tableau, and even Excel. On the frontend we use Javascript and React.
You do not have to have experience in any one technology but you should be aware of industry trends and be excited to evaluate and learn new technologies.
We value simple and reliable solutions over customizability and buzzword compliance - a good solution now is better than an expensive custom, perfect solution in a year.
You need:
Excitement about helping Khan Academy bring a free, world-class education to the world.
Experience writing code. Students, teachers and internal decision makers need trustworthy data, and that means building stable, reliable and scalable systems that can regularly process terabytes of historical data with minimal errors.
Expertise in statistical modeling, machine learning, and/or numerical analysis, so you can spot biases or bad probabilistic inference and look for patterns in the data.
Cross-team communication skills. Youll need to understand the product implications of data transformations or A/B test conversions and be able to convey uncertainties and caveats with the data
Empathy for learners around the world. You love learning and are excited about helping others learn to love learning. Youre motivated to learn new things and share what you learn with the world.
We offer the following benefits:

We may be a non-profit, but we reward our team well!
The opportunity to improve real lives, solve hard problems, and change the world.
Competitive salaries and annual bonuses.
Ample paid time off as needed. Well support you in maintaining a healthy life-work balance.
Delicious catered lunch every day, plus tons of snacks and beverages.
An inclusive, excited, and friendly team that trusts you and gives you the freedom to be brilliant.
A great location  were only 0.5 miles from the Caltrain and downtown Mountain View. We also pay for remote employees to fly out a few times a year, and our San Francisco dwellers are encouraged to work from home a few days a week to save the commute time.
Awesome team events and weekly board game nights.
Oh, and we offer all those other typical benefits as well: 401(k) + 4% matching & comprehensive insurance including medical, dental, vision, and life.
How to apply:
Attach your resume below
Address the following prompt in the space provided below: At Khan Academy, we believe that reaching all learners requires an engineering team that is diverse in every respect, and we are looking for individuals who will help us fulfill our mission by adding to the diversity of our team's experiences, perspectives, and mindsets. With that in mind, tell us briefly how your experiences and perspectives whether personal, professional, academic, or otherwise could contribute to the diversity of our team.
Optional:
Links to projects (we really like these, but they are not a requirement). We especially like living, breathing projects, demos, or commentaries on your work. Please dont just send code  show or tell us why you wrote it, what its for, what you like about it, how it helps someone, etc. No project is too small if its something you care about. If you're feeling especially creative feel free to include a link to a project that you've built on our programming platform.
If youd like to provide a more traditional cover letter addressing your interest in KA (in addition to the diversity-based response), please feel free to attach that below.
To learn more about our work:
Data Science team blog
Sals TED talk
KA Engineering team blog
Lead engineering manager Ben Kamens's blog
Our team
What our learners have to say about Khan Academy
As an organization and as individuals, we are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, gender identity, national origin, sexual orientation, age, citizenship, marital status, pregnancy, medical conditions, genetic information, disability, or Veteran status. We strongly encourage all candidates to complete the EEOC questions below so that we can continue to monitor our processes and ensure we are creating an environment that allows candidates to feel comfortable and display their best selves across many lines of difference.

Finally, if you feel under-qualified, remember that weve all been there too. Here are some great posts on building up the courage and skills to apply for your dream job, from awesome people who have worked at KA:
Courage
Skills"
Data Engineer,BMW North America,"As an independent group of companies, the BMW Group has a commitment to creativity and breakthrough ideas that goes well beyond the racetrack. In order to continuously create ultimate driving machines, we drive our growth and design excellence by staffing our teams with individuals who are innovative and always looking for the next great idea. If you share our vision and view yourself as an independent, creative thinker we invite you to join our team in this exceptional role located in Mountain View, California.

The Data Engineer is responsible for managing large scale data sets, data collection requirements, data processing, and data storage, in collaboration with partner organizations within BMW and external partners. This position will be responsive to both Software Engineers developing algorithms on the data, as well as Vehicle Fleet Managers coordinating the deployment of autonomous development vehicles. The Data Engineer is principally responsible for data availability and data quality.

Collaborate with algorithm designers to specify required data sets.
Request specific data sets (driving scenarios) by adding requests to recording plan.
Collaborate with machine learning algorithm designers to specify requirements on data labeling tools.
Specify labeling requirements (automatic, manual, categories for labeling).
Coordinate labeling process with both internal and external collaborators.
Clean data, remove unusable traces, correct/remove errors.
Post process data (eg: extract information from vehicle bus data).
Annotate data to make it searchable in database.
Responsible for quality of available data and maintenance of groundtruth.

Because of the international development process, the ideal candidate will be comfortable working independently and effectively in a team environment and must have excellent communication skills and an attention to detail.

Join the team at BMW of North America and enjoy a high-performance employment package which includes:
Company paid Medical, Dental & Vision Insurance
Employee car program
401(k) Savings Plan
Retirement Income Account

Minimum Requirements:
Master's Degree in Computer Science or Computer/Electrical Engineering or related field.
2 to 3 years experience in machine learning.
Strong prototyping skills.
Strong project management and communication skills.
Hands-on implementation of projects is required. Interaction with a backend cluster, aggregating data, writing algorithms to analyze and combine the data.
Self-starter, able to tackle problems alone, and also to work well with internal and external development partners.
Must integrate well into a multi-cultural team environment.
Ability to travel to destinations in the United States as well as to Europe.
The ideal candidate for this position will focus on software development for machine learning and will be/have:
Previously worked in large scale deployment.
Knowledge of Sensor Fusion and Big Data analytics (hadoop, yarn, map/reduce, spark).
Experience with one or more general purpose programming languages likes C, C++, Python.
Knowledge and experience with labeling/annotation frameworks and key players.
Good working knowledge of Linux.
Real-time and embedded systems experience.
Understanding of Caffe or TensorFlow preferred.
Knowledge of ROS and/or ADTF preferred but not required.

What are you waiting for...jump into the driver's seat and help us define the future of mobility!

BMW NA is an EO employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status."
Data Engineer,Green Chef Corporation,"is not designed to cover or contain a comprehensive listing of activities, duties, or responsibilities that are required of the employee for this job. Duties, responsibilities, and activities may change at any time with or without notice.

Apply online if this sounds like your next challenge. EOE / Minorities encouraged to apply.

Please visit our website at: www.greenchef.com"
Data Engineer,Nauto,"NAUTO is an autonomous car startup on a mission to make cities safer and more efficient with better drivers. We're looking for engineering and business professionals who love to design, build and scale real-time distributed systems to join our growing team in Palo Alto.

Merging artificial intelligence with computer vision, Nautos systems detect and react to whats happening on the road ahead of a driver and within the vehicle. Nauto algorithms sense when there is an issue on the road ahead, or a distraction within the vehicle, and helps the driver respond. Nauto also automatically understands when a collision is about to happen, and records the scene inside and outside of the car then. Images and data about the incident are stored in the cloud, and can be shared via Nautos app and fleet management tools.

The company's systems are already running on several classes of high-volume commercial vehicles. Longer term, Nauto will continue to partner with auto manufacturers to evolve its systems from an onboard safety AI into a platform that accelerates the development of autonomous vehicles.
Investors in Nautos recently announced massive $159M B-round included Softbank, Greylock, Andy Rubins Playground Global, Draper Nexus as well as leading strategics including BMW, GM, Toyota, and Allianz Insurance.

Team Description The Data Team is responsible for building and operating the Data Warehouse and Analytic systems for Nauto. The Data Warehouse provides a full 360 view of the Customers, their Users and Nautos Products and Business Operations. The ETL processes, Anomaly Detection and Analytic Dashboards operate at near real time at scale.

We work as a cross-functional/full stack team. Each person is responsible for being competent in each of the tools, processes and operations of the team.

We maintain a fully secure data environment with PII, Client and Country requirements.

We use AWS tools and services. Python is our primary language.

As our scale grows we will evolve our tools and infrastructure.

Responsibilities You will be responsible for and have the opportunity to learn (we mentor):
Leading projects from conception to Production release.
Partnering with Customers, Product Engineers and Business Operations to understand needs and designs - writing up and verifying specifications.
Creating and enhancing Data Models and Architectures for the new features.
Developing the code, tools, calculations and dashboards.
Doing thorough QA and testing to meet the specifications, data and calculation accuracy, 24x7 robustness and scale performance.
Documenting the new modules. We are cross-functional, 24x7. The other team members need sufficient information to be able to jump in when you are not available. For the users of our data we need to have full documentation on the data sources, ETL processes and all calculations and assumptions so they understand what they are looking at.
Conducting User Training and Education for the Dashboards and Analytics.
Partnering with DevOps, Engineering and Infrastructure to use the Nauto tools and best practices.
Maintaining a fully secure data environment following Nauto standards.
Investigating new tools, techniques and products.
Improving our internal processes, Agile methods and management tools so we operate at our best as a team internally, with Nauto and our Customers.
Participating in our 24x7 on-call support rotation. (approx. 1 week per month)
Experiences and Skills
B.S. degree in computer science, mathematics, statistics or a similar quantitative field
2+ years of experience in the arena of Data, Databases, ETL and/or Analytics.
Experience with OLTP, OLAP and Data Warehouse environments
Proficiency in a major programming and/or a scripting language (C++, Java, Python, Ruby, etc.)
Excellent SQL skills
A commitment to data accuracy and security
Experience with building and operating 24x7 production systems.
Pluses:
AWS tools and administration
Experience with In-Memory, New SQL, NoSQL databases
Performance Tuning Databases
Postgres, Redshift
Machine Learning techniques

Perks
Ground floor opportunity with the team; shape the strategic direction of the company
The rare opportunity to change the world such that everyone around you is using the product you built to get around safely. Were not just another app, were moving real people and goods and helping to reinvent transportation and logistics
Beautiful modern work space. Many perks including cafe with meals, snacks and drinks provided
Sharp, motivated co-workers in a fun office environment"
Data Engineer,Aaptiv,"About Aaptiv

Aaptiv is the fastest growing mobile fitness product on the marketwith nearly 200,000 paid subscribers in less than 2 years of business. We are the only service that turns your phone into an on-demand fitness studio with all your favorite classes and music. That drive you feel to run, push, or pedal harder in a class or when working with a personal trainer? Aaptiv recreates the experience by synchronizing the voice of a trainer with a playlist of all the music you love, to deliver fun and motivating guided workouts straight to your earbuds.

At Aaptiv, we're building a fitness platform that enables us to develop long-term and personal relationships with users based on their data profiles. By analyzing user behavior patterns, we are able to both create and recommend content customized to specific fitness levels and interests.

Want to join our team? We're looking for people who are passionate about building a world-class fitness experience. There are over 80 million Americans who value fitnessand we believe every one of them should be an Aaptiv user.

About the role

You will partner with other engineers to design data access patterns, optimize integrations, and solve data problems. You will work closely with cross functional teams, and your input will have a large impact on how our product is built. This is a role that provides a huge opportunity to have a meaningful impact on our business. At Aaptiv, we thrive on building data driven solutions to help our members find workouts that best fit their lifestyles. If you want to join a team that values this mission, and has fun working towards it together, keep reading!

What Youll Do

Design data warehouse schemas that accurately represent our business model, and store data efficiently to enable fast application access and easy reporting
Enforce data standards, help define and manage SLAs for various data sets
Enforce data precision, implement guidelines and support software engineers in creating software that access and store data in the most optimal way while generating reliable data
Manage and support all data sources
Plan and coordinate data migration projects
Create and maintain ETL processes
Work collaboratively in an agile environment with cross functional teams to help brainstorm ideas and solve data problems
Who You Are

5+ years of experience as a data engineer working on building data pipelines and transformations
Expert knowledge of SQL (PostgreSQL, SnowSQL)
Strong Python skills
Strong understanding of data access patterns and performance impacts in relational and dimensional models
Experience with noSQL datastores (MongoDB)
Experience using AWS services (RDS, EC2, Data Pipeline, Lambda)
You value code quality, and always deliver code with an accepted level of unit test coverage
You are always willing to pitch in to help your team no matter what the task may be"
Software Data Engineer,Adobe,"The Challenge:
Build real time bidding and ad serving features for Adobe Advertising Cloud. The RTB team is responsible for delivering ads for brands in video and display for hundreds of billions of auctions per day.
The Adobe Advertising Cloud is looking for a Senior Data Science Engineer to join the Optimization team to help us build and maintain the next generation of predictive models and optimization algorithms. You will develop production ready features for hundreds of customers across dozens of partners in a fast paced environment. For this role we require:
Develop classifiers, predictive models and optimization models on large datasets
Build and maintain a production level data modeling pipeline using big data technologies like Spark, Scala, Hive, Python and cloud services like Amazon AWS
Develop techniques for monitoring and visualizing performance of all deployed algorithms
Proven ability to work well in a high performing team with agile development approaches and technology
Translate high level requirements to actionable tasks/deliverables and creative problem solving a must
Ability to work cross-functionally with various teams including: site reliability engineers, project managers, product managers, and other teams
What you need to succeed:
Must have strong command of SQL, Hive, Python
Knowledge of machine learning, NLP, classifiers, statistical modeling and multivariate optimization
3+ years Object oriented programming experience with Java, C++, or Scala, etc
Distributed computing principles and experience in big data technologies including performance tuning
Expert in data structures, algorithms, and multi-threading
Test driven approach - Junit, TestNG; Experience with Jenkins, Maven, Gradle or build tools
Strong analytical and quantitative problem solving ability
Ideal Candidate Profile:
MS/PhD in Computer Science or related technical field or BS with 5+ years of applied experience
Experience with Real Time Bidding / Ad Tech background
Experience with big data techniques (Hadoop, MapReduce, Spark, Hive, Amazon AWS)
Experience with production Kafka, Aerospike, and Docker instances
Good communication skills
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
Data Engineer,Spotify,"Spotify is looking for a Data Engineer to join us. You will build data driven solutions to bring music and digital media experiences to our 100 million active users and millions of artists either by working directly on product features, publishing and insight tools for artists, or by improving the quality of our data tools and large scale data infrastructure. You will take on complex data-related problems using some of the most diverse datasets available  user behaviors, acoustical analysis, revenue streams, cultural and contextual data, and other signals across our broad range of mobile and connected platforms. Above all, your work will impact the way the world experiences music.

What youll do
Build large-scale batch and real-time data pipelines with data processing frameworks like Scalding, Scio, Storm, Spark and the Google Cloud Platform.
Leverage best practices in continuous integration and delivery.
Help drive optimization, testing and tooling to improve data quality.
Collaborate with other engineers, ML experts and stakeholders, taking learning and leadership opportunities that will arise every single day.
Work in cross functional agile teams to continuously experiment, iterate and deliver on new product objectives.
Who you are
You know how to work with high volume heterogeneous data, preferably with distributed systems such as Hadoop, BigTable, and Cassandra.
You are knowledgeable about data modeling, data access, and data storage techniques.
You care about agile software processes, data-driven development, reliability, and responsible experimentation.
You understand the value of collaboration within teams.
We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music."
Data Engineer,Capital One,"Plano 1 (31061), United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Engineer

As Data Engineer, you will be part of an innovative team working on cutting edge technology and data science. You will contribute to building insightful visualization using open source technologies to solve critical business problems.
We envision, create, deploy, and maintain technology solutions powered by streaming big data, state of the data analytics, open source technologies, and intuitive visualizations in the cloud. Our goal is to harness and deploy the latest technologies to predict and prevent internal and external customer-facing issues.

Responsibilities:
Work as part of our Critical Transaction Availability team to develop tools, write reports, manage data collections, manage dashboards, and provide reporting of critical business transaction availability
Meet monthly reporting deadline
Investigate and development of improved analytics
Develop code to meet Agile story acceptance criteria
Conduct design and code review to ensure compliance with standards
Estimate level of effort to develop code to meet acceptance criteria
Ensure adherence to continuous improvement practices as required to meet quality / time to market imperatives
Maintain records to document program development and revisions
Provide support of software and processes as needed
Share best practices and improve processes within and across the agile team
Basic Qualifications:
Bachelor Degree or Military Experience
At least 2 years of experience with SQL, UNIX Scripting, or JavaScript
At least 2 years of experience with Python or R
At least 4 years of software development experience
Preferred Qualifications:
Masters Degree
1+ year experience in business transaction management
1+ year of SAS experience
1+ year of Tableau experience
1+ year of Teradata experience
1+ year experience working in an Agile environment
1+ year experience with Big Data Hadoop
1+ year experience working with SPLUNK
1+ year of using Statistics
1+ year of H2O Machine Learning
At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,Dynpro,"Title : Data Engineer
Location : Santa Clara, CA
Position Type : Contract

Description :

Build data pipelines and ETL using heterogeneous sources to Hadoop using Kafka, Flume, Sqoop, Spark Streaming etc.
 Experience in real time data streaming
 Ability to dynamically adapt to conventional big-data frameworks and open source tools if project demands
 Knowledge of design strategies for developing scalable, resilient, always-on data lake
 Experience in agile(scrum) development methodology
 Strong development/automation skills
 Must be very comfortable with reading and writing Scala, Python or Java code

Experience in real time data streaming
 Ability to dynamically adapt to conventional big-data frameworks and open source tools if project demands
 Knowledge of design strategies for developing scalable, resilient, always-on data lake
 Experience in agile(scrum) development methodology
 Strong development/automation skills

Due to client requirements, must be US Citizen or GC Holder"
Data Engineer,NAMCO BANDAI,"BANDAI NAMCO Entertainment America is looking for a skilled, experienced, and energetic Data Engineer to join the Digital Platform team. This role will help the company and our game developer partners discover critical information hidden in vast amounts of data that help make informed decisions that help deliver a better a gaming experience. Your primary focus will be in correcting accurate data, applying data mining techniques, helping with statistical analysis, and supporting the framework for visualizing the data to make it understandable for non-technical audiences. You will partner closely with Data Analysts, Producers and Marketing to strengthen our analytical and business intelligence capabilities within our company. Your goal is to help build, support and evangelize the importance of data as it relates to monetization, player engagement, retention and other core KPIs for our existing games, new releases and online experiences.

ESSENTIAL DUTIES AND RESPONSIBILITIES  Collaborate with business stakeholders and analysts to understand business intelligence needs
 Processing, cleansing, and verifying the integrity of data used for analysis
 Creating automated anomaly detection systems and constant tracking of its performance
 Extending companys data with third party sources of information when needed
 Work with analysts to examine and visualize marketing/service performance and user conversion funnel data to identify opportunities to improve ROI, user engagement, and user retention
 Work with analysts to apply data science techniques onto identify meaningful relationships, patterns, trends, and user behavior models from complex data sets to guide service roadmaps
 Develop executive dashboard reports that are self-serve and provide daily KPI visibility
 Support and conduct ad-hoc data analysis requests to help solve more complex problems
 Help to research new modeling algorithms, packages, visualization options and statistical tools that help enhance the overall productivity of BI for the company

QUALIFICATIONS/REQUIREMENTS  You have 3+ years work experience analyzing large datasets in an analytics or product centric role
 You are familiar with SQL, Hive, or other querying languages
 You are familiar with Python or other scripting languages
 You are familiar with R, MatLab, or other statistical packages
 You are proficient with data visualization like Google Data Studio, Tableau, Superset, d3, Redash, Metabase, or other tools
 Ability to communicate technical information to technical and nontechnical audiences
 Ability to work in a fast-paced agile environment
 Selfstarter with a positive attitude

BONUS POINTS:  Experience building systems for analyzing data from online and free-to-play games
 Experience working on Cloud based infrastructure (e.g. AWS, GCP)
 Experience with some of these technical stacks: Python, Java, Golang, Shell scripting, fluentd, Kafka, Kinesis, Gobblin, Airflow, Spark, Storm, Hadoop, Hive, Presto, Aurora, MySQL, MSSQL, MongoDB, Redis, Cassandra, Elasticsearch, BigQuery

EDUCATIONAL REQUIREMENTS: BS/MS or the equivalent in Engineering, Computer Science, Science, Math, Performance Marketing or related technical degree

WORK ENVIRONMENT The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

The above statements are intended to describe the general nature and level of work being performed by employees in this position. They are not intended to be an exhaustive list of all duties, responsibilities and qualifications of employees assigned to this job.

About BANDAI NAMCO Entertainment America Inc. BANDAI NAMCO Entertainment America Inc., part of BANDAI NAMCO Holdings Inc., is a leading global publisher and developer of interactive content  including all major video game consoles, iOS, mobile and online. The company is known for creating and publishing many of the industrys top video game franchises, including PAC-MAN, TEKKEN, SOULCALIBUR, NARUTO, NARUTO SHIPPUDEN, Dragon Ball, GALAGA, RIDGE RACER and ACE COMBAT. More information about the company and its products can be found at www.bandainamcoent.com or www.facebook.com/BandaiNamcoUS ."
Data Engineer,Hewlett Packard Enterprise,"Aruba, a Hewlett Packard Enterprise Company, is a leading provider of next-generation networking solutions for the mobile enterprise: http://www.arubanetworks.com/company/about-us . The company designs and delivers Mobility-Defined Networks that empower IT departments and #GenMobile, a new generation of tech-savvy users who rely on their mobile devices for every aspect of work and personal communication. To create a mobility experience that #GenMobile and IT can rely upon, Aruba Mobility-Defined Networks automate infrastructure-wide performance optimization and trigger security actions that used to require manual IT intervention. The results are dramatically improved productivity and lower operational costs.

We are looking for a Data Engineer who is responsible for designing and implementing data pipelines at Big Data scale. Your Responsibilities Implement parsers and validators for new Log sources Implement ETL transformers to reformat and enhance the data Implement ETL correlators to update the data from multiple data sources Work on tools and APIs to visualize the backend data Troubleshoot performance and data related problems Work with the Analytics team in defining the schema for new data sources Our Minimum Requirements for This Role Are 4+ years java and/or Python development experience Experience working with Hadoop or Big Data (HDFS, Parquet, HBASE) Experience working with Large scale databases like Cassandra Experience working with Map Reduce or Spark, ElasticSearch, Kafka Experience working with Databases like Postgres, SQL Education Bachelor's or Master's degree in Computer Science, or equivalent and typically 4-6 years experience. Benefits youll enjoy At Aruba, a Hewlett Packard Enterprise Company, we offer an exciting and fun work culture, driving innovation, collaboration, and growth. We place our customers first, deliver some of the most innovative technologies to the market, and have fun doing it all!

Come join our team and be part of an exciting organization poised for success! Thanks for taking the time to review our job, if you think it is a match to your experience and interests please apply today  we are eager to learn more about you! We have dozens of openings, so encourage your friends to apply as well! #ArubaNetworks #GenMobile #ArubaNetworksJobs #HPE #HewlettPackardEnterprise Please note the above statements describe the general nature and level of work only.

They are not a complete list of all required responsibilities, duties and skills. Other duties may be added, or this description amended at any time."
Data Engineer,Walmart eCommerce,"Position Description
SEM Engineering team is a part of Growth at Walmartlabs and is in charge of optimizing paid and free search for walmart.com. We are tasked with optimizing ad spend on RoAS while making sure as many items from our catalogue as practical get conversion, while doing it at scale, meaning, the traffic quality should be maintained regardless of amount of ad spend. We use data science methods to dynamically modify keyword and product ad bids across top search engine providers. The team is responsible for building data pipelines and API integrations enabling us to do so, with concerns of SLA and data quality squarely in focus, as well as building internal tool to provide our business partners control and visibility of SEM operations. Our challenge is immense. Presently, the catalog is expanding by 10s of thousands of items daily and keyword universe is growing by 10s of thousands monthly.

#LI-DC

Minimum Qualifications
Stellar Software Engineer, capable of reliable and predictable delivery of complex projects working as part of a team.
Automation champion who is passionate about productivity and unyielding in applying best Software Engineering practices.
Role model for Software Engineers on the team and a mentor to junior talent.
Advocate of metrics-driven approach to solving business problems, operating software systems and managing agile development processes.

Masters degree in CS or EE, or related field and 5+ years experience in building and evolving complex software systems, both customer-facing and offline.
Expert knowledge of algorithms, databases, Java and/or Python.
Expert knowledge of technologies used in building scalable, distributed systems in virtualized cloud-like environments.
Solid grasp of principles and approaches used in Data-driven systems and processes.
Solid devops skills in troubleshooting live applications, assessing performance bottlenecks of applications running on Linux.
Ability to successfully lead multiple projects while remaining highly technical, hands on contributor.

Additional Preferred Qualifications
Prior experience working within Growth Engineering organizations: SEO, SEM, etc.
Prior experience extracting actionable data as well as evaluating success metrics in form of reports.
Solid understanding of mobile as well as desktop web-based applications, frameworks and processes around building and deploying those.

Company Summary
Walmart Global eCommerce is comprised of Walmart.com, VUDU, SamsClub.com, and our technical powerhouse @WalmartLabs. Here, innovators incubate next gen e-commerce solutions in real-time. We integrate online, physical, and mobile shopping experiences for billions of customers around the globe. How do we do it? We continuously build and invest in new technology including open source tools and big data innovations. Data scientists, front and back-end engineers, product managers, and web and UX/UI teams collaborate alongside e-commerce experts to envision, prototype, and bring revolutionary ideas to life in a dynamic, flexible and fun work culture.

Position Summary
SEM Engineering team is a part of Growth at Walmartlabs and is in charge of optimizing paid and free search for walmart.com. We are tasked with optimizing ad spend on RoAS while making sure as many items from our catalogue as practical get conversion, while doing it at scale, meaning, the traffic quality should be maintained regardless of amount of ad spend. We use data science methods to dynamically modify keyword and product ad bids across top search engine providers. The team is responsible for building data pipelines and API integrations enabling us to do so, with concerns of SLA and data quality squarely in focus, as well as building internal tool to provide our business partners control and visibility of SEM operations. Our challenge is immense. Presently, the catalog is expanding by 10s of thousands of items daily and keyword universe is growing by 10s of thousands monthly."
Software Engineer- Maps Data Metrics Team,Apple,"Apple seeks a software generalist to join an exceptionally skilled research group working with data science, statistics, geospatial analysis, big data, and deep learning. The Apple Maps Data Metrics team is responsible for reporting on the quality of base map, POI, traffic, satellite imagery, cartography, and map tile data. Improve Apple customer experience around the world by creating systems that provide actionable insights. The successful candidate will partner with cartography, data science, and infrastructure teams to architect and implement applications, pipelines, and dashboards that quantify product quality.

Key Qualifications
You must have previous experience in computer science and software engineering. Ideally you have done some systems level development in C-based languages or Go. You have also written user-facing applications and APIs in a variety of languages, and have familiarity with various scripting languages. Five years of professional experience is required.
We would like to discuss your experience designing full stack applications, designing APIs, wrangling data, defining software architecture, writing and debugging code, and delivering compelling solutions to your customers.
Description
The Maps Software Engineer is responsible for creating software that automates quality measurements for Maps data and services. Common projects include integration with existing applications, defining and building entirely new applications, construction and support of microservice APIs, and research into using emerging technology to measure map quality.

Education
A bachelors or masters degree in Computer Science or equivalent experience.
Additional Requirements
Candidates must have superior written and verbal communications skills."
Software & Data Engineer,Apple,"Apple's Global Business Intelligence team is seeking an experienced Data Engineer to be part of GBI Sales reporting area, who will be responsible for delivering highly performance optimized reporting solutions.

The ideal candidate for this position will be able think out of the box and come up with solutions quickly to help business in making time sensitive decisions, have strong communication and analytical skills, passion for data visualization, always drives for excellence and self motivated.
Key Qualifications
Should be an expert in Teradata or other MPP systems
Expertise in a programming language (Preferably Python or JAVA)
Experience with design and development of ETL processes
Fluent in writing Advanced SQLs
Must have good experience in development of unix shell scripts
Performance tuning of SQL queries
Experience in developing automated test scripts to help with regression testing Sharp troubleshooting skills to identify and fix issues quickly.
Data modeling experience in building Logical and Physical data model Experience in building Tableau Dashboards is a plus
Description
Work closely with Analytics, Product teams to understand data and analysis requirements
Design and build dimensional data models and schema designs to improve accessibility, efficiency, and quality of data
Develop the ETL for data induction and implement the application logic to meet the business requirements
Strong understanding of analytics needs and proactive-ness to build generic solutions to improve the efficiency
Develops new reports and perform ad-hoc reporting by working independently
Benchmark application performance periodically and fix performance issues
Education
Graduate degree required in Computer Science or related field
5+ years of experience in the related field"
Data Engineer,Plus3 IT Systems,"Location: Reston, VA or Washington, DC or Quantico VA or Colorado Springs, CO

Start Date: Immediately

Who YOU are:
As a future Data Engineer at Plus3 IT Systems, you:
Are passionate about working on cutting edge, high profile projects and are motivated by delivering solutions on an aggressive schedule
Arent satisfied with status quo, insatiably curious, and regularly look for creative ways to solve problems and help your team meet commitments
Love learning new technologies and sharing them with your team
Have a keen interest in using any and all appropriate tools, especially Cloud-based and Open Sourced, to solve the problem at hand
Have strong verbal and written communication skills, and enjoy participating in dynamic, face-paced collaborations with customers, vendors, and other engineering teams to solve complex business problems together
Use your experience and leadership skills to motive your teammates to deliver high quality results in a fast-paced work environment

What youll be doing:
Work within a team of like-minded professionals to design, build and deploy critical business and mission data-centric applications in a production environment
Design and implement appropriate data extraction, transform, and load (ETL) processes to properly prepare data, ensuring data quality and accuracy, for consumption by business and mission applications
Identify, retrieve, manipulate, relate and/or exploit multiple structured data sets from various sources
Design and implement data storage, sharing, and dissemination environment, ensuring support for all relevant agency and community policies
Engineer suitable data management and governance procedures and provide production support when required
Design and develop automation workflows, perform unit tests and conduct reviews to make sure your work is rigorously designed, elegantly coded, and effectively tuned for platform performance, and assess the overall quality of all delivered components

Qualifications:
Master's Degree preferred, or a Bachelors degree and 4 years experience, or 10 years of specialized experience
Minimum 4 years experience working on complex data/database projects as a data analyst, data architect, or database engineer
TS Clearance with ability to obtain an SCI and CI poly

Desired Technical Skills and Competencies:
Certified Data Management Professional (CDMP), Microsoft Certified Solutions Associate (Business Intelligence) or equivalent certification(s) strongly desired
Experience building n-tier web-based applications using SQL and non-SQL back-ends
Experience with large-scale data processing tools, such as Spark, NiFi, Hadoop, Kafka, etc.
Programing experience in multiple languages, such as Java, Python, Ruby, and Scala
Unix scripting (Ruby, Perl, Python, shell)
Ability to produce clear and concise documents and diagrams capturing application requirements and dependencies (UML), operational processes and procedures, and data flows using MS Visio, MS Excel and MS Word."
Data Engineer,DNAnexus,"DNAnexus is an industry-leading cloud genomics and bioinformatics platform used by pharmaceutical, clinical diagnostic and academic organizations. DNAnexus provides a global network for advanced analysis, collaboration and management of genomic data. The DNAnexus cloud-based platform accelerates science by addressing the challenges of security, scalability, and collaboration required to pursue genomic-based approaches to health, in the clinic and in the research lab. We are a team of experts in computational biology and cloud computing who work with organizations to tackle some of the most exciting opportunities in human health, making it easierand, in many cases, feasibleto work with genomic data. With DNAnexus, organizations can stay a step ahead in leveraging genomics to achieve their goals.

---------------
Job Description
---------------

We are building a sophisticated team that are passionate about using data to help drive informed business and product decisions for DNAnexus. As a Data Engineer, you will be responsible for using big data tools and techniques to build the data foundation to support business and product decisions. You will work closely with Data Analysts, Product Managers, and Engineering to define and build the data pipelines and platform used to gain a deeper understanding of the business and drive strategies for success. As a Data Engineer, you will collaborate with Data Analysts working with Product, Finance, Marketing, Sales and Science to define the required data models and metrics. The ideal candidate will have a experience working with operational data from cloud businesses and using big data technologies to provide a data layer that can be queried and visualized in dashboards and reports. We are looking for people that are self-starters, focused on results, and have a demonstrated track record of success in fast-paced, cloud companies.

----------------
Responsibilities
----------------

Apply your expertise in data and software engineering to support quantitative analysis and presentation of data to see beyond the numbers and understand the system. In particular, gain a better understanding of our global multi-region, multi-cloud platform.
Work with the Data Analysts and Product Platform Engineering team to understand the requirements and how they map to the all the available data sources including the DNAnexus platform.
Translate data requirements into well-designed, efficient data models and schemas that empower operational and exploratory analyses
Work with Product Platform Engineering and other data source owners to design and develop data extraction pipelines
Design, develop and launch production pipelines that load and transform the data into a form for the data analysts.
Design and develop key data sets on a data platform that is continuously refreshed to support up-to-date analyses.
Design and develop automated quality control steps that ensure reliable data sets
Support ad-hoc requests from data stakeholders to triage data issues and investigate data questions.
--------------
Qualifications
--------------

4+ years of experience in the data warehousing or analytics space
Strong experience in designing, building, and maintaining ETL/ELT pipelines
Strong experience with schema design and multi-dimensional data modeling
Fluent using SQL on databases and data warehouses
Fluent in programming and analysis using Python, R, Java, Scala
Experience with streaming and queryable big data technologies
Proficient with data visualization tools such as Tableau, Looker
Proficient with tools such as Jupyter notebooks, RStudio
Excellent verbal and written communication skills with ability to communicate data driven insights at both the technical and financial levels.
BS in Computer Science, Math, Physics, Engineering, Statistics or other technical field. MS or PhD a strong plus.
Based in Mountain View, California, DNAnexus is rapidly growing and searching for the best talent to join and enhance our team.

If you are interested in joining our team, please apply today!"
Data Engineer,CGI,"Help CGI develop the next generation of mission-enabling solutions for one of our federal clients. We are seeking Data Engineers to support our Federal Regulatory Agency Programs Business Unit in the DC metro area. Our team is focused on delivering results. We are looking for people with deep financial industry expertise, analytical capabilities and an innovative mindset to solve problems.

Your future duties and responsibilities:  Develop data and metadata policies and procedures for cross-functional teams to build, maintain and leverage data models, ensuring compatibility with customer and industry data standards.
 Work with Business Process Reengineering (BPR) specialists to correlate business requirements to domain entities and data elements.
 Work with Database Administrators (DBAs) to implement data and metadata designs into physical structures.
 Work with developers to design, implement and maintain scalable software solutions that use database structures designed and maintained by a Data Architect.
 Review and evaluate database performance, risk and financial analysis feasibility studies.
 Monitor industry trends and directions to apply to current and future initiatives.
 Provide technical and strategic guidance to the clients executive team on the creation and implementation of new data standards and databases.
 Provide assessments of the technical characteristics of proposals and alternatives considered to optimize database performance.
 Assemble data storage and processing infrastructure that allows working nimbly through extensive amounts of data.
 Design and build robust and scalable solutions for managing structured and unstructured data.
 Develop and document service Application Program Interfaces (APIs) for the collection, cleansing and storage of data.
 Work with data scientists, data architects, data analysts, and business users to identify and develop data processing use cases, infrastructure (including data layers), APIs, applications and processing pipelines.
 Develop interfaces and applications for accessing and visualizing data.

Required qualifications to be successful in this role:  5+ years of system analysis experience
 3+ years of DBA and data modeling experience
 3+ years of experience in designing data processing pipelines using distributed parallel processing technologies
 2+ years of experience with big data tools and technologies
 Hands-on experience working with distributed file systems such as Hadoop Distributed File System (HDFS) and distributed programming using Java MapReduce, Apache Spark, Pachyderm, etc.
 Experience with building stream-processing systems using solutions such as Storm or Spark-Streaming
 Algorithm development for high-performance systems
 Experience with data pipeline management, ETL/ELT and data/system architecture
 Experience with integration of data from multiple data sources
 Experience with NoSQL databases such as HBase, Cassandra, MongoDB, etc.
 Experience with Cloudera, MapR, Hortonworks and Databricks
 US citizenship or Permanent Resident with the ability to obtain and maintain a Public Trust Clearance required

Desired Skills/Experience
 Experience with Cloud Service Providers (CSPs) such as Amazon AWS, Microsoft Azure
 Experience in large scale machine learning
 Knowledge of data exploration and visualization tools such as Tableau, Qlik, D3, etc.
 Big data certifications with Cloudera, Hortonworks or Databricks

What you can expect from us: Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this changesupporting our clients digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer.

Qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, gender Identity, sexual orientation, national origin, age, disability, veteran status, pregnancy, or other status protected by law. CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information.

Skills:
Systems Analysis
Data Modeling
Database Administration
Big Data

Have you been referred by a CGI Member for this position?* Yes No"
Data Engineer,Cogo Labs,"Data is the life blood of Cogo Labs and impacts nearly all decision making within the company. Cogo Lab's data team serves the important responsibility of maintaining, planning, and scaling these vital systems. As a curator of these systems you will ingest and process billions of events each day and have a direct impact on Cogo's bottom line.

As our Data Engineer you will:
Build and maintain robust, fault-tolerant ETL pipelines for Cogo's internal and external data resources.
Work with incubatees and teams to plan their data pipeline, optimize their SQL, and act as a sounding board for all data questions.
Maintain, plan, and scale Cogo's MySQL and PostgreSQL relational database infrastructure.
Respond to occasional off-hours outages.
Skills and requirements:
Familiarity with five of the following: Spark, MySQL, PostgreSQL, EMR, S3, Redshift, Airflow, Presto, Hive, Kafka, RabbitMQ, Map Reduce.
Proficiency with Python and one of the following languages: Java, Scala, Golang
Comfort and experience with the Linux command line and Git.
Knowledge of relational database best-practices like replication, high availability, and performance tuning.
Strong problem-solving/analytical aptitude.
Ability to self-manage tasks and context-switch as necessary.
The willingness and ability to learn.
Nice to haves:
Experience or familiarity with LVM, md, ext4, or ZFS
Exposure to Docker and/or containers"
Data Engineer,Reverb.com,"As part of the Reverb Data Engineering team, youll help build the platform to enable data-driven decisions and products that scale along with our business. Were using Python and Scala to build a streaming architecture to support production machine learning. Were a small, eager team so were looking for engineers who can take a high degree of initiative and enjoy working across team boundaries.

Everyone at Reverb takes creative initiative, helps set their own priorities, and comes up with new ways to grow the business. Our engineers take pride in building great software but take even more pride in shipping great features for our customers. If want to learn more, check out
this video on working at Reverb
[https://www.youtube.com/watch?time_continue=45&v=rROUwUytfDU].

Requirements :
Professional experience in Python and/or Scala.
Professional experience with one or more of: Hadoop, Spark, Flink, HBase, Kafka, Kinesis.
Nice to Have Skills:
Numpy/Scipy/Scikit-learn.
General ETL pipeline architectures.
Some familiarity with standard machine learning techniques (for example: regression techniques, data cleaning, clustering algorithms) is a plus.
Some exposure to deep learning is also a plus.
What you'll get :
Competitive salary and stock options in a high growth company.
No-bureaucracy environment where ownership and initiative is valued.
Health insurance and a healthy work environment  no 80 hour weeks.
401k with 4% match.
Flexible vacation and sick days.
A MacBook Pro, monitor, keyboard, mouse of your choice, and even a stand up desk if you want!
Discounts on music gear.
This is a local position in Chicago, please no remote workers or recruiters. Please send us a link to your github!"
Data Engineer,Posh Technologies,"Title: Data Engineer

Duration: Long Term, 12-18 Months

Location: Redmond, WA

Job Description:

Responsibilities:
Manipulate and analyze complex, high-volume, high-dimensionality data from varying sources using a variety of tools and data analysis techniques
Apply statistical concepts and techniques to analyze experiments and user behavior
Use and promote data-exploration techniques to discover new or previously unasked questions SQL programming (BI focused preferably skilled with SSIS, SSAS, MDX, DAX, Data mining)
Define metrics and methodology that accurately quantify the value of improvements
Present findings and recommendations to development partners and key decision makers at various management levels
Influence stakeholders to make product improvements that yield business value by effectively making compelling cases through story-telling, visualizations, and other influencing tools
Deliver on ambiguous projects with multiple stakeholders, unclear requirements and incomplete data Provide input to software engineering teams on new analytical capabilities needed
Qualifications:

Required Skills:
Big Data -- Microsoft Cosmos / Azure Data Lake (or equivalent experience with Hadoop/Hive)
Scope / U-SQL programming (or equivalent experience with Hadoop/Hive)
C# / .Net stack (or equivalent programming language/stack), exhibiting strong algorithmic, design, coding, and documentation abilities)
SQL programming (BI focused preferably skilled with SSIS, SSAS, MDX, DAX, Data mining)
Optional Skills:
PowerShell (or any other scripting language)
Stats and data analysis experience working with Advanced tools like R, SAS, JMP, Advanced Excel (Pivot tables, Power BI)
Experience with Microsoft internal tools like AzureML, TLC, VIZFX, ScopeML, Power BI, xflow/sangam etc. is a plus
Experience with web programming to build data visualization web portal is a plus"
Data Engineer,healthgrades,"Healthgrades is seeking a Data Engineer to join us in Madison, WI or Denver, CO. Are you looking for a company where you can grow and make a difference?

Millions of people use Healthgrades monthly to research, compare and connect with physicians and other healthcare professionals. We provide consumers with the information they need to make more informed decisions, including information about the providers experience, patient satisfaction and hospital quality. Physicians and healthcare professionals rely upon Healthgrades, as well, to accurately represent their practice online, make them more relevant and discoverable, and connect them with consumers. We are dedicated to delivering solutions that bring a new level of transparency to healthcare.

The Data Engineer designs, develops, tests, and maintains database functionality to support company clients, products, and services.

Primary Responsibilities:
Develops data management framework that is effective, scalable, and reliable
Builds and maintains database structures, ETL processes, stored procedures, audit reports, and data extracts
Contributes to development of data warehousing solutions to optimize performance
Researches and prototypes new technologies
Ensures compliance of structures and systems with data management objectives
Participates on the Enterprise Architecture team
Assists in defining the data management strategy across data domains and products
Assists in defining and implementing data governance standards, policies, and best practices
Oversees data transformation, normalization, cleansing, aggregation, workflow management, and business rule application
Creates and maintains documentation to support developed processes and applications
Performs quality assurance checks
Loads, processes, and migrates incoming data feeds
Creates outgoing data extracts
Creates relational database models
Builds messaging queues
Provides decision support and fulfills ad hoc requests
Implements and maintains database training and assessment programs
Required Skills:
Bachelors degree in Database Development or equivalent experience
5+ years experience with complex database management systems
Proficiency with Microsoft database technologies, Java, and ETL
Solid understanding of software development methodologies
Ability to set priorities and manage time to complete tasks and meet deadlines
Ability to translate ideas into concepts and designs
Ability to adhere to strict design principles, policies, and implementation deadlines
Proficiency in other database technologies may be required for specific structures and/or systems (e.g. Python, Informatica)
Why Healthgrades?
At Healthgrades, we recognize that our people drive our greatest achievements. We are passionate about maintaining a fulfilling, rewarding and energetic work environment while setting the stage for your continued success.
Meaningful Work  helping millions of Americans connect with their healthcare providers
Changing the Game - evolving, fast paced culture with career advancement opportunities
Community Builders- partnering with local charity organizations and wellness initiatives
Robust Perks  generous PTO, 401k contributions, tuition assistance, entertainment discounts & more!"
Data Engineer,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities The Data Engineer will work with a team supporting a wide range of activities, including information systems development, integration of scalable solutions using various platforms, and architecting automated and scalable data process monitoring processes.

Qualifications Required

Bachelors degree in computer science, IT or equivalent technical discipline, or approximately 3-5 years work experience in relevant focus areas.

Approximately five years of experience in information systems development, focused on processing large volume, near-real time data feeds to meet data analytics and security requirements.

Demonstrated experience as a SME to prioritize and meet tactical and strategic requirements for frameworks and systems to process data.

Demonstrated experience in information systems development across the IT lifecycle, with a focus on systems to perform high volume and velocity data processing on disparate data types/formats.

Over two years demonstrated experience with delivering integrated and scalable solutions using the LexisNexis, ECL, and Pentaho platforms.

Demonstrated experience with identifying and delivering new, integrated and scalable solutions using current large-scale open-source and GOTS solutions for processing, storing and enabling analytics from high volume and velocity data collections ) on datastores and comprised of billions of records.

Demonstrated experience architecting and delivering automated and scalable data process monitoring and data quality systems and processes.

Desired

Demonstrated expertise delivering solutions to address user requirements and enrich analytic models using common industry tools to perform natural language processing, entity extraction, and data aggregation on human and machine generated data.

Demonstrated work experience delivering solutions applying industry standard Agile methodologies.

Over one year in relevant technologies (current examples include Java, ECL, Pentaho, Oracle, NoSQL, Kafka, FLUME).

Prior experience delivering data processing systems to adhere to Sponsor or IC data handling, tagging, compliance, and security requirements.

Demonstrated work experience in architecting new system solutions using current technologies, as well as transitioning existing systems into modern technologies without negatively impacting operational and compliance requirements.

Demonstrated experience performing data assessment, data engineering, modeling and analytics to enable new methodologies for end user analysts, data scientists, etc.

Experience with data analysis; target needs assessment; systems engineering and integration; data acquisition/distribution/ management/ enhancement; data prioritization; Sigint/Humint Targeting and Analysis."
Data Engineer,"Navstar, Inc.","Job Summary:

Would you like to perform rewarding work while contributing to the success of an established, growing company? Navstar is an award-winning organization that has a proven track record of successfully providing IT services and solutions both as a prime and sub-contractor on mission focused IT programs. Our employees are integral players in support of mission-critical programs focused on our National Security.

The program has a need for a Data Engineer to support the development of a data lake for a project located in Mclean, VA. In this role, the candidate will work at a customer site to support the agile development of tools and leverage standard tools (particularly Apache-NIFI) for Extract, Transform, and loading data between databases for the sponsor. The successful candidate will create custom code to quickly extract, triage, and exploit data across domains in support of analytic work while supporting the strategic development of replicable processes.

The successful candidate will use NIFI to ETL data into a secure Hadoop environment. As such, prior NIFI experience is strongly desired, but not required. They must write NIFI processors or, in instances where NIFI cannot be implemented, write custom Java code to ingest existing and new data sources. The candidate will conduct product usability tests and must work efficiently with a cross functional team members to include analysts, data scientists, project managers, and software solutions integrators.

Required Qualifications:
5-10 years of experience in data engineering and/or database administration.
Experience with Apache-NIFI (highly desired), Kafka, and Spark Streaming for ETL work.
UNIX experience.
Familiarity with HBase, solr, Spark, Oozie, and Impala.
Java and Python proficient.
Understanding and proficiency in cross-domain solutions (ETLing data from unclassified to classified systems and across classified environments).
Agile development and proficiency in continuous integration/delivery tools such as Jenkins, Artifcatory, and Git.
To be considered for this position you must hold an active TS/SCI security clearance with polygraph.
Desired Skills, Education, and Abilities :
Proficiency with AWS and container technologies such as Docker desired but not required."
Data Engineer,Insikt,"Our Company
Insikt is a rapidly growing technology company that connects borrowers seeking loans with investors seeking better, stable returns. Our mission is to bring trust and transparency back to the world of lending and securitization while also helping underserved families and small businesses find responsible credit options. We are accomplishing this mission by enabling:

1. Lending: Our proprietary white-label lending platform (Lendify) allows any retail store, online brand, or distribution partner to make loans or provide purchase financing to its customers, in its own brand name, without any prior credit expertise, and with minimal upfront investment.
2. Investing: Our proprietary investing platform allows accredited investors to buy into a variety of investment instruments backed by distinct loan portfolios underwritten in-house. Each investor has online access to all the same data as our Risk and Analytics teams, ensuring transparency into all investment instruments.

We have brought together the best and brightest from top-tier technology companies, leading financial service providers, and world-class investment firms. We are funded by some of the best investors and entrepreneurs in the world, including First Mark Capital, Capital One, Continental Investors and Serengeti Asset Management.

Our Values
We are a tight-knit team of highly talented and motivated people from a diverse set of backgrounds and industries all tenaciously driven by our mission. We value creativity, courage, and collaboration and we believe in letting the best and most innovative ideas rise to the top. We are looking for the absolute best people who want work hard, play hard, and build a socially-minded, profitable, and enduring business in the process.

What we are looking for at Insikt
You work on every aspect of the problem: data munging, feature engineering, modeling, evaluation launching models to production and monitoring
Apply advanced machine learning models and statistical techniques
You are a great programmer, you leave the code in substantially better state than you found it
You believe in teams. You know that the whole is greater than the sum of its parts. You rely on others candid feedback for continuous improvement
Work collaboratively with our credit, risk and growth teams to drive end-to-end implementation
Learn from some of the smartest and most experienced data scientists and engineers in fintech
Must Haves
B.S., M.S in Computer Science
4+ years of experience
Proficiency in Python and Java
Background in statistics and machine learning
You have an architectural perspective
You can lead large projects with feedback from other engineers
You have the ability to power decisions at the core of Insikt on lending, growth, operations, and more.
We want you to apply the best machine learning techniques to lending, and help us find a way to lend to the tens of millions of Americans that banks will not lend to.
Deep understanding of relational and non-relational databases
Familiar with distributed systems
Bonus points to have
Practical experience with modern deployment tools, and infrastructure as code, we run on AWS
Familiarity with lending space
Experience designing microservices
Compensation
Compensation is competitive and will be based on prior work experience.
Insikt Options.
401(k)
Complimentary snacks in the office
Onsite gym
Convenient location in Downtown San Francisco less than two blocks from BART
We offer equity packages that allow you to have ownership in this growing company.
Full medical, dental and vision coverage. Insikt pays 100% of employee premiums and 50% dependent premiums for medical, dental and vision benefits.
You can expect a collaborative environment, team building activities, dinners, and fun monthly gatherings, along with mentorship and career growth.
We believe in getting people to where they need to go. This value extends beyond our business principles into Insikts internal career development philosophy. As such, you will work closely with a highly-experienced management team to mentor you and help you achieve your personal career goals."
Data Engineer,Facebook,"(Menlo Park, CA - Seattle, WA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. At Facebook, we have many opportunities to work with data each and every day. How would you like to work on data and build some of the tools that are critical to moving & transforming this data into valuable & insightful information? If so, this is the right job for you. Our Enterprise Data Warehouse team works very closely with all aspects of data, both internal and external. We are looking for a Data Engineer with the Software Engineering chops to not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's data consumers and analysts. You will get the opportunity to work with focused and scaled objectives in a company that has some of the most challenging problems to tackle. This is a full-time position based in our office in Menlo Park. Responsibilities
Build data expertise and own data quality for the awesome pipelines you build
Architect, build and launch new data models that provide intuitive analytics to your customers
Design, build and launch extremely efficient & reliable data pipelines to move data (both large and small amounts) to our ridiculously large Data Warehouse
Design and develop new systems and tools to enable folks to consume and understand data faster
Use your expert coding skills across a number of languages from Python, Java and PHP
You have developed applications within the LAMP Stack environment
Work across multiple teams in high visibility roles and own the solution end-to-end
Minimum Qualifications
2+ years of Java and/or Python development experience
2+ years of SQL (Oracle, Vertica, Hive, etc.) experience
2+ years of LAMP stack development experience
2+ years of experience in custom or structured (i.e. Informatica/Talend/Pentaho) ETL design, implementation and maintenance
Experience working with either a MapReduce or a MPP system on any size/scale.
Preferred Qualifications
Experience working with or in support of diverse communities"
Data Engineer (ETL),C.H. Robinson,"C.H. Robinson has embarked on an initiative to better execute corporate strategy using advanced Business Intelligence (BI). To take the Enterprise Data Warehouse and Business Intelligence to the next level of providing business value, we are looking to expand our technical expertise with individuals that have passion for data analytics and want to enhance our business and analytical skills.
The Enterprise Data Warehouse team provides analytical experiences through secure, consistent and trusted data that empowers our users to make impactful decisions. That information is consumed by teams at all levels of the organization and also by CHR customers. Prepared data is utilized by our business users to answer critical strategic questions and continuously enhance our trading partner reporting platforms. The work is considered critical and vital to C.H. Robinsons worldwide logistics operations and its continued growth and success.
This position will mainly engage in designing and implementing solutions for Enterprise Data Warehouse
Some core characteristics include being adaptable, innovative, empathetic, goal setting, and business oriented
Create ongoing value for the business through iterative development/delivery, and working on continuous improvement within an Agile environment

Responsibilities:
Contributes to the definition of the software development process as well as the definition of best practices.
Designs technology solutions for complex scenarios that may influence architectural implementations of technology and business process, taking in to account performance and availability.
Translate/Convert the business high-level requirements into detailed designs, partnering with Business Analysts, business stakeholders, and Architects. Analyzes requirements with business analyst(s) and peers for improvement
Estimates for application development effort including identifying dependent teams and developing code to satisfy complex requirements.
Participates in the development of enterprise tools to be used by multiple teams/applications.
Designs de-normalized database schemas to provide meaningful business context.
Utilizes advanced source control features and provides input on strategy for the team.
Uses automation to build extensive end-to-end system tests and creates reusable test scripts for regression testing.
Tunes performance and resolves availability issues of application and its dependencies. Debugs multiple applications including service boundaries and dependent processes.
Maintain knowledge of emerging technologies
Participates in code development that defines the business logic rules for large scale (enterprise wide) solutions, that work with large data sets and process in bulk
Creates KPIs that will drive business decisions and actions

Qualifications:
2+ years collaborating with a team and working in a team environment
Experience with object oriented design
Proven track record in delivering in an agile environment
Practical Experience with test driven development (TDD) and Continuous Integration/Continuous Deployment (CI/CD)
Skilled with GIT and GITHub or comparable distributed version control system
Self-motivated, able to work with minimal supervision
Passion for using technology and data to solve business problems
Strong analytical and problem-solving skills
Solid interpersonal communication and collaboration skills. Able to communicate technical and business concepts across teams and various business stakeholders. Ability to provide honest and timely feedback to business stakeholders, peers, and management
2 years of experience working with database objects
2 years in the collection and organization of data
2 years of experience coding an ETL tool to load a data warehouse or create a data store
Experience in performance tuning in a SQL environment
Experience in Dimensional, Entity-Relationship, Tabular models and OLAP data modeling

Education:
Undergraduate degree in Computer Science, Software Engineering, IS, MIS, or other Technology degree or equivalent work experience.
Coding Bootcamp graduates also considered with equivalent work experience

Benefits

We offer a competitive compensation package and excellent benefits including medical, dental, and vision insurance, prescription drug coverage, paid holidays and vacation, disability insurance, life insurance, 401K with company match, profit sharing, Employee Stock Purchase Plan, and the opportunity to prosper in a Fortune 500 company.

About C.H. Robinson

Become a part of our team of over 500 talented IT Professionals. Work in collaborative, Agile development environment. Find continuing challenges and work with committed leaders. Stay with us  were large enough to build global solutions, but small enough to make real impacts as individuals.

C.H. Robinsonaccelerating careers with immense opportunities and professional growth within the global supply chain industry. Start here. Accelerate here.

Every individual working at C.H. Robinson is integral to the success of our customers and our company. C.H. Robinson is a Fortune 500, global company that values teamwork, initiative, accountability, and integrity from its employees. We work globally and innovate daily to enhance and execute supply chains that move goods around the world. The fast pace of the logistics industry translates into a high-energy and collaborative workplace environment. We are empowered to make decisions, help our customers grow, and accelerate our careers.

No matter the product being shipped or from which corner of the globe, C.H. Robinson can help make it happenquickly, securely, and reliably. Through personal connections and solid relationships, our employees use their in-depth knowledge, robust tools, and global network to help customers reach their goals quickly. Whether shipping by plane, rail, ship, or truck, C.H. Robinson has the knowledge, flexibility, and dedication to deliver the goods that make our world go round.

Join the 12,000 employees worldwide who are accelerating their careers at C.H. Robinson.

Equal Opportunity Employer

C.H. Robinson - Affirmative Action Employer/EOE/M/F/Disabled/Veteran"
Data Engineer,AXA,"The Data & Analytics Office (DAO) is responsible for enterprise-wide governance and utilization of information as an asset, via data acquisition, data processing, analysis, data mining, and other means to create value for the organization. The DAO oversees data collection, use and strategy throughout AXA. This data includes large structured and unstructured data sets (e.g., speech analytics, social media listening, digital footprints, customer behavior, financial information, proprietary market research, and secondary sources) with the goal of discovering meaningful implications for business decisions.

The Data Engineer is responsible for the design, creation, and use of data in support of the analytics and business/operational needs of a large financial services organization. The Data Engineer will be expected to consume data generated by dozens of disparate systems to produce new data sources that are elegantly abstracted, accurate, and reusable. The Data Engineer is expected to have expert level mastery of Hadoop (CDH), Spark, and relational databases. The Data Engineer must have excellent communication skills and independently collaborate with actuarial and business experts.

Qualifications

Required Skills:
Bachelors Degree and 4 years of relevant work experience

Proficiency with the following: JAVA, Hadoop (CDH), relational databases and SQL, ETL development, python/scala, spark, data validation and testing

Experience with collaborative development workflows (e.g., GitHub)

Excellent written, verbal and interpersonal skills, a must as there will be significant collaboration with the business and IT

Preferred Skills:
Experience with legacy systems (i.e., mainframes), additional software development experience, project management experience and production deployment experience

Expertise in machine learning or statistics

Other information

NOTE: AXA participates in the E-Verify program.

In addition to competitive compensation and an outstanding benefits package including 401 (k) and medical programs, we offer the opportunity for continued professional development in a congenial corporate environment.

AXA is committed to providing equal employment opportunities to our employees, applicants and candidates based on individual qualifications, without regard to race, color, religion, gender, gender identity and expression, age, national origin, mental or physical disabilities, sexual orientation, veteran status, genetic information or any other class protected by federal, state and local laws ."
Data Engineer,The Hartford,"Ready to grow your career leveraging the latest DATA technologies?

Join a fast-paced and talented Agile Scrum team to unlock Data Capabilities for The Hartfords Commercial, Claims and ERM business. You will have an opportunity to participate in the entire software development lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies. We use the latest DATA technologies, software engineering practices, Agile delivery framework, and are passionate about technology and building well architected and innovative solutions that drive optimal business value generation.

This cutting edge and forward focused team presents the opportunity for collaboration, self-organization within the Scrum Team and visibility as we focus on continuous Business data delivery.

Whats in it for you?
Experience deeper understanding of Data analytics, Emerging technologies and Development practices
Collaboration with a high-performing, forward-focused team, Product Owner(s) and Business stakeholders engagement
Opportunity to expand your communication, analytical, interpersonal, and organization capabilities
Experience working in a fast paced environment  driving business outcomes in Agile ways of working
Enable and influence the timely and successful delivery of business data capabilities and/or technology objectives
Enhance your e ntrepreneurial mindset  network opportunity and influencing outcomes
Hone your development capabilities using various tools such as Informatica, B2B, PL/SQL, Hadoop, etc. to build data assets that enable business value generation
Appreciation and opportunity to learn and support rapid software construction and deployment using DevOps and Cloud based future technologies
Supporting environment that fosters can-do attitude and opportunity for growth and advancement based on consistent demonstrative performance
Optimize business value by leveraging your DATA experience and depth
Be part of a Scrum Team  driving work independently or collaboratively towards achieving business outcomes
Act as a resource for colleagues with less experience
Qualifications

What is The Hartford looking for?
Bachelor degree with at least 2-3 years of applicable work experience
Desired educational experience include, but are not limited to: Computer Science, Engineering, IT, Management Information Systems, Data Analytics, Applied Mathematics, and Business
Desire candidates with prior Data Engineer competencies and prior experience with successful enablement of Data Delivery initiatives
Understanding of current and emerging IT products, services, processes and methodologies
Prefer experience with Big Data technologies and concepts on a Hadoop platform (e.g. Scoop, Hive, Pig, NoSQL, etc) and willing/adapting to future technologies
Prefer working knowledge of ETL process across various tools and experience with SQL skills
Behaviors at the Hartford
Deliver Outcomes  Demonstrate a bias for speed and execution that serves our shareholders and customers.
Operate as a Team Player  Work together to drive solutions for the good of The Hartford.
Build Strong Partnerships  Demonstrate integrity and build trust with others.
Strive for Excellence  Motivate yourself and others to achieve high standards and continuously improve.

Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression

** NO AGENCIES PLEASE **

Job Function : Data Engineering
Primary Location : United States-North Carolina-Charlotte

Schedule : Full-time
Job Level : Individual Contributor
Education Level : Bachelor's Degree (16 years)
Job Type : Standard
Shift : Day Job
Employee Status : Regular
Overtime Status : Exempt
Travel : Yes, 10 % of the Time
Job Posting : Dec 22, 2017, 2:37:36 PM
Remote Worker Option : No"
Senior Data Engineer,DISNEY,"As a member of the DCPI Data engineering team, the successful candidate will possess a work style that promotes progress through collaborative teamwork in a dynamic environment, be enthusiastically committed to delivering world-class data and analytics engineering solutions to the highest levels, and work with other team members to provide creative analysis and approach to optimize the existing data platform and contribute to developing innovating solutions to solving complex business problems.
This position is a legal entity of Disney Consumer Products & Interactive Media, an equal opportunity employer.

BS or MS in Computer Science or related field or equivalent work experience
Knowledge of Hadoop, Hive, Spark and Pig in a Cloud environment preferred.
3+ years solid engineering background with SQL, Linux script, Java or Python.
1-2 years experience in Web front development: HTML, CSS, DJANGO
3+ years working experience working knowledge of Hadoop/Spark/Hive
3+ years solid background with data mining and/or machine learning
Must be a self-motivated, energetic, detail oriented team player passionate about producing high quality data & analytics deliverables and thrives in a collaborative environment
Strong analytical aptitude required
Passionate about working with large volume of data
Ability to work in an agile development environment with minimal documentation and process
Strong problem solving skill to identify the root cause and implement the fix
Work smarter, not harder  Proponent of not re-inventing the wheel; generally, prefer wielding the power of open source and community-driven software to solve hard problems, quickly
Perform data platform development, support and enhancement
Develop data pipeline and engine to support application engineers to use data to implement personalization and recommendation
Perform data-warehouse design and programming
Develop engineering analytical solutions and program by following analytical models and solutions to help solve business problems.
Provide engineering support for the usage and interpretation of data to various business partners.
Perform engineering support for data-mining, deep learning and performance measurements on business-information systems utilizing analytics tools and methodologies
Be part of the team to perform the technical proof of concepts, explore new technologies for machine learning such as google tensor flow or open source APIs/packages. This position is a legal entity of Disney Consumer Products & Interactive Media, an equal opportunity employer. 509823"
Data Engineer,Piper Companies,"Piper Companies

is currently looking for an awesome Data Engineer to join a rapidly growing company in Conshohocken within walking distance to the train, bike trail, great restaurants, gyms, etc. We are looking for clever, self-motivated performers, who are not happy unless they're getting things done.

Attributes of the Data Engineer:
You are clever, self motivated, and not happy unless youre getting things done.
You understand the appropriate architectural and design issues involved in building scalable, reliable, high-performing software solutions for web, mobile and other cutting-edge delivery channels.
You excel in a collaborative environment where your co-workers rely on your contributions to help guide the team to success.
You have experience in working with Agile/Lean approaches and you understand the difference between being Agile/Lean and doing Agile/Lean.
You are excited by the prospect of working in a dynamic, diverse, collaborative, transparent, non-hierarchical, and ego-free culture and you are excited by the prospect of proactively helping this culture evolve.
You are committed to helping your team be as high performing as possible.
You will not compromise your belief that software best practices, like TDD, automated testing, and continuous integration are essential to delivering quality software.
You know that work life balance is important and act accordingly.
You don't object to writing minimally sufficient documentation or doing whatever else is needed to ensure delivery of a complete product.
You are not happy unless you understand and can see how your efforts directly impact your customers.
Qualifications of the Data Engineer:
Bachelors degree in Computer Science/Engineering preferred.
Proficiency building data processing systems with Hadoop, MapReduce and HDFS using Python and Java.
Experience using querying tools such as Pig and Hive.
Experience building stream data processing systems using Apache Spark and PySpark.
Experience administering, optimizing and troubleshooting NoSQL databases such as MongoDB or DynamoDB.
Demonstrable experience utilizing Agile (Scrum/Kanban) and creating an effective Agile culture as well as implementing the appropriate Agile practices.
Perks/Benefits for the QA Automation Engineer:
You get to work with all of the latest and greatest technologies including the hottest Big Data technologies/tools.
Flex work schedules, unlimited vacation days, weekly catered lunches, full stocked kitchen
Newly constructed, state of the art office, walking distance to train station
Personal & professional development is a top priority
Please send resumes to Joseph Marshall at JMarshall@pipercompanies.com"
"Sr. Data Engineer - Oracle Data Cloud, Shopper Intelligence - CT",Oracle,"Sr. Data Engineer - Oracle Data Cloud, Shopper Intelligence - CT-17000FZM
Preferred Qualifications

Sr. Data Engineer / Data Engineer Shopper Intelligence

Oracle Data Cloud: Powering connections to customers.
Join Oracle Data Cloud and be part of the team creating innovative and groundbreaking data-driven solutions for Marketing, Sales and Customer Intelligence.

As a Data Engineer on the Shopper Intelligence team, within Oracle Data Cloud, you will be working with retail transaction data to ingest and map data into our solutions. You will help drive our future roadmap and transition to cloud based technologies and support our continual focus on scalability improvements to provide quick, actionable data insights. Strong analytic skills are necessary to help break down problems and provide solutions.

In this role, youll be using your database expertise and knowledge to ingest data from hundreds of different companies, and the process you develop will revolutionize the way products and services are sold. We rely on all of our Database Developers to develop efficient solutions to process this data and provide innovative ways to access and use the data for our digital and offline marketing products. Our solutions will require you to work effectively with your teammates, of course. But your real success with Oracle will be measured by how well you couple critical thinking with self-motivation, enthusiasm and determination.

Day-to-Day Responsibilities:
Work as a member of an Agile Software Development Team to help implement new functionality based on business requirements
Help drive our migration to cloud based data technologies including Qubole, Spark and Hadoop
Develop in a variety of database technologies including; MS SQL Server, Cassandra, Redshift, Hive, MySQL
Developing and automating solutions for the enterprise data warehouse including ETL, tables, views, stored procedures, and indexes.
Analyzing and understanding business requirements that impact the data warehouse and working with business customers to define and document their data and reporting needs and developing solutions that meet those needs.
Developing processes and procedures to manage ETL into the data warehouse.
Develop data cleansing and error processing routines, which ensure data is loaded into database correctly and completely. This is critical to ensuring the data in the data warehouse is accurate, complete and timely and can therefore be relied upon for decision making purposes within the business.
Interact with the Oracle business and operations teams and facilitate working through technical implementation and customer service issues.

Preferred Qualifications:
Bachelors degree in Computer Science or related field
Strong SQL Server experience including SSIS
Advanced knowledge of T-SQL / Stored Procedure / Function programming and design preferred
Solid understanding of ETL, database design and data warehousing preferred
Other experience we look for includes: Perl, Python, Linux, Hadoop, Spark, Scala, SQL, big data experience.
Familiarity with AWS (Amazon Web Services) is preferred.
Familiarity with retail POS systems and associated data contained in transaction logs (TLOGS)
Strong analytical skills
Knowledge of basic client/server architecture, secure coding practices, and computer networks
Able to work independently and as part of a team
Excellent written and verbal communication skills
Must have the ability to work in a dynamic, fast-paced environment
Strong communication skills to interact with Agile team members
Talent Advisor: Whitney Lindsey
whitney.lindsey@oracle.com

Detailed Description and Job Requirements

Work with a world class team to provide enterprise-wide, Oracle Database Administration support for production systems and provide DBA services to application development teams, including database design, database generation, coding, and database production support. Provide DBA support with a high degree of customer service, technical expertise, and timeliness. Provide accurate and creative solutions to user problems of moderate nature to ensure user productivity.

Provide Development and Production support for databases. This includes creating primary database storage structures; designing and creating primary objects; modifying database structure as necessary for enhancements or performance; monitoring and optimizing performance of the database; planning for backup and recovery of the database; allocating system storage and planning future storage needs; and creating database-related scripts and programs to support development and production environments.

Duties and tasks are standard with some variation. Completes own role largely independently within defined policies and procedures. 3 years of experience supporting relational databases as a DBA, with multiple distributed relational DBSM*s or a depth and focus in Oracle and related tools. Exposure and practice at using the DBMS to optimize performance of large databases for enterprise use is preferred. Experience with an organization with a key 24 X 7 reliance on its database is desirable. BS or equivalent desired.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
Data Engineer,ViaSat,"ViaSat builds high-capacity satellite networks across the world, challenging the notion that broadband connectivity only happens on terrestrial networks. We need your help leveraging the latest cloud technologies to enable ViaSat's Commercial Satellite Products group, which is hard at work on our new satellite network designed to handle millions of customers.

The Analytic Tools Group (ATG) uses the latest Big Data technologies (Spark, Kafka, Impala, Map/Reduce) to store, process, and analyze the continuous stream of data our systems produce. We have a unique set of skills to support these goals. As a team member you'll build a breadth of knowledge across knowledge areas including software development, distributed systems architecture, machine learning, and data science.

As a software engineer, you'll build the systems and tools that the data scientists use to perform their analysis. Working with people of varying skill sets provides a unique collaboration opportunity for those creating the tools (developers, i.e. you) and those analyzing the data (data scientists). You'll interact with each other daily, encouraging innovative solutions to Big Data problems accounting for both the how and the why.

We encourage learning through osmosis, ensuring no one is confined to their specific area of work. Similarly, we value ability and willingness to learn, over experience in a particular area. This isn't to say we don't value expertise and mastery of a subject, rather we recongize the need for evolution of knowledge in our rapidly changing field.

We are looking for individuals who are excited by challenging problems and love to dive into new technologies to solve them.

Familiarity with any scripting language (Python, Ruby, BASH, etc )
Familiarity with Linux environment and common tools
Experience designing highly scaled SAAS systems
Bachelor's Degree in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related field

Hadoop, map/reduce, Spark, Kafka, Impala, or other big data technologies experience
Familiarity with RDBMS technologies (PostgreSQL, MySQL, Oracle, etc.)
Understanding of grid/parallel computing architectures
SAAS application development capability
Web application design (Angular.js, bootstrap, d3, etc)
2 to 3+ years Java, C++, Scala, or other similar development experience
Master's Degree in Computer Science, Computer Engineering, Electrical Engineering, Math, Physics or related field

Our corporate headquarters in Carlsbad is just a hop and skip away from the beautiful Pacific Coast. Are you ready to take the next step in your career where you will thrive and enjoy coming to work? Then submit your resume. It only takes a few minutes and could start you on your new path to a fulfilling career at ViaSat. We look forward to hearing from you.

** ViaSat is an EEO/AA/Disability/Protected Veteran Employer. U.S. Citizenship or Lawful Permanent Residence status may be required for certain positions. For positions requiring U.S. Citizenship or Lawful Permanent Resident status, verification of such status will be required upon accepting employment.
HOW TO APPLY
Follow the link below.
Alternatively, email directly to sean.berger@viasat.com for an initial, confidential discussion.

Once you have submitted your profile, please ensure you regularly sign in via the online portal to check for regular updates on the status of your application.

#LI-POST"
Data Engineer,Mitel,"Data Engineer

Department: Customer Activation

The Customer Success Data Engineer (CSDE) will work directly with the Customer Success support teams to support current Mitel VOIP customers both premises based and hosted solutions. The CSDE will be tasked with identifying network issue in the customers environment that directly impact the Mitel solutions in a negative matter and fail to meet Mitel outlined best practices. The CSDE will also provide the technical recommendations for the customers environment and maybe tasked with completing the recommended changes. The CSDE will also be responsible for the continued ongoing education of the Customer Success Support teams with regards to network troubleshooting and identification of network based issues in a VOIP environment.

Responsibilities :
Identify customer network deficiencies and provide the technical guidance to resolve them
Design, configure, test, and verify routers, switches, and other network devices for use at the clients location(s)
Manage ad-hoc customer relationships during crisis periods
Clearly communicate findings to Management and Executives both internally and client facing
Provide mentorship and support of Customer Success Support teams
Design, configure, test, and verify 3rd Party MPLS, GRE, and NAT redundancy solutions for clients
Design, and configure BGP, Route Maps, VRFs and Prefix-lists to allow for peering and route filtering in hosted core router infrastructure (Cisco and HP)
Design and configure OSPF, EIGRP, HSRP, LLDP, Access-Lists, VLANs, DHCP and QoS protocols
Be an escalation point for the Customer Success Support team
Troubleshoot network and Telco issues including but not limited to call quality, network design, 3rd Party MPLS, analog gateways, IP phone firmware, and discovered carrier network issues
Requirements:
5+ years hands-on data networking experience
Robust, large-scale network design experience
Expertise with Cisco router & switch administration
Detailed understanding of sub netting and network segmenting in a VoIP environment
Detailed knowledge of VoIP and SIP messaging
Desire to both design and implement superior network solutions
Hands-on network troubleshooting experience; packet capture and inspection
Strong knowledge of OSPF and BGP protocols required
Knowledge of IPsec and GRE tunnel configuration required
Telco circuit knowledge & troubleshooting skills
MPLS configuration experience a plus
Strong communication and documentation skills are essential
VOIP experience preferred but not required
Enthusiasm and ability to develop technical skills and take on additional responsibilities in order to add value to departmental and Mitels mission and goals

Education:
BS in Technical or Engineering field (or equivalent) preferred, or equivalent work experience.
CCNA/CCNP or other data network equipment certifications required
Additional experience with HP Procurve, Juniper, Dell, Extreme, Brocade or Adtran a plus
Mitel is one of the largest providers of cloud, premises-based and hybrid business telephony and unified communications (UC) solutions. Our award-winning, brilliantly simple communications offerings lead the industry in customer satisfaction and lowest total cost of ownership. Our innovative business phones, application integration, collaboration tools, mobility, and contact center applications empower organizations and employees to collaborate, connect and work no matter time, place or device.

With Mitel, organizations enjoy freedom of choice: our award-winning, premises-based IP phone system owned and managed in-house, or cloud-based communications managed offsite by our expert telephony teams, or a combination of both. Regardless, Mitel solutions are built to scale, grow, and evolve as needed. So companies can buy or subscribe to the business communication solutions they need today, with no risk to their original investment should their needs change tomorrow.

Mitel purpose-built solutions remove the barriers and complexity common with other providers solutions  whether an organization is seeking to replace an old phone system, add new office sites or branches, upgrade to unified communications, or modernize its contact center. With fewer resources tied up in a phone system, a companys resources are freed to work on the real task at hand: moving the business forward.

Mitel is an Affirmative Action and Equal Opportunity Employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender, sexual orientation, national origin, genetic information, age, disability, veteran status, or any other legally protected basis."
Data Engineer,Central Intelligence Agency,"*Higher starting salary possible depending on experience level

Data Engineers focus on the design, implementation, and operation of data management systems to meet the CIA's business needs. It includes designing how the data will be stored, consumed, integrated, and managed by different data entities and digital systems. Data Engineers work together with data consumers to determine, create, and populate optimal data architectures, structures, and systems. Data Engineering requires an extensive knowledge of data manipulation, databases, data structures, data management, and best engineering practices.

Data Engineers must also plan, design, and optimize for data throughput and query performance issues. This requires constantly updating expertise in areas such as platform, network and storage technologies, bandwidth management, data bus implications and design.

Additionally, Data Engineers play a key role in the selection of backend database technologies (SQL, NoSQL, HPC, etc), their configuration and utilization, and the optimization of the full data pipeline infrastructure to support the actual content, volume, ETL, and periodicity of data to support the intended kinds of queries and analysis to match expected responsiveness.

Offices of the CIA - Directorate of Digital Innovation

The Directorate of Digital Innovation (DDI) is at the forefront of defining the future of digital expertise within the CIA. DDI focuses on developing the workforce with cutting-edge skills, investing in IT infrastructure, and modernizing the way the Agency does business. DDI officers help accelerate the integration of innovative methods and tools to enhance the CIA's cyber and digital capabilities on a global scale and ultimately help safeguard our nation. Learn more about the Directorate of Digital Innovation

Life at CIA:

In addition to a comprehensive benefits package, the CIA offers exciting career opportunities and a dynamic environment. We're on the forefront of world-altering events - as they happen. So working here isn't just a job, it's a mindset and a lifestyle.

Minimum Qualifications:
Bachelor's degree, preferably in Mathematics, Computer Science, Engineering, Management Information Systems or related fields
GPA of at least 3.0 on a 4.0 scale
The following items must be attached to your on-line application (PDF format preferred):
Your resume.
A cover letter in which you specify your qualifications for one or more positions.
Unofficial transcripts for all degrees.
ALL POSITIONS REQUIRE RELOCATION TO THE WASHINGTON DC METROPOLITAN AREA.

All applicants must successfully complete a thorough medical and psychological exam, a polygraph interview and an extensive background investigation. US citizenship is required.

To be considered suitable for Agency employment, applicants must generally not have used illegal drugs within the last twelve months. The issue of illegal drug use prior to twelve months ago is carefully evaluated during the medical and security processing.

Important Notice: Friends, family, individuals, or organizations may be interested to learn that you are an applicant for or an employee of the CIA. Their interest, however, may not be benign or in your best interest. You cannot control whom they would tell. We therefore ask you to exercise discretion and good judgment in disclosing your interest in a position with the Agency. You will receive further guidance on this topic as you proceed through your CIA employment processing.

To Apply:

Save the position(s) that interest you in the job cart. You can add up to four (4) positions. Job cart selections will only be retained during this site visit, so be sure to click Apply Now before closing the browser window. After clicking ""Apply Now"" you will be taken to the application account creation page. The positions will appear in the cart once you have created an account. DO NOT submit multiple applications; this will only slow the review of your application and delay processing. Please read the Application Instructions carefully before you begin the online application process."
Data Engineer,University of Pennsylvania Health System,"The Data Engineer is responsible for the development of complex data transformations for support of data warehousing, visualization, and self-service products within Penn Medicines Data Analytics Center.

We are looking for a developer for our ever-growing team who enjoys working in an Agile methodology, wants to be part of solutions designs, and is comfortable working with billions of rows of information. We are a tight-knit team of experienced developers who enjoy pushing the boundaries of innovation and improving patients lives one byte at a time. Our commitment is to our patients.

We will be seeking a teammate to fit into our easygoing environment yet shares the values of hard work toward a meaningful result. We are a team of constant learners, always looking for new ways and new technologies to solve ever larger problems and will expect the candidate to be comfortable with this as well.

We will expect the candidate to leverage their knowledge of SQL, scripting, programming, and/or past history of utilizing an enterprise ETL platforms to expand our boundaries and continue to build life-changing products. This individual will be required to ensure that the extracted, transformed, and displayed information meets all clinical and business requirements of the organization and end-users. This will entail creating new as well as maintaining and modifying existing development.

In addition, this individual will be required to provide technical counsel and guidance to other team members, consultants, IS Staff, and end-users. This candidate will be responsible for creating and maintaining accurate project and process documentation.

If youd like to learn more about the Data Analytics Center, please visit our website!

Minimum Requirements:
Degree Requirements and Minimum Experience Required:
 Bachelors Degree required.
 Current Internal Penn Medicine Information Services division employees may be considered with proof of active and continued enrollment in an approved bachelor degree program.
 Minimum of 5 years data warehousing, ETL and/or data visualization experience required.

Required Skills:
 Experience with databases and data architecture is required.
 Superior problem solving skills, the ability to work independently, and great communication skills are required.
 Experience with MS-SQL, T-SQL, Oracle, PL/SQL, and/or advanced data processing tools such as IBM DataStage, Oracle Endeca, etc is required.

Preferred Skills:
 Experience working with a variety of data formats, such as XML, JSON preferred.
 Experience with program and script languages such as C#, Java, JavaScript, or Shell desired.
 Experience with very large data sets.
 Experience with Big Data Solutions.
 Knowledge of healthcare data/systems is preferred.

Additional Information:
Be a part of the exciting and ground-breaking upcoming years for the Penn Medicine Information Services department!

Because growth is essential to continuing to meet the current and future needs of patients, Penn Medicine continues to expand its capabilities.

Penn Medicine's Information Services (IS) Department focuses its efforts on the clinical and financial systems that support the day-to-day operations of four hospitals, several satellite practices, and more than 2,000 physicians.
Learn more about Information Services

We believe that the best care for our patients starts with the best care for our employees. Our employee benefits programs help our employees get healthy and stay healthy. We offer a comprehensive compensation and benefits program that includes one of the finest prepaid tuition assistance programs in the region. Penn Medicine employees are actively engaged and committed to our mission. Together we will continue to make medical advances that help people live longer, healthier lives.

Penn Medicine
http://www.pennmedicine.org/careers/

Live Your Life's Work

EOE/AA, Minority/Female/Disabled/Veteran

We are an Equal Opportunity and Affirmative Action employer. Candidates are considered for employment without regard to race, ethnicity, color, sex, sexual orientation, gender identity, religion, national origin, ancestry, age, disability, marital status, familial status, genetic information, domestic or sexual violence victim status, citizenship status, military status, status as a protected veteran or any other status protected by applicable law."
Data Engineer,elasticiti,"We are seeking a high caliber Data Engineer to help our client take their ETL game (and yours) to the next level. Lots of cool data and tech to work with.

Responsibilities

Retrieve data via API and FTP

Create ETL procedures on new datasets

Establish real-time and batch processes

Implement data model and populate with data

Establish data ingestion frequency rules

Define, detect, and correct data quality issues

Design and develop data pipeline

Define data models to achieve business and scale goals

Closely monitor and debug data pipelines

REQUIRED

Programming

2+ years of experience with ETL

2+ years development experience with Python / Java / Scala / Node.js

2+ years experience in SQL (both querying and modeling)  Intermediate Level

2+ years of experience in OLTP and OLAP

Strong experience with SQL query performance tuning

Experience with system scaling for exponential growth

Solid debugging skills and experience with automated testing

Experience working with multiple heterogeneous data sources

Platforms

AWS ecosystem: experience with S3, EC2, EMR, Lambda, Redshift

Data pipelines like Airflow, Luigi, Talend, or AWS Data Pipeline

APIs: work with at least one of the following - Google, YouTube, Facebook, Twitter, or Oauth

Experience with version control (Github, Stash etc.)

Comfortable with Linux

PREFERRED

2+ years professional Python coding experience

Experience with the media industry or an advertising agency

Experience with structured and semi-structured data

The Position:
This is a full-time role working closely with our client who is one of the most recognizable and rapidly growing media companies. It is based out of NYC and does not require travel.

Why Elasticiti:
By developers, for developers. We are supportive of your continuing professional development. We respect you as a person and the work you do

Competitive compensation, benefits, and perks

Great projects; work with stimulating teams

Work with some of the most iconic media brands as well as rapidly growing, disruptive adtech and social media platforms."
Data Engineer,Custoria,"Marketing used to be an exercise in one-to-many communication: billboards, magazine ads, and - more recently - having a powerful social media presence.

At Custora, were helping turn this model on its head, using mountains of data to help our customers direct the right campaigns to the right users rather than blasting the same message to the largest audience possible.

We're moving at a rapid pace, taking on bigger customers every day and increasing the amount of data we collect by several orders of magnitude. As we leap into the next phase of our growth, we need developers who are motivated to solve interesting problems, experiment with new technology and help us change the way e-commerce marketing works in ways that will benefit both companies and consumers.

We have hard problems to solve.

We're already analyzing data for well over 500,000,000 end customers, and this number is growing fast. We're pulling data from numerous sources, computing sophisticated statistics on the fly and building a marketing experimentation platform that's usable by the entire marketing team.

You'll have lots of responsibility.

We're a small team, and each of us leads the projects we are most passionate about. You'll be able to own large parts of the software. Specifically, were looking for someone who can make strong contributions to our infrastructure. Someone with expertise or looking to gain expertise in the areas of automation, security, monitoring, performance or data engineering can be expected to take the lead on a project and expand the platform for Custora.

Try new things.

Experimentation is one of our core values at Custora and we need engineers who will apply new approaches to solving our problems. As our data grows and we move from a batch processing model to real-time analytics, we are ready and able to try new solutions.

Have fun while youre doing it.

We believe solving hard problems is easiest in a fun, relaxed, and collaborative environment. Our team enjoys a good joke as much as fixing a tough bug. We want to find engineers who are eager to continue this balance  creating a place where people look forward to going to work.

The Stack:
While we make use of a wide variety of tools, our primary web stack is ES6/React and Ruby on Rails deployed on AWS. We make extensive use of R for statistical analysis, and our primary data stores are Hive, MySQL, and Redis.

What its like to work here:
On Monday we eat and meet as a team to chat projects and progress.
Were 35 genuinely nice people. We work together (to varying degrees) and experiment with (figure out) how to do things. Its an amazing environment to both explore your professional interests and shape your career trajectory.
We move quickly. You build something and the next day it comes to life. You see and feel an immediate impact with the collective efforts of the team.
We have a book of cultural values that helps guide how we communicate and collaborate.
Were building a company and a team we love. Were in it for the long run.
Read more about what makes us, us here.

Custora is an equal opportunity employer. We value diversity. We dont discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, marital status, veteran status, disability status, or socioeconomic status.

Qualifications:
5 or more years of experience as a software engineer.
Degree in Computer Science or a deep competency achieved via other means.
Familiarity with Ruby on Rails, AWS, and SQL-based databases.
High standards for code quality and maintainability.

Nice to Have's:
Experience with R, Scala, Spark, and/or Chef.
Consistent record of delivering significant features or building out platforms and services.
Experience working in e-commerce.

Benefits:
Competitive salary and meaningful equity
Health, dental and vision insurance (100% covered)
Free lunch every day, plus free water  hot and cold!
Unlimited vacation: take as much time as you need, although we recommend 3 weeks per year at minimum.
Monthly unlimited MetroCard."
Data Engineer,UNIQLO,"UNIQLO is a brand of Fast Retailing Co., Ltd., a leading global Japanese retail holding company that designs, manufactures and sells clothing under seven main brands: Comptoir des Cotonniers, GU, Helmut Lang, J Brand, Princesse tam.tam, Theory, and UNIQLO. With global sales of approximately 1.14 trillion yen for the 2013 fiscal year ending August 31, 2013 (US $11.62 billion, calculated in yen using the end of August 2013 rate of $1 = 98.36 yen), Fast Retailing is one of the worlds largest apparel retail companies, and UNIQLO is Japans leading specialty retailer.

UNIQLO continues to open large-scale stores in some of the world's most important cities and locations, as part of its ongoing efforts to solidify its status as a truly global brand. Today the company has a total of more than 1,200 stores in 14 markets worldwide including Japan, China, France, Hong Kong, Indonesia, Malaysia, Philippines, Russia, Singapore, South Korea, Taiwan, Thailand, U.K. and U.S. In addition, Grameen UNIQLO, a social business established in Bangladesh in September 2010, opened its two first stores in Dhaka in July 2013. UNIQLO operates an integrated business model under which it designs, manufactures, markets and sells high-quality, casual apparel. The company believes that truly great clothes should be supremely comfortable, feature universal designs, are of high quality and offer a superb fit to everyone who wears them.

With a corporate statement committed to changing clothes, changing conventional wisdom and change the world, Fast Retailing is dedicated to creating great clothing with new and unique value to enrich the lives of people everywhere. For more information about UNIQLO and Fast Retailing, please visit www.uniqlo.com and www.fastretailing.com .

Position Overview:
UNIQLO USA is looking for a Data Engineer who will build and maintain a robust, scalable and sustainable data platform. As a pioneering member of the US Data Analytics team, your role is to build a best-in-class data warehouse management capability connecting retail offline and online data and powering the growth of our customer centric business.

You should have deep expertise in the design, creation, management, and business use of extremely large datasets. You should have excellent verbal and written communication skills to be able to work with business owners in a fast paced environment to develop and define key business questions, to build data sets that answer those questions, as well as implement sustainable reporting solutions. Above all, you should be passionate about working with huge data sets and someone who loves to bring information together to answer business questions that drives change.

Responsibilities
Be responsible for the design, implementation, deployment and maintenance of a robust and sustainable data platform
Implement best practices of data warehousing, but also maintain flexibility to think outside the box as necessary
Work closely with the Analytics team and business stakeholders to gather technical requirements and build data sets to answer business questions
Contribute the continual improvement of Uniqlos data platform via your observations and knowledge gained from your involvement and KPI's
Proactively recognize important data anomalies and outliers for further investigation
Work in a collaborative environment -- meetings, iterative development and design and code review sessions

Qualifications
Degree in Computer Science or related field
Expert level proficiency in SQL, Python and Luigi
Experience handling large data sets in a business environment and processing data on relational databases like Oracle/SQL Server/ Redshift
Deep understanding of retail and/or e-commerce KPIs
Basic AWS skills
Familiar with ETL and DW processes
Be comfortable with working with complex data models and large amounts data coming from various sources.
Experience with data visualization tools such as Tableau and Looker a plus
Experience with Amazon EMR+Spark and Hadoop a plus
Excellent verbal and written communication
Strong troubleshooting and problem solving skills
Thrive in a fast-paced, innovative environment
Experience building consumer-facing data products (like recommendations or productionalized algorithms) a plus

As an Equal Opportunity Employer, Fast Retailing does not discriminate against applicants or employees because of race, color, creed, religion, sex, national origin, veteran status, disability, age, citizenship, marital or domestic/civil partnership status, sexual orientation, gender identity or expression or because of any other status or condition protected by applicable federal, state or local law.
Posting Notes: New York || New York (US-NY) || United States (US) || Experienced Professional || Corporate || UNIQLO || EX: OUT || UNIQLO ||"
Data Engineer,Columbia University,"Columbia University's Mortimer B. Zuckerman Mind Brain Behavior Institute (Zuckerman Institute) supports interdisciplinary neuroscience research and discovery by scientists and scholars across the university and promises to be the most comprehensive institute for brain science. Located in a state of the art facility, approximately 50 independent world class labs are brought together to transform our understanding of the brain and mind.

The Zuckerman Institute is seeking a highly motivated individual with an interest in neuroscience to fill the position of a Data Engineer. The Data Engineer will work directly with Zuckerman Institute faculty and researchers to facilitate and pilot new methods of data sharing and analysis, with an emphasis on reproducible research. Working closely with the Zuckerman Institute Research Computing group, the data engineer will facilitate data analysis and sharing across ten (10) dynamic laboratories investigating the computational and circuit mechanisms underlying motor control.

Responsibilities include, but are not limited to the following:
Works in close collaboration with the Principal Investigator (PI) faculty and researchers.
Assists with standardization, documentation, and packaging of open source code across multiple laboratories, including releases for the broader community.
Works directly with researchers developing novel algorithms (by researching algorithms, helping to translate proof of concept algorithms into well engineered software with proper APIs, and helping researchers unit test their code to ensure proper functionality) and subsequently supports the use of software by end users.
Establishes pipelines for data sharing (e.g., of electrophysiological data from multielectrode arrays or 4D largescale calcium imaging microscopy data) across laboratories.
Organizes training courses for scientists who wish to extend their knowledge of programming and best practices in software engineering.
Performs related duties and responsibilities as assigned / requested.
Minimum Qualifications for Grade
Applicant MUST meet these minimum qualifications to be considered an applicant

A bachelor's degree or equivalent work experience, including a minimum of four (4) years of related experience is required.

Additional Position-Specific Minimum Qualifications
Applicant MUST meet these minimum qualifications to be considered an applicant

Strong software engineering skills, with proficiency in Matlab, Python, and Java as well as standard tools (e.g., git), and some familiarity with machine learning. The ability to confer and consult directly with senior faculty research staff, and students on technical topics Excellent organizational and time management skills to support research faculty and staff sound judgment with a collaborative style that fosters teamwork. Familiarity with at least one workflow/datasharing platform (e.g. osf.io) and expertise with low-level data representation an interest in teaching.

**Applicants must submit a cover letter and CV/resume to be considered**

Special Instructions

Preferred Qualifications

Significant research and/or industry experience, with demonstrated skills in algorithm development is preferred.

Essential Functions

Additional Essential Functions (Limit to 3950 characters.)

Special Indications

This position works with:
There are no special indications for this position

HIPAA Compliance training required

No

Participation in Medical Surveillance required

No

What type of posting? Is this a waiver request?

Standard Posting

Requisition Open Date

11-22-2017

Requisition Close Date

Open Until Filled

Quick Link

jobs.columbia.edu/applicants/Central?quickFind=166012"
Data Engineer,Kaplan,"We are seeking an outstanding and forward-thinking Data Engineer to help manage our ever-growing information needs and answer increasingly complex business questions. This person will work on collecting, storing, processing, and analyzing huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring them. This person will also be responsible for integrating these data sets with the existing architecture used across the company.
Because we work on the cutting edge of a lot of technologies, we need someone who is a creative problem solver, resourceful in getting things done, and productive working independently or collaboratively.
As a member of the growing data engineering and business analytics team, this person will be a key contributor to the successful deployment of strategic future state capabilities critical for our business.

Responsibilities
Gather, profile, analyze and process raw data at scale (including writing scripts, web scraping, calling APIs, write SQL queries, etc.)
Design and develop code to load and transform the source data from various formats (like relational, unstructured, semi-structured) into a form suitable for analysis
Identify reporting requirements and translate those into supporting data structures including dimensional (star-schema) marts, aggregate and normalized tables
Conduct ETL, SQL and DB performance tuning, troubleshooting and capacity estimation to meet SLAs and ensure highest data quality standards
Support business decisions with ad hoc analysis as needed
Technical Skills: (Background/Experience)
Bachelor's degree in related area (Computer Science, Information Systems, Engineering, Statistics) or an equivalent combination of education and experience
5+ years of successfully designing and implementing big data-centric applications, such as data lakes, data warehouses, operational data stores, analytical engines
Extremely proficient in at least one of the programming language, ideally in Java or python but JavaScript, Scala, R is acceptable too
Experience working with streaming data ingestion services like Amazon Kinesis, Firehose, Kafka, Flume, etc
Proficient in at least one of the commonly used data integration tool like Microsofts SSIS, Informatica, SnapLogic, PL/SQL, or any other open source tool
Powerful hands-on SQL based data extraction skills to manipulate data from common relational database systems (Oracle, Postgres, SQL Server), distributed systems (Amazon Redshift, Teradata), NoSQL databases (Mongo DB)
Good understanding of Lambda Architecture
Experience working on big data technologies like Hadoop v2.0, Spark, Amazon EMR, MapReduce, Cloudera or Hortonworks
Proficient in using one of the big data tools such as Pig, Hive, Spark SQL, Impala, Presto, Sqoop, Oozie, etc..
Experience with Business Intelligence and reporting tools (preferably Tableau, R, SSRS)
Understanding and experience in 3NF & dimensional (STAR schema) approaches for data warehouse environment
Understanding of various compression techniques and storage formats
Familiar with the workings of network, servers, AWS infrastructure & cloud services
Familiar in data mining, machine learning, natural language processing or Statistical analysis
Demonstrated experience translating business and technical requirements into data models
Soft Skills & Capabilities:
Ability to work independently on assignments with little oversight
High aptitude for logical thinking and problem solving techniques
Be proactive and passionate about delivering good quality product with efficient time management practices
Ability to sense risks and research issues before hand so as to analyze underlying problems before recommending possible solutions
Strong interpersonal skills (including written and verbal communication) to work effectively in a team environment and also part of a matrix organization
Work with a culturally diverse team from varying backgrounds
Must be flexible for changing business requirements and assignments with an ability to work on multiple projects simultaneously
Proven ability to mentor and develop others and help them grow their skills
Has a positive and creative attitude to unknown challenges
Passionate with an eagerness to learn & try out new open source and emerging technologies based on distributing and/or cloud based computing"
Data Engineer,Success Academy Charter Schools,"As the fastest growing, highest-performing charter school network in New York State, Success Academy has reimagined public education. Nationally recognized for achieving outstanding academic results for students from all backgrounds and zip codes, we have created an innovative K-12 school model that is preparing current and future generations of children with the subject mastery and skills to succeed in college and life. Since opening our first school in Harlem in 2006, we have swiftly grown to 46 high-performing schools, from Bed-Stuy to the Bronx, serving approximately 15,000 kids. We are scaling to 100 great schools serving 50,000 children across New York City, bringing transformational change here and across the country.

Our Data Management & Analytics team ensures we all work collaboratively and more efficiently to support our growing network of schools. By implementing cutting edge technology, built in-house and from 3rd party vendors, we are able to help the organization scale by streamlining processes and making sure everyone has access to mission-critical information 24/7. In effect, the DM&A team's goal is to help everyone at Success work better, more efficiently, and with access to all available sources of information. Were looking for a tactical, organized, and detail-oriented Data Engineer to help manage initiatives for our DM&A team.

Our Data Engineer will be responsible for implementing aspects of our enterprise data warehouse that have an immediate effect on our organization and scholars. Reporting directly to our Managing Director, s/he will be comfortable jumping in the deep end and learning new skills, and should be someone who...

- Can effectively construct complex SQL queries (mostly Postgres and MS SQL Server, but some MySQL as well);
- Possesses a high level of comfort using Python data libraries, particularly Pandas and SQLAlchemy;
- Possesses knowledge of data warehouse systems, structures, and approaches;
- Is experienced in using third-party APIs to collect data from various sources into a central database;
- Works comfortably in Linux environments with some bash scripting and light administration;
- Seeks a fast paced, collaborative environment with capacity to work with multiple teams to complete a variety of projects; and
- Has a strong interest in education reform and working for a mission driven organization.

Preferred (but not required) qualifications also include...

- Experience with Python web development frameworks (Flask or Pyramid);
- Experience with data analysis and visualization methods; and
- Experience creating PDFs, PowerPoints files, or other documents programmatically.

To join our team, upload a cover letter and resume that outlines your candidacy. Your cover letter should explain in detail your qualifications for the position. Resumes without cover letters will not be reviewed.

Success Academy Charter Schools is an equal opportunity employer and actively encourages applications from people of all backgrounds. Compensation is competitive and commensurate with experience. Success Academy offers a full benefits program and opportunities for professional growth."
Data Engineer,NT Concepts,"Overview

NT Concepts is seeking an Applications Architect to join our team supporting an Intelligence Community client in the Washington-Metro Area. The successful candidate will be responsible for maintaining and enhancing new geospatial software applications, and responding to customer requirements for platform operations.
Responsibilities

Duties and Responsibilities Include:
Acts as highest-level technical expert and customer-facing point of contact, addressing problems of cross-platform systems integration and compatibility
Supports analysts with RFI responses and daily operations with geospatial applications
Performs feasibility analysis on potential future projects to management
Provides technical support and training, including the creation of technical documents

Qualifications

Education and Experience Requirements:
Bachelors or Masters degree in Computer Science, Information Technology, GIS, or a related field
Active TS/SCI clearance
US Citizenship
A minimum of 10 years of relevant experience with software applications, geospatial analysis, and the Intelligence Community
Expert understanding of software design and architecture principles and techniques
Clear and effective communication skills (oral and written)
Ability to adapt quickly in a rapidly changing dynamic environment; prioritize and multi-task across numerous work streams
Familiarity with the following technologies is required: python, operating linux servers in an enterprise environment, mongoDB, jquery
Familiarity with the following technologies is desired: node.js, three.js, Open Layers, WebGL, Google Cloud Platform (GCP)

Founded in 1998, NT Concepts is a woman-owned technology solutions company focused on data analytics, software engineering, investigative services, and geospatial information systems. We provide a broad range of Federal Civilian, DoD, and Intelligence Community customers with solutions for enterprise, cloud-based, and mobile environments.

NT Concepts is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer (M/F/D/V), making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.

All resumes are held in confidence. No recruiters or agencies without a previously signed contract. No faxes please. Only candidates whose profiles closely match requirements will be contacted during this search.

NT Concepts participates in E-Verify.

If you need a reasonable accommodation for any part of the employment process, please send an e-mail to jobs@ntconcepts.com and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis."
Software Engineer (Data Services),Hewlett Packard Enterprise,"Job Description Summary Were looking for a Software Engineer to join the Data Services group at HPE/ Nimble Storage. This is a perfect position for someone who is passionate about using the latest techniques and best practices to build and improve systems software that is scalable and best-in-class. The purpose of your job: Design and develop software in the NimbleOS data path. Essential Functions : Work with technical leaders and architects to understand new product requirements.

Create functional specs and other design documentation. Design, code, test and debug components in Nimble Storages Data Services group. Implement new features in the CASL File System, Protocols and Performance. Work with other Data Services teams, QA, Customer Support and Platform teams to solve complex customer and release issues.

Provide guidance to management and participate in technical communication with customers. Requirements: BS in CS or EE required. MS degree preferred. Must have very strong system programming background with C/C++ for large enterprise class software.

Must have proficiency with data structures, algorithms and multi-threaded programming Must have in depth knowledge of OS internals. Must be capable of debugging issues in multi-threaded and clustered environments. Prior experience in one or more of the following areas is a huge plus: Data-path on large and complex modules. Distributed Systems, Clustering or HA Memory management, Virtualization or De-duplication Replication, QoS , Storage Protocols (iSCSI/SCSI, FC, NFS, CIFS).

Performance tuning and modeling Use of Performance analysis tools e.g. Oprofile, gprof. Experience in developing commercial software products, passion for quality and excellent software engineering practices. Ability to work collaboratively in small-medium size engineering teams is a plus.

Ability to write clear and articulate functional and design specifications Ability to work independently and manage multiple priorities in a fast-paced environment and take on a variety of roles as needed About Nimble Nimble Storage, recently acquired by HPE, is seeking talented software engineers for its Data Services team. As a member of the team, you will be responsible for driving innovation for the most dynamic company in the storage industry. In 2010, Nimble upended the data storage industry with its Cache Accelerated Sequential Layout (CASL) architecture. Since then, the company established itself as the leader in the fast-growing flash storage market, and was acquired by HPE in early 2017 to become a key part of its Storage Division.

Nimble broke away from the storage industry pack with innovations like its scale-out architecture, which allows its solutions to independently scale performance and capacity with minimum disruption; and InfoSight, its automated storage support system that uses Big Data to identify and resolve problems in real-time. Adaptive Flash, Nimbles latest contribution to storage innovation, is the most efficient flash platform in the industry and the companys all-flash shelf scales to 64 terabytes within a single system. Nimble has garnered an Impressive list of awards, including CRN Storage Product of the Year, Wall Street Journal Next Big Thing award, InfoWorld / Network World ""25 New IT Companies to Watch"" Life at Nimble Nimble was named one of the Bay Area Newsgroups Top Workplaces in 2013 and 2014, based on independent employee surveys. Nimble employees cited the companys collaborative and innovative culture as two key factors contributing to their overall job satisfaction.

Nimble employees strongly believe they have been given a rare opportunity to make a lasting impact on their company as well as the industry at large. In addition, Nimble offers competitive salaries and perks. The collaborative and innovative culture continues under the helm of HPE. HPE/Nimble is a fun place to work, offering a highly stimulating work environment.

In their leisure time, Nimble employees connect through Nimble Neighborhood, an online bulletin board for arranging hikes, games, fitness events, jam sessions, cricket matches and more."
"Data Engineer, Analytics",MediaMath,"MediaMaths strength is in numbers. Our technology analyzes 200 billion customer opportunities daily more volume than the top 10 stock exchanges in the world, combined.
Over 700 Mathletes in 16 global offices are trusted by two-thirds of the Fortune 500 and partner with thousands of marketers to ensure brands connect with right audiences, in the right place, in the right time.
We believe consumers want to have meaningful conversations with their favorite and yet-to-be-discovered brands across all digital touchpoints. Our omnichannel, integrated programmatic platform unites digital media and big data to maximize the return on every marketing dollar spent by making advertising relevant, personalized, measurable and controllable. From inventing the DSP category in 2007 to being named a DMP Forrester Challenger (our first year participating in the DMP Wave!) in 2017, we continue to deliver results for marketers more quickly and accurately than any other solution.

Technology is changing the way brands interact with consumers. MediaMath is powering that change. Come be a part of it!
MediaMaths Analytics Engineering team is currently seeking a Data Engineer with the knowledge, passion and capability to develop data pipelines to surface insights for internal and external clients. The Analytics team fulfils customers advanced analytics and reporting needs through custom reports and analyses, advanced statistical applications, predictive modelling and interactive web dashboards to help clients effectively manage campaigns and optimize performance. This role, on the Analytics Engineering team within the Analytics team, will build and optimize services to work with our petabyte sized datasets.
Responsibilities
Carry deep knowledge of relevant MediaMath systems (databases, APIs) and data workflow.
Design and develop tools to improve internal workflow and processes.
Benchmark and evaluate new software packages and infrastructure.
Field technical escalations and aid in technical training for our team of Data Analysts and Software Engineers.
Identify areas of improvement with existing scripts and existing database/ETL infrastructure.
Become proficient in optimization, debugging and building of production ready services using Hive and Spark.
Provide feedback for report delivery and infrastructure workflow improvement efforts.
This is not an exhaustive list of responsibilities. As part of our global team, you may be required to be work off-hours or be on-call on a rotating basis. Other duties may be assigned, as needed. MediaMath retains the right to change job duties at any time.
Qualifications
Bachelor Degree or higher, preferably with a concentration in a computational field such as Computer Science, Mathematics, Statistics, Physics, Engineering; Masters Degree preferred
Strong SQL, Python programming; experience of at least one big data technology, e.g. MapReduce, Hadoop, Hive, Spark, Pig, AWS/S3 a plus
Experience with AWS services (or similar) like EC2, RDS, S3, EMR and Redshift a must.
Strong verbal communication skills, extremely well-organized
Exposure to online advertising systems or ad tech industry a plus
MediaMath is privately held, employee owned, and headquartered in New York. Mathletes enjoy: Company equity. Performance Bonus. Comprehensive Insurance. Global Internal Mobility. Open Paid Time Off, Philanthropy and Holidays. 401(k) match. Paid Parental Leave. Cell Phone Reimbursement. Modern office space. Onsite Fitness & Wellness. MediaMath.org.

If there might be a match, you'll be scheduled for a first round interview; a 30-minute phone call with our recruiting team so we may get a better understanding of why you are interested in MediaMath and why you think it's a fit. We do our best to respond to everyone, however due to the volume of applications received, only those selected for interviews will be contacted. If you really think weve missed the mark, please follow up with globaltalentacquisition@mediamath.com and let us know why youre the perfect fit!"
Data Engineers,American Express,"American Express Travel Related Services seeks Data Engineers to w ork with product teams to understand business data requirements, identify data needs and data sources to create data architecture, and support changes and implementation. Document data requirements and data stories, and maintain data models to ensure seamless integration into existing data architectures. Create and maintain information about stored data and understand database requirements and translate those requirements into physical database design. Build and enhance database design and infrastructure that supports the business portfolio. Perform database design review, support database testing, and provide production environment support for database systems and processes. Design database features for ongoing sprints and monitor database requirements based on industry trends, new technologies, known defects, and issues. Qualifications
Position requires a Masters degree in Computer Science, Engineering, Information Systems, or a related STEM field, and 2 years of experience with database administration. Experience must include a minimum of: 1 year of experience with developing and supporting large OLTP or OLAP enterprise systems, relational and dimensional data modeling and metadata management, installing, and configuring DBMS software, and requirements analysis. Experience must also include a minimum of: 1 year of experience with applying Agile or Waterfall methodologies, business intelligence, data architecture definition, data model design tools, Erwin, ETL, IMS, Oracle databases, SQL, SQL Server, schema development, and TOAD.

JOB LOCATION: Phoenix, AZ.
ReqID: 18000293
Schedule (Full-Time/Part-Time): Full-time"
Data Engineer,WeddingWire,"Are you looking for a unique employee experience where you can apply your passion for business intelligence and data solutions to drive performance, growth and learnings? WeddingWires data team is seeking a data engineer with multidisciplinary experience utilizing technologies ranging from low level ruby, python and shell coding to enterprise ETL and Business Intelligence reporting tools such as Talend and Tableau. This key role will develop, implement and provide the data and information needed for WeddingWire to compete globally as the leading data-driven software product company in the events industry.

ABOUT THE ROLE:
Extract, transform and load data from global WeddingWire sites and other sources to our DataWarehouse and datamarts for the consumption of our end users
Design and develop solutions for integration between disparate systems which include cloud-based sources such as Salesforce and ExactTarget
Create dynamic dashboards and reports to help visualize trends and forecasts
Maintain current ETL processes and resolve daily ETL job failures as they arise
Resolve Ad-hoc data questions from the organization in a timely and accurate manner
Continuously improve on current ETL processes to ensure accuracy, timeliness and scalability as data volumes grow
Document build processes and front end reporting solutions
SUCCESSFUL DATA ENGINEERING CANDIDATES HAVE:
A minimum of 2+ years experience with SQL development, specifically with a strong programming experience and a demonstrated interest in reporting, statistical analysis, informatics, analytics and business intelligence
An understanding and hands-on experience with at least one of the following Databases: MYSQL, PostgreSQL, Redshift, MSSQL
An analytical mind with the ability to conduct research and make recommendations on BI requirements, products and services across all leadership levels
A Bachelors Degree in Computer Science, Information Systems, or related field; an advanced degree or professional licensing is a plus.
IT'S A BONUS IF:
Youre knowledgeable on database schema design and architecture
The fast pace excites you and you thrive in variety.
You keep your cool when things get a little hectic.
You demonstrate self-confidence, energy and enthusiasm.
You pay attention to details and take pride in thoughtful work.
You are self-motivated, manage your own time and use down time to be proactive.
You multi-task, but prioritize tasks by urgency and importance.
Youre comfortable collaborating with all levels of management.
You ask ""what if?"" and then like to use your resources to find the answers.
You have strong written and oral communication skills.
You understand and uphold confidentiality.
WHY WEDDINGWIRE?

Our Amazing Team! Our team is led by the industrys top leaders. We are a supportive team that wants every employee to succeed. WeddingWire employees are deeply proud to work here and are eager to onboard employees who want to promote the mission of our organization and achieve our business objectives.
Training & Development! WeddingWire provides each employee with company orientation during week one. From there, each team onboards and trains each employee specific to their role. We also provide continuous education and other learning opportunities for the broader team through guest speakers, Leadership Development and Managers Certification Programs and crash courses.
Award-winning Culture! We are proud to have been selected by numerous organizations as one of DC's Best Places to Work! We strongly believe in promoting a unique employee experience and that happy employees lead to happy customers: our competitive advantage! We make significant investments in cultivating our culture, which consistently ranks the highest attribute of working for WeddingWire on our internal and external employee surveys.
OUR PERKS:
Unlimited Vacation - Cutting-edge technologies - Casual attire - Commuting Allowances - Stipends for Professional Development for Every Employee - Give-Back Programs with Matched Donations - Full Menu of Health Benefits - Matched 401(k) Plan - Flexible work schedules - Monthly events - Catered breakfast every morning - Company-sponsored team outings - Monthly catered lunches - State of the Art Facility

ABOUT US:
WeddingWire, Inc. is the leading global online marketplace connecting consumers with events and creative professionals. Operating within a $200 billion industry, WeddingWire, Inc. hosts 10 million monthly unique users across its mobile and web platforms. Consumers are able to read over 3 million vendor reviews and search, compare and book from a database of over 400,000 businesses. Globally, it provides these businesses the technology they need to serve their clients through advertising, marketing, and business management tools such as websites, payment processing, invoicing and contracts. Founded in 2007, the WeddingWire portfolio of sites serves couples and businesses across 15 countries in North America, Latin America, Europe, and Asia, making it the worldwide leader in weddings with brands including Bodas.net, Casamentos.com.br, Matrimonio.com and more. The company employs more than 800 and maintains global headquarters in Chevy Chase, MD and international headquarters in Barcelona, Spain.

---

WeddingWire provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, or disability. In addition to federal law requirements, WeddingWire complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. WeddingWire expressly prohibits any form of workplace harassment based on race, color, religion, gender, sexual orientation, gender identity or expression, national origin, age, genetic information, disability, or veteran status."
Data Engineer,Slalom Consulting,"Data Engineer Consultant

As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure and Google Cloud using core data warehousing tools, Hadoop, Spark, Event Stream platforms, and other big data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.

Who are you?
Youre a smart, collaborative person who is passionate about technology and driven to get things done.
Youre not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
What technologies will you be using?

Everything. Its about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board.

Why do we work here?

Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in San Francisco leveraging the coolest technologies. At Slalom, we found our people.

What does our recruitment process look like?

Our process is highly personalized. Some candidates complete their process in one week, others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless how long or short the process may be for you, the most important thing is that you find your dream job.

Qualifications:
Bachelors degree in Computer Engineering, Computer Science or related discipline
5-7+ years relevant experience
Understand different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
4+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
2+ years of experience working on AWS, GCP or Azure
Experience working with relational databases
Strong analytical problem-solving ability
Great presentation skills
Great written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and cloud specific data pipeline tools such as Airflow, Glue and Dataflow"
Data Engineer,TopTal,"At Toptal, we measure everything and always rely on data to guide all of our initiatives, including both our long-term strategy and our day-to-day operations.

As a Data Engineer, your main goal is to be one step ahead of data scientists and analysts, and support them by providing infrastructure and tools they can use to deliver end-to-end solutions to business problems that can be developed rapidly and maintained easily. This is more than building and maintaining ETL pipelines. We need innovation, creativity and solutions that will have significant impact on our velocity. We, in turn, will give you autonomy and freedom to turn your ideas into reality.

This is a remote position that can be done from anywhere. However, we do things like rent out hotels in Africa or mansions in Thailand , and you will certainly be invited to come work with us.

Responsibilities:
Build scalable, highly performant infrastructure for delivering clear business insights from a variety of raw data sources.

Develop batch & real-time analytical solutions, prototypes, and proofs of concept for selected solutions.

Implement complex analytical projects with a focus on collecting, managing, analyzing, and visualizing data.

Build frameworks and tools to empower our data scientists and analysts.

Be in constant communication with team members and other relevant parties and convey results efficiently and clearly.

Requirements:
Working experience with Python , Pandas . Prior experience with Luigi is a plus.

Working experience with Scala and Spark is a big plus.

Familiarity with Google Cloud Platform (e.g. GCS and BigQuery) is a plus.

Working experience with Ruby and Rails is a plus.

Familiarity with the basic principles of distributed computing and data modeling .

Extensive experience with object-oriented design and coding and testing patterns, including experience with engineering software platforms and data infrastructures. Familiarity with functional programming concepts is a plus.

Outstanding communication and interpersonal skills.

Be excited about collaborating daily with your team and other groups while working via a distributed model.

Be eager to help your teammates, share your knowledge with them, and learn from them.

Be open to receiving constructive feedback.

You must be a world-class individual contributor to thrive at Toptal."
Data Engineer,Adobe,"The challenge
This is an exciting new opportunity for a data/software engineer with business acumen. You will build and maintain scalable, rock solid self-serve solutions for data analysts and support them by understanding the content and context of data as well as collaborating with them to determine the best way to extract, transform, load and access it. The role will be located in San Francisco Bay Area, but our team is distributed across the United States and Europe.
The ideal candidate will have a passion for simplifying and automating workflows, for making things better, for being efficient and helping others do more with less. To this person, containers don't just belong in ships, Mesos is not a misspelled soup name and Marathon doesn't mean 42.195 kilometers of toil.
What you will do
Work with data analysts to determine best schemas to translate available data sources into business domains
Build connectors to integrate with postgres-based RDBMSs
Write scripts to retrieve data from RDBMSs, logging systems (e.g. Splunk), APIs, text files and unexpected data sources
Work with external teams to negotiate data access
Work with own team to make workflows more efficient and automate them when possible
Work with the following stack: Git, Jenkins, Docker, Marathon, Mesos
Required skills
Passion for continuous betterment (i.e. better workflows, automation, efficiency)
Experience choosing database architectures (determining best schemas)
Experience collaborating through Git
Experience using Jenkins for continuous integration (CI/CD)
Experience configuring and launching containers
2 years of experience coding in python (or two scripting languages)
Preferred skills
Experience managing a Mesos cluster
Knowledge of statistics or experience with R, to better communicate with analysts
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
Data Engineer,etouch,"The ideal candidate:
Enjoys the challenge of building a system from the ground up
Has significant experience in Big Data environment (Cloudera, HortonWorks) with 7+ years of experience in managing and organizing terabytes of data
Possesses expertise in the following areas (will be thoroughly checked!):
Agile (Scrum, Kanban, Lean) and Test-Driven Development with Python
Advanced knowledge of computer algorithms to build and/ or leverage distributed systems
Proven track in building SOA-based systems
Languages: Python, SQL, Shell scripting. C with gcc compiler is a plus but not required
Big Data  Hadoop (HDFS, YARN) with Hive on Map-Reduce or Tez
Linux (RHEL) with experience in interfacing with APIs from Salesforce, SAP, Tableau
Building and leveraging CI/CD pipelines with use of such solutions as Jenkins
Willingness and ability to tackle problems outside the candidates areas of expertise
BS, MS, or PhD in Computer Science, Electrical Engineering, or a related field"
Data Engineer,MassMutual,"Since 1851, MassMutuals commitment has always been to help people protect their families, support their communities, and help one another. This is why we want to inspire people to Live Mutual. Were people helping people.

A career with us means you will work alongside exceptional people and be empowered to reach your professional and personal goals. Our employees are the foundation of what makes MassMutual a strong, stable and ethical business. We invite you to bring your bright, innovative ideas to MassMutual as we continue to help millions of Americans rely on each other.

Together, were stronger.
Description


The MassMutual Data Engineering Group in the Customer Experience Organization is seeking an outstanding individual to be a member of the enterprise data platform team. The successful candidate will work with data scientists, engineers, product managers, and enterprise stakeholders to help deliver innovative, data-driven systems aimed at transforming the insurance industry.

The successful applicant will be expected to quickly learn new cutting-edge technologies, technical concepts, and complex problem spaces. Current projects include:

1. The design and implementation of the enterprise data platform, a very large data processing and analytics system.
2. Advanced distributed analytics workflows on Hadoop/Spark
3. Using Python, Java and Scala/Spark to integrate, manage and analyze multi-terabytes of diverse financial and life insurance data
4. Building pipelines to clean, prepare, and enrich data and ultimately integrate into a consolidation layer presented to customers and enterprise stakeholders.
5. Working closely with a team of highly skilled data scientists to analyze and develop predictive data products that advance and transform MassMutuals businesses.

The applicant must possess a solid understanding of computer science fundamentals, including database fundamentals, distributed algorithms, data structures, and functional programing.

RESPONSIBILITIES

You will be expected to:
Work with a team to develop creative and scalable solutions to difficult technical problems
Be self-supportive in collaborating with your peers to effectively deliver robust solution for the business
Stay current on other trends and technologies in the field and be a force for disseminating that information throughout the organization
Work with data scientists to architect and build systems that use Machine Learning Data Analysis techniques
Develop reliable data pipelines that convert data into features that can be used to train machine learning models
Analyze, characterize and understand data sources, participate in design discussions and provide guidance about database technology best practices
Write tested, robust code that can be quickly moved into production
BASIC QUALIFICATIONS

3 years of related hands-on experience building large scale software and data systems
Experience designing and implementing creative solutions to difficult data processing problems.
Experience in building large scale database systems with emphasis on performance and near real-time data analytics
Experience in Data Modeling and Database Design
Strong communication and interpersonal skills
Strong technical leadership skills
Strong understanding and practice of agile software development processes
Strong experience with Python, Scala, Spark and Hadoop
Experience with distributed data processing and management systems
Authorized to work in the US without sponsorship now or in the future
PREFERRED QUALIFICATIONS

8 years of experience in software development and engineering role preferred
Experience in AWS cloud technologies and stack
Experience with polyglot databases, NoSQL, RDBMS and other new SQL
Experience with data mining techniques
Experience in the insurance and financial industries
Advanced degree in Computer Science
Ranked No. 77 in the annual FORTUNE  500 Ranking (FORTUNE  Magazine, June 2017) and recognized as a Worlds Most Ethical Company by Ethisphere, MassMutual is guided by a single purpose: We help people secure their future and protect the ones they love. As a company owned by our policyowners, we are defined by mutuality and our vision to put customers first. Its more than our company structure  its our way of life. We are a company of people protecting people. Our company exists because people are willing to share risk and resources, and rely on each other when it counts. At MassMutual, we Live Mutual.

CORE VALUES

Focus on the Customer: We understand our customers well and look for every opportunity to deliver an experience that is clear, easy, personal, human, empowering and trustworthy.

Act with Integrity: We deliver on our promises by being open, honest and humble and by adhering to the letter and spirit of applicable laws, rules, regulations and company policies.

Value People: We respect and learn from each others diverse backgrounds, experiences and ideas. We engage and develop people to their greatest potential.

Work Collaboratively: We work together to achieve results by actively listening, seeking, understanding and creating solutions as a unified team driving toward one company, one culture, one brand.

Achieve Results: We focus on winning by exceeding expectations and getting better  everyone, every day.

For more information, visit www.massmutual.com or find us on Facebook, Twitter, LinkedIn, YouTube, Google+, Instagram and Pinterest.

MassMutual is an Equal Employment Opportunity employer Minority/Female/Sexual Orientation/Gender Identity/Individual with Disability/Protected Veteran. We welcome all persons to apply."
Data Engineer,PatientPing,"We are actively hiring a full-time Data Engineer who wants to create something meaningful that will greatly impact peoples lives. PatientPing supports a number of customer facing web-based applications that facilitate care-coordination. This is a green-field opportunity to build out key components of our data pipeline and analytics infrastructure. The ideal candidate will be able to build and scale-out solutions for processing large data sets in near real-time.

What You Will Do:
Work closely with the Engineering Team to ensure the highest standards of design, implementation, performance, and security are met
Work closely with the Product team to ensure all customer needs are met
Build and maintain data systems supporting the application, internal users and analytics requirements

What You Need:
Expert knowledge on RDBMS & NoSQL (Elasticsearch, Redis, MySQL, PostgreSQL or Oracle)
3+ years of automation using coding/scripting languages (such as shell scripting, Perl, Python, C/C++, or Java)
3+ years experience in custom ETL design, implementation and maintenance.
Experience architecting database deployments on cloud platforms like AWS
Experience with Hadoop (or similar) Ecosystem (MapReduce, Yarn, HDFS, Hive, Spark, Presto, Pig, HBase, Parquet)
Experience with one of the messaging system (Kafka, SQS, Kinesis) and different data serialization (json, protobuf, avro)
BA/BS degree in Computer Science, Computer Engineering, or related field, or equivalent practical experience

What You Get:
Be on the ground floor of a rapidly growing company with many opportunities to expand the role and responsibilities
Have the autonomy to build something with a brilliant and enthusiastically supportive team
Learn best practices from world class investors and advisors
Become an expert on healthcare delivery system transformation
Receive cash and equity compensation with health, dental, and other benefits"
Data Engineer,Zipcar,"Zipcar is the worlds largest shared-mobility company. We are adding talent to our engineering team at our Boston office. We have a rich software stack with which we solve a wide range of challenges: embedded custom hardware and firmware, mobile-first user experience, analytics, scalable real-time scheduling and monitoring, etc.

The technology choices were standardizing on are:
Service-oriented architecture (SOA) using message queues for internal communication, with services mainly written for the JDK (Java/Groovy/Scala and other JVM languages) and Ruby

Modern continuous delivery, using technologies including Concourse, Docker, and BOSH

Native mobile applications on Android and iOS  mobile-first wherever it makes sense

Big Data technologies like Hadoop, Spark, Scala, Python, AWS, and other data science tools

We are looking for a data engineer to join our team implementing Zipcars new data infrastructure centered on Hadoop and related technologies that will serve reporting, analytics and data science needs. The team focusses on the full data pipeline: ingest and transformation of raw data from many data sources; further transformation for use by analysts and other data consumers; and data sharing with other systems at Zipcar, ABG, and partners. A software engineering background, preferably using Java, Python, or Scala, is essential. You will collaborate with other data engineers, operations, data analysts, data scientists, and business stakeholders throughout the organization.

If youre excited about Zipcars ideals of sustainable resource sharing and urban mobility, join our growing team of capable, passionate people. Were results-oriented, dedicated, and love contributing to an unparalleled experience for our community of Zipsters. Our work environment is as supportive, diverse, and as fun as our brand.

To learn more about Zipcar, what we do, and how we do it, visit zipcar.com. For further employment opportunities, visit http://www.zipcar.com/about/jobs

Responsibilities:
Develop software for data collection and ingestion, data aggregation, and feeds to other systems.

Build and support scalable and durable data structures for advanced data analytics using Big Data technologies (Hadoop, Spark, AWS, and other technologies).

Help administer the data platform to maintain continuous data availability.

Use the best software engineering tools and techniques. Review and test code, practice pair programming, help manage automated testing and deployments. Document data models.

Collaborate with internal partners from engineering, product, and business. Respectful and thoughtful communication skills are critical to our success.

Contribute to continually improving the processes and workflows that enable us to rapidly operationalize data to make our business more efficient and more responsive to our members.

Build a cooperative, skilled and knowledgeable team of engineers, scientists, analysts and data stewards. Share your skills and be willing to improve yours.

Requirements:
BS in Computer Science or equivalent experience

2-5 years software development experience, ideally using Java, Scala, or Python

1+ years recent experience with Hadoop or related technologies

Data warehousing and BI tools experience are desirable

Clear verbal and written communication

Collaborative approach with peers

As a member of the Zipcar Team you get a great benefits package including health and dental insurance, 401k, vacation time, paid holidays, personal and sick leave, along with a full complement of other insurance and support programs.

Because our work has a global impact, we enthusiastically support each employees commitment to creating a better world by offering:

Free Zipcar membership & discounted driving rates for you and your significant other

Paid volunteer time off

Bicycle commuter reimbursement

Paid parental leave for mothers & fathers

Training and development programs To accelerate career growth

Global footprint of job locations and opportunities

Flexible and open work arrangements

Fun, respectful, passionate, and collaborative environment

Educational Assistance Program

Company sponsored parties, outings, and events

Discounts on a variety of products and services through Zipcar partners

Zipcar is an EEO Employer

To all recruitment agencies: Zipcar does not accept agency resumes. Please do not forward resumes to our jobs alias, Zipcar employees or any other company location. Zipcar is not responsible for any fees related to unsolicited resumes"
Data Engineer,Quartet,"Quartet is a healthcare tech company transforming the way behavioral healthcare is delivered in this country, by making it more accessible and integrated into primary care. Millions of Americans are not getting the behavioral healthcare they need. Quartet partners with insurers and works with physicians to identify their patients in need of behavioral care, match them to the right care, and support them throughout their journey. We are a team of engineers, data scientists, and clinicians, finding solutions for a broken healthcare system. Venture-backed by the very best healthcare tech investors, including GV(formerly Google Ventures), Quartet is currently live in 6 U.S. markets and growing rapidly.

Mission
We're creating a platform to improve outcomes for patients with mental health and chronic medical conditions. We do this in many ways, from identifying patients at risk and routing them to the right specialists to creating a feedback loop between the primary care physicians and behavioral health specialists so they can collaborate on each patients treatment. These patients are often lost in our healthcare system and don't get the right care at the right time. We want to fix that.

Outcomes
We are looking for a Data Engineer that will work on ingesting, transforming, and organizing disparate sources of healthcare data from our partners and internal teams into canonical data models for application and data science insights. Our data platform team works to not only clean, standardize, and load external data sources into our platform but also generates key base features from that data to be utilized by downstream systems and internal customers  from the backend API to data science teams. This role will also work to vend tooling and automation to support our growing data needs and allow data integration team members working with customers to self service data prior to it being ingested and distributed to the rest of Quartet. You are passionate about data and will collaborate with and learn from other engineers, data scientists, and clinicians.

Responsibilities
Develop and maintain tools and systems to ingest, transform, and distribute numerous application streams as canonical data models and features.
Implement and vend data ingestion, transformation, and feature generation tools to data science, ML, and partner services teams.
Work with subject matter experts and product managers in architecting models to improve reliability and interpretability of data for analytical and business needs.
Understanding Quartet business objectives and design services that couple business logic with reusable components for future expansion.
Monitoring performance and advising any necessary infrastructure changes.
Leverage best practices in testing, continuous integration, and delivery.
Build a world class, scalable health data ingestion pipeline.
Drive down the time to insight by improving the accessibility and organization of data.
Competencies
Must Haves:
Experience with integration of data from multiple heterogeneous data sources
Experience developing in Python, Spark
Experience working in distributed systems
Experience with SQL and data modeling
Experience standing up and maintaining infrastructure at all levels of the stack
Interest in solving real-world healthcare problems
A great sense of humor and an ability to add to our dynamic culture
A desire to work on a highly-collaborative, mission-driven team
Nice to Haves:
Ideal candidate has 5+ years of engineering experience
Experience with HL7 and other healthcare data exchange standards a big plus
Experience with Scala
Experience with AWS
Experience with Apache Airflow
Quartet has been named among Modern Healthcare's 100 Best Places To Work in 2016 and 2017! Benefits and perks of working at Quartet include:
Medical, dental, vision and life insurance
Enhanced mental health benefits
Paid membership to One Medical
Pre-tax health, transit and dependent care flexible spending accounts
Fee-free 401(k) program
Unlimited vacation and sick leave, and competitive family leave policy
Amazing office with stocked kitchen, weekly yoga, meditation services and regular company gatherings
Working with some of the most talented and mission-driven minds in the industry!"
Data Engineer,OneMain,"We are looking for a Data Engineer to join our Digital Analytics team, contributing to the success of our web presence through OneMainFinancial.com and the OneMain mobile app.

The ideal candidate will be highly motivated, reliable, detailed, and driven to produce high quality work. This individual must thrive in a fast-paced environment.

Responsibilities:
Gather and process raw data at scale (including writing scripts, write SQL queries, etc.).

Process unstructured data into a form suitable for analysis

Perform in-depth analysis to identify key business risks and opportunities and make recommendations to improve overall profitability.

Design and help implement randomized experiments (A/B testing, split testing, bucket testing, etc.) to improve performance across KPIs.

Collaborate with business teams to build daily, weekly, and monthly reporting for KPIs.

Contribute insight to strategic and tactical decisions that impact competitive position, marketing mix, product design, analytics, and operations.

Use data mining and exploratory analysis to help identify opportunities using advanced analytics.

Work with the necessary teams to implement the full analytical solution.

Implement key recommendations by identifying cross-functional stakeholders, developing requirements, ensuring accurate implementation, monitoring impact, and optimizing results.

Always be curious about finding better ways of utilizing existing data as well as discovering new creative sources of meaningful data.

Skills or Experiences we want you to have :
BS/MS degree in an analytical or data driven discipline

2-4 years relevant experience, preferably in consumer lending

Knowledge of different data structures and their benefits and limitations

Strong knowledge of relational databases, creating complex SQL queries, and the ability to build and maintain database views

Experience with database technologies including, PostgreSQL, Redshift, Snowflake. and Greenplum

Experience with BI tools a plus, especially Looker

Experience with data driven programming (R, Python/Pandas, SAS, etc.)

Proven capability for critical thinking and problem solving

Ability to perform complex quantitative analysis and translate results into meaningful business insights

Have demonstrated verbal and written communication skills, and ability to interface with Business, Analytics, and IT organizations"
Data Engineer,Next College Student Athlete,"Data Engineer

ABOUT NEXT COLLEGE STUDENT ATHLETE (NCSA)

NCSA is the worlds largest and most successful collegiate athletic recruiting network. A wholly-owned subsidiary of Reigning Champs ( www.reigningchamps.com ), NCSAs team of more than 650 former collegiate and professional athletes leverages 17 years of exclusive data, proprietary matching algorithms, and personal relationships to connect tens of thousands of college-bound student-athletes to more than 35,000 college coaches nationwide across 31 sports every year. You can learn more about NCSA at www.ncsasports.org .

Data Engineer Job Details

NCSA (Next College Student Athlete) is growing our Software Development team and we are looking for someone to lead a portion of the team, define and refine the process used across the team, and provide clear reporting on the progress made.

Youll be working with a collaborative team of QA engineers, data scientists, analysts, developers, and product managers, so youll need to be able to articulate your decisions and take feedback into the next iteration. Want to work with a great team to make students lives better? Great!

We need a top-notch Data Engineer to help vault forward our growing data team. You will be integral in improving the structure of our data, expanding our data sources, maintaining the integrity of our data systems and taking our data warehousing to the next level. We have a lot of interesting data and we want you to help us get the most out of it.

Responsibilities
Develop systems and processes to collect, transform, store, maintain and audit structured and unstructured data sources
Define and implement standards for data hygiene to ensure accuracy, immediacy and integrity
Build data pipelines to support organizational reporting needs and customer product requirements
Coordinate data accumulation from internal and external sources, including paid sources and scraping projects
Own the ETL workflow including reliable error and exception handling, with a robust rollback framework.
Aggregate diverse and numerous data sources and create complex SQL queries
Analyze, maintain and improve database servers and systems (MS-SQL, Postgres, MySQL), both on-prem and in the cloud
Provide assistance in query optimization and data structure definition

Qualifications
Bachelors degree in Computer Science or a related Engineering field (or relevant experience)
3+ years of experience in a Data Engineering or DBA role
MS-SQL and Postgres experience, MySQL a plus
AWS Redshift or similar cloud warehouse experience
Solid understanding of relational and/or columnar databases
Excellent oral and written communication skills
Experience with a scripting or programming language (R, Ruby, Python, Java, etc)

Preferred Qualifications
Un-afraid to manage up or sideways
Familiarity with standard source control methods
A team-player, always looking to make things better and easier for a co-worker

Benefits
Comprehensive Medical, Dental, & Life Insurance
Pre-Tax Flex Spending Account
401(k) Match Up to 3%
Parental Leave
Wellness Reimbursement Program
Online Health Assessments
EAP

National Collegiate Scouting Association LLC is committed to providing equal employment opportunities to all employees and applicants without regard to race, religion, color, sex, national origin, citizenship status, uniform service member status, age, disability, sexual and gender orientation, genetic information or any other protected status in accordance with all applicable federal, state, and local laws."
Data Engineer,INTL FCStone,"This position requires a hands on approach from the ground up and will require a strong technical background in order to liaison between ground staff and core IT functions.

Primary Accountabilities/Responsibilities:
Design, implement and support Operational and related data systems and processes.
Report generation, ETL or Business Intelligence is the foundation but a long term focus on fast queries and improve the way data is gathered and disseminated. You will learn different business processes and workflows to determine the best way to organize and segment data.
Scrutinize and guide the technical side to ensure technical specifications for the business user are correctly met and are able to scale beyond the current business needs.
Work in a team of complementary skills sets to add efficiencies in processes and disseminate business intelligence.
Maintains prompt and regular attendance.
This list of duties and responsibilities is not intended to be all-inclusive and can be expanded to include other duties or responsibilities that management deems necessary.
Job Requirements:
Master Degree in Computer Science, Computer Engineering, Software Engineering, Electrical Engineering, Math, Physics or related major preferred
Working knowledge of the following languages / Technologies:
Node.js
C++/C#/.NET
JSON/XML/FIX
JAVA
ELASTIC SEARCH/ELK Stack
InfluxDB
SQL
Stream Processing
Distributed Systems Design
Strong interpersonal skills, able to comfortably and pleasantly deal with a variety of people.
Excellent organizational and planning skills.
Ability to effectively learn and acquire new knowledge and skills.
Ability to share knowledge and work in a strong team oriented environment.
Strong work ethic and emphasis on attention to details.
Proficient in Word, Excel, PowerPoint, and e-mail
Must be authorized to work in the US for any employer
Physical requirements/Working conditions:
Climate controlled office environment
Minimal physical requirements other than occasional light lifting of boxed materials
Dynamic, time-sensitive environment

We encourage applicants of all ages and experience, as we do not discriminate on the basis of the applicant's age."
Data Engineer,"eShares, Inc.","Carta - We are building the central registry of asset ownership.

Our team believes that the way in which assets are created, issued and transferred is inherently flawed. We live in a world where individuals can execute public market trades in a fraction of a second, but startups pay tens of thousands of dollars (and spend up to a month) to close a seed round. Carta is on a mission to change this by providing companies of all stages the best software to manage their ownership.

We are looking for extraordinary individuals to work alongside our team with experience from top technology companies. You will be building Carta from the ground up and helping build the world's next great financial infrastructure company.

Carta is backed by many of the best investors in the world, including Social Capital (Slack, Intercom, Box), Union Square Ventures (Twitter, Twilio & Coinbase), Menlo Ventures (Uber & Warby Parker) and Spark Capital (Oculus VR, Slack and Cruise Automation).

Carta is looking for an experienced data engineer to strengthen the architecture of our data processing infrastructure. The successful candidate should have extensive experience with ETL pipelines and stream processing architectures, particularly implementing them in a fast-paced startup environment. At Carta, work ownership is crucial, meaning that data engineers are not only responsible for building high quality data infrastructure but also maintaining it over time.

In this new phase of Carta, data engineering will primarily build and maintain a Data Lake architecture, an architecture that will provide high flexibility for different stakeholders to access data via dashboards, APIs, and data science tools for complex analyses. This architecture will also be responsible for warehousing the business metrics that drive the work of our multiple business units.

The basic requirements of a successful candidate are the following:
At least three years of experience with data engineering projects
At least one year of experience developing ETL pipelines in a startup environment
At least one year of experience building and maintaining stream processing architectures
Experience managing and maintaining AWS services and environments
A strong background in data security
A strong foundation of Python and Spark
Ability to write production-ready SQL queries and process
Core Linux admin skills
Plus:
NoSQL knowledge
Data Warehouse modelling"
Data Engineer,Red Ventures,"The mission of our data team at Red Ventures to make data easy-to-use for everyone. We are hiring brilliant Data Engineers who are excited to overcome challenges, work with the latest AWS technologies, and continue making our data better.

Our Data Engineer will collaborate with the data team in all aspects of database management, pipeline building and data warehouse design. This role requires experience in engineering, ETL development, data transformation, data modeling, and BI dashboard development. If you want to be a part of a dynamic team solving business problems using data, this is the role for you.

What we're looking for:
BS in Computer Science/similar field or equivalent work experience
Experience with Scala and Spark
Familiar with Microsoft SQL Server 2012 and/or above
Microsoft BI Stack experience
SQL SSIS and SSRS
Experience with T-SQL
We'd be ecstatic if you have any of the following:
Knowledge of Git, BIML, C#
Experience with business intelligence visualization software (such as MicroStrategy, Tableau, Jaspersoft, Spotfire, or Looker)
Experience with Big Data Technologies like Hadoop or Hive
About Red Ventures:
Red Ventures is a leading digital consumer choice platform based in Charlotte, North Carolina. Through deeply integrated brand partnerships and consumer-facing assets, Red Ventures connects online customers with products and services across high-growth industries including home services, financial services, and healthcare. Founded in 2000, Red Ventures has more than 2,700 employees in offices across the Carolinas, Seattle, Washington, and Sao Paulo, Brazil."
DATA ENGINEER,Mount Sinai Health System,"The Mount Sinai Health System

Do you have what it takes to wear the badge?

The Mount Sinai Health System's commitment to
excellence extends beyond delivering world-class health care. The System's
ongoing success is dependent upon our highly motivated, nonclinical
professionals working to improve business operations. Our leadership team is
driven to provide exceptional service by cultivating a workforce that is
dedicated to upholding Mount Sinai's mission of delivering innovative,
breakthrough medicine with compassion and integrity.

Are you ready to discover the world of limitless
possibilities that comes with wearing the badge? Explore more about this
opportunity and how you can help us write a new chapter in our story of
unrivaled patient care!

What You'll Do:
The Data Engineer II focuses
on data collection, movement, storage, transformation processing, and storage
of Big Data. This individual works
with both current ETL/Data Warehousing and future Big Data/Streaming/Pipeline
architectures. The focus is on choosing
optimal solutions to use for these purposes, then implementing, maintaining,
and monitoring them, always being mindful of the overarching goal of
accelerating translational research and improving clinical care.

Facilitates
data collection from a variety of sources, getting it in the right formats,
assuring that it adheres to data quality standards, and assuring that
downstream users can get that data quickly and with a common standard
interface.
Ensures
that data streams/pipelines are scalable, repeatable, and secure, and can serve
multiple users within the Institute.
Develops
as a core member of an Agile team, using Agile tools and methodology. Work
closely with other team members including Application Developers, Database
Developers, and Data Scientists.
Responsible
for creating the infrastructure that provides insight from raw data and handles
diverse sources of data seamlessly.
Enables
big data and batch/real-time analytical solutions that leverage emerging
technologies.
Additional
responsibilities include developing prototypes and proof of concepts for the
selected solutions, and implementing complex big data projects with a focus on
collecting, parsing, and managing large sets of data using multiple platforms
to allow for Research and Data Science initiatives.
Translates
business requirements into modern data pipeline solutions. Create centralized
documents and diagrams of all solutions.
Creates
a data catalog store of all metadata.
Designs
and implements monitoring, backup, and disaster recovery of data systems.
Approaches
all relationships with a world-class customer service approach. Maintains a customer-focused approach with
users to provide solutions that are science/research-driven.
Responsible
for the integrity and security of data in all forms of storage throughout the
Data Architecture.
Works
with other IT professionals through Mount Sinai effectively. Comply with the
Institutional Review Board and HIPAA to follow all applicable policies and
procedures.
Assists
in the development of standards and procedures affecting data management,
design and maintenance. Documents all standards and procedures.
Provides
presentations and training to other team members in the above.
Possesses
an extremely flexible attitude. Willing to work with multiple types of
technologies and languages with an open mind and without technology bias.
Continuous interest in updating skill sets and knowledge of trends in the Big
Data Technology space.
Other
duties as assigned.

What You'll Bring:
Bachelor's
degree in Computer Science or a related discipline; Advanced degree preferred

4+ years relevant professional development
experience, preferably in a LINUX environment.

Strong SQL and NoSQL Database Knowledge:
Oracle, PostgreSQL/MYSQL, and Mongo DB (or similar).
Proficiency with at least 2 programming
languages among Scala/Python/ Java. Must be flexible and fast to pick up new
languages.
Proficiency in Restful service development,
preferably with Node JS, Django and PHP.
Experience with micro-services and SOA.

Strong SQL and NoSQL Database Knowledge:
Oracle, PostgreSQL/MYSQL, and Mongo DB (or similar).
Proficiency on installation and configuration
of big data software and technology
Knowledge of Hadoop, Spark, Kafka and other big
data technology stacks and streaming tools.
Familiarity with and the ability to leverage a
wide variety of open source technologies and tools.
Working knowledge of cloud architecture and
implementation on Azure or AWS, is a big plus. Experience with server-less
computing, creating VMs, cloud security, and other cloud services is also a big
plus.
Experience working in an Agile methodology.
Experience working with JIRA is a plus.

Who We Are:
Over 35,000
employees strong, the mission of the Mount Sinai Health System is to provide
compassionate patient care with seamless coordination and to advance medicine
through unrivaled education, research, and outreach in the many diverse
communities we serve.

Formed in
September 2013, The Mount Sinai Health System combines the excellence of the
Icahn School of Medicine at Mount Sinai with seven premier hospital campuses,
including Mount Sinai Beth Israel, Mount Sinai Beth Israel Brooklyn, The Mount
Sinai Hospital, Mount Sinai Queens, Mount Sinai Roosevelt, Mount Sinai St.
Luke's, and New York Eye and Ear Infirmary of Mount Sinai.

The Mount
Sinai Health System is committed to the tenets of diversity and workforce that
are strengthened by the inclusion of and respect for our differences. We offer
our employees a highly competitive compensation and benefits package, a 403(b)
savings plan, and much more.

The Mount
Sinai Health System is an equal opportunity employer. We promote recognition
and respect for individual and cultural differences, and we work to make our
employees feel valued and appreciated, whatever their race, gender, background,
or sexual orientation.

EOE
Minorities/Women/Disabled/Veterans"
Data Engineer,Epic Games,"For over 25 years, Epic Games has been making award winning games and game engine technology that empowers others to make visually stunning games and 3D content that brings environments to life like never before. Epics award-winning Unreal Engine technology not only provides game developers the ability to build high-fidelity, interactive experiences for PC, console, mobile, and VR, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. As we continue to build our Engine technology and develop remarkable games, we strive to build teams of world-class talent.

Our Analytics Team is looking for a Data Engineer ready to cut their teeth and apply their scholastic knowledge in a real time setting.

The person in this role will be responsible for the following:
Assisting in creating SaaS (Software as a Service) applications in near real-time framework
Working with the team to help architect, design and implement a Spark Ecosystem for data transformation
Performing performance tuning tasks on large compute clusters
Pulling and analyzing data from both batch and near-real-time sources
Using software development methodology to accurately perform unit testing, performance tuning and integration testing
The ideal candidate will have a mix of the qualifications below:
Knowledge of one or more programming languages (Java, Scala or Python)
Expertise using SQL
Understanding of Apache Spark
Experience working with complex data types (JSON, XML)
Exposure to Hadoop a plus
Ability to be flexible working in a dynamic organization with minimal documentation
Comfort with software development methodology around unit testing, performance tuning, integration testing, etc.

This is going to be Epic!"
Data Engineer,Nordstrom,"Data Engineer - 305696

Come help Nordstrom's Cloud Data Pipeline team modernize data capture and products through clickstream analytics! https://github.com/atotech

We are data and application engineers, serving and empowering all Nordstrom teams and functions with high-quality data and data services and products.

We instrument Nordstrom sites and apps to measure customer experiences, ingest and process that data, and build amazing applications and real-time feeds on it. We are a small but highly agile and productive team of full-stack engineers. We value work/life balance but love what we do and there is no clock for us.

You are a data geek and puzzle solver, agile in node.js, Python, SQL, Docker, Kubernetes, APIs, AWS, Google Cloud, and more. You are left-brained, right-brained, or both. You are looking for a two-pizza team that has fun but is serious about building cool stuff, getting it done, and measuring data upon data to document its impact on the business.

The Data Engineer is experienced in the clickstream analytics and data engineering practice areas, leading by example, responsible for instrumenting optimization, personalization, and clickstream analytics data in a variety of languages; working with internal partners to provide accurate data capture and feedback; working with external partners to support complex marketing programs and data feeds; improving client performance of all related code; monitoring and improving data processing applications in Scala, Hadoop, Spark, and other Big Data technologies; developing and maintaining data warehouses, particularly Redshift; developing and maintaining BI applications, dashboards, and tools in a variety of languages, settings, and workflows; and guiding junior engineers to be subject matter experts in the analytics engineering space.

Cloud Data Pipeline supports merchandising, marketing, finance, technology, web and mobile apps, product management, customer experience, and data lab teams in capturing, processing, and analyzing data through a variety of third-party and proprietary platforms.

Experience with business intelligence, third-party analytics solutions (IBM Coremetrics, Google Analytics, Omniture), and/or digital marketing is helpful; experience with Snowplow Analytics is a huge plus.

This position requires a self-starter with the ability to prioritize multiple tasks in a deadline-driven environment. Every day is different. Never boring. Continuous planning and delivery, and the range of services we cover, requires a fast, curious, and optimistic learner. Bring your track shoes, because you'll hit the ground running.

Responsibilities:
Develop clean and well-structured JavaScript for front-end data capture with an eye for compliant, cross-browser, cross-device, and standards-based code.
Influence development and deployment patterns and best practices through code contributions and reviews
Develop real-time data feeds and services leveraging AWS Kinesis, Lambda, nodeJS, etc. to enhance useful analytic opportunities and influence customer content and experiences.
Develop and monitor batch Hadoop data and query processes and apply Spark, Spark Streaming, and other evolving tools to improve toward central unified real-time logging and microbatching.
Research and monitor data quality and issues via structured and unstructured query languages (SQL, ElasticSearch/Kibana, etc.).
Influence and improve data quality by developing and instrumenting application monitoring, health checks, health metadata, and self-healing processes to ensure high reliability and uptime.
Automate all of the above where possible to further improve code, application, and data quality
Grow to be a technical subject matter expert for proprietary optimization and analytics efforts.
Partner with optimization analysts, system administrators, and project managers to design, build, deploy, and capture effective metrics.
Support fellow engineers, business and technology partners, and project stakeholders, contributing to both internal and external open source projects and standards.
Prioritize daily workflow and demands on quality, time and resources.
Maintain current knowledge of industry trends and developments.
Help your teammates (and yourself!) grow by fostering open, positive discussion and experimentation with developing projects, frameworks, tools, systems, and standards. (Participation in local meetups, conferences, and technology groups is highly encouraged.)
Meet all requested development objectives and deadlines as assigned by the engineering manager.
Participate in agile and continuous planning ceremonies and provide input on stories, requirements, and acceptance criteria as needed.

Qualifications:
4-6 years professional experience in JavaScript
4-6 years professional experience with data-driven commercial websites
2-4 years experience with another server scripting language (node, Python, Java, Ruby)
1-2 years experience with cloud infrastructure and ops desired (AWS preferred)
Advanced Git
Demonstrated experience with test and build automation tools (PhantomJS, Casper, Selenium, Jenkins, Luigi)
Experience with Test- and Behavior-Driven Development preferred (Mocha, Chai, Cucumber, etc.)
Experience with bug tracking and workflow applications/tools (Atlassian)
Experience working in agile frameworks and processes (Kanban)
Advanced problem-solving and troubleshooting skills
Sound organizational, negotiation and decision-making skills
Superior verbal and written communication skills
Ability to work independently or as a part of an interdisciplinary team
Demonstrated ability and drive to learn new technologies, tools, and processes
Desire and ability to train and share knowledge with team members and other technical teams
A bachelors degree or better in Computer Science, Management Information Systems or equivalent experience
Since 1901, Nordstrom has offered a wide variety of quality apparel, shoes and accessories for men, women and children at our stores across the country. We're proud to be named to Fortune TM magazine's list of '100 Best Companies to Work For.' We believe this recognition comes from our desire to empower our employees to set their sights high and deliver exceptional service to customers.

As a Nordstrom employee, you can feel confident that your health and well-being are among our highest priorities. We offer a comprehensive, flexible employee benefits package that includes medical/vision and dental coverage, a generous merchandise discount, an employer-matched 401(K) savings and profit sharing plan and much more.

We are an equal opportunity employer committed to providing a diverse environment.

The above information has been designed to indicate the general nature and level of work performed by employees within this classification. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. No visa sponsorship is available for this position.

#LI-TD1

Job : Technology
Date Posted : 11/06/2017, 6:46:52 PM
Location : United States-Washington-Seattle
30+ days ago - Save Job
- Original Job
View or apply to job
Other Jobs You May Like
Data Engineer

Amazon.com
-
Seattle, WA

2 days ago


Hadoop Data Engineer

Horizon Consulting Inc
-
Seattle, WA

2 days ago

Easily apply

Data Engineer

UnitdForce
-
Seattle, WA

3 days ago

Easily apply

Data Engineer

ViaSat
-
Seattle, WA

25 days ago

Data Engineer jobs in Seattle, WA
Jobs at Nordstrom in Seattle, WA
Data Engineer salaries in Seattle, WA"
Data Engineer,Redfin,"Redfin is redefining real estate in the consumer's favor by combining our own technology and agents to create a service that's better, faster, and costs less. As a Redfinnian, you'll make a difference in one of life's most important events-buying or selling a home. Our hiring standards are high, yet our culture is humble. No matter what role you have, youll be both challenged and inspired every day. We've got all the perks, but if what you value most is doing great work in a creative, collaborative, and disciplined environment, join us.

As a Data Engineer for the Data Engineering Team, your job is to integrate, sanitize, and productize our massive store of market and user data to turn it into a competitive weapon. You will have ownership of Redfins Data Warehouse platform, overall architecture, data integration and operational excellence.

The Role
Design data warehouse solutions using dimensional methodologies to support ETL processes and data analytics applications
Develop, implement and tune ETL processes
Write and tune SQL including database queries, ddl and dml
Own features that you develop end to end. Work with end users on requirements gathering, develop and test your code, implement new processes in production, then maintain and support them over time
Work with Analytics and Digital Marketing teams to provide them the data they need to make efficient decisions
Assist with adhoc report generation and data analysis for customers
Be part of monthly on call rotation

Desired Skills & Experience
3-5 years experience in database technologies (Postgres, MySQL ,SQL Server, Oracle, RedShift etc.)
Minimum 3 years of experience in Data Warehousing
Working knowledge of dimensional modeling techniques
Working knowledge of data quality approaches and techniques
Experience with AWS tools (S3/Redshift/DynamoDB/IAM) is a plusExperience working with a standard ETL tool (i.e.,Informatica, SSIS, Talend, Pentaho, etc.)
An entrepreneurial spirit, a drive to ship quickly, and familiarity with agile software development practices
The ability to deal with ambiguity, communicate well with partner teams - both technical and non technical, and a strong empathy for the customer experience
The ability to work within an Agile/Scrum development process

What We Offer
Competitive compensation packages with a salary, bonuses, and stock grants
Generous benefits, including three weeks of paid vacation, medical, dental, and vision insurance, and fully paid family leave
Strong startup culture with catered lunches, casual dress code, happy hours, and more

Redfin is an equal opportunity employer."
Data Engineer,Object Partners,"Why consider OPI, and why do people dig working here?
Variety of consulting; new technologies, projects, and people on a regular basis.
Stability; weve been around since 1996 and have a diverse mix of clients and technologies to keep us busy, very busy. And we keep a bench. If youre not on a project, youre writing software for our internal business functions or youre learning new technologies. Its in our benefit to make our consultants as marketable as possible. Thats good for your career.
No politics or management; we dont get in the way. Why sit in meetings all day when you can code and be productive?
Awesome benefits; free healthcare for your entire family (yes, free), 24 days of PTO + 10 days of sick time, quarterly profit sharing bonuses, you get paid OT, company trips (to Mexico), 3 company lake cabins/homes, various quarterly company events, new Macbook Pros, free beer/soda, chips, candy, and so much more.
You work with the best. Do an Object Partners search on LinkedIn and see the types of talent we hire. You truly get to work with intelligent, passionate engineers that share the same goal of building great software the right way.
Low company overhead. It all means more money back into our consultants pockets (profit sharing) or company trips and events to share in the financial success.

Qualifications
Architecture and design of highly available/scalable distributed systems.
Design and development of streaming data platforms using Kafka Streams, Spark, Flink, Storm, Beam or Cloud Dataflow.
Experience with functional/event-driven programming.
NoSQL technologies such as ElasticSearch or Cassandra.
Messaging technologies such as RabbitMQ, Kafka or Kinesis.
SQL on Hadoop technologies such as Hive, Impala or Presto.
Cloud services such as AWS, Azure, GCP or OpenStack.
JVM languages such as Java, Groovy, Kotlin or Scala.
Service frameworks such as Spring Boot, Ratpack, Vert.x, or Play.
Knowledge of data analytics, visualization and governance.
Knowledge of operating big data production solutions at scale.
Knowledge of CI/CD pipelines and DevOps culture.
Passion for software development. Someone that loves what they do, that arent just in it for a paycheck. Do you have dev projects going on at home at all times?
New projects, new teams, new technologies means having to adjust and learn all the time. You might get thrown on a DevOps project as well, so having the ability to jump in and get your feet wet without hesitation is important. If you dont want to grow and learn, were not a fit.
Positive, can-do attitude. We can teach the technology, but we cant teach how to approach client challenges with a positive, helpful demeanor. The best consultants arent the most technical (although that sure helps), its the consultant that will do whatever it takes to see a client be successful, no matter what they throw at you."
Data Engineer,Western Digital,"We are looking for a savvy Data Engineer to be part of a data science and cross-functional team that typically include data scientists, engineers, software engineers, product managers, and end users on projects that involve large, complex and dynamic data sets. These project are aiming at building blocks toward Smarter Factory.

As a Data Engineer, you will need to understand production data and associated manufacturing processes; design and develop procedures and tools in a scalable fashion that customize and optimize the process of assembling, manipulating, and managing large and dynamic datasets to support large data based projects and other analytics needs.

Responsibilities:
Leverage existing big data platform and data pipeline tools to develop optimal automated processes to assemble and process large data sets from different sources and ensure optimal and consistent delivery of analysis-ready data at scale.
Work alongside data scientists, product engineers and software engineers to translate off-line analysis to viable product implementable in production. Strong expectation of playing as part of a team
Identify data processing process improvement opportunities and design customized tools to turn the opportunities to applications.
Identify data issues, evaluate their impacts on analytic projects, and find/implement solutions
Execute complex data engineering projects
#LI-BJ1
MS in Computer Sciences, Engineering, other quantitative fields or equivalent plus a minimum of 5 years relevant experience
Proficient in object-oriented scripting languages. 3+ year experience in Java or Python programming.
Expert in SQL and relational databases. Hands-on experience in developing database applications.
3+ year experience on integrating, manipulating, processing, and extracting value from large disconnected datasets.
Strong grasp of data structures
Working knowledge with big data tools (Hadoop, Spark, Thrift API, HiveQL) and familiar with binary encoded files (AVRO, ORC, Parquet, etc.)
Experience supporting and working with cross-functional teams in a dynamic environment

Desired Skills:
Knowledge in HDD production and testing engineering or failure analysis
Experience in R or SAS
In-depth understanding to multivariate analysis, regression and statistical inference.
General understanding of machine learning/deep learning methods and experience in building ML models.
Experience with AWS cloud services: EC2, EMR, RDS, Redshift"
Senior Data Engineer,Bosch Group,"Job Description

Design and implement fault-tolerant data pipelines to integrate large amounts of data from many diverse storage systems
Promote a culture of self-serve data analytics by minimizing technical barriers to data access and understanding.
Execute complex data engineering projects that have a significant impact on Bosch global business.
Share knowledge by clearly articulating results and ideas to customers, managers, and key decision makers.
Stay current with the latest research and technology and communicate your knowledge throughout the enterprise
Take responsibility for preparing data for analysis and provide critical feedback on issues of data integrity
Up to 10% travel may be required. Qualifications

MS in Computer Science or other engineering discipline
3+ years of in-depth knowledge and working experience with distributed systems
3+ programming skills in Scala or Java
Strong understanding in tuning and performance optimization of Apache Spark jobs
Experience with integration of data from multiple data sources
Experience with various messaging systems, such as Kafka or RabbitMQ
Ability to manage and solve ongoing issues with a Spark/Hadoop cluster

Desired:
Familiarity with distributed machine learning frameworks like Spark MLlib
General understanding of machine learning / deep learning methods

Additional Information

BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives
FIRST Robotics (For Inspiration and Recognition of Science and Technology)
AWIM (A World In Motion)
By choice, we are committed to a diverse workforce  EOE/Protected Veteran/Disabled."
Data Engineer,Viant,"Viants culture is a unique combination of strong leadership, ambition, collaboration, and fun, with incredible growth opportunities for our employees! We offer a fast-paced work environment, with a strong focus on employee engagement, recognition, and development. With a commitment to innovation from the top down, Viant continues to remain ahead of the curve in the rapidly changing ad tech marketplace. This creates a very exciting workplace that allows our employees to continuously expand their knowledge in all facets of digital marketing, while becoming industry thought leaders.

Viants Engineering Department seeks a Data Engineer to join our Irvine, CA office. The Data Engineer will be involved in the full product lifecycle, contributing to Viants core product: The Identity Management Platform. Our Engineering team designs and implements big data systems, working with 50 terabytes of data and real-time map reduce to match against billions of transactions in real-time. For the right candidate, this is a great opportunity to work with distributed systems in a dynamic, fast-paced environment.

Responsibilities:
Writing/Analyzing/Optimizing complex SQL queries.
Writing ETL processes using various tools and programming languages.
Designing, developing, testing, debugging and deploying applications and reports using various technologies.
Able to easily switch between different technologies.
Solving complex performance problems, architectural challenges and production issues.
Working with product owners, QA engineers and fellow engineers.
Ad-hoc duties as needed.
Qualifications:
Minimum B.S in computer science, mathematics or related field.
Experience and understanding of at least one of main cloud technologies. (AWS/Google/Azure)
Experience with OOP and related languages.
Proficient with at least one of these core programming languages: Java/Python/Go
Knowledge of scripting languages: Javascript/Bash/Shell
Well versed with Unix/Linux environments.
Experience with web services.
Experience with MySQL and Cloud database technologies.
Ability to troubleshoot queries, stored procedures, functions, packages and triggers.
Excellent problem solving, attention to details, critical thinking and good communication skills.
Ability to learn newer technologies in a short period of time.
A team player.
Benefits:
Competitive Salary and Bonuses
Paid benefits for the employees (Medical, Dental, Vision, LTD, Life insurance/AD&D)
Paid parental leave
401k
Summer Work from Anywhere Fridays
Health and Wellness programs
Employee discounts  e.g. gym memberships, wireless plans, entertainment tickets
Fully stocked kitchen
Casual Office Atmosphere
Commuter Benefits Program
Ongoing Education & Training
Company Sponsored Events & Team Building Experiences
About Viant:
Founded in 1999, Viant Technology LLC is a leading global people-based advertising technology company, enabling marketers to better plan, execute, and measure their digital media investments through our cloud-based platform. Built on a foundation of people instead of cookies, the Viant Advertising Cloud provides marketers with access to over 1 billion registered users globally, one of the largest registered user databases in the world, and infuses accuracy, transparency, reach and accountability into cross device advertising.

Viant owns and operates Adelphic and MySpace, and is a member of the Xumo joint venture. In 2016, Viant became a subsidiary of Time Inc. (NYSE: TIME), one of the worlds leading media companies with over 100 influential brands including People, Sports Illustrated, Fortune and Time. In 2017, Viant acquired Adelphic, a mobile-first, programmatic demand side platform (DSP), forming the industrys first self-service people-based DSP."
Data Engineer,Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Data Engineer
Key Role:
Contribute to the functional implementation of Enterprise Data Management (EDM), working closely with business functions in all aspects of data, including data governance, data quality, master data management, data analytics, and data discovery. Identify and promote best practices, standards, and methodologies to assist with the execution and implementation of EDM best practices. Support EDM initiatives, including building, evolving, or maintaining an enterprise business glossary, conceptual data models, logical data models, data naming standards, an enterprise data dictionary, a metadata repository, data asset inventory, data or information flows, or data lineage analysis.

Basic Qualifications:
-8+ years of experience with enterprise data management work
-5+ years of experience with the DAMA-DMBOK2 data management framework for data governance and management
-Experience in data management, data architecture, data governance, and/or enterprise architecture
-Knowledge of current technology trends, including Cloud, Cloud Native, AWS, DevOps, Agile, Free and Open Source, Big Data, and equivalent technologies
-Ability to use technical and data modeling expertise to lead discussions in refinement and planning meetings
-Secret clearance
-BA or BS degree

Additional Qualifications:
-Experience in working with Metadata repositories
-Experience in working with relational databases and business intelligence systems
-Experience with data lineage
-Ability to show enthusiasm for data and information and communicate with relevant data stakeholders to research and help resolve data lineage, data anomalies, or data quality issues
Ability to serve as a technical leader of a team by using a listen-first approach, bringing up the team through mentoring, team work, and collaboration, and being an innovative, calculated risk-taker
-Ability to work in a dynamic, fast-paced, work environment
-Ability to manage multiple projects and tasks concurrently
-Ability to be self-motivated and work under minimal supervision
-Ability to work both independently and on project teams
-Possession of excellent analytical, problem solving, and organizational skills
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet."
Data Engineer,Dropbox,"The Data Engineer is responsible for designing and developing robust, scalable solutions for collecting, analyzing large data sets, creating and maintaining the data pipelines, structures and reports to be used by the revenue organization at Dropbox. Responsibilities Understand business processes, applications and how data is gathered; and to tie application telemetry to transactional data model. Develop and manage data pipelines at enterprise scale Build data expertise and own data quality for various data flows Launch and support new data models that provide intuitive analytics to your customers Design and develop new framework and automation tools to enable folks to consume and understand data faster Use your expert coding skills across a number of languages like SQL, Python and Java to support analysts and data scientists Collaborate with multiple teams in high visibility roles and own the solution end-to-end Requirements 5+ years of Python or Ruby development experience is necessary; 5+ years of SQL (Oracle, AWS Redshift, Hive, etc) experience is required, No-SQL experience is a plus; 5+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation and maintenance; Nice to have: Experience working with either a Map Reduce or a MPP system on any size/scale Nice to have: Experience working with visualization tools like Tableau or MicroStrategy Communication skills including the ability to identify and communicate data driven insights BS or MS degree in Computer Science or a related technical field"
Data Engineer,HelloSign,"If you love clean data, we want to hear from you. We are looking for a Data Engineer who will keep our exec dashboard accurate and take the Analytics teams data infrastructure to the next level. You will work with our data infrastructure, datasets and analytics tools that are used by the Executive, Product, Marketing, Sales, Sales Engineering, Finance and Customer Success teams every day--in short, your work will impact the whole company. This is a new role for HelloSign and you will play a critical role in shaping our Analytics foundation.

You will contribute to a variety of projects that range from designing robust and fully automated ETL processes to building tools for improving company-wide productivity with data. You have a passion for designing, implementing and operating stable, scalable and efficient solutions to flow data from production systems into the data warehouse.
We know new analytics technologies are emerging every day and we are excited about the impact they will have  we hope you share our enthusiasm!

Responsibilities:
Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources
Ensure we have data consistency on both production and analytical databases. You will own the integrity of our data from end to end and the company will make high impact decisions based on this data.
Architect and build data warehouse to provide timely data to a variety of third party applications (Salesforce, Marketo, etc)
Design and build tools that make our data pipelines and surfacing more reliable and easier to use
Work closely with backend engineers to roll out new tools and features
Triage, identify, and fix scaling challengesCollaborate with internal data customers to gather requirements
Help develop our data engineering function in areas of data architecture, business intuition & insight or potentially big data.
Required Skills and Qualifications:
3+ years experience with at least one relational databaseMySQL, Postgres, Oracle or other.
SQL Scripting and Data Warehousing experience using RDBMS
Experience with large-scale data pipelines and ETL tooling.
Ability to write efficient SQL queries
Some coding experience. Our ETL process is in Ruby and our development environment is LAMP stack.
Ability to maintain confidentiality of sensitive customer data.
Bonus Points:
Experience with EDA (Exploratory Data Analysis) and Data Visualization (we use Tableau)
Amazon RedshiftExperience working with data using Python (pandas, numpy, scikit-learn, etc)
Business Intelligence, Analytics or Finance experience
Experience with any Hadoop type technology (Hive, Pig, Spark, etc)
About Us:

HelloSign is creating the next generation eSignature platform, with a focus on clean usability. We were recently voted #1 eSignature solution for Small Business and #1 software solution for Mid-Market business on G2 Crowd! Were proud of these awards because they are voted on by real users and reflect our commitment to making life better for our customers. In fact, we have the highest customer satisfaction score out of all our competitors.

In addition to our sleek end user product, developers love our API, clean, straightforward documentation and extensive SDKs. On average, full-featured integrations average less than 2.5 days of development--the fastest in the industry. And our API Engineers offer the best support youll never need. Our vision is to deliver Frictionless Agreements by making our users awesome.

Life at HelloSign:

We are centrally located in downtown San Francisco in SOMA near Embarcadero. Currently at 60 employees, we are growing the company deliberately, with an eye towards maintaining a culture that values lifestyle, fun and continuous improvement. We were awarded the 2015 Hirepalooza Culture Award for Lifestyle and continue to maintain an overwhelmingly positive presence on Glassdoor and The Muse.

We have raving fans who love what we make  We're user-focused and product-driven  We're always evolving with an eye towards improvement  Committed to building a product people want  We have a well-defined culture of fun, continual learning and collaboration  A supportive, familial atmosphere  An open, airy, creative space with communal dining and lounge spaces  We love dogs  Happy hour for unwinding and partaking in shenanigans  A fully stocked kitchen with drinks and snacks  And we'll never forget your birthday!

HelloSign is an equal opportunity employer committed to hiring a diverse team of qualified individuals  HelloSign conducts background checks; pursuant to the San Francisco Fair Chance Ordinance, HelloSign will consider for employment qualified applicants with arrest and conviction records  HelloSign participates in E-Verify."
Data Engineer,Reflektive,"ABOUT REFLEKTIVE'S ENGINEERING TEAM

Reflektive is seeking a Data Engineer to deliver our talent development platform to the world's best places to work. Reflektive is a rapidly scaling company making this the best environment to take on ownership as well as learning how to grow a company.

You'll join a lean, prolific team where everyone, including you, is active in the product defining and development process (where deploying new features every 2 weeks is common). You'll know the customers we're talking to, and the needs of each one. As a result, you know where your initiative and drive can best make a difference (and be recognized!)

Our engineering team consists of developers from a wide array of backgrounds. Our data team primarily focuses on Java, SQL and Python. Our team is a tight knit, friendly group of engineers that are dedicated to learning from and teaching to each other. Team members regularly contribute to and optimize our engineering practices and processes. Our team wants to make software engineering fun, easy, and fulfilling, so we've come up with a set of values that we apply to our software every day: Simple, Flexible, Consistent, Predictable, Efficient, and Pragmatic.

Responsibilities
Working on Reflektive's Data & Infrastructure Projects
Experience or demonstrated interest in Big Data Technologies
Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data
Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack
Help promote best practices for Big Data development at Reflektive
Act as a bridge between the infrastructure and application engineering teams
Provide infrastructure support with a focus on cloud based computing
Build and support visualization and exploration capabilities around our Data Sets
Work with the Data Extraction and Data Science engineers on normalization and analytical processes
Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries
Help design and implement disaster recovery efforts
Analyzes requirements and architecture specifications to create detailed design
Research areas of interest to the team and help facilitate solutions
Desired Skills and Experience
1-5 Years of professional experience as a Data Engineer or a Backend / Full Stack Software Engineer looking to move into Data Engineering
Skills required: Java, SQL
Great if candidate has experience in any of the following Python, Ruby on Rails, PostgresSQL, Redshift, Elasticsearch
Startup experience
Flexible team player
Willingness to roll up your sleeves and get stuff done
Willingness to learn and do whatever it takes to meet deadlines in a quick, ever-changing environment
In 30 days, you will have deployed fixes to production and have an understanding of how Data is consumed/transformed/stored and the Key Infrastructure Projects
In 60 days, you will have taken on a deeper understanding of projects, working independently or with team mates to implement key features
In 90 days, owning your own project and working collaboratively with your peers"
Data Engineer,Alloy,"Over $20 trillion worth of goods flow through increasingly complex global supply chains each year--the items we use, wear and consume every day. Alloy is building the first platform that provides a comprehensive, low-latency view of demand and inventory from production to consumption. Our platform connects manufacturers, logistics providers, distributors, and retailers, allowing them to more efficiently manufacture, distribute, and sell their products.

We are early stage, well funded by leading VCs, and growing. Our small team studied at top institutions including MIT, Stanford, Waterloo, and ETH Zurich and has diverse backgrounds and experience in analytics, large-scale enterprise software, and retail and financial technology. Culture really matters to us: we value diversity in all forms and strive to foster respect, integrity and open communication.

We're committed to making enterprise software inspiring. We use Google Compute Engine, BigTable, PostgreSQL, Redis, Python, Java and React, all wrapped in strong design.

About you You thrive in an environment where you can bridge the gap between engineering and its applications. You tend to be the most business-savvy person in the engineering team and the most technical person in the business team.

You dont shy away from even the most challenging problems and are relentless in always looking for better solutions. You are self-motivated and enjoy working with others towards a common objective; and when you know a better way, you voice your opinion. Building software is the means to an end: you want to change the way an entire industry operates.

About the role As a Data Engineer at Alloy, you will oversee and expand our entire data integration layer, allowing Alloy to seamlessly communicate with a wide variety of companies across the supply chain. This includes retailers, distributors, logistics operators and e-commerce platforms.

Specifically, you will:
Build, automate and maintain integrations with retailer/distributor portals, e-commerce platforms, logistics providers, ERP platforms, manufacturers, and other relevant data sources
Build and improve internal libraries to streamline data integration across multiple sources, including web scrapers, EDI files, REST APIs and flat files
Be the internal expert on how each player in the supply chain shares and interpret data. Maintain internal Alloy logic to automate the interpretation of data across channels through the unified Alloy data model
Your experience will include:
Strong knowledge of Python and SQL, especially in data wrangling and ETL applications
Familiarity with Java is a plus
Experience in interpreting and manipulating supply chain-related datasets (Point-of-sale, logistics/EDI, product master)
Working knowledge of Selenium and other web-scraping tools"
Data Engineer,Fintech Fund Services,"Over $20 trillion worth of goods flow through increasingly complex global supply chains each year--the items we use, wear and consume every day. Alloy is building the first platform that provides a comprehensive, low-latency view of demand and inventory from production to consumption. Our platform connects manufacturers, logistics providers, distributors, and retailers, allowing them to more efficiently manufacture, distribute, and sell their products.

We are early stage, well funded by leading VCs, and growing. Our small team studied at top institutions including MIT, Stanford, Waterloo, and ETH Zurich and has diverse backgrounds and experience in analytics, large-scale enterprise software, and retail and financial technology. Culture really matters to us: we value diversity in all forms and strive to foster respect, integrity and open communication.

We're committed to making enterprise software inspiring. We use Google Compute Engine, BigTable, PostgreSQL, Redis, Python, Java and React, all wrapped in strong design.

About you You thrive in an environment where you can bridge the gap between engineering and its applications. You tend to be the most business-savvy person in the engineering team and the most technical person in the business team.

You dont shy away from even the most challenging problems and are relentless in always looking for better solutions. You are self-motivated and enjoy working with others towards a common objective; and when you know a better way, you voice your opinion. Building software is the means to an end: you want to change the way an entire industry operates.

About the role As a Data Engineer at Alloy, you will oversee and expand our entire data integration layer, allowing Alloy to seamlessly communicate with a wide variety of companies across the supply chain. This includes retailers, distributors, logistics operators and e-commerce platforms.

Specifically, you will:
Build, automate and maintain integrations with retailer/distributor portals, e-commerce platforms, logistics providers, ERP platforms, manufacturers, and other relevant data sources
Build and improve internal libraries to streamline data integration across multiple sources, including web scrapers, EDI files, REST APIs and flat files
Be the internal expert on how each player in the supply chain shares and interpret data. Maintain internal Alloy logic to automate the interpretation of data across channels through the unified Alloy data model
Your experience will include:
Strong knowledge of Python and SQL, especially in data wrangling and ETL applications
Familiarity with Java is a plus
Experience in interpreting and manipulating supply chain-related datasets (Point-of-sale, logistics/EDI, product master)
Working knowledge of Selenium and other web-scraping tools"
Data Engineer,Blue Owl,"A million people a year die in car collisions around the world. That number can be zero . You can help us build an InsurTech company that uses the latest technology, data science, and behavior modification methods to save lives by preventing car collisions before they happen. To this end, we helped launch hiroad.com , a cloud native insurance solution that rewards people for the act of driving well. We are a well funded team of elite developers, data scientists, and business people who truly care about making a difference in the world, located in the financial district of San Francisco. The field is rich with data and we will be pushing the boundaries of what is possible with it. If this sounds like a match for you and what you are up to, please apply. We'd love to hear from you.

Skills and requirements
You've built streaming data applications using open source tools
You're deeply familiar with the SMACK stack and Scala
You've deployed machine learning models in production
You are a solid software engineer
Ideal, but not required
You have been responsible for supporting large-scale, data-intensive deployments and have the scars to prove it!
You know how to put together a machine learning model
You have wrangled trip data (location, accelerometer, gyroscope, etc).
BlueOwl, LLC is an equal opportunity workplace and affirmative action employer. We are committed to equal employment opportunity regardless of race, color, ancestry, religion, sex, national origin, sexual orientation, age, citizenship, marital status, disability, gender identity or Veteran status.

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
Data Engineer,Circle Internet Financial,"At Circle, we're shaking up the global economy by making money simple. With us, you can send money like a text  instant, secure and free  whether your friend is across the table or across the ocean. How exactly are we doing this? Unlike existing systems that are closed and proprietary, we use open internet standards and protocols, including the blockchain (allowing us to offer this service for free). Our mission is to connect every person in the world by exchanging value across borders, between currencies and among friends.

As a Data Engineer at Circle, youll work closely with the Data Science team productizing machine learning solutions and developing the scalable data pipelines that support these solutions. Youll have the opportunity to significantly impact internal data consumption and decision making (risk and compliance, growth, and marketing) as well as our customer experience.

As part of our team, youll own software throughout the entire development life-cycle. Youll contribute ideas to improve our product and processes, make decisions that have significant impact, and provide feedback so we can continue to get better. Youll get the opportunity to develop your skills, collaborate across teams and continue to learn.

Projects we are working on (and you may work on, too)
Productize machine learning models and build a world class automated evaluation system to manage risk exposure while delivering the best possible experience to our customers
Design, build, and implement scalable streaming data pipelines and ETL frameworks to increase data access and decrease analysis and decision times across the organization
Build an open transaction protocol where users can send money and exchange value that leverages both new and traditional technologies to deliver value the fastest and cheapest way possible
Leverage blockchain technology in a new transaction framework so that Financial Institutions an exchange value and associated metadata in a compliant manner.
What you'll bring to Circle
Experience writing high quality code in Python plus another OOP language (Java, Scala, C++, Go, etc)
Experience working with RDBM systems, particularly familiarity with SQL
3+ years experience building distributed solutions in Spark, MapReduce or other MPP system with associated data models and datastores (e.g., Redshift, Cassandra, HBase, Parquet)
Production development of event-based applications using frameworks such as Kinesis, Kafka, Spark Streaming, or similar
Familiarity with machine learning techniques, continuous deployment pipelines and tools, and AWS technology stack a plus
Desire to work across internal teams to identify requirements and iterate on solutions
Debug complex production issues across various levels of the tech stack
Interest in impacting the way money moves between people

Circle was founded in 2013 by internet entrepreneurs Jeremy Allaire and Sean Neville and we're backed by $140 million from investors including Jim Breyer (Facebook), Goldman Sachs, IDG Capital (Baidu, Tencent), General Catalyst (AirBnB, Snapchat) and Accel Partners.

Check us out at circle.com and download Circle Pay for iOS and Android today."
Data Engineer,ThirdLove,"ThirdLove is looking for a Data Engineer to provide technical leadership and hands-on development of our data driven projects. This role will lead the charge for selecting and implementing the right technologies and infrastructure for data warehousing, analytic reporting, recommendation engines and analytics engines based on business requirements and best practices.

THE JOB
Identify database requirements by interviewing stakeholders, evaluating existing systems, and analyzing department applications and operations
Recommend, document, and implement database systems, data architecture, schema design, security, backup, and disaster recovery
Master of ETL/ELT
Relational Data modeling including de-normalized dimensional modeling (Star and Snowflake schema design)
Work closely with engineers to design and maintain scalable data models
Develop, document and maintain enterprise ETL processes
Be the expert on end-to-end data flow for the enterprise
Develop an enterprise reporting and data warehouse solution to track business metrics
Implement systems for tracking data quality and consistency
Work with the tech team to establish data standards, ensure standard adherence and maintain data quality
Collaborate with business and product stakeholders to develop clear business objectives, KPIs and measurements
Programming/Scripting of tools for task automation
THE QUALIFICATIONS
3+ years experience in data warehousing and business intelligence
Expertise in ETL (eg, SSIS, Ansible, Informatica, SAP, OWB or Scripts)
Prior experience working with data processing platform
Proven ability to work with varied forms of data infrastructure, including relational databases
Experience with configuring and deploying databases on AWS
Experience with creating frameworks to extract data via APIs
Expertise with various database platforms (SQL, NoSql, On-Prem, Cloud)
Scripting and programming in one or more of the following: PHP, .NET, Python, R., Javascript
Ability to write, analyze, and debug SQL queries
Experience with non-structured data (eg, free form, json, xml, images, audio, video)
Experience with data visualization, data mining, and preferably statistical tools
Good understanding of the Big Data technology trends
A master of all trades mentality and an ability to embrace new challenges regularly
Able to take individual ownership of a project from start to finish
Excellent critical thinking, problem solving, and analytical skills
Excellent communication skills, and the ability to work effectively with others
BS or MS degree in Computer Science or a related technical field
THE PERKS
Comprehensive health benefits
401k plan
Equity
Subsidized lunches
Quarterly product allotment
Wellness benefits including in-office massage and chiropractor visits
ThirdLove is a fast-growing intimates brand designing bras with real women in mind. Instead of using standard industry measurements, we developed proprietary half-cup sizes based on thousands of real womens measurements. For us, it's not just about the bra  it's about how you feel in it.

Were a growing team of designers, operations experts, mobile and web engineers, and marketers in San Franciscos Design District. Through our combined expertise and passion, we strive every day to deliver elegant design and brilliant solutions to difficult-to-solve problems.

Our culture is entrepreneurial and dynamic, with a passionate dedication to creating beautiful products and great user experiences. We are funded by a long list of tier 1 VCs, celebrity angels, and strategic investors. Just a few ThirdLove perks include lunches, fun team outings, a fully stocked organic kitchen, health care, equity and, to top it all off, in-office chiropractic!

Please note before submitting an application: as a company, we take hiring very seriously. Interviewing with ThirdLove may include video and phone interviews, written projects, and/or on-site interviews. Although we are unable to follow-up with each and every applicant, we do our best to run a thorough process for candidates with whom we identify a potential fit.

ThirdLove is an equal opportunity employer and values diversity at the company. We do not discriminate on the basis of race, religion, color, national origin, gender, gender identity or expression, sexual orientation, age, genetics, marital status, veteran status, or disability status."
Data Engineer,Retention Science,"So you've acquired a customer; now what?

Marketers have long known that acquiring a customer is not the hardest part  keeping them is. During a relationship with a brand, the customer goes through different lifecycle stages. It's only logical that each stage requires different tactics to keep them engaged. Our platform uses machine learning to predict the lifecycle stage a customer is in and helps marketers automatically deliver the right campaign that captures their attention.

We are growing rapidly and looking for great engineers to join our team.

Our client list is growing, and we're hitting larger and larger amounts of data. We are looking for passionate data engineers to scale out our platform and collaborate with data scientists to implement and improve our machine learning models that crunch billions of data points. Currently, our platform powers thousands of campaigns and processes billions of behavioral signals each day. Weve sent over a billion individually personalized campaign interactions since our inception!

Like challenging problems? We got you covered.

Our engineers love a challenge. Every day we work with the latest technologies to solve extremely difficult problems involving data, analytics, machine learning, concurrency, scaling, real time stream processing, distributed systems, caching, and more. We optimize every chance we have. For each new challenge, our engineering team focuses on planning up front, prototyping thoroughly yet quickly, then executing with well-tested code. We value simple solutions to difficult problems and have endless thirst to learn learn learn!

We are looking for an experienced data engineer who is passionate about writing clean, well-tested code. You should want to make a huge impact in a fast-paced and cutting-edge start-up environment. You are a spark/scala expert, and youll be building robust real-time data pipelines that process billions of events per day and working with our world-class data science team to implement production-quality, scalable machine learning algorithms.

In addition to having meaningful responsibilities and improving engineering chops, you will also receive comprehensive exposure to all aspects of our business. The code and ideas that you contribute will have a tangible impact on the business as a whole. Your code will touch millions of end-users. You will have full responsibility over your projects, and have a real sense of product ownership. You will also have the opportunity to learn tremendously from our awesome team of humble yet super engineers. You'll work with and learn the latest technologies and apply them across our distributed systems.

Our Stack:

We are building the next generation of marketing and data science products, and have a large variety of tools available to us. Our production data stack involves Scala / Spark / Hadoop / Kinesis / Redshift. However, as an engineering team we also make use of Python, Ruby, NodeJS, Java, Dynamo, Mongo, Redis, Flume, Elasticsearch, and whatever the job requires.

Requirements:
3 to 5 years experience working on production data products in Scala or similar
You are a Hadoop expert, and also have 1+ years experience with Scala / Spark
Proficient in at least one statically typed language
Like functional programming
Experience designing and implementing large, scalable services
Passionate about enforcing software engineering principles, production code quality, and regular use of design patterns
Experience interfacing with APIs - SOAP, REST, etc.
Experience creating robust RESTful APIs
Comfortable using Git, Bitbucket/Github
Strong belief that tests and code go hand-in-hand
Deep understanding of SQL, query optimizations, joins etc.
Excellent CS foundation: data structures, time complexities, algorithms, etc.
Startup work experience a major plus!
About Us:

Weve been named in CRNs Top 10 Big Data Startups of the Year, Fast Companys Innovation Agents of 2013, SocialTechs Top 10 Software Company in Southern California, and one of Fox News LAs most promising startups to watch. Our founders have received awards like the Ernst & Young Entrepreneur of the Year. We come from schools like Berkeley, Caltech, Carnegie Mellon, Stanford, and Yale. Weve been featured in the Wall Street Journal, Forbes, Entrepreneur, Inc. Magazine, TechCrunch, Bloomberg, and Reuters, among other notable publications.

Our powerful profiling engine uses machine learning techniques and statistical models to analyze purchasing trends based on massive data sets, which we then use to predict customers' behavior and maximize customer retention for our clients. We love creative brainstorming for solutions and the pursuit of innovation-fueled knowledge.

We also like long walks on the beach, the crackle of turning Rubik's cubes, and dogs with old-soul eyes. Because who doesn't?"
Data Engineer,System1,"System1 is an organization centered around data and data products. We firmly believe data should inform and guide every quantitative decision process. We are building exciting new data products and a state-of-the-art data platform aimed at large scale data collection. At the same time, we are building novel approaches to analyze such data and extract knowledge from it in the form of machine learning algorithms. This enables our platform to optimize our users experience through product recommendations highly customized to individual preferences and interests. Our Data Platform team is pragmatic and focused on applying the best tools and development practices. Knowledge essential for this role includes RESTful APIs, relational database technologies like Postgres and MySQL, as well as Amazon AWS technologies like SQS, SNS, Lambda, Kinesis, and Redshift. Of great value is also interest in NoSQL frameworks like Dynamo, MapReduce, and Spark, as tools to address some of the scalability requirements of our platform. The engineering team as a whole, and the Data Platform team in particular, have an open culture and work in an agile style. The ideal candidate for this role will manifest a passion for distributed data platforms and scalable data architectures. In this role, you will be entrusted with the entire end-to-end engineering processes to manage rich datasets from tens of millions of visitors per month. We are looking for self-motivated, creative thinkers, and for people who are flexible and enjoy working as a team.
Responsibilities
Create, build, and maintain a coherent and performant data architecture
Prototype, develop, deploy, and debug data ingestions and data management services
Participate in peer code reviews and produce high quality documentation
Construct queries and reports to guide architectural design, business decisions, and optimization algorithms
Be embedded within the Data Science team and aid in identifying and exploiting patterns and trends
Take projects through the full engineering lifecycle: designing, ticketing, building, testing, deploying, and debugging tools and products
Help grow a team and work with a tight knit group of engineers and data stakeholders
Qualifications
Bachelors in Computer Science or equivalent
2+ years of experience with Python development
Working with large SQL datastores to answer business intelligence questions using PostgreSQL & Redshift
Experience with Linux, the bash shell, Docker, and AWS infrastructure
Working knowledge of web browsers, cookies, fingerprinting, and how these technologies fit in online advertising
Experience with Spark
Understanding of NoSQL datastores like DynamoDB and Redis
Knowledge of queueing systems like SQS and Kinesis Firehose
Understanding of how to build RESTful APIs
Perks
Free Uber/Lyft to and from work every day!
Collegial and collaborative team with highly intelligent and motivated coworkers
Cross-team lunches and demos to foster learning
Unlimited Paid Sick Time, Competitive PTO and Benefits package
Catered meals and fully stocked kitchen
Weekly happy hour at various bars, restaurants, and venues across Los Angeles
Weekly fitness class with private trainer: high intensity training, yoga, beach volleyball, beach soccer, ultimate frisbee
Competitive Relocation Package
Company parties and outings: Skyzone indoor skydiving, Medieval Times, Karaoke, etc."
Data Engineer,"TrueCar, Inc.","TrueCar is seeking to add Engineers to our Data team. This team applies subject matter expertise to source, analyze and validate the automotive data required from internal and external sources. These data engineers are also responsible for data modeling and structuring and aggregating the data in an efficient, accurate and customer relevant manner. Implementing in the following technologies but not limited to:

ABOUT THE JOB
 Design and develop ETL processes that move and transform data in batch and streaming.
 Build complex workflows and orchestrate data dependencies.
 Work within state of the art engineering practices (i.e. SCRUM, unit/integration testing, design review, code reviews, continuous integration, etc.).
 Work productively together with and learn from other top-level players.
 Ability to learn and adapt to continually evolving technologies in the big data ecosystem.

WHAT YOU NEED:
 4 + years of experience programming in Java.
 2+ years of experience in the Big Data field.
 Experience in any of the following: MapReduce, Spark, ElasticSearch, HBase, Cassandra, Presto, Pig.
 Proficient in SQL and experience in RDBMs/NoSQL databases.
 Experience working with AWS technology stack  EMR, S3, Data Pipeline, Redshift, Kinesis
 Ability to self-manage tasks and proactive in working with other teams to accomplish them while taking pride and ownership in their work.
 Has strong interpersonal communication skills; effectively communicates in verbal and written form.

NICE TO HAVE:
 Experience in Scala
 Experience in AWS Machine Learning, DataRobot or other Machine Learning technologies.
 BA/BS in related field.
Location(s): Santa Monica Offices"
Data Engineer,Comcast,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
Data engineering and data science skills combined with the demands of a high volume, highly-visible analytics platform make this an exciting challenge for the right candidate.Are you passionate about digital media, entertainment, and software services? Do you like big challenges and working within a highly-motivated team environment?As a Data engineer in Data Platform Engineering and Research (DPER) team, you will research develop, support and deploy solutions using real-time distributed computing architectures. You will also employ your skills to deliver insights into customer and network behavior on a rapidly-growing video-over-IP platform. The DPER data team is a fast-moving team of world-class experts who are innovating in end-to-end video delivery. We are a team that thrives on big challenges, results, quality, and agility.Who does the data engineer work with?Data software engineering is a diverse collection of professionals who work with a variety of teams ranging from other software engineering teams whose software integrates with analytics services, service delivery engineers who provide support for our product, testers, operational stakeholders with all manner of information needs, and executives who rely on data for data based decision making.What are some interesting problems you'll be working on?Develop systems capable of processing millions of events per second and multi-billions of events per day, providing both a real time and historical view into the operation of our wide-array of systems. Design collection and enrichment system components for quality, timeliness, scale and reliability. Work on high performance real time data stores and a massive historical data store using best-of-breed and industry leading technology. Design, develop, and apply advanced statistical methods and Machine Intelligence algorithms.Where can you make an impact?Comcast DPER is building the core components needed to drive the next generation of data platforms and data processing capability. Running this infrastructure, identifying trouble spots, and optimizing the overall user experience is a challenge that can only be met with a robust data architecture capable of providing insights that would otherwise be drowned in an ocean of data.Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.Responsibilities:
Analyze massive amounts of data both real-time and batch processing
Prototype ideas for new tools, products and services
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Lead and mentor junior engineers
Work collaboratively to deploy and operate our systems
Help automate and streamline our operations and processes
Build and maintain tools for deployment, monitoring and operations
Troubleshoot and resolve issues in our dev, test and production environments
Here are some of the specific technologies we use:
Spark (AWS EMR), AWS Lambda
Spark Streaming and Batch
Avro, Parquet
Kafka
MemSQL, Cassandra, HBase, MongoDB, RDBMS
Caching Frameworks(ElasticCache)
Elasticsearch, Beats, Logstash, Kibana
Java, Scala, Go, Python, R
Git, Maven, Gradle, Jenkins
Rancher, Puppet, Docker, Ansible, Kubernetes
Linux
Hadoop (HDFS, YARN, ZooKeeper, Hive), Presto
Skills & Requirements:
3+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in software development of large-scale distributed systems including proven track record of delivering backend systems that participate in a complex ecosystem
Knowledge in data related technologies and open source frameworks preferred
Good current knowledge of Unix/Linux environments
Knowledge of network engineering and security
Test-driven development/test automation, continuous integration, and deployment automation
Enjoy working with data analysis, data quality and reporting
Good communicator, able to analyze and clearly articulate complex issues and technologies understandably and engagingly.
Great design and problem solving skills
Adaptable, proactive and willing to take ownership
Keen attention to detail and high level of commitment
Comfortable working in a fast-paced agile environment. Requirements change quickly and our team needs to constantly adapt to moving targets
About Comcast DPER (Data Platform Engineering and Research): Data Platforms Engineering and Research (DPER) is a result driven, data platform research and engineering team responsible for the delivery of multi-tenant data infrastructure and platforms necessary to support our data-driven culture and organization. DPER has an overarching objective to gather, organize, and make sense of Comcast data with intention to reveal business and operational insight, discover actionable intelligence, enable experimentation, empower users, and delight our stakeholders. Members of the DPER team define and leverage industry best practices, work on extremely large scale data problems, design and develop resilient and highly robust distributed data organizing and processing systems and pipelines as well as research, engineer, and apply data science and machine intelligence disciplines.
Comcast is an EOE/Veterans/Disabled/LGBT employer"
Data Engineer,Boeing Intelligence & Analytics,"**THIS POSITION REQUIRES AN ACTIVE TS/SCI **

BI&A seeking Software Engineer with data transformation (ETL) experience working with latest industry tools; elastic search, apache Kafka, and Apache NiFi.

DUTIES ENTAIL:

 Work with a teammate on data integration requirements.

 Write code on ETL platform to transform data to a suitable formats as defined by IC ITE initiatives.

 Add features to ETL platform to shorten timelines for future data integration efforts.

 Develop, maintain code, and integrate software into a fully functional software system.

 Participate in daily scum meetings, sprint retrospectives, and other agile processes.

 Work with external teams to validate data ingest.

 Provide and maintain documentation of system architecture, development, and enhancements.

EDUCATION:

 Bachelors Degree and 6 or more years experience or Master's Degree with 3 or more years' experience from an accredited course of study, in engineering, computer science, mathematics, physics or chemistry.

REQUIRED EXPERIENCE:

 6+ years of software development experience

 Demonstrated understanding of high scale cloud architecture

 Linux/Unix experience

 Object Oriented programming language

 Possess strong verbal and written communication skills

 Possess strong analytical skills, with excellent problem solving abilities in the face of ambiguity

DESIRED EXPERIENCE:

 Expertise in data ingestion, data transformation (ETL), and data modeling.

 Experience with Java, Ruby, or Python

 Experience in Agile/SCRUM enterprise-scale software development

 3 years experience working with batch-processing and tools (eg, Nifi, Midpoint, MapReduce, Yarn, Pig, Hive, HDFS, Oozie)

 1 year working with Restful web services Experience with code development, deployment, versioning, and build tools (eg, Eclipse, git, svn, maven, Jenkins)

 Experience working with tools in the stream-processing (eg, Storm)

 Experience developing applications that work with NoSQL stores (eg, ElasticSearch, Hbase, Cassandra, MongoDB, CouchDB)

 Working in cloud architecture with AWS EC2, RDS, S3, VPC, Elastic Search"
Data Engineer,Children's Hospital of Philadelphia,"Req ID: 13421
Shift: Days
Employment Status: AF - Active - Regular - Full Time

Job Summary

Join a world renowned pediatric healthcare organization recently recognized in Computerworld as one of America's Best Places to Work in IT. The Children's Hospital of Philadelphia Enterprise Analytics team is responsible for the build and support of enterprise data warehousing as well as analytics in the form of creating standard reports, dash boards, and advanced data visualization. Our team is focused on delivering and supporting the data needs of the enterprise, including financial reporting and analytics, improving patient quality, advancing clinical reporting, analytics and research.

This position is responsible for the development of complex data integration processes, data transformation, and building on the data warehouse model within CHOP. The successful candidate will be required to ensure that the extracted, transformed, and displayed information meets all clinical and business requirements of the organization and end-users. This will entail creating new as well as maintaining and modifying existing ETL processes. This individual will be required to provide technical counsel and guidance to other team members, consultants, IS Staff, and end-users. Creating and maintaining accurate project and process documentation is also necessary.

This role will be focused on the day to day development of the CHOP logical data warehouse and taking part in high impact, high visibility projects in our data driven organization. The Agile Scrum methodology is utilized to drive the completion of tasks and goals. The opportunity to work hand in hand with key decision makers, as well as with small teams in your own department will be available.

We are looking for a teammate with an eager personality willing to expand our boundaries, provide new insights into future opportunities, build outside the box solutions, and have a willingness to find and deliver new and unique solutions. Ideally, you will have a strong desire to be on the forefront of advanced medicine and play a part in progressing the personalized approach to medicine. Interest in advanced data warehousing and reporting tools/techniques is welcomed, such as IBM Pure Data for Analytics (Netezza), IBM BigInsights (Hadoop), Apache Spark, Streaming Analytics, Cloud Data Integration and QlikView.

We offer a strong potential for growth and career advancement, and to make a major impact in the way we use data to provide a higher standard of care to our patients. If you have experience with data integration, ETL, APIs and cloud data integration, then apply today for an opportunity to join our seasoned and rapidly growing team.

Job Responsibilities

Job Responsibilities:
1. Exhibits advanced knowledge of SQL; relational and multidimensional models, Data Warehousing methodologies (STAR and Snowflake Schemas), ETL concepts; and source and target structures.
2. Demonstrates advanced knowledge of and expertise in the technical facets of ETL (Extract, Transform and Load), Data Warehousing and BI (Business Intelligence) concepts and tools.
3. Exhibits ability to perform advanced aspects technology to install, test, maintain, develop, provide technical support of, understand and modify existing code and develop new code.
4. Understands cloud data integration concepts
5. Ability to work with APIs and object oriented programming languages for example Python and Java
6. Has the ability to navigate within a linux environment and build shell scripts.
7. Understands advanced uses of data in statistical environments and advanced analytic algorithms.
8. Understands institutional data model standards and can employ them in the environment.
9. Demonstrates basic knowledge of project management and related tools.
10. Demonstrates the ability to create detailed work-related documentation, operations documents and change management plans.
11. Exhibits the ability to communicate effectively with clients and colleagues.
12. Works with analysts to identify and understand source data systems.
13. Exhibits the ability to work with minimal supervision in support of team initiatives.
14. Demonstrates the ability to serve as a resource for other team members and to instruct other staff on various technical matters. Codes complex programs in required languages as defined in specifications prepared by IS personnel or vendors.

Required Education and Experience

Two (2) Certifications or proficiency in appropriate Business intelligence/Data Warehousing technology or subject domain.
Bachelors degree in computer related field required.
3-5 years of Business Intelligence/Data Warehousing experience, preferably in a healthcare environment.

Additional Technical Requirements

Information Security Requirements
Understands and complies with all enterprise and IS departmental information security policies, procedures and standards.
Supports the integration of information security in the development, design, and implementation of Hospital Technology Resources that process, transmiton or store CHOP information.
Supports all compliance activities related to state, federal regulatory requirements, healthcare accreditation standards, and all other applicable regulations that govern the use and disclosure of patient, financial, or other confidential information.

All CHOP employees who work in a patient building or who provide patient care are required to receive an annual influenza vaccine unless they are granted a medical or religious exemption.
Children's Hospital of Philadelphia is committed to providing a safe and healthy environment for its patients, family members, visitors and employees. In an effort to achieve this goal, employment at Children's Hospital of Philadelphia, other than for positions with regularly scheduled hours in New Jersey, is contingent upon an attestation that the job applicant does not use tobacco products or nicotine in any form and a negative nicotine screen (the latter occurs after a job offer).
Children's Hospital of Philadelphia is an equal opportunity employer. We do not discriminate on the basis of race, color, gender, gender identity, sexual orientation, age, religion, national or ethnic origin, disability or protected veteran status.
VEVRAA Federal Contractor/Seeking priority referrals for protected veterans. Please contact our hiring official with any referrals or questions.
CHOP Careers Contact
Talent Acquisition
2716 South Street, 6th Floor
Philadelphia, PA 19146
Phone: 866-820-9288
Email: TalentAcquisition@email.chop.edu"
Data Engineer,Chef'd,"At Chefd we believe great food creates memories, enhances moments, and nurtures the soul. Our mission is to enrich lives through culinary adventure with our hand-cultivated, meticulously curated, delicious experiences.

Chefd is the only subscription-free meal marketplace allowing you to discover new foods the way you wantno strings attached. We have over 700 recipes from some of the best names in foodcelebrity chefs, major brands, and culinary influencers.
Founded in 2014, Chef'd is Headquartered in El Segundo, CA with production facilities in El Segundo, CA, Brooklyn, NY, and Pico Rivera, CA.

Chef'd, a venture-backed, series-B company, is beginning to experience high growth as we scale our operations to new markets and distribution channels. There are plenty of areas where we can innovate and optimize. We're at the beginning of business growth and innovation. Join us and help build a world-class company!

Chefd is looking for a data engineer to join the growing data team at Chef'd HQ in El Segundo. Our goal is to enable our leaders with data to power decisions, inform strategy, and identify opportunities for innovation across the company. As a data engineer, you will collect and transform data by building streaming pipelines using the latest in big data solutions. You will enable the data scientists to access what they need to solve complex problems and personalize the user experience.

Responsibilities:
Enable Chefd to build next generation data solutions
Provide the data needed to power our soon to be released recommendation engineEnable the collection of streaming data by integrating with our Google Cloud pipeline
Work with our architect to build logical models for all data sources
Provide our data scientists needed data through ad hoc queries and data wrangling
Optimize ETL processes to provide data to fulfill production requirements
Work with the analytics team to ensure our BigQuery data warehouse is fulfilling business requirements
Requirements:
Experience loading data into any of the Cloud Platforms technology stack: AWS, Azure, Google Cloud PlatformData Concepts (ETL, streaming, data structures, metadata management)
Big data experience (Hadoop, Redis, Mongo, etc)
Experience with any data integration tools (DataFlow, Informatica, etc)
Strong programming skills in Python
Experience with scikit, numpy and scipy a plus
Education: M.S. in Computer Science, Data Science or related field with 3+ years of experience applying data engineering techniques to real business problems
Self-starter who is excited about learning new technologies
Chef'd offers competitive compensation packages including full medical, dental, and vision benefits, flexible PTO, pet insurance, and discounts on meal kits!"
Data Engineer,Support.com,"Support.com, Inc. (NASDAQ: SPRT) is a leading provider of tech support and turnkey support center services, producer of SUPERAntiSpyware anti-malware products, and the maker of Support.com software. Support.com services and products help leading brands deepen their customer relationships. Customers want technology that works the way it's intended. By using Support.com services and software, companies can deliver a fantastic customer experience, leading to happier customers, greater brand loyalty and growing revenues.

Reducing customer effort through advanced tools embeddable in mobile and web apps

Optimizing the interaction of support agents with customers via intelligent, contextual guidance; data-driven, step-by-step solutions; and modern visual remote tools

Providing actionable insights into support practices and real-world product performance through support interaction analytics and data from external systems (IoT platforms, CRMs, ticketing, etc.)

Join us as a Data Engineer building world-class support software and be part of its success in the market place.

Why work with us

Use cutting edge cloud technology stack built on Linux, Spark, Kafka, Hadoop, AWS, Redis, Vertica, and PostgreSQL. Don't have experience with some of these? You'll get it here!

We love collaborative, agile software development, iterative design and testing. We form tight teams with PM and UX which build rapid prototypes and release frequently. You won't get bored!

We encourage learning and integration of new technologies. If you're passionate about technology, we'd love to hear your story. Support.com is a public company, with mature products established in the marketplace and continued innovation in support technology software.

What you will be doing

Build and deploy the infrastructure for ingesting high-volume data from various endpoints -- consumer devices and apps, IoT platforms Design and implement the processes that turn data into insights. Model and mine the data to describe the system's behaviour and to predict future actions.

Enable data driven change -- Build effective visualizations and reports for the consumption of data insights by all stakeholders, internal (corporate) and external (our SaaS customers)

Develop and maintain the data-related scripting for build/test/deployment automation Research individually and in collaboration with other teams on how to solve problems

What you need for the job

MS in Computer Science, Statistics, Applied Mathematics, or a related field; or BS and 3+ years of relevant work experience Experience with a modern Big Data processing stack including Apache Spark, Storm, Kafka, Kinesis or equivalent technologies

Strong SQL abilities and experience with massive relational database systems. Familiarity with the MPP Databases (Redshift or Vertica), Hadoop ecosystem, and HQL a plus.

Strong knowledge of all traditional Data Warehouse-related components (Sourcing, ETL, Data Modeling, Infrastructure, BI, Reporting) and the modern tools to support those components

Proficiency with UNIX development environments Excellent programming skills in Python or Java

Ability to understand business problems and translate them into technical requirements

Your benefits

Flexible work environment -- e.g. ability to work from home 1 - 2 days a week! Catered lunches every Friday!

Competitive benefits and compensation!

Fun atmosphere! Weekly happy hours, activities and team builders! Ability to work with exceptionally creative and talented people!

Support.com is an Equal Opportunity Employer

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"
Data Engineer,theSkimm,"-----------------------
ABOUT THE TEAM AND ROLE
-----------------------

theSkimm makes it easier to be smarter. We started as two friends on a couch and have since purchased more couches and gotten new friends, er... employees. Skimm HQ works hard but knows when to take breaks and when it's time to update the Spotify playlist. We drink the kool-aid but know we're drinking it. Through our suite of products, we are disrupting routines and information consumption one Skimm'r at a time. Come join us, we're changing things.

theSkimm's Tech team is a group of experienced, resourceful programmers who are comfortable building tools throughout the stack.Our mission as an engineering team is to enable theSkimm's audience to experience our products as seamlessly as possible. We believe that through rigor and creativity, data engineering efforts will lead the way to achieving this goal.

--------------------
WHAT YOU'LL BE DOING
--------------------

In this role, you will be encouraged to use emerging technologies to develop innovative solutions for common data challenges. Put more simply, we want someone who's familiar solving the data engineering challenges of today, but who's eager to pioneer the solutions of tomorrow.

Primary responsibilities include:
Designing and centralizing theSkimm's data infrastructure in the cloud (primarily AWS)
Building connections between theSkimm and its partners, often via RESTful APIs
Creating frameworks and tooling that allows engineers and analysts to introduce new data views quickly and efficiently
Managing the development, configuration, and reporting on data tasks responsible for communicating with millions of Skimmr's
--------------------------------
WHAT WILL SET YOU UP FOR SUCCESS
--------------------------------

A minimum of five years experience in modern web programming at scale
Proficiency with ETL development
Experience gathering data from disparate sources and building data pipelines among systems
Experience with data modeling, data access, and data storage techniques
Smarts, humility, and comfort being resourceful in a fast-paced environment
Strong analytical and communication skills
------------------------
WHAT WOULD SET YOU APART
------------------------

Familiarity and enthusiasm for theSkimm: passion for our audience and mission
Familiarity building systems that leverage AWS storage and database services
Experience administering and tuning distributed databases like Redshift
Proficiency with task processing tools such as Airflow or Luigi
------------------
WHAT WE OFFER
------------------

Medical, Dental, Vision, Life and Disability (starting day one)
$400 annual gym reimbursement
MassMutual 401(k) program
Flexible vacation policy and generous holiday observances
Competitive salary & equity packages
Catered lunch on Fridays
Snacks, coldbrew on tap, and lunch on Fridays
Collaborative office space and roof deck
This is a full-time role based in New York, NY"
Data Engineer,Earnest Research,"THE EARNEST RESEARCH COMPANY

Earnest Research is a VC-backed data innovation startup driven to change the way professionals understand consumer and business behavior. Working with world-class data partners, we transform raw data into a source for business and investment professionals to ask better questions so they can make better decisions. We believe, in the right hands, data has the power to change the way we work.

DATA ENGINEER

Earnest is seeking a data engineer to help scale up our data infrastructure. You will be part of a data-driven decision-making culture and collaborate with software engineers in building out the data tools and processes to support the creation of insights that will drive our business. The role will involve the design and implementation of the entire data pipeline, from capturing and storing disparate data sources to processing that data and making that data available to other team members. You will be working across the company to understand their data needs, and creating systems that provide consistent and complete information to help solve various business problems.

RESPONSIBILITIES

Maintain and implement tools and systems that ingest, transform, organize, and expose data insights
Collaborate with other engineers to help implement and design our next generation data warehouse system
Work closely with our data analyst team to gather technical requirements and provide support on analytics processes
Develop and maintain data pipelines, with a focus on writing scalable, clean, and fault-tolerant code to handle disparate data sources
Implement new product features and performance improvements to existing products
Help drive optimization, testing and tooling to improve data quality across the product line
QUALIFICATIONS

Required Skills:
Proficiency in one of Python, Java, Scala, or a similar programming language
Experience with Hadoop and related technologies (Hive, Pig, Spark, Presto, Impala)
Strong SQL experience (MySQL, Redshift/Postgres)
Comfort with source control (GitHub) and working in a Linux environment
Experience with handling and processing large data sets in a business environment
Understanding of structured and unstructured data design/modeling
Strong analytical, quantitative, problem-solving, and critical thinking skills
Excellent verbal and written communication skills
Additional Preferred Skills:
Experience with AWS tools, i.e. especially EMR, Redshift, Data Pipeline
Experience working with large volumes of time series financial data
Exposure to Data Science
Knowledge of machine learning and natural language processing
NoSQL experience: HBase, MongoDB
Familiarity with BI and analytics tools (e.g. Looker, Tableau)"
Data Engineer,CB Insights,"Build data-driven products and help us predict the next big thing

At CB Insights, we build products to gauge and predict technology trends. This requires gathering information from disparate sources, analyzing it, extracting useful information and surfacing that on our platform. As a data engineer at CBI, you will be a core part of this process end-to-end and help us in building data pipelines and the infrastructure that enables this. You will help build products that use natural language processing and machine learning models and make them run efficiently with large amounts of data to enable the best user experience whether they be end-users or our data analysts.

Were looking for engineers that, through hard-won practical experience, know how to build maintainable and testable data pipeline processes and infrastructure. We are looking for engineers that love solving problems and are willing to take on hard ones. Sounds a tad clich but as engineers, we believe that the best professional satisfaction comes from knowing our customers use the software weve built and love it.

Key Responsibilities

Engineer efficient, adaptable and scalable data pipelines that power our data products
Design and build efficient ETL infrastructures for unstructured textual data sets and various other types of data sources
Take a prototype of a data product built with NLP and/or machine learning models and make it run reliably in production.
Monitor and maintain existing data products running in production including identifying when models need to be retrained
Design and implement internal tools to make this data processing infrastructure easily accessible to and usable by other software developers
Develop solutions that are well-engineered, maintainable, tested and delivered on time.
Participate in code reviews and sprint planning, help to identify problems and share knowledge with your colleagues.
Required Experience and Qualifications:
2+ years software/data engineering experience
2+ years professional experience with using Python, SQL
Knowledgeable about data modeling, data storage techniques, data warehousing and general data architecture
Experience with engineering data pipelines to capture, store and process unstructured data
Experience with building and maintaining a Hadoop or Spark cluster and other related tools in the big data ecosystem
Excellent written and verbal communication skills
Excellent problem solving and analytical skills
Proficiency developing in a Mac/Linux environment
Technologies/Languages: Python, SQL, NoSQL, Spark, Hadoop
Additional Qualifications:
Experience with Go, Scala
Perks and Benefits:
-------------------

Subsidized health, dental and vision insurance
401K with up to 4% match
$1,000 yearly continuing education stipend
Daily lunch stipend
Equal Opportunity Employer: CB Insights is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status.

------------------------------------------------------------------------------------------------------------------------------------------------------------------
If you know someone who'd be perfect for the role, submit here ( https://www.cbinsights.com/research-jobs-referrals ) and you'll be eligible for $5,000!
------------------------------------------------------------------------------------------------------------------------------------------------------------------"
Data Engineer,Match,"OkCupid is looking to hire a Data Engineer!
If you're a Data Engineer that likes the idea of working on a product used by millions of people, read on.

What's it like being a Data Engineer at OkCupid?
We're a team of engineers and data scientists that manage all aspects of OkCupid's BI and data stack. Every person we hire is given lots of ownership; your input will be a big part of what we do and how we do it.

There's also a sense of satisfaction that comes from helping millions of people find love. If that's the kind of thing that appeals to you, you're probably a good fit for OkCupid!

We're seeking someone who:
Has 4+ years experience with ETL or BI platforms
Is fluent in SQL and Python
Understands the differences between row-based and columnar, relational and no-sql
Has a Bachelor's degree, preferably in a related field (Engineering, Computer Science, Statistics, Applied Math)
What you'll work on:
Write complex and efficient code to transform raw data sources into easily accessible models for our teams (e.g., Product, Growth, Finance, Fraud)
Select the right tools for the right job and make them all work well together
Help architect and build our machine learning infrastructure
Interpret and analyze data from various source systems to support data integration and data reporting needs
Troubleshoot & determine best resolution for data issues and anomalies
Manage exploratory data analysis to support database and dashboard development, as well as advanced analytics efforts
Bonus Points if you have:
Experience with a mass market consumer facing product
Familiarity with the pydata stack (numpy, scipy, pandas)
Whats in it for you?

Competitive salary and full benefits, including Medical, Dental, Vision, and 401k
Workstation and tools of your choice
Paid attendance to conferences
Regularly catered lunch and a bomb shelter stockpile of snacks and drinks
Company funded happy hours/events
A sunny office full of incredibly curious and friendly people"
Software Engineer,C.H. Robinson,"We are seeking a highly motivated developer/programmer to join our Global Forwarding System development team. The Global Forwarding System development team is responsible for delivering solutions to our business users to help them efficiently and effectively manage the global freight network they are responsible for. We are responsible for the development of the primary application our business representatives use in many branches across North America, India, Asia, South America and Europe. We are growing team with high growth potential in the future and are looking to add that developer who wants to be on forefront of where CH Robinson is going as a company.
The individual in this role will perform analysis, design, programming, and testing tasks to deliver on projects specific to the application. The position demands excellent problem solving skills and interaction with internal customers and other IT teams. The individual will have a passion to solve business problems, pay great attention to detail, take pride in their work and have a good sense of product requirements. The ideal candidate will apply design patterns and SOLID principles using a variety of tools and components as the technology landscape constantly evolves.
The successful applicant will be a creative, self-motivated individual, capable of working in a team environment, and on multiple projects at once. Additionally, applicants should have excellent communication skills and demonstrated ability to work in a high-paced, high-pressure environment. As a continuously evolving Agile team (currently practicing Scrum), we demand strong participation and courage to constantly inspect, adapt, and experiment. We expect strong passion and commitment for building unique and customized solutions to make our users lives better and differentiate our services in the marketplace.

Responsibilities:
Participates in designing and developing applications using .NET and web based technologies that meet data integrity, performance, business, and security objectives for complex application features using tools like Visual Studio and interaction with SQL Server databases
Contribute to high-level technical options that may span multiple systems
Translate high-level requirements into detailed designs
Develop and maintain thorough technical documentation
Provide technical estimates
Perform thorough unit testing and some functional testing as needed
Provide innovative ideas and solutions in an ever-changing environment
Possess solid communication skills and a strong customer focus
Maintain knowledge of emerging technologies and using those technologies to solve business problems

Qualifications:
Required Qualifications
A minimum of 4 years programming experience in Information Technology
Experience in object-oriented programming languages (willingness to work in C# and VB.NET)
Experience with various design patterns and SOLID Principles
Experience with object-oriented design and development methodologies
Experience with relational database technology including schema design, stored procedure development and SQL queries. SQL Server preferred.
Proven track record of effectively delivering results in a fast paced environment while managing multiple priorities
A proven track record of delivering UIs using technologies such as Angular, WPF, and WinForms
A minimum of 1 year of experience in Unit or automated testing
Focus on building clean code and quality and designs that yield a more maintainable and longer running product
Preferred Qualifications
Experience with transportation or logistics information systems
Experience with javascript frameworks i.e. Angular & React
Experience with javascript test frameworks i.e. Jasmine or Protractor
Experience with REST services i.e Web API, Service Stack or Node.js
Experience with Agile/Scrum methodologies
Functional experience data modeling
Experience with full stack development
Experience in system integration projects or global projects
A minimum of 1 year of experience in Agile/Scrum methodologies
A minimum of 1 year of experience in continuous build practices
Required Education
Undergraduate degree in computer science, software engineering, business administration or equivalent work experience.

Benefits

We offer a competitive compensation package and excellent benefits including medical, dental, and vision insurance, prescription drug coverage, paid holidays and vacation, disability insurance, life insurance, 401K with company match, profit sharing, Employee Stock Purchase Plan, and the opportunity to prosper in a Fortune 500 company.

About C.H. Robinson

Become a part of our team of over 500 talented IT Professionals. Work in collaborative, Agile development environment. Find continuing challenges and work with committed leaders. Stay with us  were large enough to build global solutions, but small enough to make real impacts as individuals.

C.H. Robinsonaccelerating careers with immense opportunities and professional growth within the global supply chain industry. Start here. Accelerate here.

Every individual working at C.H. Robinson is integral to the success of our customers and our company. C.H. Robinson is a Fortune 500, global company that values teamwork, initiative, accountability, and integrity from its employees. We work globally and innovate daily to enhance and execute supply chains that move goods around the world. The fast pace of the logistics industry translates into a high-energy and collaborative workplace environment. We are empowered to make decisions, help our customers grow, and accelerate our careers.

No matter the product being shipped or from which corner of the globe, C.H. Robinson can help make it happenquickly, securely, and reliably. Through personal connections and solid relationships, our employees use their in-depth knowledge, robust tools, and global network to help customers reach their goals quickly. Whether shipping by plane, rail, ship, or truck, C.H. Robinson has the knowledge, flexibility, and dedication to deliver the goods that make our world go round.

Join the 12,000 employees worldwide who are accelerating their careers at C.H. Robinson.

Equal Opportunity Employer

C.H. Robinson - Affirmative Action Employer/EOE/M/F/Disabled/Veteran"
Machine Learning Engineer,Lenovo,"Functional Area: University Facility: Corporate Office Relocation Provided: No Education Required: Bachelors Degree Experience Required: 1 - 3 Years Travel Percent: 0 Position Description At Lenovo, our belief is that there is no cookie-cutter way of achieving success. We love innovative thinkers, risk-takers, and people who Never Stand Still.Through our internship program we are looking to cultivate the minds of fresh career seekers with an interest in joining a fast-paced technology environment. We are looking for passionate and driven students that are willing to learn! Different Works Better. Different Hires Better. Different Is Better. We are looking for a Machine Learning Engineer to join the Software Cloud Development team. Responsibilities may include:  Write clean code for our cloud platform, create robust high-volume production applications, and develop prototypes quickly.  Participate in cutting edge research in artificial intelligence and machine learning applications.  Develop software solutions for real world, large scale problems.  Help the data prediction by integrating scripts with post prediction web services. *Multiple Positions Available* May 2018 graduates are encouraged to apply. Position Requirements Minimum Requirements:  BA/BS degree in Computer Science, related technical field or equivalent practical experience.  1-2 years of work or educational experience in Machine Learning or Artificial Intelligence.  Experience with one or more general purpose programming languages including but not limited to: Java, C/C++ or Python Preferred Qualifications:  MS or PhD degree in Computer Science, Artificial Intelligence, Machine Learning, or related technical field.  Have strong problem solving and analytical skills.  Strong consistency and attention to detail.  Passionate about innovation and discovery  Experience with one or more of the following: Natural Language Processing, text understanding, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.  Prior experience with Agile development preferred Lenovo is a $46 billion global Fortune 250 company and leader in providing innovative consumer, commercial and enterprise technology. Our portfolio of high-quality, secure products and services covers PCs, workstations, servers, storage, smart TVs and a family of mobile products like smartphones (including the Motorola brand), tablets and apps. Everyone here at Lenovo is an integral part of the company, working together, across continents, cultures and innovations, all comprised in a friendly, fast-paced, work environment that focuses on one common goal: to be known as the best in what we do. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. ---"
Software Engineer - Big Data,Intel,"Job Description

Saffron is building the AI software that's making it possible for Intel-powered devices to solve problems in the real word. We're building a new team that can build incredible new software products for customers in multiple industries.

We're looking for software engineer to join this new team. You will design and develop full stack software for a big data system. You will understand requirements and work with cross-org teams, will translate advanced business analytics problems into implementable approaches and will influence all aspects of the system from ingestion of rich data sources utilizing big data methodologies to solving the data science problem to present the results to user in an understandable format

Qualifications

Education: Master's degree +8 years of industry experience. or Bachelor's degree + 10 years of industry experience
Industry Experience: - At least 8+ years of experience in Software Development- At least 4+ years of big data experience with different languages
Strong programming skills - C++, Java, Python, Scala along with ability to pick up new languages

At least 5 years of hands-on experience in architecture, design or development of enterprise applications and integrations with rest of the systems
Past demonstrable programming work
Exposure to big data methods - Hadoop, Hive, Spark, MapReduce
Understanding of cloud based system, distributed systems
Experience with source code management tools
Good understanding of software methodologies for complete software development life-cycle from design to development to test to integration Inside this Business Group

Intel Labs, the New Business Initiatives group (NBI), the Perceptual Computing Group and the New Devices Group (NDG). Additionally, New Devices Group and the New Business Initiatives group were combined into the New Business Group (NBG) which nests into NTG.
Posting Statement. Intel prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status."
DATA ENGINEER,Sound Transit,"GENERAL PURPOSE: Under minimal supervision, organizes and structures data for analytical or operational uses. Has expertise in building data pipelines to pull together information from different source systems, integrating, consolidating, cleansing data and structuring it for use in individual analytics applications. Has extensive experience in database technologies, cloud based platforms, ability to analyze business needs for data acquisition, data modeling, extract-transfer-load (ETL) processes, data analysis, report creation and implementation. Leverages pragmatic data management practices and principles to ensure the coherent and understandable storage and movement of data for the delivery of information to end users. Work complements the agency business and technical teams on many non-standard and unique business problems and uses creative problem solving to collect, organize and deliver useful reports and data insights.

This job posting may be used to fill multiple positions.

Essential Functions:

ESSENTIAL FUNCTIONS:
The following duties ARE NOT intended to serve as a comprehensive list of all duties performed by all employees in this classification, only a representative summary of the primary duties and responsibilities. Incumbent(s) may not be required to perform all duties listed and may be required to perform additional, position-specific duties.
Analyzes business data requirements and design strategies for the gathering, analyzing and storing of information for end users.
Owns the design, development, and maintenance of ongoing data marts, metrics, reports, analyses, dashboards, etc. to drive key business decisions.
Is proficient with writing complex SQL queries, stored procedures, ETL scripts, views, temporary tables, and other objects related to solving questions with data.
Has expertise in data warehouse lifecycle development, dimensional data modelling, Online Transaction Processing (OLTP) and Online Analytical Processing (OLAP) modeling.
Focuses on efficiency, scalability and performance tuning by partnering with Database Administrators (DBAs) so that applications, ad-hoc queries, reports and dashboards perform well and can be modified with ease.
Analyzes, creates and maintains scripts to monitor and report on data quality and troubleshoot issues which may compromise the credibility of reports.
Maintains and administers reporting platforms, including server and client tools, organizing reporting in portals and user security.
Develops and maintains reports, graphs and visualizations, including a catalog of available reports, allowing users to easily find and understand information.
Partners with the IT Enterprise Architect and other IT analysts to define and implement a sustainable data management practices.
Works with business users to understand data and how to use the data to satisfy information needs.
It is the responsibility of all employees to follow the Agency safety rules, regulations, and procedures pertaining to their assigned duties and responsibilities, which could include systems, operations, and/or other employees.
It is the responsibility of all employees to integrate sustainability into everyday business practices.
Other duties as assigned.

Minimum Qualifications:

Education and Experience:
Bachelor's Degree in computer science, information technology, business administration, engineering, or closely related field and six years information technology and data analysis experience in a business intelligence environment; OR an equivalent combination of education and experience.

Required Licenses or Certifications:
None.

Required Knowledge Of:
Principles and practices of computer science and information systems
Principles and practices of technologies including, but are not limited to, MS Azure, MS SQL-Server, SSIS, SSRS, Crystal Reports, Power BI, Pyramid, Business Objects Enterprise, SharePoint, Excel, and any scripting languages such as PowerShell, Python, Java or C#.
Data and information domains including those related to Operations, Finance, Enterprise Resource Planning, Asset Management, Transit Rider Technologies, and other subject areas as needed.
Business intelligence, data management, data warehousing concepts, data modeling, reporting and related activities
Methods and techniques for evaluating business requirements and developing information systems solutions
Operational characteristics of a variety of computer and network systems, applications, hardware, software and peripheral equipment, including enterprise business systems
IT best practices, including ITIL (Incident and Change Management)
Principles, practices, methods and techniques used in the installation, troubleshooting and maintenance of systems and applications
Quality Assurance techniques and practices
Principles and practices of project management.
English usage, spelling, grammar, and punctuation.
Principles of business communication and basic report preparation.
Principles of customer service.
Required Skill In:
Utilizing personal computer software programs affecting assigned work and in compiling and preparing spreadsheets and reports.
Experience with SQL, Microsoft Azure, SQL Server, Microsoft Analysis Services and/or Business Objects, Integration Services, Reporting tool (either of Reporting Services, Crystal Reports, Power BI, Pyramid, etc.)
Logical and physical modeling for both OLTP and OLAP.
Establishing and maintaining effective working relationships with other department staff, management, vendors, outside agencies, community groups and the general public.
Experience using SharePoint for delivery of reports and information.
Experience with large IT implementations, development life cycles, change management and IT processes.
Working knowledge of Microsoft Office, with advanced skills in Excel as a data source and reporting tool.
Creating and consuming various raw data formats such as CSV and XML.
Experience in programming and/or scripting in languages such as PowerShell, PHP, Python, or Java including scripting to APIs or web services for data acquisition.
Interpreting and administering policies and procedures sufficient to administer, discuss, resolve, and explain them to staff and other constituencies.
Demonstrating a positive customer service orientation with both internal and external clients.
Responding to general inquiries and in effective oral and written communication.
Working cooperatively with other departments, Agency officials, and outside agencies.

Physical Demands/Work Environment:

Physical Demands / Work Environment:
Work is performed in a standard office environment.
Subject to standing, walking, bending, reaching, stooping, and lifting of objects up to 40 pounds; may occasionally be exposed to dangerous machinery, extreme temperatures, and extreme noise when working in the data center environment or when working in the field.
The Agency promotes a safe and healthy work environment and provides appropriate safety and equipment training for all personnel as required.

Sound Transit is an equal employment opportunity employer. No person is unlawfully excluded from employment action based on race, color, religion, national origin, sex (including gender identity, sexual orientation and pregnancy), age, genetic information, disability, veteran status or other protected class."
Data Solution Engineer,Pharmaceutical Strategies Group,"PSG has a 20-year legacy of delivering innovative expertise and solutions for pharmacy. PSG is the largest independent pharmacy programs consulting firm in the country, with a breadth of knowledge and experience unmatched in the industry. Additionally, PSG has market leading technology and administration services for hospitals and health systems participating in the 340B program.

The successful candidate will have a solid understanding of SSIS and SQL Server, as well as a solid history of designing and building automated data ETL solutions utilizing Microsoft technologies.

Essential Duties & Responsibilities
Works with software development, ETL and QA teams to perform product design, implementation, defect verification and remedy on data automation projects.
Works directly with Product Management team to implement against product requirements, and solicit more information as required.
Identifies opportunities for improving ETL processes and procedures and communicates recommendations appropriately.
Works within Scrum team environment to deliver on commitments within the product release cycle.
Scope of responsibilities/ownership will span multiple features; owning them from conception to delivery.
Competencies
Effectively applies broad, in-depth, and up-to-date knowledge of pertinent software development; project management; and technical, business, and professional issues.
Visualizes complex processes to identify and analyze key issues and recommend quality solutions. Able to participate in development of project estimates, decision analysis, and project schedules.
Demonstrates excellent written and verbal communication skills. Listens effectively, transmits information accurately and understandably, and actively seeks feedback. Effectively presents and explains information to others with various levels of knowledge.
Well-organized, self-directed team player. Remains open to others ideas, and exhibits willingness to try new things.
Acts as a mentor to junior team members and regularly brings new approaches and ideas to the table.
Adapts to changes in the work environment.
Requirements
BS degree in a field directly related to software development or data engineering.
Minimum of 8 years of relevant technical experience in software engineering and/or an equivalent combination of education and experience.
Minimum of 3 years in healthcare industry, particularly handling Rx/medical claims data.
Ability to write and debug complex stored procedures in Transact SQL and considerable experience within SSIS.
Experience with SQL Server 2016.
Experience working in secure environments, handling PHI data.
Preferred
Experience with Agile and Kanban development methodologies.
Basic C# scripting abilities for use within SSIS.
Experience with TFS version control.

Salary and Benefits

Salary based on background and experience. Benefits include full medical, pharmacy, dental, vision, life & disability insurance, paid vacation."
Simulation Data Engineer,CGI,"Find your fit at CGI! We are seeking a Simulation Data Engineer. This opportunity offers the flexibility to support operations remotely.

Your future duties and responsibilities: Develop the meta and modeling data for simulations. Coordinate with scenario team to ensure data accuracy and the simulation team to ensure data validation and verification.

Required qualifications to be successful in this role: A Bachelor's Degree AND 1 year experience are required for this role.

The ideal candidate will have an understanding of meta-data and simulation data: how to find it and how to use it effectively.

Desired qualifications:

Prior modeling and simulation experience desired, especially experience with Athena.

What you can expect from us: Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this changesupporting our clients digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer.

Qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, gender Identity, sexual orientation, national origin, age, disability, veteran status, pregnancy, or other status protected by law. CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information.

Have you been referred by a CGI Member for this position?* Yes No"
Senior Data Engineer/Big Data/Hadoop,JP Morgan Chase,"As an experienced Software Engineer, your mission is to help
lead our team of innovators and technologists toward creating next-level solutions
that improve the way our business is run. Your deep knowledge of design,
analytics, development, coding, testing and application programming will help
your team raise their game, meeting your standards, as well as satisfying both
business and functional requirements. Your expertise in various technology
domains will be counted on to set strategic direction and solve complex and
mission critical problems, internally and externally. Your quest to embracing
leading-edge technologies and methodologies inspires your team to follow suit.
And best of all, youll be able to harness massive amounts of brainpower
through our global network of technologists from around the world.

This
role requires a wide variety of strengths and capabilities, including:

Expertise in application, data and infrastructure
architecture disciplines

Advanced knowledge of architecture, design and business
processes

Keen understanding of financial control and budget management
Ability to work collaboratively in teams and develop
meaningful relationships to achieve common goals

Masters or Bachelors in Computer Science with 5+ years of experience in big data and
analytics.

Strong CS fundamentals, data structures, algorithms with
good understanding of big data

Experience with Hadoop data management including
technologies like Apache sqoop, SQL, Hive, HBase to name a few.

Experience with RDBMS technologies including Oracle
Land data from various firm
sources into big data warehouse

Design and tune schema for
data landed on platform

Partner with information
modelling teams on firmwide logical data models.

Primary subject matter expert (SME) for data in
analytics platform.

The Macro BIG technology team is part of
the Macro front office technology that supports the global Sales, Trading,
eCommerce and research organizations across FX, Rates and Commodities lines of
business. The group is responsible for building analytics and the data
platform. This platform delivers insight to sales and trading to optimize sales
and trading decision making in addition to strategic decision making

The
BIG group is hiring data engineers to help us build our next generation
business analytics platform. We will land financial data from various front
office and back office platforms and will be the primary source of data for
running business analytics. This is a great
opportunity to work at the intersection of critical front office facing data
analytics and big data technologies

Our Corporate & Investment Bank relies on
innovators like you to build and maintain the technology that helps us safely
service the worlds important corporations, governments and institutions.
Youll develop solutions for a bank entrusted with holding $18 trillion of
assets and $393 billion in deposits. CIB provides strategic advice,
raises capital, manages risk, and extends liquidity in markets spanning over
100 countries around the world.

When you work at JPMorgan Chase & Co., youre not just working at a global
financial institution. Youre an integral part of one of the worlds biggest
tech companies. In 14 technology hubs worldwide, our team of 40,000+
technologists design, build and deploy everything from enterprise technology
initiatives to big data and mobile solutions, as well as innovations in electronic
payments, cybersecurity, machine learning, and cloud development. Our $9.5B+
annual investment in technology enables us to hire people to create innovative
solutions that will not only transform the financial services industry, but
also change the world.

At JPMorgan Chase & Co. we value the unique skills of every employee, and
were building a technology organization that thrives on diversity. We
encourage professional growth and career development, and offer competitive
benefits and compensation. If youre looking to build your career as part
of a global technology team tackling big challenges that impact the lives of
people and companies all around the world, we want to meet you"
Data Engineer,IBM,"Job Description

We are in a data science renaissance.

Companies that embrace data science will lead and those who do not will fall behind.

To help IBM's clients lead, we are building an elite team of data science practitioners to help them learn how to succeed with data science. The team will include data engineers, machine learning engineers , operations research / optimization engineers and data journalists.

The team will engage directly in solving real-world data science problems in a wide array of industries around the globe with IBM clients and internally to IBM. The elite team of data professionals will work with other IBMers and client data science teams to solve problems in banking, insurance, health care, manufacturing, oil & gas and automotive industries, to name a few. We will teach our client data science teams how to execute on key responsibilities.

Key Responsibilities :
Design and implement optimal data pipeline architectures
Assemble large, complex data sets that meet business requirements
Identify, design, and implement internal process improvements: including process automation, optimizing data delivery, etc.
Design optimal ETL infrastructures from variety of data sources
Incorporate governance processes and tools into the data landscape
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with executive, LOB, design and IT stakeholders on data-related technical issues and infrastructure needs
Keep data separated and secure across national boundaries through replication and failover techniques
Guide and mentor clients to become self-sufficient practitioners Preferred Work Location: New York

While working across all these industries, you will also get to travel the World as these engagements will require that the team spend several weeks at client sites working on data science problems with a diverse team.

As a member of the team you will have a T-shaped skill set, having a broad knowledge base in Data Science and Industry Solutions in general, but also in- depth expertise in data engineering.

Required Technical and Professional Expertise

At least 4 years experience with relational SQL and NoSQL databases, including DB2, Oracle, Postgres, Cassandra
At least 4 years experience with big data tools: Hadoop, Spark, Kafka, etc.
At least 3 years experience with data pipeline and workflow management tools: InfoSphere, Informatica, etc.
At least 3 years experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
At least 3 years experience working on Agile teams
At least 1 years experience working with stream processing, message queuing

Preferred Tech and Prof Experience

Strong analytic skills related to working with unstructured datasets
Demonstrated examples of root cause analysis on internal and external data and processes that address business questions and improvements
Experience with at least 3 of the following: SQL, Java, Python, Scala or Ruby
Experience in team leadership, project management
Training in design thinking
Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Senior Data Engineer,Microsoft,"The Azure Customer Advisory (AzureCAT) is an elite team of engineers and architects within the Cloud and Enterprise engineering organization that works with interesting and strategic customer deployments on Azure.

By joining this team, you will have the opportunity to work closely with the Azure Engineering teams to spearhead customer adoption of new and bleeding edge technologies. You will engage with customers in lab environments for deep technical learnings on new products and services. You will have the opportunity to greatly impact the overall direction of the platform through early customer engagements. Your insights from these customer engagements will influence future product releases. You will be required to present technical challenges, solutions and guidance to audiences including senior customer executives, conference attendees, and leadership.

Required qualifications for this role:
5+ years of experience in building or shipping products or services or building and architecting software solutions.
Preferred skills for this role include:
Bachelors or higher degree in technology, Computer Science or Engineering or related discipline
Deep experience designing, developing, tuning and troubleshooting complex data warehouse applications using MPP (massively parallel processing) technologies, such as Teradata, Netezza, Snowflake, Redshift, etc.
Exceptional communication and presentation skills.
Ability to design and architect distributed data systems, defining and recommending best practices and principles of data architecture.
Experience working with multiple top tier customer scenarios and workloads to meet customer needs for performance, scalability, latency, reliability.
Experience with designing schemas, debugging and optimizing data loading and query performance on MPP platform
Knowledge of data warehousing and data pipeline concepts, such as ETL, data integration, streaming
Knowledge of Cloud (Azure or AWS or Google Cloud) platform infrastructure to run and optimize distributed data applications. Ability and flexibility to learn and adapt to a spectrum of data technologies running on multiple platforms.
Ability to identify patterns and trends, synthesize customer feedback for actionable product improvement, and deliver to the development team
Practical experience in planning, sizing & migrating large enterprise workloads from on-premises to cloud platform.
Knowledge of other open source database technologies is an added advantage.
Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include, but are not limited to the following specialized security screenings:

Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to askstaff@microsoft.com."
"Data Scientist/Quantitative Analyst, Engineering, University Graduate",Google,"At Google, data drives all of our decision-making. Quantitative Analysts work all across the organization to help shape Google's business and technical strategies by processing, analyzing and interpreting huge data sets. Using analytical rigor and statistical methods, you mine through data to identify opportunities for Google and our clients to operate more efficiently, from enhancing advertising efficacy to network infrastructure optimization to studying user behavior. As an analyst, you do more than just crunch the numbers. You work with Engineers, Product Managers, Sales Associates and Marketing teams to adjust Google's practices according to your findings. Identifying the problem is only half the job; you also figure out the solution.

As a key member of the team, you work with engineers to analyze and interpret data, develop metrics to measure results and integrate new methodologies into existing systems.

Google is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on millions, if not billions, of users. At Google, engineers not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From AdWords to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another.

Responsibilities
Work with large, complex data sets. Solve difficult, non-routine analysis problems, applying advanced analytical methods as needed; Conduct end-to-end analysis that includes data gathering and requirements specification, processing, analysis, ongoing deliverables, and presentations.
Build and prototype analysis pipelines iteratively to provide insights at scale. Develop comprehensive understanding of Google data structures and metrics, advocating for changes where needed for both products development and sales activity.
Interact cross-functionally with a wide variety of people and teams. Work closely with engineers to identify opportunities for, design, and assess improvements to Google products.
Make business recommendations (e.g. cost-benefit, forecasting, experiment analysis) with effective presentations of findings at multiple levels of stakeholders through visual displays of quantitative information.
Research and develop analysis, forecasting, and optimization methods to improve the quality of Google's user facing products; example application areas include ads quality, search quality, end-user behavioral modeling, and live experiments.

Qualifications Minimum qualifications:
PhD degree in a quantitative discipline (e.g. statistics, bioinformatics, computational biology, computer science, applied mathematics, or similar) or equivalent practical experience.
2 years of experience in data analysis or related field.
Experience with statistical software (R, S-Plus, SAS, or similar).
Experience with databases and scripting languages (such as Python).

Preferred qualifications:
1 year of relevant work experience (i.e., data scientist role), including deep expertise and experience with statistical data analysis such as linear models, multivariate analysis, stochastic models, sampling methods.
Experience with Survey Sampling.
Ability to draw conclusions from data and recommend actions.
Demonstrated leadership and self-direction. Demonstrated willingness to both teach others and learn new techniques."
Data Engineer,ICW Group,"Defines and builds the data pipelines that will enable faster and better, data-informed decision-making within the business to improve system and process efficiencies and reduce overhead cost in terms of time and effort.

ESSENTIAL DUTIES AND RESPONSIBILITIES

Data collection, integration and management
Designs and develops scalable ETL packages and data streams, and integrates data from internal and external data sources.
Optimizes performance, implements schema changes, and maintains data standards to enhance reporting and analytics capabilities.
Ensures proper data governance and quality across data repositories and schema. Identifies best data sources among multiple sources to use for data pipelines to improve trust in data.
Incident resolution and troubleshooting
Monitors production databases for potential migration issues.
Troubleshoots incidents with data sources to minimize disruption to data architecture and data access.
Conducts training users and colleagues related to assigned projects.
Advanced analytics projects
Gathers and implements database requirements, analyzes performance and troubleshoots existing issues.
Collaborates with product management, BI and advanced analytics team in implementing various data streams.
Analyzes complex data elements and systems, data flow and dependencies in order to contribute to conceptual physical and logical models.
Catalog maintenance and documentation
Documents the design and architecture of new and existing data models.
Develops documentation and reports that provide intuitive analytics to customers.
Maintains comprehensive catalog of all the data streams, data mapping and data dictionaries
Requirements

EDUCATION AND EXPERIENCE Bachelors degree in Computer Science, Applied Mathematics, Engineering, or any other technology related field. 3+ years of experience in a data engineering related role.

KNOWLEDGE AND SKILLS Advanced skills in SQL, ETL, building and integrating with API, analytics products like Alteryx, conceptual knowledge of database modelling, large and complex data sets, system management, SQL server reporting services or similar reporting platforms, analysis services and integration services. Experience with internet technologies  SaaS, PaaS, IaaS; Tableau, Power BI or similar visual analytics platforms as well as experience with SalesForce and Data Virtualization preferred."
Data Engineer,Nielsen,"962
Technology and Engineering - USA Richardson, Texas

Nielsen Panel Engineering measures mobile and PC usage for 400,000 PCs and 80,000 mobiles across several countries. The Engineering team is responsible for the metering application, collection of raw logs, processing and applying business rules to define mobile application and web usage. This is a challenging environment to add more products and value while maintaining the technologies in the ever evolving mobile environment. This team is part of a wider Digital organization to ultimately measure all digital usage. Nielsen Engineering is seeking a strong Data Engineer with proven background in Engineering data analytics, using a wealth of data to proactively find and diagnose issues from devices, software, industry

The candidate will perform the role of data analytics in the engineering systems. The role will work in the existing java/SQL /Oracle data environment and also work as part of the team as we migrate to AWS technologies. A strong focus in bringing in near real-time monitoring, and systems to continuously check products systems and metrics health. The candidate must demonstrate deep understanding of logic for using analytics for troubleshooting issues. Experience on current web and database technologies as well as competency to work on the most complex projects. There is a particular focus for SDK measurement via panel devices

This position works on broad, highly visible, software development projects in an extremely complex and evolving technical and business environment driven by committed client deliverables. Key responsibilities also include hands-on analysis and correlation of database data to support business/client inquiries, resolution of incidents plus the ability to proactively identify issues using data. The ideal candidate will possess excellent external engagement and communication skills as well as the technical background to fluently interact and provide guidance to software developers and support engineers.

Responsibilities:
Mining through millions of data records to find root causes for issues that arise within data collection.
Function as a hybrid between data collection and data engineering.
Upon finding issues within data collection, work to not only find the root cause but to suggest possible fixes to the problem.
Work on both PC and mobile platforms.
Past experience working with Data Mining/Investigation on both Oracle and SQL Server platforms.
Possess Subject Matter Expert (SME) knowledge for data analytics
Manage the impact analysis for changes introduced to the product
Drive the team for more advanced real time monitoring
Support the flow of Nielsen SDK data collected via the panel
Engage other areas and stakeholders within Nielsen including Business, R&D, Application Development, Measurement Science, Operations, Infrastructure and Support.

Qualifications:
5+ years of experience in digital engineering with an emphasis of data analytics
Extensive and practical experience and knowledge specifically on software development technologies, principles, methodologies, tools, practices, industry standards and trends
Demonstrated experience with building and maintaining large-scale, complex application systems in a team environment
Strong background Oracle / SQL as well as working with relational databases, techniques, data modeling and associations
Strong background in monitoring tools
Experience in AWS technologies / migration is preferred
Knowledge of Unix and Windows environments plus basic scripting
Experience in Agile software development methodology
Proven experience in working with a technical team spanning different geographic locations (US EST, US PST, India)
Demonstrated leadership skills: proven ability to direct, mentor and provide technical leadership to a team of developers, deliver entire end-to-end solutions using a process-oriented focus and able to make independent, sound decisions
Must be able to assess existing systems and design a strategy to improve performance, efficiency and monitoring
Track record for being detail-oriented with a demonstrated ability to self-motivate and follow-through on projects
Strong problem-solving skills; the ability to analyze problems and develop actionable and appropriate tactical plans quickly with commitment to excellence
Excellent verbal and written communication and presentation skills
Exceptional interpersonal and relationship management skills
Ability to communicate effectively with both technical and non-technical audiences
Ability to succeed in a fast-paced, innovative, and rapidly evolving industry and business organization

About Nielsen:
Nielsen N.V. (NYSE: NLSN) is a global performance management company that provides a comprehensive understanding of what consumers Watch and Buy. Nielsens Watch segment provides media and advertising clients with Total Audience measurement services across all devices where content  video, audio, and text  is consumed. The Buy segment offers consumer packaged goods manufacturers and retailers the industrys only global view of retail performance measurement.
By integrating information from its Watch and Buy segments and other data sources, Nielsen provides its clients with both world-class measurement as well as analytics that help improve performance. Nielsen, an S&P 500 company, has operations in over 100 countries that cover more than 90 percent of the worlds population. For more information, visit www.nielsen.com
Nielsen is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class.
Job Type: Regular
Primary Location : Richardson , Texas
Seconday Locations: , , ,
Travel: Yes, 5% of the Time"
Data Engineer,Classy,"Position Description:
We want you to help us establish Classy as THE thought leader in social impact and fundraising. Help us build our dataset and leverage it to publish reports that support awesome organizations achieve their goals to do good.
Were looking for a data engineer who understands that the key to great analytics and machine learning is a clean, diverse dataset, and has experience building great datasets from challenging and varied sources:

Accessing / querying a variety of data sources (relational databases, no-SQL databases, Salesforce, csvs, JSON APIs, web scraping).
Filtering and transforming such data to dedupe records and clean the individual data fields.
Cross-referencing data with public datasets or paid APIs, to augment the dataset.
Ability to use AWS tools, such as Lambda (via serverless), Cloudformation, and Redshift.
You will work on a small, talented team of web developers, data analysts, and data scientists to help build the best dataset possible to guide our business and help guide our customers in online fundraising. With this dataset, well aim to answer questions such as:

What charities are most likely to succeed on our platform?
What factors predict whether a fundraising campaign is successful?
How do we identify top fundraising users as early as possible?
Can we automatically score how good a users text and images are for fundraising?
Much, much more!
You will have chances to leverage your existing skills while learning new ones, in a fast-paced startup environment. You will be among the first to work with our unique in-house dataset, to help discover new insights in how Classy can best support organizations and their supporters to do good in the world.

Who You Are:
Basic requirements:
2+ years of work experience in programming, data scraping, and data querying (preferably all three).
Prefer individuals who have completed one or more self-led data science projects, from data aggregation to publishing text and visualizations (the more diverse the data sources, the better!).
Great attitude
Eagerness to learn
Motivated self-starter
Excellent communicator
Desirable traits also include:
Ability to query such data to build analytic reports (e.g. MySQL, MongoDB, Pandas/Python, Node/Javascript).
Using appropriate statistical / machine-learning methods to analyze the data
Publishing insights (visualizations and text, internal presentations & blogging)
Prototyping tools (via web programming) to expose these insights internally and/or externally (to non-profit organizations)
Our awesome perks!

New expansive office in downtown San Diego overlooking Petco Park baseball stadium and the ocean
29 days off annually including vacation + holidays
10 half-day Fridays a year
20 hours paid volunteer time off
Monthly parking pass, public transit pass, or ride-share allowance
Employee Stock Options + health benefits available day one
Pet friendly office (woof!)
Compensation will be competitive and commensurate with experience, including equity in a SaaS company backed by prominent technology investors, including JMI Equity, Mithril, Salesforce Ventures, and Bullpen Capital. Classy has raised $53 million to-date in growth capital. For more information, visit www.classy.org ( http://www.classy.org/ ) or follow Classy on Twitter @Classy
About Classy:
Classy ( http://www.classy.org/ ) is a social enterprise that creates world-class online fundraising tools for nonprofits, modernizing the giving experience to accelerate social impact around the world  like curing disease, responding to disasters, providing access to education and other inspirational programs. Classys goal is to be a growth engine for nonprofits and social enterprises, helping them engage with supporters through a remarkable giving experience. Since 2011, fundraising on the Classy platform has doubled each yearresulting in thousands of nonprofits collectively raising over half a billion dollars.

Did you know that over $390 billion is donated annually to U.S. nonprofits, yet only a small fraction of this number is donated online? This massive market has been devoid of significant innovation in technology for decades - with most modern technology companies focusing elsewhere. Therefore, this market is ripe for disruption.

Classy is a VC-backed SaaS company headquartered in San Diego, boasting a culture that is exciting, result-oriented, and above all else, committed to our mission. Our customers deserve a technology solutionand a team that can transform the giving experience. Wed love for you to be a part of it.

Today, the 225-person Classy team services over 3,000 nonprofit customers and also hosts the Collaborative and Classy Awards to recognize excellence in social innovation."
Data Engineer,ICF,"This is a cross functional position with significant responsibility and opportunity to shape the future direction of our team. The selected candidate will work with our Data Scientists and Software Delivery Teams to code within SQL Server to collect, store, validate and process large datasets in a Microsoft SQL Server environment. This position will be part of a small analytical team working on quantitative software products and services so experience working in a product development environment is desired. Successful candidates will be very analytical and detail-oriented to ensure a stable, repeatable, and accurate production data stream. Successful candidates will also enjoy a highly independent work environment where they can take a high level of ownership of their accomplishments.
Key Responsibilities:
 Develop and execute T-SQL code to facilitate data movements and ETL processing
 Develop and execute stored procedures to report on processing statuses and data conditions
 Design and code workflows, create database releases, and optimize production data pipelines to ensure data accuracy
 Perform hands on data research to provide insights and uncover potential data issues
 Map data feeds and combine them with third party content ensuring consistency
 Develop data processing T-SQL Code with a focus on consistency, reliability, and accuracy
 Develop and code operational processes to automatically report on data conditions and KPIs during processing
 Be a part of fast moving development teams using agile methodologies

Qualifications

Basic Qualifications:

 8+ years of professional database solution development with Microsoft SQL Server
 Strong Analytical and reasoning skills that result in clear, robust, and accurate solutions
 Strong data debugging skills
 5+ years of experience working within the full lifecycle SDLC as part of a software product and operations team environment
 Experience developing SQL scripts, data processing processes, and performance tuning quantitative queries
 Superior organizational and end to end delivery skills in an Agile environment
 Ability to manage multiple tasks and shifting priorities with proactive communication in mind
 Ability to maintain strong working relationships with development and business teams based on a proactive communication style

Additional Preferred Skills/Experience:
 Experience with PowerShell scripting
 Ability to work in a distributed team environment
 Experience working on quantitative software products
 An outstanding academic record with a focus on Energy, Science, Math, and Engineering or similar.

Professional Skills:

 Excellent listening, interpersonal, written, oral, and phone communication skills
 Highly self-motivated and self-directed to solve complex technical problems
 Ability to exercise independent judgment
 Builds and maintains relationships with stakeholders to ensure buy-in and adoption of technology solutions
 Self-motivated to continuously improve technical and professional skills
 Ability to effectively prioritize and execute tasks

ICF offers an excellent benefits package, an award winning talent development program, and fosters a highly skilled, energized and empowered workforce.

ICF is an equal opportunity employer that values diversity at all levels. (EOE  Minorities/Females/ Protected Veterans Status/Disability Status/Sexual Orientation/Gender Identity)

Working at ICF
Working at ICF means applying a passion for meaningful work with intellectual rigor to help solve the leading issues of our day. Smart, compassionate, innovative, committed, ICF employees tackle unprecedented challenges to benefit people, businesses, and governments around the globe. We believe in collaboration, mutual respect, open communication, and opportunity for growth. ICFtogether for tomorrow.

About ICF
ICF (NASDAQ: ICFI) is a global consulting and technology services provider with more than 5,000 professionals focused on making big things possible for our clients. We are business analysts, policy specialists, technologists, researchers, digital strategists, social scientists and creatives. Since 1969, government and commercial clients have worked with ICF to overcome their toughest challenges on issues that matter profoundly to their success. Come engage with us at icf.com.

Primary Location : United States-Colorado-Louisville"
Big Data Engineer,GEICO,"At GEICO, were about more than just insurance. Technology is at the core of everything we do, which means youll get to work on innovative and influential tech projects in a successful company. Our associates are top-notch tech pros who are changing peoples lives  just ask Forrester and Dynatrace-Keynote whove awarded our teams with top honors.
Our company culture is simple to sum up, its all about giving our customers the very best. And, we know that the best way to do that is by giving our associates that crucial edge to succeed. Our team leaders and managers truly believe in the power of mentorship, and we host a number of monthly professional certification classes and user groups to keep your skills sharp.

The Decision Sciences and Transformation (DSaT) team is seeking a driven Big Data Engineer to work on the Single View of the Customer (SVOC) initiative. The SVOC project will focus on leveraging Big Data and new models to significantly improve personalization and customization for our customers. At the same time, SVOC will enable GEICO to optimize our internal processes to make our systems substantially more efficient.

So, how are we going to do this? The SVOC system will integrate with a large number of GEICO applications and will ultimately help make nearly all of our systems ""smarter"". And thats where you come in  when you join the team, youll bring:

- Minimum experience of 5 years
- Real project experience as a Data Wrangler/Engineer across design, development, testing, and production implementation for a Big Data Project
- Minimum of 1 year experience on Spark. Exposure to Spark Streaming and MLLib preferred
- Minimum of 2 years of experience on Hadoop AND MapReduce AND Oozie AND Hive AND Pig
- Minimum of 2 years of experience on core Java OR Scala
- Expertise in performance tuning of Hive and Hadoop
- Exposure to Python OR iPython OR any other Scripting language
- Experience with NoSQL databases, such as HBase OR Cassandra OR MongoDB
- Exposure to Big Data Exploration, Profiling, Quality and Transformation
- Proficient in designing efficient and robust ETL/ELT workflows, schedulers, and event-based triggers
- Ability to quickly learn, adapt, and implement Open Source technologies
- Exposure to Data Mining preferred
- Ability to work independently with limited supervision as well as contribute to team efforts is required
- Strong critical thinking, decision making, troubleshooting and problem solving skills
- Outstanding time management skills and attention to detail
- Ability to support multiple projects simultaneously and work in a fast-paced environment
- Excellent verbal/written communication skills, including communicating technical issues to non-technical audiences
- Ability and desire to learn new skills and techniques
- A Bachelor's degree in a computer related field or equivalent professional experience is required

Let us know if youve also got experience with:
- Scrum methodologies

About GEICO

For more than 75 years, GEICO has stood out from the rest of the insurance industry! We are one of the nation's largest and fastest-growing auto insurers thanks to our low rates, outstanding service and clever marketing. We're an industry leader employing thousands of dedicated and hard-working associates. As a wholly owned subsidiary of Berkshire Hathaway, we offer associates training and career advancement in a financially stable and rewarding workplace.

Our associates' quality of life is important to us. Full-time GEICO associates are offered a comprehensive Total Rewards Program*, including:
401(k) and profit-sharing plans
Medical, dental, vision and life insurance
Paid vacation, holidays and leave programs
Tuition reimbursement
Associate assistance program
Flexible spending accounts
Business casual dress
Fitness and dining facilities (at most locations)
Associate clubs and sports teams
Volunteer opportunities
GEICO Federal Credit Union

* Benefit offerings for positions other than full-time may vary.

GEICO is an equal opportunity employer. GEICO conducts drug screens and background checks on applicants who accept employment offers."
Freelance Data Engineer,New York Media LLC,"New York Media energizes people around shared interests, igniting important conversations with a cosmopolitan point of view and providing the map to shrewdly navigate a fast-moving culture. We want to be a beloved habit for people around the world, with groundbreaking journalism at our core. By connecting our consumers to indispensable content and experiences, our media becomes the starting point from which we can provide innovative offerings across multiple platforms.

The Product team analyzes and creates the platforms that connect New York Medias content with our audience. Were adept problem-solvers and visionaries who can transform creative ideas into successful products.

The focus for this temporary Data Engineer role is to provide a solid foundation of data to support the expansion of New York Medias digital audience. The top priorities are to build out requirements that track key trends in our websites metrics and find opportunities to drive consumer engagement. This position provides analysis and reporting relevant to guiding improvements to our websites and marketing efforts.

The role will create and support deadline-driven reports and data tables for NYMag.com , Vulture.com , TheCut.com , SelectAll.com , and much more. You'll enjoy the company of kind, smart people while creating the responsive web and learning every day how to make it a better place.

Key Responsibilities
Help implement new reporting tools deemed necessary for the advancement of audience understanding. Use regular and ad hoc reporting to discover actionable audience metrics.
Work with the Product Development team to create and automate reports that our Data Team relies on for analysis.
Build out data pipelines and update current queries to optimize processes.
Help implement new reporting tools deemed necessary for the advancement of audience understanding to help the Data Team discover actionable audience metrics via regular and ad hoc reporting.
Build out a data dictionary for the Technology Teams to use to understand the schemas and relationships between data sets.
Role Requirements
5+ years of experience in managing ETL processes and a deep understanding of SQL
Strong Experience with Google Cloud Platform to send daily reports into spreadsheets for the various editorial, ecommerce, and executive teams
Skilled in automating health checks and quality assurance for the various datasets and reports
Experience with web data sets like Google Analytics and DFP log files
Experience with affiliate marketing data
New York Media is a place where you can be yourself and make an impact. Our team members are creative and collaborative, and we want to be just as engaged and diverse as our audience. We know that cultivating diversity and fostering an inclusive work environment is crucial in maintaining our strength as a prominent media organization.

We create an environment where no individual is advantaged or disadvantaged because of their background. We offer equal opportunity employment regardless of race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, or protected veteran status.

With a commitment to maintaining a bias-free environment in which harassment is prohibited, we respect cultural diversity and comply with the laws of the places in which we operate. We expect our business partners, suppliers, clients, and all of our team members to uphold these commitments."
Software Engineer,EDF Renewable Energy,"Background:
EDF Store & Forecast was founded in 2014 as a spin-off from Electricit de France (EDF) Group. We develop autonomous control software and forecasting products to optimize the operation of local electric systems, such as microgrids, grid batteries, and renewable energy projects. We have successfully deployed software solutions for 36.5 MW of renewable energy + battery projects across the world, with another 74.6 MW to be deployed in 2018. Our team is made up of 15 software engineers, data scientists, and energy experts. We are now growing our presence within the U.S. market and have an open position for a Software Engineer in San Diego, CA. The candidate will work closely with EDF Renewable Energy, another EDF subsidiary that is a leader in renewable project development in the U.S.

Who you should join us:
This position offers the opportunity to be on the front-lines of the rapidly changing renewable energy industry. Renewable energy has enormous economic and environmental benefits, but in an increasingly competitive market, projects require advanced software solutions based in artificial intelligence. EDF Store & Forecast is a leader in this domain and benefits from the stability and reputation of our parent company, EDF Group, which is one of the largest energy companies on the planet. At EDF Store & Forecast, you will work in a startup environment where you will own your projects and make direct contributions  all while having the resources of a global energy leader.

Job description:
The Software Engineer will develop software that implements and operationalizes machine-learning and optimization models (developed by various EDF teams) that control the energy management of battery /microgrid projects in the U.S. The position will require moderate travel to France and within the U.S. (2 - 6 weeks per year).

Near-term projects / assignments :
 Adapt existing energy management software from projects in France to projects in the U.S.
 Design and implement new software architecture for existing models
 Develop web-based system for launching existing models
 Design and develop IT/communication infrastructure for a pilot project in San Diego
 Quality assurance/ testing of existing models

Required qualifications:
 Bachelors degree in Computer Science (Masters a plus)
 3 years of experience in developing, deploying, and maintaining commercial software in at least one of the following languages: Python, Go, Java, C++
 Demonstrated experience with front-end web development using Javascript (w/ Angular, React, or other framework), HTML, CCS and back-end development using Python w/Django or other (i.e., Node JS, PHP, etc.)
 Experience with SQL databases: SQLite, MySQL, PostgreSQL, NoSQL
 Comfortable working in a start-up environment where priorities shift on a regular basis. Autonomous, rigorous, and well-organized. Result/goal oriented. Punctual with deliverables. Ability to manage multiple projects.
 Comfortable writing/ presenting technical material to audiences with diverse backgrounds
 Preferred experience
o IT / cyber security: Firewall, VPN
o Networks: Ethernet, IP, configuring DNS/NTP /SMTP on servers
o Working with AWS highly preferred

We are proud to be an EEO/AA employer M/F/D/V. We maintain a drug-free workplace and perform pre-employment substance abuse testing."
Senior Data Engineer,Capital One,"1750 Tysons (12023), United States of America, McLean, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer

As a Capital One Data Engineer, you'll be part of an Agile team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. You will participate in detailed technical design, development and implementation of applications using existing and emerging technology platforms. Working within an Agile environment, you will provide input into architectural design decisions, develop code to meet story acceptance criteria, and ensure that the applications we build are always available to our customers. You'll have the opportunity to mentor other engineers and develop your technical knowledge and skills to keep your mind and our business on the cutting edge of technology. At Capital One, we have seas of big data and rivers of fast data.

Who you are:
You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule
Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results
You love to learn new technologies and mentor junior engineers to raise the bar on your team
It would be awesome if you have a robust portfolio on Github and / or open source contributions you are proud to share
Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.
The Job:
Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art, next generation Big Data & Fast Data applications
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Open Source frameworks like Apache Spark, Apex, Flink, Nifi, Storm and Kafka on AWS Cloud
Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift
Utilizing Hadoop modules such as YARN & MapReduce, and related Apache projects such as Hive, Hbase, Pig, and Cassandra
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance
Basic Qualifications:
At least 3 years of professional work experience in data warehousing / analytics
At least 3 years of ETL design, development and implementation experience
Preferred Qualifications:
Master's Degree
2+ years of Python development experience
2+ years of Agile engineering experience
2+ years of experience with the Hadoop Stack
2+ years of experience with Cloud computing (AWS)
2+ years Java development experience
4+ years of scripting experience
4+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)
4+ years of UNIX/Linux experience
What we have:
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
A startup mindset with the wallet of a top 10 bank
Monthly innovation challenges dedicated to test driving cutting edge technologies
Your choice of equipment (MacBook/PC, iPhone/Android Device)
At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,Hollar,"Who we are:
Hollar is the ultimate mobile destination for cool products at incredible deals starting at $1. Featuring thousands of items from toys and electronics to home, beauty and apparelincluding unique curated gift and party collectionsHollar is all about the joy of discovery.

With $80 million in venture capital funding from top investors such as Kleiner Perkins Caufield & Byers, Lightspeed Venture Partners, Index Ventures and more, Hollar is primed to revolutionize the extreme value industry.

Our Headquarters is 10 minutes from Downtown LA, 20 minutes from Mid-Wilshire and 20 minutes from Pasadena.

Help us build the next revolution in retail. Join our growing and collaborative team!

What were looking for:
We are looking for a Data Engineer that will work on the collecting, storing, processing, and analyzing of sets of data. The primary focus will be coding, pulling data, translating it for analysts and developing optimal solutions and processes to use for these purposes. You will also be responsible for integrating new processes with the warehouse data systems used across the company. If youre someone with strategic vision and a desire to work for a dynamic and fast-growing eCommerce startup, this role may be for you.

What youll do:
Develop and support Hollars warehouse data systems and BI tools
Improve and optimize data flow processes and scripts
Create new dashboards and reports to support major database information
Communicate effectively within and across teams
Youll love this job if youre:
A metrics driven go-getter with an aptitude for meeting aggressive business goals
A self-starter who loves collaboration. Youre comfortable working independently as well as influencing cross-functional teams
Passionate data cruncher. You're passionate about getting data available for other peoples use
Enthusiastic and motivated. Youre excited to share ideas and find creative ways to improve processes
What youll need:
Bachelors degree
2-3 years working in business analyst capacity, eCommerce a plus
Top-notch technical and SQL skills
Hands-on experience with open source tools (Pentaho, Zeppelin)
Proven experience with programming/scripting languages (i.e. Python, Javascript, Java, Scala) and Linux
Strong understanding of data warehouse concepts and practices
Open source background preferred
Practical familiarity with standard RDMS
Whats in it for you:
Comprehensive benefits package
401(k) retirement plan
Fitness reimbursement
Unique and fun culture!
Dog friendly office
Fully stocked kitchen and weekly catered lunches"
Data Engineer,RiskIQ,"RiskIQ is the leader in digital threat management, providing the most comprehensive discovery, intelligence, and mitigation of threats associated with an organizations digital presence. With more than 80 percent of attacks originating outside the firewall, RiskIQ allows enterprises to gain unified insight and control over web, social, and mobile exposures. Trusted by thousands of security analysts, RiskIQs platform combines advanced internet data reconnaissance and analytics to expedite investigations, understand digital attack surfaces, assess risk, and take action to protect business, brand, and customers. Based in San Francisco, the company is backed by Summit Partners, Battery Ventures, Georgian Partners, and MassMutual Ventures

Our Data Science team is at the core of advancing our technologies that will compose our next generation of internet and mobile security products. A successful candidate will be a highly motivated team member with a passion for solving the challenging problems our business faces. As a Hadoop Engineer, you will design, build, and scale our Hadoop ecosystem, which empowers our Data Science team to deliver game-changing intelligence from both existing and new/novel data sources. Relocation is available for the right candidate.

Your responsibilities will include

Participate in architecting and building scalable applications using elements of Hadoop ecosystem.
Determining feasibility and rapidly developing proof of concept code based on ideas that are produced from the Data Science team.
Partnering with our Engineering team to implement successful products into the RiskIQ platform.
Work with our DevOps team to scale our data infrastructure.
Creating new ways to extract interesting insights from existing research analysis.
Analyzing, designing, developing, and testing of your projects to meet deadlines and exacting standards.
Requirements

BS or higher in Applied Math, Statistics, Computer Science, Physics, or related field
Senior Developer with 6 years overall experience in Java technologies and Big Data.
Proven ability to build large, reliable, scalable infrastructure
4+ years of advanced, hands-on experience with the Hadoop eco-system:
MapReduce
Hive
MapR
HDFS
HBase
Spark
Strong SQL experience to analyze, transform, and integrate high volume, complex data sources.
Extensive knowledge of Data Infrastructure at both the physical and the logical level
Excellent communication skills with the ability to identify and communicate data driven insights and technical approach through conversation, documentation, and presentation.
Proficiency with the following:
Job orchestration systems (e.g. Oozie, Luigi, Airflow)
Advanced file formats (e.g. Parquet, Avro, ORC)
Stream processing systems (e.g. Spark, Storm, Flink)
Search engines (e.g. ElasticSearch, Solr, Lucene)
Desired experience with some of the following technologies:
Kafka
Cassandra
Redis
Machine learning experience
Amazon Web Services or other cloud-based providers/solutions
Why work at RiskIQ?

Fascinating work - Welcome to the dark underbelly of the Internet. We detect, expose, and investigate malware, exploit kits, botnets, affiliate fraud, advertising fraud, and illicit mobile apps, and much more. It is the golden age of internet crime, and we are at the forefront of defensive efforts to stem the tide. Internet security is a global growth industry, and the knowledge you acquire here will be a marketable skill for decades to come.
Were a company on the forefront of a burgeoning industry - We've recently celebrated several new milestones headlined by 80% year-over-year growth revenue growth, the closing of $30.5 million in Series C funding, and recognition by Forrester in its Forrester Wave: Digital Risk Monitoring, Q3 2016 report, which named RiskIQ a leader.
Top Leadership - Our CEO is a renown cybersecurity veteran known for his expertise. Our leadership group is poised and experienced with a track record in technology and cyber security.
Unbounded opportunity - We are small, but were growing. At RiskIQ, youll be provided with as much responsibility as you can handlenew career development opportunities constantly arise given our rate of growth Want to design a new data center from the ground up? Architect a big data backend to increase our storage and analysis capabilities? These challenges are yours for the taking if you prove you're capable.
Flexibility - Youll have a large workload, but also the freedom to accomplish it on your own terms. RiskIQ has unlimited PTO and flexible hours."
Data Engineer,Collective Health,"Collective Health is seeking an experienced Data Engineer with a real passion for converting data into relevant information. We are on a mission to revolutionize health care. With a team of healthcare experts, engineers, data scientists and designers - think of us as a smart replacement to your traditional health insurance carrier. Were not building another health app or gadgetwere building a complete solution to replace your employer health insurance; all while delivering the health care experience that people deserve.

As a rapidly growing organization with complex regulatory requirements around data access, your code will serve as the glue that enables a wide variety of internal teams to access claims and member data securely and reliably. You'll also work collaboratively with health data scientists and actuaries to productize insights derived from their analysis.

Your end goal is to own, architect, and maintain our data pipelines. Your systems will help our immediate clients - employers - ensure their health care funds are spent efficiently. You will both resolve problems and teach people how to resolve their own problems (and also possess the wisdom and know-how on which approach is most appropriate). You display the people skills necessary to collaborate across teams, analytical skills and programming skills to understand data, and automate the process of arriving at insights.

If you are looking for a challenge and most or all of the following items apply to you, please apply!

Minimum 5 years experience working as a software engineer
Experience building data models, infrastructure, and ETL pipelines for reporting, analytics, and data science
Experience with AWS - including EC2, S3, EMR, RDS, and Redshift
Proficiency with SQL and relational database query performance
Knowledge of prestodb, hive, and mapreduce/hadoop is preferred
Experience with statistical analysis packages (SAS, R, python/pandas/numpy) a plus
Loves open source software, ships reliable code, and is serious about security
Collective Health is a technology company working to create the healthcare experience we all deserve. Founded in 2013, our team of engineers, designers, product managers, and actuaries are redefining the $1 trillion market of employer-sponsored health benefits with data-driven and people-focused products. Our complete health benefits solution helps great companies like Activision Blizzard, Palantir, Restoration Hardware, and Pinterest take care of their people by harnessing the power of design and technology. Based in San Francisco, CA, were backed by some of the best investors in Silicon Valley including Google Ventures, Founders Fund, NEA, and Redpoint Ventures. For more information, visit us at https://www.collectivehealth.com ( https://www.google.com/url?q=https%3A%2F%2Fwww.collectivehealth.com&sa=D&ust=1512070986645000&usg=AFQjCNHOhJtqeMBDZS6UnjZ58pJ_HkPnww ).

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Data Engineer,RAYMOND JAMES FINANCIAL,"DESCRIPTION

In an enterprise data environment, designs and develops enterprise level integration infrastructures, information pipes and services using leading data technologies. Takes action and makes decisions on software development in high exposure areas including application availability, stability, sustainability and reliability. Has extensive contact with business technology teams and other members of Architecture and Core services.

RESPONSIBILITIES

Responsible for data engineering functions including, but not limited to: data extract, transformation, loading, integration in support of enterprise data infrastructures  data warehouse, operational data stores and master data management.
Responsible for data services and data movement infrastructures.
Implements concepts of programming such as data structures, error handing, data manipulation and I/O processing. Implements database concepts and practices including definition and query language.
Prepares project plans and uses flowcharts and data flow diagramming to create program design concepts.
Creates detailed system design documentation.
Analyzes requirements and architecture specifications to create detailed design.
May provide technical advice and training and mentor other associates in a lead capacity."
Data Infrastructure Engineer,Apple,"Design and implement frameworks to manage complex workflows and monitor data quality
Design, build and deploy ETL pipelines that are efficient, reliable and easy to operate
Research and build efficient and scalable data storage and retrieval systems that enable interactive reporting on high dimensional data
Research and build the next generation data dashboard and visualization solution
Build libraries and frameworks to empower data scientists to effectively work with our data products
Key Qualifications
Strong understanding of object oriented design and experience writing Hadoop and/or Spark jobs to process large amounts of data
Proficiency in Java, Bash, Python, SQL, HDFS, and other Hadoop tools (ie Hive, Hue, YARN, etc)
Experience with data visualization is a big plus
Experience with A/B experimentation is a plus
Experience with open source projects is a plus
Aptitude to independently learn new technologies, prototype and propose software design and solutions
Excellent communication skills; you will work cross-functionally across multiple teams
Description
Maps Evaluation Metrics is responsible for defining, implementing and measuring actionable metrics summarizing the quality of algorithms, services and data. We achieve this by mining massive amount of rich data including human judgements, ground truth, and user feedback logs. We primarily work on Maps but our platform is used by other applications like Siri, iTunes and News.

As a Data Infrastructure Engineer, you will be working with some of the most unique and interesting data sets in the world including geo spatial data, probe, search logs, traffic data, human judgements and A/B experiments data. You will partner with data scientists and engineers to acquire valuable signals on where and how we have the most opportunity to improve user experience for Apple customers around the world. You will build large scale data pipelines and end-to-end analytics solutions to transform rich data at Apple scale into actionable insights that directly impact customers. As a member of a small and dynamic team, you will have significant responsibility and influence in shaping all parts of the data platform.
Education
BA, MS or PhD in Statistics, Computer Science, or other quantitative fields."
Reliability Data Engineer,Finisar,"Finisar is looking for a strong Engineer to impact the Reliability Engineering team in support of laser and photodetector development and production. This is an exciting time as our Fremont wafer fab is developing leading edge products and the necessary new technologies.

The Reliability Data Engineer will lead the Reliability team in creating new calculations, extracting and analyzing data, and generating reports. Stretch goal is to become a subject matter expert in statistical and reliability analysis. Prior experiences in Product Engineering, Statistical Analysis, Failure Analysis and Reliability are beneficial. Occasional travel may be required.

KEY JOB DUTIES & RESPONSIBILITIES:

Essential Functions & Duties
 Create accurate, compelling visualizations of reliability data.
 Create re-useable queries and jmp scripts.
 Analyze reliability data, generate reports, and make recommendations.
 Design, coordinate and perform studies to assess and improve device reliability.
 Publish Qualification Test Reports (QTRs)  tabulate and graph data, summarize results, obtain reviews, disseminate results.
 Comply with all applicable quality, health, safety, and environmental regulations.

The ideal candidate would additionally be able to perform the following duties:
 Develop new measures of devices and new parameterizations of data, towards better characterizing and predicting device reliability.
 Advise Engineers and Management on statistical and reliability methods.

SKILLS/QUALIFICATIONS:

Experience
 Experienced with SQL or similar languages, comfortable writing queries, data processing scripts, and understanding data structures.
 Experienced with jmp (such as statistical analysis, partition of variation) and jmp scripting.
 Experienced with creating calculations (e.g. MS-Excel, MATLAB, python, VB).
 3+ years of relevant experience in two or more of these areas:
o Reliability analysis of semiconductor components.
o Familiarity with semiconductor and/or laser device physics and measurements of these devices.
o Familiarity with failure analysis, its techniques, and driving improvement in products and processes.
o Understanding of failure modes and acceleration factors in reliability testing.
o Understanding of wafer fabrication and of InP-based epitaxy.

Education
 B.S., M.S. or Ph.D. in Physics, Electrical Engineering, Engineering or related fields.

Skills
 Able to visualize data effectively.
 Mathematical maturity.
 Ability to quickly learn new technologies.
 Work on multiple projects, plan own work and drive to meet strict deadlines.
 Positive inter-personal skills necessary to support cross-functional teams.
 Strong technical (written and verbal) communication skills required."
Data Engineer,PlayQ,"About Us:
PlayQ is a rapidly growing mobile gaming studio based in the heart of sunny Santa Monica. Our titles have been downloaded over 60 million times worldwide, with millions of users playing our games every day!

Our goal is to deliver tomorrow's nostalgic game experiences to a global audience and we rely on our dedicated team to craft these unforgettable games that our users love. We believe that the best games come from diverse teams with a shared desire to create something amazing. From our beachfront headquarters to our innovative tech stack, its no surprise that PlayQ was named as one of the 2016 Best Places to Work in Los Angeles by the Los Angeles Business Journal.

We're looking for people who have a genuine love for their craft and we invite you to join us in this exciting endeavor.

Job Overview:
We are looking for a Data Engineer to join our top performing team. You will work as an integral member of our Engineering team to design, implement and maintain the systems which support our mobile game titles with millions of daily players. The technical scope of the project is broad and leverages a variety of cutting-edge commercial and open source tools to get the job done. The ideal candidate has a wide range of skills including database and cloud hosting experience as well as a desire to learn new technologies.

Responsibilities:
Collaborate with development teams on design, architecture and expansion of infrastructure
Perform system configuration, administration and support for services
Collaborate with development teams to ensure smooth and reliable operation of software and systems for fulfilling business objectives and processes
Create and maintain documentation as it relates to system configuration, mapping, processes, and service records
Gauge the effectiveness and efficiency of existing systems; develop and implement strategies for improving or further leveraging these systems
Requirements:
Computer Science, and/or Math degree from an accredited college/university
4+ year operations experience with Linux environments
Solid knowledge in AWS (EC2, S3, ELB, VPC)
Experience working with bash, Python, or other scripting languages
Knowledge of Git (Github, Bitbucket, or similar)
Perks:
Competitive compensation
Comprehensive medical, dental, vision insurance
401K plan with company match, and flexible time off
Stock options/equity for highly qualified candidates
Stocked kitchen with free snacks and beverages of your choice
Penthouse office with rooftop deck offering panoramic views of Santa Monica, located 1 block from the beach
Help build and support awesome GAMES. For a living! Who doesn't love games?
Interested? Please get in touch!"
Data Engineer,XPO Logistics,"Logistics done differently.
You are a data enthusiast who loves the challenge of looking at data differently, maximizing the informational benefits. As the Systems Engineer Analyst BI at XPO Logistics, youll have the opportunity to work in data solutions and advanced analytics, as well as evolve new capabilities to increase operational efficiencies of the logistics business. We will give you the team, resources, and freedom to build a career you love.
Pay benefits and more.
We are eager to attract the best, so we offer competitive compensation and a generous benefits package, including full health insurance (medical, dental and vision), 401(k), life insurance, disability, and the opportunity to participate in a company incentive plan.

What youll do on a typical day:
Maintain a system-wide architectural view of the data solutions
Work with customers and business teams to identify relevant customer-user cases, customer needs, and business opportunities that enhance the products we support
Develop product requirement documents to communicate data product features and decisions clearly to all stakeholders; communicate business needs in clear technical terms to the technical resources, and the technical details in simple business terms to the leadership and non-technical people
Identify, evaluate, and integrate the data sources and ideal solutions that would best fulfill business requirements
Collaborate closely with the engineering team to align system design decisions to drive technical product goals
Provide solutions and guidance to the team members on technical and business issues facing them
Partner with enterprise data science team to prioritize product development priorities

What you need to succeed at XPO:At a minimum, youll need:
Bachelors degree in CS, MIS, or equivalent; successfully completed recognized data science/advanced data analytical courses
Professional experience in business intelligence/data warehouse development preferably in a medium to large data warehouse environment
Experience in delivering solutions in at least two areas: data modeling, data integration, data analysis, SQL, R/Python, and machine learning
Experience building logical data models and physical data models, and normalizing techniques with 3NF
Experience in MPP databases like Netezza or Teradata
Demonstrated success in guiding data science projects to generate business insights and recommendations
Understanding of core principles of data warehousing, data science, and machine learning
Excellent verbal and written communication skills

Itd be great if you also have:
Masters degree or PhD in data science, statistics, computer science, or related field; or equivalent work experience
10 years of professional experience in business intelligence/data warehouse development preferably in a medium to large data warehouse environment
Experience in transportation/logistics industry
Knowledge in cloud solutions; preferably in Amazon suite of products
Be part of something big.
XPO provides cutting-edge supply chain solutions to the world's most successful companies, including Disney, Pepsi, L'Oral, Toyota and many others. Were the fastest-growing transportation company on the Fortune 500 list, and were just getting started.
We are proud to be an Equal Opportunity/Affirmative Action employer. Qualified applicants will receive consideration for employment without regard to race, sex, disability, veteran, or other protected status.
The above statements are intended to describe the general nature and level of work being performed by people assigned to this classification. They are not intended to be construed as an exhaustive list of all responsibilities, duties, and skills required of personnel so classified. All employees may be required to perform duties outside of their normal responsibilities from time-to-time, as needed."
"Software Engineer, Data Product Infrastructure",Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Facebook is seeking an experienced Software Engineer to join the Warehouse Product Infrastructure team. The Warehouse Product Infrastructure team builds large-scale logging, data processing and analytics at Facebook. Our stack serves all Facebook products to monitor and make critical product decisions. We handle everything from Facebook scale logging across client and server, to metrics computation, to unified pipeline management across streaming, batch and machine learning workloads. We are looking for candidates who share a passion for tackling complexity and building platforms that can scale through multiple orders of magnitude and for enable Facebook analytics to be fast and high quality. This position is full-time and is based in our main office in Menlo Park, CA. Responsibilities
Design core, backend or frontend software components.
Code using primarily PHP, Hack, Python, Scala or Java.
Interface with other teams to collaborate in transforming the landscape.
Conduct design and code reviews.
Analyze and improve efficiency, scalability, and stability of various system resources.
Design and implement a workflow language for efficient data processing and machine learning.
Minimum Qualifications
6+ years of relevant coding experience in C,C++, Java.
2+ years relevant coding experience in PHP, Hack, Python, Java or C++.
2+ years relevant experience building logging infrastructure, metrics infrastructure or pipeline management.
Preferred Qualifications
B.S in Computer Science or related technical field.
Experience working directly with data engineer or data scientist teams.
Experience working with product team in logging instrumentation on both client and server side.
Experience working on data modeling from logging to analytics.
Experience working in building metrics computation or consumption framework.
Experience working on extracting and implementing analytics patterns."
Machine Learning Engineer,Toyota Research Institute,"The Toyota Research Institute (TRI) is an R&D enterprise designed to pursue research in artificial intelligence in order to improve the quality of human life. Our mission is to bridge the gap between fundamental research and world-scale industrial impact. TRI was launched with mandates to (1) enhance the safety of automobiles, with the ultimate goal of creating a car that is incapable of causing a crash; (2) increase access to cars to those who otherwise cannot drive, including those with disabilities and the elderly; (3) help translate outdoor mobility technology into products for indoor mobility; and (4) accelerate scientific discovery by applying techniques from artificial intelligence and machine learning. Toyota believes artificial intelligence has significant potential to improve the quality of life for all people, bringing ease, comfort and safety to all aspects of life.

As a machine learning engineer, you are at the forefront of advancing our core artificial intelligence efforts. You work alongside top research scientists in machine learning and are responsible for deploying cutting-edge machine learning techniques across truly vast amounts of data. You interact closely with our data teams to design and deploy large-scale distributed infrastructure for training and inference. You are passionate about applying machine learning to real-world problems in autonomous driving and robotics and about building the required frameworks and tools to do so.

Responsibilities:
Help create new deep learning approaches and apply them to solve a plethora of real-world problems, such as large-scale scene understanding, semantic segmentation, action recognition, etc.
Engineer big-data tools and pipelines to train, evaluate and deploy deep neural networks across TRIs datasets
Design and implement features for our distributed (cloud and bare-metal) machine learning infrastructure
Work on creating new pathways through TRIs data stack for machine learning using AWS and other compute platforms
Build smarter, more efficient ways of automating deep learning processes, including labeling, training, testing and visualization
Evaluate, adapt and apply popular deep learning frameworks for global, petabyte-scale use
Qualifications (candidate profile 1):
MS in Computer Science or related field
Qualifications (candidate profile 2):
BS in Computer Science or related field with 3+ years of relevant work experience
Proven software engineering skills in Python and/or C++
Hands-on experience with TensorFlow (preferred), Caffe, Keras, Theano or Torch
Hands-on experience with cloud frameworks such as AWS, GCP or Azure
Excited about building powerful yet efficient big-data infrastructure for machine learning"
Data Engineer,Ramsey Solutions,"Ramsey Solutions continues to grow at an incredible velocity. The key to sustaining this growth is maintaining a high-quality customer experience, evolving our digital products and services in new and engaging ways, and being innovative in how we deliver our mission. This role will be responsible for all aspects of data integrity for the B2Church business unit that supports all of our Financial Peace University products, impacting millions of families across the country.
We need someone that can help take us to the next level with data warehousing, analytics and event streaming. In this role, youll be utilizing our data platform to consolidate and design data structures for our critical business metrics and reporting systems. Your efforts will directly affect how we measure our key performance indicators, inform our sales commissions system, and track users engagement with our products. Youll be working with our business leaders and technology teams to deliver solutions that support our data analysts and web developers. We need someone who can bring their ideas to the table and help drive improvements in our data platform system.

Qualifications :
Proficient in database deployment, change management, version control, and migration tools such as LiquiBase, Flyway, or DBmaestro TeamWork
Experience with designing and implementing ETL processes using various tools such as Boomi, Informatica or Ab Initio
Experience with data virtualization including the concepts of data federation and logical abstraction
Experience with components in Hadoop ecosystem (Hive, Pig, Impala, Ambari, Oozie, Sqoop, Zookeeper, Mahout)
Experience architecting and building data fabric with Flume, Scribe or Kafka would be a plus
Strong understanding of data quality, data cleansing, data lifecycle, and metadata management
Strong database design and development experience for very large database applications
Understands and has worked in an Agile Environment (Scrum/Kanban) and agile application development techniques
Proficient in creating, maintaining, and debugging system configurations and database code
Experience with data analysis and analytics systems (eg, Hue, Zeppelin)
Familiar with Linux (Red Hat and CentOS) and/or UNIX systems and networks
PostgreSQL, Mysql, MSSQL, Mongo (or other NoSQL)
Cloud: Amazon Web Services (AWS)

Core Attributes :
Ability to use a wide variety of open source technologies and tools
Ability to code and script all things database
Comfortable with frequent, incremental code testing and deployment
Strong grasp of automation tools
A strong focus on business outcomes
Comfortable with collaboration, open communication and reaching across functional borders
Ability to communicate and express design models as well as listen to needs
Some of Our Benefits
Ongoing personal and professional development training by some of the best in the industry
40-hour workweeks (We mean it. Work-life balance is a real thing here!)
401(k) match on first 4%
PTO, sick time, and ministry time off
Wellness reimbursement to put towards hitting the gym
Being part of a company that has been named Best Places to Work in Nashville nine times
And so much more  being a debt-free company enables us to be crazy in other non-corporate ways

EOE/M/F/Vet/Disabled"
Data Engineer,Paypal,"Who we are
Fueled by a fundamental belief that having access to financial services creates opportunity, PayPal (NASDAQ: PYPL) is committed to democratizing financial services and empowering people and businesses to join and thrive in the global economy. Our open digital payments platform gives PayPals 210 million active account holders the confidence to connect and transact in new and powerful ways, whether they are online, on a mobile device, in an app, or in person. Through a combination of technological innovation and strategic partnerships, PayPal creates better ways to manage and move money, and offers choice and flexibility when sending payments, paying or getting paid. Available in more than 200 markets around the world, the PayPal platform, including Braintree, Venmo and Xoom, enables consumers and merchants to receive money in more than 100 currencies, withdraw funds in 56 currencies and hold balances in their PayPal accounts in 25 currencies.
When applying for a job you are required to create an account, if you have already created account - click Sign In.
Creating an account will allow you to follow the progress of your applications.
Note:
Provide full legal first Name/Family Name
Provide full address details
Multiple attachments can be uploaded including Resume and Cover Letter for each application
Job Description Summary:
This position is for PayPal's Enterprise Data Services (EDS) under Enterprise Core Services (ECS), an organization focused on delivering enterprise solutions using modern technology to serve the various needs of the business. At EDS, we are committed to bringing innovation, passion and customer focus to the business of enterprise solutions.

The charter of ECS is deliver on data driven companywide transformational initiatives like the data lake to integrate data from cross property seamlessly using big data technology and serve through a common model as the data glue. In this role, you will get an opportunity to work closely with Venmo stakeholders closely to integrate, orchestrate, and enable critical use case across multiple business partners all over PayPal delivering value through the power of data. This position requires an individual who is comfortable working in cross-functional teams with a very high degree of technical and product management skills.
Job Description:
Areas of focus:
Engineering: Works with PayPal business units and Product Dev teams to design, develop and deliver data solutions on one of the largest data platforms in the world.
Business Support: Supports PayPal business units by providing data in a ready-to-use form to data analysts and data scientists for Business insights, predictive analytics, machine learning, etc.
Owns and is accountable for the design and development of a Data solution feature or a Data Pipeline
Develops data pipelines/ETL code for solving various data requirements using traditional ETL tools and big data tools but also has experience & skills for building custom data pipelines.
Recognized as the go-to developer for a product or major sub-system and is seen as a leader in their specialized field.
Participates in architecture discussions, proposes and discusses solutions to system and product changes that are directly related to their area of focus.
Responsible for managing multiple Applications, providing necessary support and maintenance activities.
Should be comfortable working in an agile environment and with cross-functional teams, should have appetite to learn and be flexible to pick up new technology.
Responsibilities:
Can be relied on to deliver a data pipeline on time and to requirements, without data quality issues.
Leads the team, and contributes effectively to the success of those that they interact with regularly.
Can triage and resolve data / process issues
Helps to resolve site data base issues and SLA, ATB impacts.
Familiar with various big data technologies, open source data processing frameworks.
Good understanding of data processing, data structure optimization and design for scalability.
Qualifications:
Good understanding of REST-oriented APIs, understanding of distributed systems, data streaming,
NoSQL solutions for creating and managing data integration pipelines for Batch and Real Time Data needs.
Excellent communication and team player skills to collaborate with cross functional teams for successful delivery of Data solutions.
Understanding of version control systems, particularly GIT.
Strong analytical and problem solving skills.
Good understanding of database principles and SQL beyond just data access.
Expert in multiple Programming/scripting languages, i.e. Unix/Linux Shell Scripting, Perl, Python, C++, Java, Python, Ruby, Scala, etc.
Intermediate level knowledge on following technologies, with expertise on few of them:
- Spark with Python or Scala
- HBase
- Hive/Pig
- Scripting (Shell, Python, Java)
- Database fundamentals (mysql, MongoDB, Cassandra)
- Hadoop echo system
- Elastic search
Basic Qualifications:
Bachelor Science degree in Computer Science or equivalent
7+ years experience in software development and 3+ years as a data engineer on Hadoop/Data Lake
Excellent English written and oral communication skills
Preferred Qualifications:
Experience in large organizations including managing dependencies across business units
Master of Science or MBA degree
Subsidiary:
PayPal
Travel Percent:
0
Primary Location:
New York
Additional Locations:
We're a purpose-driven company whose beliefs are the foundation for how we conduct business every day. We hold ourselves to our One Team Behaviors which demand that we hold the highest ethical standards, to empower an open and diverse workplace, and strive to treat everyone who is touched by our business with dignity and respect. Our employees challenge the status quo, ask questions, and find solutions. We want to break down barriers to financial empowerment. Join us as we change the way the world defines financial freedom.
Paypal provides equal employment opportunity (EEO) to all persons regardless of age, color, national origin, citizenship status, physical or mental disability, race, religion, creed, gender, sex, pregnancy, sexual orientation, gender identity and/or expression, genetic information, marital status, status with regard to public assistance, veteran status, or any other characteristic protected by federal, state or local law. In addition, PayPal will provide reasonable accommodations for qualified individuals with disabilities."
Data Engineer,Weight Watchers International,"At Weight Watchers, we inspire and guide the healthier choices that transform lives. We meet our members where they are and leverage the latest nutritional and behavioral science to help them eat healthier, move more, and shift their mindset. We are a purpose-driven organization and we understand how to create community and deliver personalized services to help our members throughout every step of their journey. We have more than 50 years of history and trust to build on, and we are leveraging technology, community, and behavior science to help more people around the world reach their goals. Our approach is science-based and evidence proven to help people lose weight and develop healthy habits to live life fully. If being part of a purpose-driven, member-focused, high-performing digital culture sounds exciting to you, we invite you to explore our open positions.

Collecting data from many unique sources we are strongly positioned to derive deep knowledge on our members behaviors. Using this data we can provide improved & personalized experiences for users as well as providing insights to the executive team that will drive the evolution of our company.

To help us achieve our goals, we are seeking Data Engineers to join our team.
Description
Uniquely positioned to lead the way in the exciting HealthTech industry, Weight Watchers is rebuilding most of our core experiences and embracing modern engineering practices and techniques.
Collecting data from many unique sources we are strongly positioned to derive deep knowledge on our members behaviors. Using this data we can provide improved & personalized experiences for users as well as providing insights to the executive team that will drive the evolution of our company.
As we rebuild many of our core experiences, we are seeking talented people who are excited to join our team. This is a rare opportunity to join a company embracing a modern Technology culture, where you will have the ability to improve peoples lives in a very meaningful way and have a major impact on Weight Watchers offerings to our members. As reliance on health and wellness awareness increases, you can be part of the team that is leading the way.

To help us achieve our goals, we are seeking Data Engineers to join our team.

Some of the opportunities this role has to offer include:
Work closely with our data scientists to help build complex algorithms that provide unique insights into our data.
Build data pipelines that clean, transform and aggregate data from many different sources.
Develop models that can be used to make predictions.
Build complex functions that answer questions for the business.
Model data at rest and enable powerful data analysis.
Enable machine learning, natural language processing and other data science methods within Weight Watchers.
Work with engineers across the organization to identify data quality issues in source systems and help keep the data clean.
Provide solutions that help share data with the enterprise.
Be an advocate for best practices and continued learning.

You should have:
Experience with a Cloud Data Platform such as GCP or AWS
If GCP: Big Query, Cloud Dataflow, Cloud Dataproc, Pub/Sub
If AWS: Redshift, EMR, Datapipeline, RDS, Lambda ,and Kinesis.
Experience programming in Scala, Python, or Java .
Experience building data pipelines using Apache Beam or Apache Spark.
Experience with some of the following data stores: Big Query, Redshift, Postgres, Cassandra, or Mongo.

It would be great if you also have:
Experience with real time streaming solutions such as Kinesis, Pub/Sub, or Kafka.
Large scale Data Modeling from a Modern Big Data perspective.
Experience producing and consuming event driven data.
Experience working on a data team building functions, models and complex algorithms.
We hire only the best people. Here are the benefits to being top-notch:

The opportunity to work with some of the best innovators in the industry.
Generous healthcare coverage.
401(K) with company match.
Paid Time Off
Paid parental leave
Tuition reimbursement
Wellness allowance
Profit Sharing
Plus, the resources to locate services including child care, legal services, pet care and more

Weight Watchers is an equal opportunity employer. Weight Watchers does not discriminate on the basis of sex, race, color, creed, national origin, marital status, age, religion, sexual orientation, gender identity, gender expression, veteran status, or disability."
Data Engineer,Hiya,"As one of the fastest growing mobile startups in the world, Hiya is revolutionizing the way people make and receive phone calls. The phone app has remained unchanged since the smartphone launched over a decade ago. Hiya is fixing this through innovations designed to finally make the phone experience a great one. We make this happen by partnering globally with leading carriers and smartphone OEMs, including Samsung and AT&T. Despite being a startup, we are already making a big impact for our almost 20 million monthly active users in 196 countries (thats all of them!). We have a solid business model, we are already generating significant revenues, and our backers include some of the worlds leading investors.
Hiya is a small, but mighty crew of 50 with offices in Seattle, New York, London and Budapest. Were growing quickly and hiring across every area of the company. Come join us as we make todays calling experience smarter and more relevant for all 5 billion mobile subscribers worldwide!
We have strong company values and culture and would love to share why ours is an opportunity worth considering. We appreciate and respect diversity, and are an equal opportunity employer.

Job Overview
We are looking for a Data Engineer to work in our Manhattan, New York City office, who thrives on designing, coding and maintaining high performance data processing applications on the JVM platform (primarily Scala). Our ideal candidate would have the skills and desire to understand our entire data stack, from DynamoDB to Kafka to Scala/Akka and the passion to champion new and exciting technologies to solve unique and challenging problems.
Your ideal job should be one where you work in a small team and are empowered to make yourself and your team more productive on a daily basis. You should want to be part of a team where your desire to grow and learn are valued and aptly rewarded; where using and contributing to open source are looked upon as an asset; where innovating and executing are core to your teams' beliefs.

Our ideal candidate will have:
A passion for building scalable web services;
Proven experience building highly scalable and available production web services starting from the data storage layer;
A desire to own design and architecture end to end;
A philosophy of iteration and continuous improvement;
BSCS + 2 years experience or M.S. in Computer Science;
Excellent communication and self-organization skills incl. working within and with other remote teams.

You must be:
Fluent in functional programming;
Familiar with data storage, processing systems and web service APIs;
Deeply versed in internet protocols and standards;
Knowledge of cloud computing;
Knowledgeable of scaling techniques (caching, message queues, eventual consistency, etc.);
Experience with *nix.

Extra points if you:
Have experience with stream processing solutions;
Have at least 2 years' experience programming in a commercial setting;
Have implemented service(s) with ML techniques;
Have experience with both SQL and NoSQL databases;
Have deployed services to the cloud (especially AWS);
Have deployed docker based services;
Fluent in Scala.
We have strong company values and culture and would love to share why ours is an opportunity worth considering. We appreciate and respect diversity, and are an equal opportunity employer."
Data Engineer,New York District Attorney's Office,"Position Summary:
The New York County District Attorney's Office (DANY) has an immediate opening for a Data Engineer - Level 1 in the Office's Cybercrime and Identity Theft Bureau. The Data Engineer will work within the bureau alongside attorneys and investigators using their unique skill set to enhance, understand, and improve casework through technological and or programing solutions.

Responsibilities include but are not limited to:
Work directly with prosecutors, analysts, forensic analysts and detectives to understand and improve case workflow through technological and/or programing solutions.
Coordinate the collection and use of large and disparate investigative data, including those with unique and/or complex technical requirements.
Collaborate on developing technological infrastructure in the form of internal web applications and/or database management.
Develop and maintain tools to identify trends, behaviors, and patterns related to Cybercrime and Identity Theft Bureau cases and initiatives.
Work directly on cases wherever programming can enhance an investigation through efficiency, visualization or analytics.
Collaborate with other data engineers and programmers throughout the office to work on DANY-wide applications and projects.
Liaise with other law enforcement agencies, private organizations, and third-party vendors.
Present findings and solutions in front of team members and management.
Other job related duties as assigned.
Qualifications:
Knowledge of programming/scripting languages such as Python, and preferably R, SQL, and NoSQL.
Experience with custom ETL design and maintenance.
Experience developing (internal) web applications via Django, Flask, Dash, Bokeh, Shiny, JavaScript, HTML, CSS, etc.
Ability to make sense out of virtually any type of incoming file, and extract and analyze the relevant content and metadata.
Ability to work in a team environment communicating effectively with other programmers internally or externally.
Must be driven to find ways to use technology to improve the way cases are brought in and investigated within your unit.
Experience with data science, network analysis, and/or big data aggregation is a plus.
Educational Requirements:
Bachelor's Degree required (in Computer Science, Mathematics, Statistics, or a related field preferred).
Preferred Requirements:
Proven experience writing code, demonstrated through academic achievement, work experience, and/or an active GitHub account.
Commitment:
One (1) year commitment to hiring bureau.
The New York County District Attorney's Office is an Equal Opportunity Employer"
Data Engineer,Apple Leisure Group,"Description

Apple Leisure Group (ALG) is a vertically integrated private equity portfolio company in the travel and hospitality space, focused on packaged travel and resort/brand management in Mexico and the Caribbean. ALG currently operates 6 subsidiaries, including a resort and brand management company, 2 tour operators, an online travel agency and a loyalty program.

CheapCaribbean.com is an Internet-based travel company specializing in beach vacations. Our mission is to Rock the World through the Wonder of Travel! We are the industry leader in Caribbean vacations and are expanding rapidly. As many fast-growing e-commerce companies, we have a fun, entrepreneurial, non-hierarchical culture, with an office that is more akin to Silicon Valley startups than Dallas corporates.

As Data Engineer, you will be responsible for advancing business analytics by working directly with your customers to define, develop, and maintain data and analytics technologies. You will also lead application and data management support for all business functions. The role will report to our Director of Data & Analytics.

Responsibilities:
Work directly with your customers to define and deliver data and analytics solutions
Design, develop, and support business intelligence ETL processes and analyses
Ensure data quality and integrity through the support of data governance processes
Identify and implement new analyses to advance analytics for all business functions
Stay current on cutting edge technologies and advancements in analytics"
Data Engineer,Vium,"About Us:
Want to use your experience to help save lives? Heres the chance for you to make this a reality. Vium empowers biomedical investigators with technology that accelerates the preclinical drug discovery and development pipeline. Researchers can design, run, and analyze experiments that rapidly surface insights and high volumes of quality data. The platform improves the speed and reproducibility of preclinical research so that it is more predictive of how drugs perform in humans. The result is that better therapies get moved more quickly to patients who need them. Founded in 2013, we have a proven product, an awesome team, happy clients, smart investors and advisors, and plans to double in 2017 and beyond.

The Role:
Weve scaled fast and this data engineering role is a critical addition to our team. Alongside talented engineers you will work up and down our stack to ensure our data platform is predictable, reliable and highly performant. In this role you will have the opportunity to explore and enhance our core platform, building tools, frameworks, and solving real world data problems. This is a unique opportunity not only to build highly scalable systems, but to work on a platform that brings together big data, advanced electronics, and the power of the cloud computing.

Meaningful Things Youll Do:
Collaborate with experts across data science and software engineering to ensure our data platform is of this highest quality.
Help drive the next evolution of our platform to ensure we can scale through our post launch growth phase.
Build new tools and frameworks that drive data lifecycle management across our systems.
Education and Experience:
3+ years professional software development experience using Python
Strong sense of software design and debugging skills
Experience with Test Driven Development (TDD) and writing unit tests
Able to explain performance considerations, alternatives, and tradeoffs of different technical approaches
Able to decompose high-level requirements into designs and technical tasks
A passion for building tools, and the ability to test them well
Experience using Git, and GitHub
Bonus points for:
Experience with Spark or other distributed computation frameworks.
Strong background in multi threading and parallel processing models.
Background in performance testing and metrics gathering.
2+ years experience with AWS and critical services such as S3, RDS, SQS, etc.
More About Our Culture:
Were a group of talented individuals across multiple disciplines, coming together to deliver great engineering!

We:
Say what well do and then do it! As a growing team, weve increased collaboration between roles of development and operations
Have an attitude of shared responsibility and value building quality into the development process.
Have a wide range of interests, which is why were in this company in the first place.
Are focused on results more than hours-in-a-chair-at-the-office.
Have great values and sense of community, while encouraging growth.
Love what we do and love being part of a team that rewards trying new things.
What We Offer:
Good salaries and benefits. Who doesnt want that?
Stock options
Catered lunch most days and a fully stocked kitchen with healthy snacks and delicious drinks (Hungry anyone?)
Flexible work hours and Friday demo days/happy hours
Relaxed office in San Mateo with a casual environment
Company teambuilding days, fun parties, including a week in the Summer when we close up shop and go on vacation
Our corporate office is in San Mateo and the remote-controlled lab is in the South Bay."
Software Engineer - Big Data (Advertising Analytics),A9.com,"A9.com, an Amazon company, creates powerful, customer-focused search and advertising solutions and technologies. Our Ad Tech team builds the technology infrastructure and ad serving systems to manage billions of advertising queries every day. The result is better quality advertising for publishers and more relevant ads for customers. Our infrastructure supports millions of Internet users and handles billions of queries per day, all delivered in milliseconds. Our data platform processes massive data sets to develop business intelligence and analytics that are critical for the efficiency and profitability of our advertising business.

In this role, you will:
Build, scale and maintain data pipelines to process tens of billions log entries daily into our Hadoop Analytics cluster and RDBMS.
Write and tune complex Java MapReduce, Pig and Hive jobs.
Explore available technologies and design custom solutions to improve our data quality, workflow and job manageability and scalability; leverage cutting-edge tools and technology to continuously improve our data analytics infrastructure and reporting capability.
Troubleshoot data issues and build customized reports to investigate key business questions.
Work closely with Amazon Display Advertising teams worldwide.
At A9, youll experience the benefits of working in a dynamic, entrepreneurial environment, while leveraging the resources of Amazon.com (AMZN), one of the world's leading internet companies. We provide a highly customer-centric, team-oriented environment in our offices located in Palo Alto, California.

Basic Qualifications:
Bachelor's degree in Computer Science or related field.
At least 3 years of experience in building scalable software systems and writing production code in Java.
At least 3 years of experience with relational databases, SQL, and map-reduce languages (Pig, Hive).
At least 3 years of experience with Unix and shell scripting; strong knowledge of regular expressions and text processing (awk, sed).

Preferred Qualification:
Hadoop tuning and administration.
Familiarity with Tableau and OBIEE.
Familiarity with PostgreSQL and Redshift.
Knowledge of web technologies and online advertising systems.
Experience with real-time Big Data analytics.
Master's degree in Computer Science or related field. Job# 17222"
Data Engineer,"SRC, Inc.","SRC, Inc. is currently seeking a big data engineer with hands-on experience creating, deploying and optimizing large-scale data systems, providing on-site support to one of our government customers located in Ft Belvoir, Va.

The ideal candidate will bring strong technical experience with big data systems/technologies and have a strong track record of deployment, maintenance and optimization of production code.

As a data engineer, you will be responsible for creating, deploying and optimizing large-scale data, while being able to effectively collaborate and communicate with others. In addition, you will collaborate with other data engineers, data scientists, software engineers, and other researchers in the DoD and industry. As a member of our team, it is vital that you can think outside the box and bring innovative and cutting-edge technology and techniques to the table daily. Your expertise at SRC in big data analytics will help develop the next generation of military, cyber and critical infrastructure defenses.

Responsibilities

Create robust, high-volume production systems/architectures and develop prototypes quickly
Work with development teams to design maintenance and support strategies
Create optimized workflows using relevant technologies (Hadoop, Spark, Elastic Search, Kafka)
Create architectural workflows, diagrams and specification documents to help define platform features/functionality
Perform experiments and analyze results to improve the performance and quality of algorithms
Work with product management and executive stakeholders to take detailed requirements and implement them using Agile Test Driven techniques
Work in an organized, team-oriented environment with shared responsibilities
Participate in trend, pattern and statistical analysis
Participate in analysis of structured and unstructured data
Work independently and collaboratively in a project team

Position Requirements :
Bachelors Degree or higher in Data Engineering, Data Science, Computer Science, Computer Engineering, or similar
5 or more years of big data technologies and database design experience (examples, but not limited to: Spark, Storm, MapReduce, Hive, HBase, Hadoop, Kafka, Impala, Cassandra, YARN, Ambari, Elastic Search)
Experience cleaning and manipulating data
Experience with implementing algorithms
Excellent interpersonal communication skills
Experience in Elastic Stack - highly desired

Security Clearance Requirements

An active Top Secret/SCI security clearance and current investigation are required"
Data Engineer,BlueCross BlueShield of Kansas City,"Position Summary

The Data Engineer works collaboratively to mine and evaluate internal and external data to create insights, solutions, and visualizations for strategic Blue KC analytical priorities. This individual will develop a deep understanding of analytical technologies in order to enable Blue KC to create valuable models and insights for both external and internal stakeholders.

Accountabilities

Develop and Collaborate extensively with Business Stakeholders, IT, Data Scientists, Researchers (internal and external), and BI Analysts to deliver insights and solutions that realize business value.

Have a solid understanding and ability to use financial, clinical, socioeconomic, industry, and third party data to perform analysis and solve unique business problems.

Incorporate analytical and problem resolution skills to effectively communicate analytic findings and approaches to management and staff.

Ensure quality and accuracy of data used by performing analysis to measure quality of data. Validate data quality content in conjunction with respective data stewards by developing reports and tools to monitor and visualize data.

Develop strategies, standards, and best practices in the areas of data wrangling, data visualization, and data integration for use in analytic solutions.

Lead and execute activities to support bringing new data sources into Blue KC data environment. Analyze new sources of data to quantify quality, uniqueness, value and overlap with existing data sources. Collaborate extensively with IT to design data ingestion, data models, and automated operational metrics for consistent high quality data loading to Blue KC data environment.

Stay current with rapidly developing analytic technologies and tools. Share knowledge with other Blue KC resources. Prototype and integrate technologies and tools into analytic solutions; lead adoption of tools throughout the department.

Competencies

Delivering High Quality Work
Displaying Technical Expertise
Solving Complex Problems
Using Math

Minimum Requirements

3+ years relevant experience or Bachelors degree in computer science, information systems, business administration or other relevant academic field; or any combination of education and experience providing the types and level of knowledge, skills, and abilities required by the job.

Strong analytical skills for effective problem solving.

Ability to rapidly learn and leverage new data assets and tools.

Strong ability to handle and interpret a wide variety of data formats and types.

Experience creating impactful reports, visualizations, and interactive dashboards.

Experience gathering manipulating and visualizing data.

Experience and knowledge of Data Visualization concepts and tools such as Power BI or Tableau.

Excellent communication skills with the ability to explain insights to stakeholders.

Sensitivity to clients' needs and able to develop warm client relationships

Excellent collaboration skills and proactive approach to sharing knowledge with peers and cross functional teams.

Independent, creative, and nimble with ability to overcome obstacles to solve problems.

Preferred Requirements

5+ years relevant experience or Masters degree in computer science, information systems, business administration or other relevant academic field; or any combination of education and experience providing the types and level of knowledge, skills, and abilities required by the job.

2+ years experience with relational databases and strong knowledge of SQL

2+ years experience with health care claims and clinical data sets.

Knowledge of R, Python, and data lake concepts.

Some prior experience with NCQAs Healthcare Effectiveness Data Information Set (HEDIS).

At Blue Cross and Blue Shield of Kansas City (Blue KC), we know that success starts with a diverse workforce. Our ability to work effectively as individuals and collectively as teams is a direct reflection of our ongoing dedication. The Blue KC business philosophy is to leverage diversity and inclusion to meet the complex and ever-changing needs of our employees, partners and customers we serve. Blue KC is truly committed to fostering an inclusive culture through equal opportunity and respect. Not accepting applicants from placement services , recruitment firms, and employment

agencies.

Blue Cross and Blue Shield of Kansas City will extend reasonable accommodations to qualified individuals with disabilities who are otherwise not able to fully utilize electronic and online job application systems. For assistance, please send an email to Recruiting@BlueKC.com .

EOE/M/F/Vets/Disability

Affirmative Action Employer

Equal Employment Opportunity

Nearest Major Market: Kansas City

Job Segment:
Database, Claims, Engineer, Computer Science, SQL, Technology, Insurance, Engineering"
Data Engineer,IMM,"Do you have an eye for building and optimizing data systems?Are you experienced with Python and SQL? We are looking for a skilled Data Engineer to join our blossoming analytics team.

A Day in the Life
You will work closely with our solutions architect, data scientists, and analysts to help build out our data aggregation and ingestion platform across multiple projects.

Here are some specifics on exactly what your days might look like:

Ingest large amounts of data from numerous sources (API, Email, SFTP, S3, & etc.)
Identify, prioritize, and execute pipeline improvements
Help drive new architectural ideas and decisions for products and offerings
Work closely with other data and analytics team members to optimize the companys data systems and pipeline architecture
Build data and analytics tools that will offer deeper insight into the pipeline, allowing for critical discoveries surrounding key performance indicators and customer activity
Troubleshoot systemic issues and lead improvements
If you meet the following requirements, we definitely want to hear from you!
Graduate degree in Computer Science, Information Systems or equivalent quantitative field or 3+ years of experience in a similar Data Engineer role.
Expert level Python ability (or willingness to get there)
An analytical mindset with problem-solving skills
Demonstrated ability to build processes that support data transformation, data structures, metadata, dependency and workload management
Strong interpersonal skills and ability to manage projects and work with cross-functional teams
Advanced working SQL knowledge and experience working with relational databases, query authoring (SQL) as well as working with NoSQL.
Experience performing root cause analysis on internal and external data and processes to answer specific business questions and identify opportunities for improvement.
Experience with the following tools and technologies is a plus:
Talend
Tableau
Relational SQL and NoSQL databases
Data pipeline/workflow management
AWS services such as Lambda, SQS, & SNS
Other Object-oriented/object languages such as Java or Ruby"
Data Engineer,DXC Technology,"DXC Technology (NYSE: DXC) is the worlds leading independent, end-to-end IT services company, helping clients harness the power of innovation to thrive on change. Created by the merger of CSC and the Enterprise Services business of Hewlett Packard Enterprise, DXC Technology serves nearly 6,000 private and public sector clients across 70 countries. The companys technology independence, global talent and extensive partner alliance combine to deliver powerful next-generation IT services and solutions. DXC Technology is recognized among the best corporate citizens globally.

For more information, visit www.dxc.technology. DXC Technology is seeking a highly motivated and qualified Data Engineer. IDMS Database Administrator (DBA) with 8-10+ years of experience working as an application DBA in a production support environment. Must possess good knowledge of industry standards.

Experience with IDMS version 18 or higher. Excellent performance and tuning skills required. Proactively prevent database issues by closely monitoring the databases on a regular basis and implementing the tools and procedures required to maintain a very stable environment. Excellent verbal and writing skills are critical.

Client facing role. Communicate and coordinate with other support groups. US Eastern/US Central Time zone work hours required. Manage database performance, capacity and availability.

DBMS software implementation, upgrade, patching. 24 x 7 on call support. Create and maintain application database disaster recovery plans. Attend regular meetings with the client and account team.

Experience using CA tools and utilities. Experience with IDD, ADS/O, OLQ, Culprit, SYSGEN. Experience with DML and DDL. Experienced with z/OS utilities, JCL, SDSF or IOF.

Experience with CM tools such as Librarian or Endevor. Experience with ticketing and notification systems such as eNOTE, Service Manager, ServiceNow. Experience with ITIL Processes for Incident/Change/Problem/Request management. Work on an application DBA team performing IDMS database capacity planning, installation, configuration, database design, migration, performance monitoring, security, troubleshooting, as well as backup and data recovery.

KEY RESPONSIBILITIES Installation, configuration, upgrade, and migration. This job role requires knowledge of the hardware prerequisites for an efficient database environment, and communicating those requirements to the system administrator. The DBA installs the database software and selects from various options in the product to configure it for the purpose it is being deployed. As new releases and patches are developed, it's the DBA's job to decide which are appropriate and to install them.

Backup and recovery. Responsible for developing, implementing, and periodically testing a backup and recovery plan for the databases they manage. Final responsibility for making sure that the backups are being done as scheduled and that they include all the files needed to make database recovery possible after a failure. When failures do occur, the DBA needs to know how to use the backups to return the database to operational status as quickly as possible, without losing any transactions that were committed.

There are several ways the database can fail, and the DBA must have a strategy to recover from each. From a business standpoint, there is a cost to doing backups, and the DBA makes management aware of the cost/risk tradeoffs of various backup methods. Database security. Because databases centralize the storage of data, they are attractive targets for hackers and even curious employees.

The DBA must understand the particular security model that the database product uses and how to use it effectively to control access to the data. The three basic security tasks are authentication (setting up user accounts to control logins to the database), authorization (setting permissions on various parts of the database), and auditing (tracking who did what with the database). The auditing task is particularly important currently, as regulatory laws like Sarbanes-Oxley and HIPAA have reporting requirements that must be met. Storage and capacity planning.

The primary purpose of a database is to store and retrieve data, so planning how much disk storage will be required and monitoring available disk space are key DBA responsibilities. Watching growth trends are important so that the DBA can advise management on long-term capacity plans. Performance monitoring and tuning. The DBA is responsible for monitoring the database on a regular basis to identify any bottlenecks (parts of the system that are slowing down processing) and remedy them.

Tuning a database is done on multiple levels. The capacity of the hardware and the way the operating system is configured can become limiting factors, as can the database software configuration. The way the database is physically laid out on the disk drives and the types of indexing chosen also have an effect. The way calls against the database are coded can dramatically change how fast results are returned.

A DBA needs to understand which monitoring tools are available at each of these levels and how to use them to tune the system. Proactive tuning is an attitude of designing performance into an application from the start, rather than waiting for problems to occur and fixing them. It requires working closely with developers of applications that run against the database to make sure that best practices are followed so good performance will result. Troubleshooting.

When things do go wrong with the database, the DBA needs to know how to quickly ascertain the problem and to correct it without losing data or making the situation worse."
Data Engineer,Trianz,"Role: Data Engineer
Project: Support Data lake, Machine Learning/Data Science use cases Location: Sunnyvale, CA

6-12 month contract

About Trianz
Trianz is a global professional services firm committed to enabling leaders to develop and execute operational strategies, leverage new business and technology paradigms , and achieve results expected by senior management in their organizations- predictably.

What We Stand For
Our clients are transforming their businesses, competitive strategies, product and service portfolios, customer-partner-employee interactions and their ecosystem. The cost of misses is not financial alone but a lost window of opportunity. So, getting things right the first time is absolutely critical.
As a result, Trianz is focusing on three important themes in our engagement model with clients.
Crystallize business impact from a top management point of view
Help Clients achieve results from strategy-by making execution predictable through innovative execution techniques
Create a positive, enriching partnership experience in everything we do

Industries, Clients & Practices
Trianz works with clients across High Technology, Banking, Insurance, Manufacturing, Retail, Telecom, e-businesses and Public Services. Most clients are Fortune 1000 organizations and our relationships are sponsored by senior leaders in Enterprise Analytics Sales, Finance, Marketing, Human Resources, Operations and Information Technology. We partner with our clients to address the following key service areas:
Information Management
Business Intelligence & Analytics
Big Data & Business Insights

Cloud Analytics About the role:
Data Engineer

Role responsibilities:
Can program using Spark with Python and Scala
Understands Hive and Hadoop configuration
Understands different file formats in the Hadoop environment and how to organize data for query performance
Supporting number of tools: Data Lake sync, SMFDB population, Metadata sync
Tuning of the cluster for performance optimization with Spark and Presto.
Understand the different formats parquet, Avro and snappy

We are Growing Rapidly: 2015-16 Highlights
Trianz is growing above the average of the professional services industry. Here are some highlights.
Voted significantly above other services firms by 90% + of clients for business impact, execution predictability and organizational commitment in the recent Trianz wide client satisfaction survey.
Won the Customer Obsession Award from Amazon Web Services for our innovation and execution record in Cloud Infrastructure and Business Applications strategy and services.
Won UNICOM awards for the #1 Digitization and #1 Analytics project over a wide array of competition.
Featured by IDC in their Spotlight series under the theme of Operationalizing Strategies through Execution Excellence: A New Paradigms in Technology Delivery.
Achieved 50%+ revenue and employee growth compared to prior years exit showing an increasing acceptance of our models and success from our differentiated methodologies in strategic execution.

Talk to us, join us & Develop into Leaders
Come join a dynamic global start-up. We are an open, non-bureaucratic and no-nonsense culture. We believe in a culture of innovation, encouraging our people to create. We believe training and development of all our associates is the most important thing we can provide to our talent. We are investing heavily- in classroom, online and on the job training. Seeing our talent develop into leaders- is whats fundamental for everyone at Trianz.
We are hiring at all levels of Trianz . And we are hiring globally. So- if you have a passion for execution and would like to develop into a leader capable of taking on anything, or are already a leader, talk to us!
Equal Opportunity Employer
Trianz does not discriminate based on race, color, creed, national or ethnic origin, gender, religion, disability, age, political affiliation or belief, disabled veteran, veteran of the Vietnam Era, or citizenship status (except in those special circumstances permitted or mandated by law)."
Data Engineer,Wayfair,"Ideal Candidate : A technical-minded analyst with a quantitative background who has a strong desire to help enable business decisions with data. You appreciate the power of a data-driven environment, predictive analytics, and the challenges of working with large datasets!
Introduction to the team: Business Intelligence (Analytics) Team is an exciting blend of business, big data, and technology. At Wayfair, the Business Intelligence team empowers people in every part of the company to make better decisions with data. Working on the Business Intelligence Team is a premier opportunity to further develop your career in business and big data analytics.
A platform developer works to improve the core datasets and processing architecture of the Wayfair BI platform, partnering with every other team in Business Intelligence  such as Marketing, Operations, or Web Analytics - to help them achieve their goals. Whether building complex data pipelines in Spark, optimizing OLAP cubes, or developing internal analytic tools you will be working with cutting edge technology and multi-terabyte datasets. Core skills include Python, SQL, Powershell, Git and shell scripting.
At their core, Business Intelligence Platform Developers at Wayfair are strong in technical abilities with the business acumen to understand where they can deliver the greatest value. They think critically to tackle complex challenges, thrive in a fast-paced environment and are seeking a high-growth opportunity where they will have an immediate impact on day one.

Responsibilities:
Consulting with BI stakeholders to guide the development of analytics infrastructure
Designing and building a modern data infrastructure in SQL, Python, and other technologies to process terabytes of data
Working with several large and complex SQL, Hadoop, and Vertica databases
Building distributed ETL systems across a range of platforms and technologies
Working to optimize and scale reporting technologies such as Tableau and OLAP Cubes

Qualifications:
BA/BS with strong academic record, ideally in Economics, a quantitative social science, Mathematics, Statistics, Computer Science, Physics etc.
0-3 years of experience in a quantitative role
Experience with SQL and either Python or R
Excellent interpersonal and team building skills
Positive, people-oriented, and energetic attitude
Analytical, creative, and innovative approach to solving problems

About Wayfair :
Wayfair Inc. offers an extensive selection of home furnishings and dcor across all styles and price points. The Wayfair family of brands includes:
Wayfair.com, an online destination for all things home
Joss & Main, where beautiful furniture and finds meet irresistible savings
AllModern, a go-to online source for modern design
DwellStudio, a design house for fashion-forward modern furnishings
Birch Lane, a collection of classic furnishings and timeless home dcor
Wayfair generated $2.25 billion in annual sales in 2015 and $786.9 million in net revenues for second quarter 2016. Wayfair employed 5,398 people as of June 30, 2016 and is headquartered in Boston, Massachusetts with operations throughout North America and Europe. The company has previously been named one of Americas Most Promising Companies by Forbes Magazine and selected by the Boston Business Journal and Glassdoor.com as a Top Company to Work For and Top Company for Well Balanced Life."
Data Visualization Engineer (People Analytics),Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. We are seeking an experienced Data Visualization Engineer to join the People Analytics team and participate in the design and development of creative and innovative data visualizations that will help us find, grow and keep the talented people that make Facebook awesome and an amazing place to work. Responsibilities
Develop deep understanding of people data model  write queries and manage data transformation to enable analytics and reporting
Help analyze, visualize, and provide analytics on data to build reporting solutions to support various company initiatives
Participate in the on-going development of the business intelligence and data warehousing functions within the wider organization
Build rich and dynamic dashboards using out-of-box features, customizations, and visualizations using d3, Angular.js or equivalent
Participate in the creation and support of BI development standards and best practices
Explore and recommend emerging technologies and techniques to support/enhance BI landscape components
Minimum Qualifications
Demonstrable skills building responsive user interfaces and data visualizations using Angular.js, d3.js
Web development experience, including Python (or other scripting languages), JavaScript, CSS, HTML, and general visualization principles
Bachelors in Computer Science, Business, Business Administration, or closely-related degree, or foreign equivalent
At least 5 years of business intelligence and data warehouse experience
At least 2 years experience with Oracle RDBMS (SQL/PLSQL) or MySQL"
"Data Engineer, Analytics (WhatsApp)",Facebook,"(Menlo Park, CA) WhatsApp is a fast, simple and reliable way to talk to anyone in the world. More than 1 billion people in over 180 countries use WhatsApp to stay in touch with friends and family, anytime and anywhere. WhatsApp is not only free but also available on multiple mobile devices and in low connectivity areas  making it accessible and reliable wherever you are. It's a simple and secure way to share your favorite moments, send important information or catch up with a friend. WhatsApp helps people connect and share no matter where they are in the world.

We are seeking a talented Data Engineer to join the WhatsApp Business and Analytics Team. In this role, you will see a direct link between your work, company growth, and user satisfaction. You will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet, at a scale that few companies can match.

In addition, you must have the ability to operate independently in a fast-paced, small but growing environment and work proactively with various teams across the organization, including Engineering, Product, Business, Customer Support, Human Resources, Finance and Legal. This is a full-time position based with the WhatsApp team at Facebook HQ in Menlo Park, California and will report to WhatsApps Head of Growth & Partnerships. Responsibilities
Manage data warehouse plans for a product or a group of products.
Interface with engineers, product managers and product analysts to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Design, build and launch new data models in production.
Design, build and launch new data extraction, transformation and loading processes in production.
Support existing processes running in production.
Define and manage SLA for all data sets in allocated areas of ownership.
Work with data infrastructure to triage infra issues and drive to resolution.
Minimum Qualifications
BS/BA in Technical Field, Computer Science or Mathematics.
2+ years experience in the data warehouse space.
2+ years experience in custom ETL design, implementation and maintenance.
2+ years experience working with either a Map Reduce or an MPP system.
2+ years experience with object-oriented programming languages
2+ years experience with schema design and dimensional data modeling.
2+ years experience in writing SQL statements.
Ability to analyze data to identify deliverables, gaps and inconsistencies.
Communication skills including the ability to identify and communicate data driven insights.
Ability in managing and communicating data warehouse plans to internal clients.
Preferred Qualifications
Experience with Python or Java"
Big Data Software Engineer,Palo Alto Networks,"Palo Alto Networks is the fastest-growing security company in history. We offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!
In this service, we ingest tens of terabytes of data daily from several sources into the system. As the historical data is always valuable for analysis and correlation (i.e. the malware does not expire), the total amount of storage can easily get to peta bytes. By doing several data correlation among this vast amount of data, we can provide intelligence to our customers to help them prioritize threats and decide what actions they should take.

Responsibilities:
As a Big Data Software Engineer, you will be an integral member of our threat intelligence service, i.e. auto focus, team responsible for architecture, design and development.
Having the dynamic ability to adapt to conventional big-data frameworks and tools with the use-cases required by the project
Ability to communicate with research teams and data scientists, finding bottlenecks and resolving them
Design and implement different architectural models for our scalable data processing, as well as scalable data storage
Build tools for proper data ingestion from multiple heterogeneous sources

Qualifications:
2+ years of experience in design and implementation in an environment with hundreds of terabytes of data
4+ years of experience with large data processing tools such as: Hadoop, HBase, Elastic Search, etc.
2+ years experience with Java
Passion for doing research on large data sets containing ill-formatted data
Can-do attitude on problem solving, quality and ability to execute
Excellent inter-personal and teamwork skills
BS in Computer Science/Engineering, or equivalent experience
Learn more about Palo Alto Networks here and check out our fast facts

#LI-PALSAAS"
Software Engineer - Big Data,Esri,"Work collaboratively with a talented team of dynamic and passionate engineers in building capabilities enabling our customers to make a difference. Develop and deploy robust backend technology to power Internet of Things (IoT) scale architectures. Join our team of exceptional software developers where youll design and build next-generation real-time and big data server software for enterprise and cloud users worldwide.

Responsibilities
Build micro-service components that, when combined, power Esris real-time and big data capabilities
Design and build components that seamlessly run on-premises and on cloud platforms
Work collaboratively with other team members within an agile software development process
Participate in hackathon efforts, bringing new innovative ideas to future versions of our software
Demonstrate latest product capabilities to stakeholders, prospects, and customers

Requirements
Strong foundation in computer science with advanced competencies in data structures and software design
Programming experience with Scala or the Java platform and familiarity with enterprise integration patterns
Working knowledge of Apache Spark including streaming and machine learning using MLlib
Experience with Elasticsearch, Apache Cassandra, or Datastax Enterprise
Good understanding of and experience with cloud computing platforms including Amazon EC2 or Microsoft Azure
Knowledge of working with a source and issue control system, preferably GitHub
Experience developing code in IntelliJ IDEA
Bachelors in computer science, computer engineering, mathematics, or related field (master's preferred)
Familiarity with DC/OS powered by Apache Mesos, Marathon, and Metronome
Working knowledge of container technologies including Docker and Open Container Initiative (OCI)
Previous project experience using deep learning frameworks such as TensorFlow or Caffe
Experience with Apache Kafka
Familiarity with how to build reactive applications using the Play framework and Akka
Knowledge of Esri ArcGIS or other web mapping technologies"
Senior Data Engineer,NIKE INC,"Your Role on the Team: Design and implement product features in collaboration with product owners, report developers, product analysts, and business partners within an Agile / Scrum methodology Provide engineering on modern data processing technology stack including Spark, EMR, Airflow, Python and Snowflake within AWS Contribute to overall architecture, framework, and design patterns to store and process high data volumes Support the culture of team empowerment and engagement through successful self-management. Own your workload and contributions to the team Proactively support product health by building solutions that are automated, scalable, and sustainable be relentlessly focused on minimizing defects and technical debt Build continuous integration and test-driven development environment Mentor and guide other software engineers within the team including review code and provide feedback relative to best practices and improving performance Skills/Qualifications Bachelor's degree in Computer Science, Information Systems or other relevant subject area 2 years' additional relevant experience in lieu of a degree 5+ years of experience with large-scale data engineering with emphasis on data analytics and reporting Strong experience with SQL and Relational database engineering (Oracle, SQL Server, Teradata) expert-level SQL abilities 3+ years of experience developing in the Hadoop ecosystem, leveraging tools such as Pig, Hive, Sqoop, Spark and others Good understanding of file formats including JSON, Parquet, and Avro Experience developing solutions in Snowflake Experience developing with the AWS EMR managed service Experience developing with scripting languages such as Shell and Python Experience with agile delivery methodologies in a fast-paced complex environment  Scrum, SAFe, Extreme Programming Strong ability to multi-task and balance multiple priorities in a fast-paced environment A Bachelor's degree in Business, Information Technology or related field Solid interpersonal and communication skills (written and verbal) Continuous Learner"
Data Engineer,Blizzard Entertainment,"Blizzard Entertainment games dont just begin with game ideas nor do they end once those games are released. A lot more goes into the creation of an epic game than the work of developers and more than you can ever imagine goes into continuing to build and hone and perfect the most epic gaming experience in the years after our games are in the hands of gamers world-wide.
Blizzard Entertainment is looking for a Data Engineer to join our Global Insights team to partner with our respective analytics teams with technical leadership and vision, innovation and agility with big data at scale, and to develop a deep and rich subject matter expertise with the data most important to the franchise.
In this role, you will be responsible for providing accurate, timely, reliable, and ultimately useful data to the analytics group. This mandate involves everything from the discovery, access, and integration of source data into the ecosystem to guaranteeing its health and robustness within that ecosystem to modeling and presenting that data in a manner that allows stakeholders within the ecosystem to perform their duties effectively. You will have a delicate balance in this role: you work most closely with the other members of your group and translate their needs into agile, highly-customized implementations for franchise-specific data; at the same time, though, you work just as closely with the Core Data team both to ensure that your group does not introduce problematic deviations from core data and that the core data infrastructure itself is supporting your groups evolving needs; you are just as closely tied to the Tools and Technology team to ensure that your franchise-specific system usage is playing nice, playing fair within the greater analytical data ecosystem. You are one step closer to the business needs of the stakeholders and act as the primary technical resource for the franchise analytics team. Your priority is to ensure that the Analytics groups can provide best-in-class analyses, insights, and recommendations in the most timely and effective manner possible.
Our ideal candidate is the druid of the analytics space: a powerful technologist whose dedication to the craft allows him or her to morph into whatever form necessary to get the job done while retaining the flexibility to see the bigger picture and respond accordingly. The ideal candidate uses the speed, agility, and targeted on-point damage of Cat form to meet the quick-shifting, high-impact immediate needs of his or her franchise group before morphing seamlessly into a Bear form, responsible for tanking the ongoing operational needs of the group as a whole. A shift to Moonkin allows for the push-and-pull flow of ideas between Blizzard Analytics and Core; and the final shift to Tree provides the stability and integration between the franchise group and the other users, approaches, policies, and procedures of the overall analytical data ecosystem.
Responsibilities
Design, develop, implement, and evolve franchise-specific data pipelines
Troubleshoot any performance, system or data related issues, and work to ensure data consistency and integrity
Work with the franchise team as technical lead and primary technical resource
Plan and coordinate franchise-specific technical projects
Innovate in a more agile environment and summarize ideas/approaches to the Core Data team for validation and potential integration into core data infrastructure
Ensure no deviation from core data
Act as a primary technical liaison between the Blizzard Analytics teams and the Core Data / Tools and Technology teams
Requirements
2+ years working in a large analytical data ecosystem
Strong technical understanding of data modeling, design, architecture principles, and techniques to take business requirements from concept to implementation
Very strong knowledge of relational databases, MPPs, SQL, with an emphasis on Teradata fundamentals
Very strong experience with the Hadoop ecosystem including HDFS, MapReduce, Spark, Hive/Pig, et. al
Strong verbal and written communication skills across both technical and non-technical audiences
Knowledge of Python, Java, Linux architecture and scripting
Extensive background extracting and transforming complex data sets (ETL process design and administration)
Experience with database design and star schema data warehouse theory
Passionate video gamer and in-depth knowledge of Blizzard Entertainment games, products, and services"
Data Engineer,CLEARLINK,"We are looking for a driven Data Engineer to join our Information Management team within our Data Science organization in our Salt Lake City office.

Responsibilities:
Optimize data (both underlying data structure and delivery method) for use by reporting platforms.
Manage large databases.
Work with new noSQL, Document, and Graph databases.
Learn and assist with our Extract Transform and Load (ETL) process for bringing data into the warehouse in order to turn it into usable information.
Partner with various users company wide to build a strong data driven culture through the use of information based decision making.
Help these users access and understand the data and metrics in the data warehouse.
Developing appropriate new metrics to be used across the business.
Building ad hoc reports for various stakeholders.
Maintaining Documentation.

Qualifications:
3 years of SQL query experience.
PHP and/or Python experience.
Clickstream web analytics data management experience.
Familiarity with Lambda Architecture.
In-depth quality assurance expertise.
Complex problem solving skills.
Requires high proficiency in both written and verbal. Most communication involves technical/specific terminology, logical development of arguments or processes and requires clarity of expression.
Requires high degree of independent judgment and problem solving of complex problems.
Data auditing skills to verify data integrity and understand discrepancies.
About CLEARLINK:

CLEARLINKs team of 1,300+ employees is headquartered in Salt Lake City and has been creating marketing content services for Fortune 500 companies for over 13 years. At CLEARLINK you will have opportunities to work with people who are as passionate as they are talented, develop yourself and your skills, and create valuable content and relationships every day. We also like to reward our employees:
Up to 100% healthcare for your entire family
Over two weeks paid time off
Paid ski days, wellness activities, and team outings
Fully-stocked break room and gourmet coffee
Award-winning wellness program with free health coaching
All-expense paid vacations for top employees
Monthly gym subsidy"
Data Engineer,Charles Schwab,"Description:
Westlake - TX, TX2050R, 2050 Roanoke Road, 76262-9616
Brian Parker
20171117-5852

We believe that , when done right, investing liberates people to create their own destiny. We are driven by our purpose to champion every clients goals with passion and integrity. We respect and appreciate the diversity of our employees, our clients, and the communities we serve. We challenge conventions strategically to create value for our clients, our firm and the world. We live and bring to life the concept of own your tomorrow every day. We champion our employee strengths, guide their development, and invest in their long-term success. We hire optimistic, results-oriented, curious, innovative, and adaptable people with the desire to help our clients and one another succeed.

As a company, we were established by Chuck over 40 years ago to champion Main Street over Wall Street, and to help Americans transform themselves from earners to owners. Through advocacy and innovation, we work to make investing more affordable, accessible and understandable for all. As we enter our fifth decade, we are looking for talented, innovative and driven people who believe they can help themselves, and our clients, create a better future.

Our Opportunity:
The Risk Data Aggregation and Risk Reporting (RDARR) team is situated in Corporate Risk Management (CRM). The team is responsible for identifying and defining strategic CRM technology initiatives that advance CRM's RDARR maturity. This position is for a Data Engineer involved in the development, maintenance and documentation of an integrated data infrastructure for corporate risk analytics and reporting. This is an individual contributor role .

What youll do:
This role provides a terrific opportunity for someone who has the technical chops and wants to solve complex business problems. This role will be uniquely positioned to work with a variety of Risk teams to implement innovative data management solutions and functionality that provide direct business value while requiring proper IT best practices and controls.

Key Responsibilities Include:
Support the current risk data infrastructure to consolidate multiple data sources into a single source using ETL

Establish productive relationships with internal business partners and information technology partners

Responsible for documenting business logic and maintaining procedures

Develop complex SQL queries for small to large sets of data and lead query optimization efforts

Understand process inputs and create solutions to automate data sources and apply data governance standards

Support daily and monthly operations and troubleshoot issues

Design and architect artifacts using Data Modeling concepts

What you have:
The following qualifications are required:
Bachelors Degree in a concentration with an analytical focus

3-5 years of experience

Experience in one or more ETL Tools: SSIS, Informatica, Alteryx, or other tool with understanding of best practices for building and designing ETL code

Database development or engineering experience in one or all of the following databases: SQL Server, Oracle, or Teradata

Experience using Database Modeling Tools: Erwin or other tool

Passion for data with a data warehousing or business intelligence background

Strong data management, analytical, communication, and technical writing skills

Individual must be self-motivated and able to bring projects to their conclusion

The following qualifications are not required, but highly desired:

Sufficient understanding of the General Ledger (ex. Balance Sheet and Income Statement)

Hands on experience with .NET or other programming languages developing custom tools

Hands on experience with R/Python/Scala

Familiarity with the Hadoop framework (NoSQL, Hive, Kafka) and/or other Big Data tools

Experience with SCM tools such as GIT, TFS, Jenkins and Bamboo is preferred

What youll get:
Comprehensive Compensation and Benefits package

Financial Health: 401k Match, Employee Stock Purchase Plan, Employee Discounts, Personalized advice, Brokerage discounts

Work/Life Balance: Sabbatical, Paid Parental Leave, New Mothers returning to work Program, Tuition Reimbursement Programs, Time off to volunteer, Employee Matching Gifts Program

Everyday Wellness: Health and Lifestyle Wellness Rewards, Onsite Fitness Classes, Healthy Food Choices, Wellness Champions

Inclusion: Employee Resource Groups, Commitment to diversity, Strategic partnerships

Not just a job, but a career, with an opportunity to do the best work of your life

Learn more about Life@Schwab .

Charles Schwab & Co., Inc. is an equal opportunity and affirmative action employer committed to diversifying its workforce. It is Schwab's policy to provide equal employment opportunities to all employees and applicants without regard to race, color, religion, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), gender identity or expression, national origin, ancestry, age, disability, legally protected medical condition, genetic information, marital status, sexual orientation, protected veteran status, military status, citizenship status or any other status that is protected by law. Schwab also does not discriminate against applicants or employees because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. At Schwab, we believe that every employee, through their diverse abilities and experiences, can contribute to our growth, innovation and client loyalty. We embrace diversity and are committed to providing equal opportunity to all employees and applicants. If you have a disability, and require reasonable accommodations in the application process, call Human Resources at 800-725-3535. We will be happy to assist you. Schwab will only share your accommodation request with those individuals who have a specific need to know. The request for an accommodation will not affect Schwab's hiring decisions. All other submissions should be performed online.

Job Specifications

Relocation Offered?:
No

Work Schedule:
Days

Languages:
English - spoken

Current Licenses / Certifications:
None

Relevant Work Experience:
Analyst/Strategy-2-5 yrs, IT-Other Specialty Engineering-2-5 yrs, Financial Services-2-5 yrs, Risk Analysis, IT-System Administration-2-5 yrs, IT-DBA-2-5 yrs

Position Located In:
TX - Westlake

Education:
BA/BS

Job Type:
Full Time

Category: Risk Management

Activation Date: Wednesday, December 27, 2017

Expiration Date: Thursday, February 1, 2018

Apply Here"
"Machine Learning Engineer, Amazon AI",Amazon.com,"Amazon is looking for a passionate, talented, and inventive Research Engineers with a strong machine learning background to help build industry-leading Speech and Language technology.
Our mission is to provide a delightful experience to Amazons customers by pushing the envelope in Automatic Speech Recognition (ASR), Machine Translation (MT), Natural Language Understanding (NLU), Machine Learning (ML) and Computer Vision (CV).
As part of our AI team in Amazon AWS, you will work alongside internationally recognized experts to develop novel algorithms and modeling techniques to advance the state-of-the-art in human language technology. Your work will directly impact millions of our customers in the form of products and services that make use of speech and language technology. You will gain hands on experience with Amazons heterogeneous speech, text, and structured data sources, and large-scale computing resources to accelerate advances in spoken language understanding.
We are hiring in all areas of human language technology: ASR, MT, NLU, text-to-speech (TTS), and Dialog Management, in addition to Computer Vision.

Basic Qualifications
Graduate degree (MS or PhD) in Electrical Engineering, Computer Science, Mathematics or Physics with specialization in speech recognition, natural language processing, machine translation, time series analysis, signal processing, or machine learning.

7+ years of related work experience OR a PhD and 5+ years of related work experience.

Familiarity with programming languages such as C/C++, Python, Java or Perl.
Preferred Qualifications
Experience in building speech recognition, machine translation and natural language processing systems (e.g., commercial speech products or government speech projects)

Solid Machine Learning background and familiar with standard speech and machine learning techniques.

Scientific thinking and the ability to invent, a track record of thought leadership and contributions that have advanced the field.

Solid software development experience.

Good written and spoken communication skills."
Data Engineer,Ignyte Group,"Who We Are

Business has changed - so should consulting. We are strategists, technologists, engineers, and designers who bridge the gap between consulting, design, and marketing to create powerful digital experiences for our customer's brands and users.

Recently named as one of Washingtonian Magazine's Best Places to Work in Washington, DC, Ignyte is looking for a Data Engineer to join our progressive start-up culture and help us build a new type of consulting company.

What You'll Do

At Ignyte, our Data Engineers are passionate about creating innovative data-driven solutions across various industries that address critical business problems. You'll be challenged daily as you wear multiple hats to drive multiple project types to completion in both a development and business-facing role.

As a Data Engineer, we give you the opportunity to create your own career path based on your interests, the skills you already possess as well as the skills you'd like to have. Your everyday tasks in supporting our analytics solution development efforts can range anywhere working with our clients to understand their reporting needs to working with our developers to create machine learning solutions.

Youll work with our firm's leadership from day one formulate your own job description and career goals. Not sure whered you fit in? Well work with you to figure it out as part of the application process.

What We're Looking For:
2+ years experience with SQL, NoSQL, relational database design, and methods for efficiently retrieving data

Strong experience developing and launching efficient and reliable ETL pipelines to move and transform data

Experience in designing, architecting and implementing data warehouses

2+ years of experience in a variety of programming languages such as SQL, Python , Java, and Scala

Ability to architect highly scalable distributed systems using open source tools and big data technologies such as Hadoop, HBase, Spark, Storm, Etc

Knowledge of reporting technologies (e.g., PowerBI, Tableau, MicroStrategy etc.); Hands on experience preferred

Experience handling structured and unstructured data from internal and third party sources

Experience with cloud computing platforms such as Amazon Web Services or Microsoft Azure is a plus

""Big 4"" consulting experience and/or a data role at a leading tech company is a plus

BS or MS degree in Computer Science, Mathematics, Statistics, Finance, or related technical field

What You'll Gain

Experience working with a team of smart and driven professionals at DC's hottest consulting startup

Exposure to complex technical business issues and challenges

Opportunities to leverage your technical knowledge and analytical skills to solve complex business problems

The ability to mold your own career path based on the skills you have and want to have"
Data Engineer,Connexity,"The Data Engineer sits in the Data Engineering practice within Hitwise, Global Product Development - a division of Connexity.
As a Data Engineer, expect to engage with account managers and research analysts to discuss whats possible in big data and insights and even what seems impossible. Youll then work to develop a custom solution, extract and model the right unstructured and existing data to bring these custom solutions to life using BI and data visualization tool sets.
You'll also provide expertise to improve the delivery mechanisms of the growing number of clients requesting custom solutions. The solutions will be delivered using cutting edge open source platforms such as Python, SQL and Hadoop to process, to extract and deliver a variety of Big Data Insights.

Main Responsibilities
Contribute to the Data Engineering pipeline and team workload
Champion regular communications with Research Analyst teams across the globe
Evaluate the feasibility of client requests from our global sales teams
Implement and deliver approved Data Engineering projects
Manage the development for data generation and extraction
Assist in data QA and data delivery
Assist in engineering new data delivery mechanisms to improve business efficiency
Assist in the commercialisation and syndication of data engineering solutions
Explore and implement new methods of deriving insights from our expansive datasets
Analyse and assess product offerings to be repurposed for potential partner channels
Maintain all documentation required to support the Data Engineering function in accordance with the Hitwise Knowledge Management Portal. Knowledge

Strong technical background in software development
Understanding of Big Data environments and the management of large datasets. Experience

Technical
Expert knowledge and experience of Python coding.
Expert knowledge and experience of SQL
Intermediate Linux environment skills
Ability to maintain security and confidentiality over sensitive information.
Able to work proactively, with customers, team members, and other staff to achieve goals.
Team player with the ability to work autonomously.
Able to engage people on all levels.Experienced with Apache Hadoop, PIG, JAVA , GROOVY or other NoSQL technologies.
Experienced with Business Intelligence and Data Visualisation tools such as Tableau. Business
High level of written and verbal communication and report creation skills.
Strong understanding of Agile methodologies
Ensure adherence to implemented processes.
High level of attention to detail.
Human Relations
Able to work proactively, with customers, team members, and other staff to achieve goals
Team player with the ability to work autonomously
Able to engage people on all levels
Headquartered in Los Angeles, the company operates sites and business services in the United States, the United Kingdom, Italy, Germany, and Australia! We offer top benefits including flexible time off, casual work environment, competitive comp, paid parental leave, free lunch everyday and much more! Could you be our next X'er?"
Senior Data Engineer,NVIDIA,"NVIDIA is hiring creative software engineers for its GPU-accelerated Deep Learning AI team. This role will offer you to opportunity to make a difference in the world by giving you a chance to create and build products used broadly. Academic and commercial groups around the world are using our GPUs to power a revolution in deep learning, enabling breakthroughs in problems from image classification to speech recognition to natural language processing and artificial intelligence. Work in interaction with the deep learning community to implement the latest algorithms.
Are you ready to join a growing team of folks accomplished in Machine Learning and Data Engineering disciplines; passionate about using the best of what lies at the forefront of technology and data science to address answer complex, real-world problems in across diverse gaming and marketing space here at NVIDIA. Some of the tools we use are: Python, Postgres, SPARK, Git, Airflow and Docker, KERAS and Tenserflow, Hadoop.
What youll be doing:
You will be crafting, Implementing and building a large scale distributed data infrastructure using the latest technologies
Develop, maintain and automate data pipelines that will perform the multiple operation ranging from collection, connection, centralization and curation of data from various source including internal and external data sources.
Implement tools to enable extraction, analysis and visualization of data.
Focus on producing high quality production ready code in an agile environment
Collaborate closely with the product team to build new features and infrastructure
Working with the data scientists to implement forecasting and prediction algorithms and models in the data infrastructure
Build and use tools to monitor and debug data pipelines
What we need to see:
BS or MS in Computer Science or a related field
5+ years of relevant experience with data-intensive backend software engineering
2+ yrs in leading projects/tech lead/team lead
Experience building and deploying large-scale data processing pipeline and with Python, Scala or Java [Python preferred]
Workflow, continuous integration and automation tools and processes, like Airflow
Distributed environments such as Spark, Kubernetes, Hadoop, Mesos etc
SQL and Relational Databases, like PostgreSQL
Ability in planning, launching and refactoring phases of code.
Ways to stand out from the crowd:
Prior work in deployment of SPARK and Hadoop applications on Supercomputers.
Experience with applied machine learning or algorithm development and experience working with gaming data and used GPUs for distributed computing
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law. #deeplearning"
Data Engineer,One Kings Lane,"About One Kings Lane
Since launching in March 2009, One Kings Lane has established itself as a premiere home shopping destination. With a breadth of the best-quality furnishings and finishing touches, One Kings Lane provides the access, inspiration, and help to create a style that is uniquely yours. A pioneer in content-led commerce, One Kings Lane speaks to a highly engaged audience via a multichannel ecosystem that includes in-house design services from The Studio at One Kings Lane; social shopping; an online Style Guide; and a seamless shopping experience across mobile, tablet, and desktop.

The Opportunity
The Data Engineer role is an integral part of the OKL Data Analytics team. This role is responsible for having a deep understanding of the inner workings of the hardware and software systems that we make use of to stay in the business of data fulfillment. This role is responsible for organizing and modeling our data to support the needs of Analytics, Marketing, and Operations, as this individual will be the chief expert in those platforms inner workings.

In a nutshell: You have a deep passion for understanding how systems work, love automation, and believe that you can create a world run by machines with your own two hands, keyboard, and cup of coffee. You love the idea of having robust, self-healing systems and thats a goal worth marching toward.

Key Responsibilities
Design, maintenance, enhancement, and simplification over all of Data Warehouse
Assist in developing new ETL jobs that integrate multiple sources
Work closely with business analysts for data acceptance and change management, while ensuring quality
Become an expert on the inner workings of a number of data integration systems and tools
Champion of performance, scalability, and reliability for our data feeds and flows
Involve yourself in the dynamic world of data security and access control
Build tools to monitor the integrity of the end-to-end data flow
Be a first responder and provide triage for data issues
Ensure the overall health of our infrastructure
Skills & Requirements
3+ years of relevant experience
Bachelors degree in Computer Science, Information Systems or similar discipline
Work effectively individually as well as part of a group
Excited by the prospect of learning and working with new or unconventional technologies
A commitment to writing understandable, maintainable, and reusable software
Proficiency with Unix/Linux process, troubleshooting and log spelunking
Advanced SQL Skills
Strong experience with Python, R, Shell scripting
Data Warehousing management a must
Automation (e.g. Chef, Capistrano, automated deployment etc)
Work effectively individually as well as part of a group
Great verbal and oral communication skills
An innate desire to deliver"
Senior ETL/Big Data Engineer,Charles Schwab,"Description:
Westlake - TX, TX2050R, 2050 Roanoke Road, 76262-9616
Brian Parker
20171130-6030

We believe that , when done right, investing liberates people to create their own destiny. We are driven by our purpose to champion every clients goals with passion and integrity. We respect and appreciate the diversity of our employees, our clients, and the communities we serve. We challenge conventions strategically to create value for our clients, our firm and the world. We live and bring to life the concept of own your tomorrow every day. We champion our employee strengths, guide their development, and invest in their long-term success. We hire optimistic, results-oriented, curious, innovative, and adaptable people with the desire to help our clients and one another succeed.

As a company, we were established by Chuck over 40 years ago to champion Main Street over Wall Street, and to help Americans transform themselves from earners to owners. Through advocacy and innovation, we work to make investing more affordable, accessible and understandable for all. As we enter our fifth decade, we are looking for talented, innovative and driven people who believe they can help themselves, and our clients, create a better future.

Our Culture:
Through Clients Eyes: https://www.youtube.com/watch?v=Qkic76FWat8

Our Opportunity:
The Risk Data Aggregation and Risk Reporting (RDARR) team is situated in Corporate Risk Management (CRM). The team is responsible for identifying and defining strategic CRM technology initiatives that advance CRM's RDARR maturity. This position is for a Senior ETL/Big Data Engineer involved in the development, maintenance and documentation of an integrated data infrastructure for corporate risk analytics and reporting.

This will be an individual contributor role.

What youll do:
This role provides a terrific opportunity for someone who has the technical chops and wants to solve complex business problems. This role will be uniquely positioned to work with a variety of Risk teams to implement innovative data management solutions and functionality that provide direct business value while requiring proper IT best practices and controls.

Key Responsibilities Include:
Support the current risk data infrastructure for consolidating multiple data sources into a single source using ETL/ELT

Provide recommendations on Big Data Technology usage and develop data flows in and out of the Big Data environment

Establish productive relationships with internal business partners and information technology partners

Own documentation of business logic and maintenance of procedures

Develop complex SQL queries for small to large sets of data and lead query optimization efforts

Understand process inputs, create solutions to automate data sources and apply data governance standards

Support daily/monthly operations and troubleshooting of issues

Design and architect artifacts using Data Modeling concepts

NOTE: Technical lead or experience managing developers is ok, but you must have hands-on development experience.

What you have:
The following qualifications are required:
Bachelors degree in Information Science / Information Technology, Computer Science, Engineering, Mathematics or a related field

5+ years of related and demonstrable hands-on experience in a development role

Experience in one or more ETL Tools: SSIS, Informatica, Alteryx, or other tools with an understanding of best practices for building and designing ETL/ELT code

Hands on experience with .NET or other programming languages developing custom tools

Database development/engineering experience in one or all of the following databases: MongoDB, SQL Server, Oracle, or Teradata

Strong scripting experience with Python/Scala

Understanding of best practices for building an Enterprise Data Lake

Hadoop and big data experience

Hands on experience with any of the following: Spark, Kafka, Hive, Pig, YARN, REST, APIs and JSON

Experience using Database Modeling Tools: Erwin or other tool

Passion for data with a data warehousing or business intelligence background

Strong data management, analytical, communication, and technical writing skills

Individual must be self-motivated and able to bring projects to their conclusion

The following qualifications are not required, but highly desired:

Sufficient understanding of the General Ledger (ex. Balance Sheet and Income Statement)

Experience with SCM tools such as GIT, TFS, Jenkins and Bamboo is preferred

Experience with Machine Learning concepts

What youll get:
Comprehensive Compensation and Benefits package

Financial Health: 401k Match, Employee Stock Purchase Plan, Employee Discounts, Personalized advice, Brokerage discounts

Work/Life Balance: Sabbatical, Paid Parental Leave, New Mothers returning to work Program, Tuition Reimbursement Programs, Time off to volunteer, Employee Matching Gifts Program

Everyday Wellness: Health and Lifestyle Wellness Rewards, Onsite Fitness Classes, Healthy Food Choices, Wellness Champions

Inclusion: Employee Resource Groups, Commitment to diversity, Strategic partnerships

Not just a job, but a career, with an opportunity to do the best work of your life

Learn more about Life@Schwab .

Charles Schwab & Co., Inc. is an equal opportunity and affirmative action employer committed to diversifying its workforce. It is Schwab's policy to provide equal employment opportunities to all employees and applicants without regard to race, color, religion, sex (including pregnancy, childbirth, breastfeeding, or related medical conditions), gender identity or expression, national origin, ancestry, age, disability, legally protected medical condition, genetic information, marital status, sexual orientation, protected veteran status, military status, citizenship status or any other status that is protected by law. Schwab also does not discriminate against applicants or employees because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. At Schwab, we believe that every employee, through their diverse abilities and experiences, can contribute to our growth, innovation and client loyalty. We embrace diversity and are committed to providing equal opportunity to all employees and applicants. If you have a disability, and require reasonable accommodations in the application process, call Human Resources at 800-725-3535. We will be happy to assist you. Schwab will only share your accommodation request with those individuals who have a specific need to know. The request for an accommodation will not affect Schwab's hiring decisions. All other submissions should be performed online.

Job Specifications

Relocation Offered?:
No

Work Schedule:
Days

Languages:
English - spoken

Current Licenses / Certifications:
None

Relevant Work Experience:
Financial Services-6+ yrs, IT-Change Management/Release Management-6+ yrs, IT-Mainframe (Systems Prog/App Dev)-6+ yrs, IT-DBA-6+ yrs, Analyst/Strategy-6+ yrs, Accounting and Finance-6+ yrs, IT-Other Specialty Engineering-6+ yrs, Risk Analysis

Position Located In:
TX - Westlake

Education:
BA/BS

Job Type:
Full Time

Category: Information Technology

Activation Date: Saturday, December 30, 2017

Expiration Date: Thursday, February 1, 2018

Apply Here"
SENIOR BIG DATA ENGINEER,Charles Schwab,"Description:
Westlake - TX, TX2050R, 2050 Roanoke Road, 76262-9616
Mary O'Brien
20171206-6110

We believe that , when done right, investing liberates people to create their own destiny. We are driven by our purpose to champion every clients goals with passion and integrity. We respect and appreciate the diversity of our employees, our clients, and the communities we serve. We challenge conventions strategically to create value for our clients, our firm and the world. We live and bring to life the concept of own your tomorrow every day. We champion our employee strengths, guide their development, and invest in their long-term success. We hire optimistic, results-oriented, curious, innovative, and adaptable people with the desire to help our clients and one another succeed.

As a company, we were established by Chuck over 40 years ago to champion Main Street over Wall Street, and to help Americans transform themselves from earners to owners. Through advocacy and innovation, we work to make investing more affordable, accessible and understandable for all. As we enter our fifth decade, we are looking for talented, innovative and driven people who believe they can help themselves, and our clients, create a better future.

Job Description

The Schwab organization is looking for a talented and highly motivated Senior Big Data Operations Engineer with a background in Enterprise Operations supporting data warehouse platforms such as Teradata and Hadoop and associated tools for process automation, ETL, job management and code deployment. Our team provides solutions to extract, load and transform data to enable the implementation of business solutions. We are looking for a highly motivated candidate who is willing to quickly learn and understand new technologies and tools to support, maintain and deliver solutions on the Hadoop platform as a Senior Operations Support Engineer on our Big Data Operations team. Our Big Data / Hadoop ecosystem is a key next generation platform for data analytics. This position is responsible for supporting key Big Data systems and applications that handle data ingestion, processing and presentation layers, and scaling the system to handle billions of events in near real-time. In this exciting and challenging role, you will be responsible for support of the Big Data platforms and all associated jobs. You will provide the highest caliber of Technical Support to the Schwab enterprise ensuring jobs are running and data is available. Our Big Data teams are working to expand our culture of Big Data production. You will work to find better ways of capturing the existing data our business units need for improved performance. In addition to being hands-on, youll also serve as a key interface with other Big Data technical development and architecture teams, building new relationships through identification and remediation of issues to improve performance and availability for your customers.

What you'll do:
Provide care and feeding of our Big Data environments and its interfaces built upon technologies in the Hadoop Ecosystem including Hive, Hbase, Spark, and Kafka

Day-to-day monitoring and troubleshooting of jobs, problems and performance issues in our clustersContribute to addressing evolving storage needs to meet changing requirements for scaling, reliability, performance, manageability and price

What youll have:
3+ years of technical support, field or application engineering experience in Big Data technologies such as Spark, Hadoop, AWS, MapReduce, Hive, Pig, Sqoop, Flume, Storm, Spark, Kafka or HBase

Programming experience in Scala, Java, Python and/or C++

Good knowledge of machine learning, data structures, graph algorithms

Understanding of Hadoop file format, data transformation and compression

Strong SQL background is a pluS

Strong Linux/Unix support skills are a plus

Excellent analytical and troubleshooting skills

Excellent customer facing, verbal and written communication skills

Strong desire to work in a fast paced, exiting, growing, environment and make it a success

Experience in supporting cloud based technology a plus

B.S in Computer Science or other equivalent technology desired

Job Specifications

Relocation Offered?:
No

Work Schedule:
Days

Languages:
English - spoken

Current Licenses / Certifications:
None

Relevant Work Experience:
Capital Markets-less than 2 yrs, IT-Change Management/Release Management-2-5 yrs, IT-Communications/Networking-6+ yrs, Business Development and Sales-less than 2 yrs, Analyst/Strategy-6+ yrs, IT-System Administration-2-5 yrs, IT-DBA-2-5 yrs, Banking-2-5 yrs, Financial Services-6+ yrs, IT-Other Specialty Engineering-less than 2 yrs, IT-Distributed and Web Development-2-5 yrs, Investment Management - 2-5 yrs, Risk Analysis

Position Located In:
TX - Dallas

Education:
BA/BS

Job Type:
Full Time

Category: Information Technology

Activation Date: Tuesday, December 26, 2017

Expiration Date: Thursday, March 1, 2018

Apply Here"
Data Engineer,ThinkCERCA,"ThinkCERCA is an education technology startup that helps educators teach critical thinking through argumentative writing. Named a ""game-changing"" education tool by Bill Gates, our product is proven to help students achieve two years of academic growth per year. We're on a mission to make these results a reality for all students across the country.

Were looking for someone is who passionate about web development, works well with a team, and is interested in joining a fast-paced startup as a data engineer to drive our product to the next level. Primary skills desired include proficiency with scripting languages, service oriented architecture, Redshift, and Postgres.

What youll be doing

Building out our data and reporting APIs by expanding our core Redshift database and creating accessible microservices for views into specific slices or targeted transformations.

Working with vast swaths of standards-based educational data to improve the critical thinking capacity and writing ability of students in the US and, eventually, throughout the world.

Maintaining the absolute privacy and anonymity of any data that you work with or expose to the wider internet.

Working with other senior engineers on occasional low level DevOps projects to improve the efficiency of our infrastructure and development process

Competing for the coveted Best With Computer award during our bi-monthly hack nights in obscure new programming languages

Interfacing directly with our QA team to ensure that the features you build have been rigorously verified and are up to spec

Meeting with our stakeholders to understand and refine feature requirements throughout their implementation

Making our team better with your unique viewpoints and interests

What youll need

Experience with or willingness to work in either Ruby or Elixir, which are already in our stack, or one of the following languages, listed in order of preference: Julia, Python, or R. (Bonus points if youve worked with any of the machine learning libraries in any of the above languages)

At least some basic knowledge of how GraphQL works to help understand the type of data that is sent and received from our main front-end

Knowledge of data integrity testing best practices and version control (we use git)

An understanding of the many difficulties and necessary tasks in data services that run over the internet, including: handling unnecessary duplication, anonymizing non-private metadata, intermittent connectivity during loading tasks (using transactions, leveraging idempotence), combining data from multiple asynchronous services, preventing vulnerabilities when decrypting and processing encrypted information, etc

A desire to continually improve both the stack that we all work in and your own skillset

The ability to give and receive thoughtful, constructive feedback on any incoming changes

An awareness of when to act, delegate, and ask for help

An appreciation of discussion and diverse viewpoints

Big Bonus points: knowledge of education or education technology"
Sr Data Engineer,Aetna,"Leads and participates in the design, built and management of large scale data structures and
pipelines and efficient Extract/Load/Transform (ETL) workflows.

Fundamental Components:
Leads and participates in the design, built and management of large scale data structures and
pipelines and efficient Extract/Load/Transform (ETL) workflows.

BACKGROUND/EXPERIENCE desired:
BACKGROUND/EXPERIENCE desired:
5 or more years of progressively complex related experience.
Has strong knowledge of large scale search applications and building high volume data pipelines.
Experience building data transformation and processing solutions.
Knowledge in Hadoop architecture, HDFS commands and experience designing & optimizing queries against data in the HDFS environment.
Knowledge in Java, Python, Hive, Cassandra, Pig, MySQL or NoSQL or similar.
Experience with bash shell scripts, UNIX utilities & UNIX Commands.
Ability to understand complex systems and solve challenging analytical problems.
Ability to leverage multiple tools and programming languages to analyze and manipulate data sets from disparate data sources.
Strong collaboration and communication skills within and across teams.
Strong problem solving skills and critical thinking ability.
EDUCATION
The highest level of education desired for candidates in this position is a Bachelor's degree or equivalent experience.

EDUCATION
The highest level of education desired for candidates in this position is a Bachelor's degree or equivalent experience.

ADDITIONAL JOB INFORMATION
Aetna is about more than just doing a job. This is our opportunity to re-shape healthcare for America and across the globe. We are developing solutions to improve the quality and affordability of healthcare. What we do will benefit generations to come.

We care about each other, our customers and our communities. We are inspired to make a difference, and we are committed to integrity and excellence.

Together we will empower people to live healthier lives.

Aetna is an equal opportunity & affirmative action employer. All qualified applicants will receive consideration for employment regardless of personal characteristics or status. We take affirmative action to recruit, select and develop women, people of color, veterans and individuals with disabilities.

We are a company built on excellence. We have a culture that values growth, achievement and diversity and a workplace where your voice can be heard.

Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
"Software Engineer, Big Data Applications",Conviva,"Have you watched Game of Thrones on hbo.com or live broadcasting of FIFA World Cup Soccer on espn.com? If the answer is yes, you have already used Conviva technologies.
Conviva is processing billions of video events globally every day, performing multistage data transformations and providing real-time analytics to our customers. In order to handle the challenges of processing these large data sets in real-life, we kept on moving quickly alongside with the rest of the industry by both utilizing cutting edge technologies, like Apache Spark, as well as providing innovative in-house platform solutions.
At the core of our engineering culture, we truly believe in the value of technological advancement and what it can do to help with our business. What you get to do

Work with billions of online video events to analyze, design, and build advanced algorithms for video analytics, alerts, and optimization on big data platforms
Develop, document, test, and release production quality code
Debug and fix issues throughout the development and deployment cycles
Analyze, evaluate, research, and prototype new and existing algorithms What you bring to the role

Strong desire to work in a startup company and make it a success
Strong knowledge of OOD/OOP principles, exceptional knowledge of CS fundamentals, distributed systems, data structures, algorithms
Experience building high-quality and scalable production software
Proven system design skills and strong desire to follow software development best practices
Strong communication skills and easy going attitude
BS (with 5+ years of industry experience) or MS/Ph.D. in CS or equivalent (with 3+ years of experience)
Plus: Knowledge of video streaming and transport
Plus: Strong mathematical background
Plus: Experience with Spark and Hadoop
Plus: Experience applying machine learning to solve real-world problems Conviva powers every internet-connected screen with the most engaging viewing experiences imaginable by elevating the way OTT businesses use data-driven intelligence. For years, HBO, Sky, ESPN and the like have been using the Conviva Platform to enlighten, reveal and inform with important insights around consumer in-screen viewing experience allowing them to connect those metrics to important business outcomes. This allows customers to not only maximize subscriber retention and growth but also understand content and viewing trends so that they can deliver more personalized viewing experiences. We make engagement a data-driven outcome based on actionable quality of experience (QoE) analytics. Conviva is privately held and headquartered in Silicon Valley, California, with offices in New York and London. For more information, please visit us at www.conviva.com"
Machine Learning Engineer,Asimov,"Asimovs Biodesign Software Team builds our computer-aided design platform for programming biological systems.

Position: Were looking for a full-time Machine Learning Engineer to develop advanced functionalities for our cell biophysics engine and -omics analysis pipeline. The ideal candidate is obsessed with the question of how to best engineer complex biological systems. This is a unique opportunity to work at a nimble, forward-thinking synthetic biology startup and help build the foundation for engineering biology.

As part of the Biodesign Software Team, you will:
Use machine learning techniques to better design and debug genetic systems
Innovate new approaches to guide molecular and cellular engineering
Collaborate frequently with the Synthetic Biology Team to incorporate biological design principles and experimental data into the software pipeline
Communicate methods and results with other scientists, industry executives, and academic researchers
Manage software project and write elegant code
Work effectively as part of a multifunctional team in support of a synthetic biology design platform
Qualifications:
B.S. or M.S. in Machine Learning, Computer Science, or a related field
Knowledge of cell and molecular biology a must
2+ years of machine learning experience
Ability to work both independently and in a collaborative team environment
About us: Were fueled by a vision to transition synthetic biology to a fully-fledged engineering discipline. Should you join our team, you will grow with a constantly evolving organization, and push the frontiers of biological engineering. Culture is key to Asimov - we believe that our mission can only be achieved by a diverse team that brings a mixture of perspectives to creating a future powered by engineered biology."
Data Engineer,Intellipro Group Inc.,"DESCRIPTION

Are you a fan of data and enthusiastic about applying data (big and small) to real product? Can you help to make significant contributions in an innovative application project with cutting edge Client, data science and AI skill? Does it sound exciting to you to improve billions of people's daily life and convenience?

If these describe you, we would love to hear from you!

DUTIES AND RESPONSIBILITIES (Including Essential Functions)

Help to evolve large scale Big Data platform and tools in all aspects, from data collection to data processing, exploration, visualization and modeling.

Create and implement large scale solutions.

Apply Client algorithms and solution in real business engagements.

Help create and demonstrate significant business values.

JOB SPECIFICATIONS

Education/Knowledge:
Major/Discipline: Computer Science/Statistics/Math/EE or related.

Degree Requirement: Master's Degree with a minimum of 3 years of experience.

Skills & Abilities (Technical or General):
Requirements:
Knowledge of basic data analysis techniques (probability, statistics, machine learning).

Experience in building analytics platform for big data and cloud application;

Experience in data pre-processing, data cleansing and access;

Scale up from prototypes to production solution in big data and cloud environment.

Experience in implementing and validating data algorithms and models with solid programming skills (such as Python, R, Java or other) and machine learning libraries(such as TensorFlow, Keras, MLlib, Scikit-learn or other)

Familiar with Big Data tools such as Hadoop, Spark and Flink.

Strong communication skills and ability to work well as part of a team."
Data Visualization Software Engineer,KPMG,"Known for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today's most important industries. Our growth is driven by delivering real results for our clients. It's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it's no wonder we're consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you're as passionate about your future as we are, join our team.
KPMG is currently seeking a Data Visualization Software Engineer , to join our Advanced Analytics Organization.

Responsibilities:
Work in cross-disciplinary teams with KPMG industry experts, data scientists, and software engineers to gauge client needs for consuming analytics, understand user profiles, competencies, and priorities
Utilize analytics to explore data, and identify key trends and insights from multi-faceted, and high-dimensional data, and work with data scientists to create diagnostic, predictive, and prescriptive insights
Rapidly iterate, prototype, and implement visualizations that meet client visualization needs, considering composition, color, texture, and optimizing the depth and information density for the target user
Utilize interactive drill-downs, movements, and highlights to add layers of insight, and engage users to explore data
Explore a variety of visualization methodologies, from Data Discovery (such as Tableau, Qlikview), custom javascript methods (such as d3.js, raphael.js), and multichannel/mobile deployment

Qualifications:
Bachelor's degree from an accredited college/university in Information Design, Interaction Design, User Experience, Human Computer Interaction, Informatics, Data Science, Computer Science, or similar field with five years of experience, Master's degree with two years of experience or PhD with one year of experience
Solid analytical skills and basic understanding of statistical and machine learning models
Ability to perform data tasks, working with a variety of SQL, NoSQL, and HDFS data stores, and preparing data for visualization
Solid understanding of visual composition and visualizations; familiarity with ethnographic research preferred
Experience with one or more of the following: data discovery software (such as Tableau or Qlik), web-based visualization (d3.js, raphael.js) or Javascript/Python visualization methods
Applicants must be currently authorized to work in the United States without the need for visa sponsorship now or in the future
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an equal opportunity employer. All qualified applicants are considered for employment without regard to race, color, creed, religion, age, sex/gender, national origin, ancestry, citizenship status, marital status, sexual orientation, gender identity or expression, disability, physical or mental handicap unrelated to ability, pregnancy, veteran status, unfavorable discharge from military service, genetic information, personal appearance, family responsibility, matriculation or political affiliation, or other legally protected status. KPMG maintains a drug-free workplace. KPMG will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable local, state or federal law (including San Francisco Ordinance number 131192). No phone calls or agencies please."
"Data Engineer, Digital Markets","Gartner, Inc.","Gartner Digital Markets is an innovative web-focused business that provides demand generation services for the software industry. Were looking for an experienced Data Engineer with a strong technical and analytical background, to help convert our substantial data assets (spanning three websites, millions of unique visitors per month, and +40,000 listed software products) into value added offerings for our clients and consumers. As a Gartner company, Digital Markets can offer you the benefits of a fast-paced entrepreneurial workplace and the professional growth opportunities of a global organization.

Establish frameworks for quickly rolling out new data analysis for standalone data-driven products and service to support our network of sites.
Have end-to-end responsibility for leading projects focused on extracting, merging, analyzing and managing large sets of data across multiple, disparate databases
Be able to transform unstructured raw data in to formats suitable for modeling
Apply statistical and advanced techniques (e.g. segmentation, machine learning) to develop both prototypes and scalable (i.e. efficient, automated) data analyses
Demonstrate a deep knowledge of, and ability to operationalize, leading data technologies and best practices
Be responsible for maintaining analytics plans, clean code, and well-written documentation
Be able to work in teams and collaborate with stakeholders to define requirements
Make decisions independently on analytical problems and methods
Be able to identify and suggest novel areas of future work for themselves or the team
Qualifications

Bachelors or Masters degree in Computer Science, Applied Statistics, or a related field
2+ years of data engineering or other data-intensive development experience
Experience processing large amounts of structured and unstructured data
Advanced knowledge of programming languages such as Python, Java, C/C++ or C#
Experience building scalable data models and performing complex relational databases queries using SQL (Oracle, MySQL), R, etc.
Familiarity and experience with web analytics tools (e.g. Google Analytics, etc.)
Deep knowledge of algorithms and statistics and at least some experience in data mining, machine learning, and/or natural language processing
Attention to detail and demonstrated ability to detect and resolve data/analytics quality issues
Primary Location

: United States-CT-Stamford
Work Locations

:
NA-US-CT-Stamford 56TGR
56 Top Gallant Road P.O. Box 10212
Stamford 06904-2212
Job

: Marketing
Organization

: 100 New Markets I - 0699
Schedule

: Regular
: Full-time
Job Posting

: Dec 22, 2017, 3:15:04 PM"
Data Engineer,Gruppo Mutui Online,"Societ facente parte di Gruppo MutuiOnline e interamente controllata, ricerca un Data Engineer per un importante progetto finalizzato alla sperimentazione di nuove tecnologie e allo sviluppo di soluzioni innovative a supporto del nostro business.

Se ti piace lidea di lavorare su grandi moli di dati, strutturati e non, rendere scalabili ed ottimizzare dei modelli di machine learning e generare valore, allora questa opportunit fa per te.

Ti inseriremo nel team di AI & Machine Learning, assumerai un ruolo chiave allinterno della nuova area strategica e avrai la possibilit di contribuire allidentificazione delle migliori soluzioni tecnologiche e innovative adatte ai nostri clienti. Lavorerai con esperti nel settore e potrai realizzare applicazioni che cambieranno il panorama finanziario italiano.

Avrai la possibilit di
 Intraprendere un percorso formativo e professionale ad alto livello, partecipare a scelte strategiche per lAzienda, con contatti diretti con il Top Management
 Analizzare e creare nuovi processi di business interpretando la vasta base dati disponibile
 Proporre soluzioni creative per lottimizzazione dei processi esistenti usando tecniche di Artificial Intelligence e Machine Learning
 Utilizzare le pi recenti tecnologie del settore Big Data ed essere driver di innovazione e digital mindset

Di che cosa ti occuperai
 Gestione, normalizzazione e trasformazione di grandi moli di dati
 Ottimizzazione di modelli statistici e matematici grazie al calcolo distribuito
 Elaborazione di algoritmi applicati a problemi reali
 Applicazione di tecniche di machine learning per generazione di risultati data-driven in tempo reale

Cerchiamo in te voglia di imparare e di metterti in gioco in questa nuova sfida dimostrando passione, creativit, curiosit, forte interesse per la tecnologia per un progetto che rappresenter una grande evoluzione del nostro business.

Se hai
 Una laurea in Matematica, Fisica, Ingegneria Elettronica, Informatica ed Economia
 Unesperienza da 1 a 4 anni nellinnovazione o nellIT presso grandi realt aziendali
 Conoscenze IT: SQL / no-SQL database, Linux/Unix/Windows server, Hadoop, Cloudera, Python, Pig, Spark, Scala, scripting, programmazione GPU, TensorFlow
 Conoscenze settoriali: matematica, statistica ed algoritmi supervisionati / non supervisionati

e possiedi anche le seguenti caratteristiche:
 Hai buone doti di analisi
 Sei attento e meticoloso
 Sei positivo, proattivo e curioso
 Sei creativo e innovativo
 Hai ottime doti di comunicazione sia scritte che verbali
 Conosci linglese

Sei il nostro candidato ideale!

Se sei interessato alla nostra proposta invia, solo dopo aver visionato il nostro sito istituzionale www.gruppomol.it, il tuo Curriculum Vitae per partecipare alle prossime selezioni, allindirizzo mail: selca@gruppomol.it, specificando il Rif.: AIML-DE"
Machine Learning Engineer,"Linc Global, Inc.","Why you should join:

Join us in building the best ecommerce SaaS platform to revolutionize how retailers and brand engage with shoppers for post-purchase activities and more. Linc is looking for a machine learning developer who is passionate about applying ML techniques from natural language understanding, dialog system, recommendation system, knowledge extraction to deep learning. We offer a fun, dynamic and result-driven environment. If scale and complexity in a high growth startup excite you, Linc is the right place to be.

About Linc:

Linc is an AI-powered Customer Care Automation Platform that transforms one time purchasers into lifetime customers through a better than Amazon experience around tracking, returns, exchanges, and reordering. We also allow retailers to re-target and offer cross sells and upsells based on purchase history rather than browser history. We make this available to e-retailer or brand client via a number of channels including; web, email, text, and the emerging channels of Facebook Messenger and Voice. The end result is higher shopper retention rates and a higher lifetime value with those clients.

As a fast growing startup, Linc has delighted tens millions of shoppers, and customer list includes world-class brands like Carters, P&G, Crocs, GoPro, Hugo Boss, eBags, and LOreal. Learn more at letslinc.com.

Responsibility:
Works on one or more technical areas of applications of Natural Language Understanding, Dialog system, Statistical NLP, Contextual Understanding, Probabilistic Inferencing/Reasoning, Product Recommendation
Develops and demonstrates viability of novel solution in the context of Lincs platform and data to deliver business results as well as best of breed user experience.
Be a part of an engineering team with total responsibility of both algorithms, systems and production scaling and support  full soup to nuts ownership and empowerment!

Key Qualifications:
3-7 years with at least 2 years in Machine Learning, Statistics and NLP related fields
MS or PhD program in Computer Science, Mathematics or a related field with demonstrated academic excellence
3+ years of programming experience, proficiency in Python and at least one of the objective oriented languages such as Java/C++/Go/Scala
2+ years of hands-on cloud platform experience: AWS or Azure
Strong knowledge in general software design, data structures, and relational database
Strong written and oral communication skills
Strong analytical skills, data driven mindset

Desired:
Previous experience working in a start-up environment
Experience with Ecommerce space, knowledge of product taxonomy building
Experience with NLP toolkits such as NLTK, OpenNLP, Stanford CoreNLP etc.
Experience with open source machine learning toolkits: TensorFlow, CNTK, scikit-learn, pandas, numpy, libsvm, Keras

Perks
We will sponsor Visas
Full medical and dental coverage
Company provided lunches and close to downtown Sunnyvale
Knowledge sharing sessions with some of Silicon Valleys best minds"
Data Engineer,Harris IT Services,"Peraton is seeking a Data Engineer in Clarksburg, WV to analyze data and produce data profiles in support of business objectives for the United States Department of Justice (DoJ), Federal Bureau of Investigation (FBI), National Crime Information Center, 3rd Generation Enterprise Business Intelligence platform and analytic services.

JOB DESCRIPTION :
This position will be responsible for the following:
Provide technical expertise in the data design, development, and analysis of IT operation environments.

Develops data analysis strategies of the client/server systems and facilitate such architecture design to support overall customer objectives.

Researches new data analysis technologies and advises client as required.

Develops strategies for conducting data studies to estimate the space, computer hardware, software and connection infrastructure resources required on a short and long term basis.

Develops data models of the computer and computer application environment, identifies and resolves computer operations systems performance problems.

Measure system data usage by workload type, identifying trends and estimating further resource requirements.

REQUIRED EXPERIENCE :
Bachelors Degree in related discipline

At least three years of experience with analysis of data to produce data profiles in support of business objectives

At least three years of experience with common scripting languages and tools such as Perl, Python, etc.

At least three years of experience with SQL

At least three years of experience generating data sets to meet specific data profiles to be used for all levels of testing, including unit, integration and performance testing

At least two years of experience with Hadoop-based analytics

Strong communication skills

PREFERRED QUALIFICATIONS :
A Data Engineer certification, such as Googles Certified Professional  Data Engineer, IBM Certified Data Engineer is a plus

Prior work experience with the Federal Bureau of Investigation preferred

SECURITY CLEARANCE :
This position requires the candidate to already have a current Top Secret security clearance and to maintain the clearance.

About Peraton

Peraton provides innovative, reliable solutions to the nations most sensitive and mission-critical programs and systems. Peraton has significant experience providing highly differentiated secure communications, space, and technology solutions to key customers, and has become a trusted partner on missions that are critical to the security priorities of the United States. Capabilities include complex software and technology services and solutions, as well as end-to-end mission operations abilities, including Software Systems Development, Cyber, Modeling & Simulation, Mission Operations, Signal Intelligence (SIGINT), and Quick Reaction Capabilities (QRC) / Research & Development. The company is headquartered in Herndon, VA, with approximately 3,500 employees across the U.S. and Canada.

We are an Equal Opportunity/Affirmative Action Employer. We consider applicants without regard to race, color, religion, age, national origin, ancestry, ethnicity, gender, gender identify, gender expression, sexual orientation, marital status, veteran status, disability, genetic information, citizenship status, or membership in any other group protected by federal, state, or local law.

CN20173011-20508"
Data Engineer,Aruba Networks,"Aruba, a Hewlett Packard Enterprise Company, is a leading provider of next-generation networking solutions for the mobile enterprise: http://www.arubanetworks.com/company/about-us.

The company designs and delivers Mobility-Defined Networks that empower IT departments and #GenMobile, a new generation of tech-savvy users who rely on their mobile devices for every aspect of work and personal communication. To create a mobility experience that #GenMobile and IT can rely upon, Aruba Mobility-Defined Networks automate infrastructure-wide performance optimization and trigger security actions that used to require manual IT intervention. The results are dramatically improved productivity and lower operational costs.

We are looking for a Data Engineer who is responsible for designing and implementing data pipelines at Big Data scale.

Your Responsibilities
Implement parsers and validators for new Log sources
Implement ETL transformers to reformat and enhance the data
Implement ETL correlators to update the data from multiple data sources
Work on tools and APIs to visualize the backend data
Troubleshoot performance and data related problems
Work with the Analytics team in defining the schema for new data sources

Our Minimum Requirements for This Role Are
4+ years java and/or Python development experience
Experience working with Hadoop or Big Data (HDFS, Parquet, HBASE)
Experience working with Large scale databases like Cassandra
Experience working with Map Reduce or Spark, ElasticSearch, Kafka
Experience working with Databases like Postgres, SQL

Education

Bachelor's or Master's degree in Computer Science, or equivalent and typically 4-6 years experience.

Benefits youll enjoy

At Aruba, a Hewlett Packard Enterprise Company, we offer an exciting and fun work culture, driving innovation, collaboration, and growth. We place our customers first, deliver some of the most innovative technologies to the market, and have fun doing it all! Come join our team and be part of an exciting organization poised for success!

Thanks for taking the time to review our job, if you think it is a match to your experience and interests please apply today  we are eager to learn more about you! We have dozens of openings, so encourage your friends to apply as well!

#ArubaNetworks #GenMobile #ArubaNetworksJobs #HPE #HewlettPackardEnterprise

Please note the above statements describe the general nature and level of work only. They are not a complete list of all required responsibilities, duties and skills. Other duties may be added, or this description amended at any time."
Data Engineer,"Visual IQ, Inc.","This role provides strong technical leadership within the Development organization ensuring were making smart technology and implementation choices. This is an opportunity to write amazing code for something at significant scale with talented peers.
Collaborate with Product Management and Quality Assurance to develop and deliver high-quality products and features in a timely manner
Have finger on the evolving technical landscape and understand how new technologies could benefit our product solution
Actively participate as a Scrum team member  participating in all phases of the Scrum process
Improve the strength of the technical team around you  sharing your knowledge and mentoring other developers
Plan, monitor and improve the performance of a distributed application to process a lot of streaming data
Ensure that code adheres to strict standards and best practices for accessibility, security, object-oriented practices, quality and performance
Qualifications
2+ years of experience developing Enterprise Software
2+ years developing software in a Software as Service (SaaS) product development environment
Experience with Java 8, Python programming languages.
Strong expertise with Big Data and Hadoop technologies
Strong expertise with Apache NIFI/Hortonworks DataFlow.
Possess demonstrated expertise in NoSQL/SQL technologies and data architecture
Experience in NoSQL/SQL database design, development and data modeling
Practical knowledge/expertise in Hive and/or Spark SQL.
Hand on and Solid understanding on HDFS.
Hands-on experience with HBase/ Streaming viewed as a value add
Strong experience with SCRUM methodologies
Working knowledge of JIRA, Confluence and Git
Experience working with Docker is a plus.
Experience working with AWS and other cloud providers.
Excellent communication skills. Able to successfully evangelize and gain buy in for a technology or solution
Bachelors degree in Engineering with Computer Science major or similar qualification. Advance degrees preferred"
Software Engineer - Big Data,Trimble Inc.,"Title: Software Engineer - Big Data

Location: : Coralville, Iowa

ISE

Innovative Software Engineering (ISE) is a leading engineering and systems integration firm that delivers innovative, end-to-end mobile and enterprise solutions. ISEs business includes three interrelated divisions:
Professional Services offers clients a complete spectrum of software engineering services, including IoT / Telematics, Big Data, Cloud, Mobile App Development and Agile Consulting

Innovation Services provides the capability to achieve and sustain short and long-term growth throughout the entire organization. This approach allows ISE to remain focused on enhancing the customers experience as they look for technology solutions

Fleet Services leverages decades of transportation technology expertise to offer a configurable end-to-end solution to manage drivers hours-of-service and vehicle maintenance compliance, which results in a safer, more profitable operation

Position Overview
Modifying existing software and creating new software

Developing and directing software system testing and validation procedures, programming and documentation

Conferring with systems analysts, engineers, programmers and others to design system and to obtain information on project limitations and capabilities, performance requirements and interfaces

Analyzing user needs and requirements to determine feasibility of design within time and cost constraints

90% Design, Development and Testing

10% Agile Activities: Backlog grooming, Sprint Planning, Daily Stand-ups, Retrospectives

Required Skills:
2+ years of experience as a Software Engineer in a professional environment or a Bachelor's degree or higher in Computer Science or related field.

Experience with Python, R, and/or Java

Big Data  experience with Hadoop, including HDFS, MapReduce, Sqoop, Oozie and Hive

Preferred Skills:
Machine Learning or AI experience

Data Science or Data Analytics

Trimble Inc. is proud to be an Equal Opportunity and Affirmative Action Employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, status as a covered veteran in accordance with applicable federal, state and local laws, or any other protected factor. EOE/M/F/"
Big Data SQA Engineer,Palo Alto Networks,"Palo Alto Networks is the next-generation security company, leading a new era in cybersecurity by safely enabling applications and preventing cyber breaches for tens of thousands of organizations worldwide. If you are motivated, intelligent, creative, hardworking and want to make an impact, then this job is for you!
Our Quality Assurance team is responsible for testing our firewall, threat intelligence cloud, and cloud services offerings.
The Cloud Services QA team is responsible for testing our cloud offerings including threat intelligent cloud (AutoFocus), cloud based Logging Service and related services.
You will be working on our Cloud Services QA team who are responsible for testing different components of the system for gathering/ingesting data from multiple heterogeneous sources and processing data for reporting/analytics.

Opportunities:
You will
work alongside our research teams including data scientists to derive intelligence from our data.
work on diverse projects relating to Big Data, distributed systems.
work on test automation and help to expand the test coverage
work on distributed systems focused on validating data across several systems.
work on integration tests with other Palo Alto Networks products.

Skills:
The successful applicant should
Have knowledge of large data processing tools such as Hadoop, HBase, ElasticSearch, RabbitMQ, or Kafka.
Strong computer networking knowledge including networking protocols such as TCP/IP.
Have experience scripting in Python, Perl or Bash.
Have relevant work experience either as a past intern or full time.
Requirements  To apply, you must be pursuing a 4-year Undergraduate Degree, a 2-year Masters Degree or a Doctorate degree. You must have authorization to work within the United States.
Learn more about Palo Alto Networks here and check out our fast facts"
Software Engineer,Change Healthcare,"Designs, modifies, develops, writes and implements software programming applications. Supports and/or installs software applications/operating systems. Participates in the testing process through test review and analysis, test witnessing and certification of software. Designs, plans, and coordinates work teams. Provides technical support to project team members. Requires a bachelor's degree in a related area and 6-8 years of experience in the field. Familiar with a variety of the field's concepts, practices, and procedures. Relies on experience and judgment to plan and accomplish goals. Performs a variety of tasks. Leads and directs the work of others. A wide degree of creativity and latitude is expected. Typically reports to a head of a unit/department.

Looking for a person to build AWS applications and integrating machine learning to lower the cost of healthcare by looking for insights based on big data, efficient payments for providers, fraud/waste/abuse, accuracy and automation. The candidate would be expected to be a strong Java developer with good exposure to core Java architecture principles who also has functional programming experience with Scala. This person would be doing both front-end application screens as well as back-end design and development on a small team that is building state of the art applications (current tech, HA, scalable, automated) that use advanced analytics to enhance the revenue generation needs of our clients. Be part of a team that is all about sharing and learning as a group and building relationships with the business and product owners and collaborating to improve the products.

Must have:
Experienced with UI frameworks. React/Angular and various *.js
Some UI/UX design/dev experience
Current and solid experience in Java 1.6/1.7 or 1.8
Frameworks: Java Servlets, JDBC, EJB
Experience building and using RESTful JSON APIs
Experience with AWS EMR Spark/Scala
Comfort in Unix/Linux including shell scripting
Proficient with various databases and object storage: Oracle, SQL Server, dynamo, mongo, Redshift, S3
Must have some CI/CD setup knowledge and a strong propensity for automation of everything (AWS cloudformation and AWS CLI strongly desired)
Internet Languages: HTML/5, DHTML, XHTML, JQuery, JavaScript, XML, and JSP

Must have soft skills:
Should be excited to work with AWS and incorporate new application design techniques (micro-services, distributed apps and DBs, etc)
Must have recent hands-on coding experience and be highly motivated to deliver functionality for the customer (customer centric)
Must have good problem solving skills and like to learn new things
Must be able to demonstrate thought leadership and not just be a coder
Must have good communication and English language skills  both written and verbal.
Must have good experience with key architectural principles and design patterns
Must have the desire and confidence to share knowledge and opinions to design applications yet also have a collaborative team focus.

Ideal:
Healthcare experience (detailed Claim/payment data knowledge strongly desired)
Machine learning experience (implementation of random forest or other algorithms)
Big data experience with Spark or Hadoop/hive
Workflow management systems experience
Middleware: JMS, SMS, ActiveMQ, AWS SQS/SNS, Spark Streaming, AWS lambda, AWS glue, AWS Batch, AWS DataPipeline
Web Frameworks: Struts, JSF/Ice Faces, Spring MVC, Java Servlets, Server Frameworks/Components : JDBC, EJB1.1/2.0/3.0, Spring Batch.
Js Libraries/Frameworks: Angular Js 1.2, React Js 0.14, Knockout Js, J Query, node Js platform
Project Management Tools: Jira
Build/Deployment Tools: Maven, Ant, Nexus, Jenkins, shell, cloud formation, rundeck
Version Tools: gitlab
Application Server: Tomcat, Glassfish."
Data Engineer,Abbott Laboratories,"At Abbott, we're committed to helping you live your best possible life through the power of health. For more than 125 years, we've brought new products and technologies to the world -- in nutrition, diagnostics, medical devices and branded generic pharmaceuticals -- that create more possibilities for more people at all stages of life. Today, 94,000 of us are working to help people live not just longer, but better, in the more than 150 countries we serve. Please note that candidates need to be eligible to work in the U.S. without Abbott sponsorship.
Main Purpose of Role
This will be a foundational member role in a small team of talented and highly motive data engineers, and big data architects to create big data based advanced analytical platforms and products

Main Responsibilities
Create, deploy and optimize large scale data
Use extensive data engineering expertise to design and build solutions/ products for analyzing large data sets and identify patterns and relationships
Manage data sources, organize data and create data assets using identified open source or proprietary tools
Work closely with SMEs, functional experts in Commercial, R&D, finance, etc. for building data pipeline from structure and unstructured data sources
Work on newest tools and technologies powering these analytics wave - Scala, Scalding, Spark, Hadoop
Deploy advanced machine learning techniques and algorithms and work in a truly Big Data way
Travel occasionally per needs of the assigned project.

Qualifications

Education
Education Level
Major/Field of Study
Or
Education Level
Bachelors Degree ( 16 years)
Bachelors degree in any of the following  Math, Physics, Computer Science, Statistics, Economics, Quantitative Sciences
Experience/Background
Experience
Experience Details
Minimum 16 years
Strong problem-solving skills
Experience in any from AWS, Python, R, Spark, hive, HBase, Hadoop, Kafka, YARN etc. will be a plus
Attention to detail and organization/ documentation skills
Ability to prioritize and triage deadline-driven tasks in a high-pressure environment
Basic knowledge of distributed computing, parallel processing and large scale data management
Experience manipulating and analyzing complex, high-volume data from varying sources
Ability to communicate complex quantitative analysis in a clear, precise, actionable manner"
Software Engineer - Distributed Computing - Big Data Research Platform,"23andMe, Inc.","Join 23andMe and solve cool and challenging engineering problems implementing a large scale data store, building data processing pipelines in the cloud, and powering novel research and analytical capabilities. Work within a collaborative environment of small fast-paced development teams that have a mix of veteran engineers and recent graduates. Come join us in our mission to be the world's trusted source of personal genetic information!

Who we are
Since 2006, 23andMes mission has been to help people access, understand, and benefit from the human genome. We are a group of passionate individuals pushing the boundaries of whats possible to help turn genetic insight into better health and personal understanding.
What you'll do
Work on the design and implementation of new infrastructure, features, and frameworks in AWS cloud.
Build data analysis pipelines for statistical phasing, imputation, & association analyses
Participate in product development process
Write unit and functional tests
Solve performance and scalability problems
What you'll bring
B.S. in Computer Science or a related field
Strong knowledge of Python and Spark
Experience engineering high volume data pipelines and scientific workflows
Experience working with Amazon cloud services: AWS, Cloud Formation, etc.
Experience with distributed computing and large scale data processing
Optional: Experience with Computing optimizations like GPU-computing, Multi-threading, etc.
Optional: Knowledge of C and Scala programming language
Experience solving problems with Hadoop and related technologies (HDFS, MapReduce, HBase, Spark, etc.)
Interest in bioinformatics
Thrive with minimum supervision to deliver well-designed high-quality code on time
About Us
23andMe, Inc. is the leading consumer genetics and research company. Our mission is to help people access, understand and benefit from the human genome. The company was named by MIT Technology Review to its 50 Smartest Companies, 2017 list, and named one of Fast Companys 25 Brands That Matter Now, 2017. 23andMe has millions of customers worldwide, with ~85 percent of customers consented to participate in research. 23andMe is located in Mountain View, CA. More information is available at www.23andMe.com .

At 23andMe we value a diverse, inclusive work force and we provide equal employment opportunity for all applicants and employees. All qualified applicants for employment will be considered without regard to an individuals race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. 23andMe will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.

Please note: 23andMe does not accept agency resumes and we are not responsible for any fees related to unsolicited resumes. Thank you."
Machine Learning Engineer,TransUnion,"But, having data means nothing without curious analytical minds to unlock the meaning within it. Our analytics team is home to some of the most brilliant minds in the market. Here, we will not only understand your stats jokes but appreciate them. We come to work every day to quantitatively explain how things happen and why. Why? Because the work that we do allows people to have access to credit, enables new generations to pursue an education and builds economies from the ground up. TransUnion is a place where its cool to be smart. Have we piqued your interest? Well, read on

Dynamics of the Role
As a Consultant on our team, you will join a world class group of statisticians, data scientists, mathematicians and modelers on a mission to extract insights from information and put them to good use. You will have an opportunity to be a part of a variety of analytical projects in a non-siloed environment and be recognized for the work you deliver. TransUnion offers a culture of lifelong learning and as a Consultant here, your growth potential is limitless.

How Youll Contribute:
Collaborate with internal and external partners to deliver innovative analytical products and insights. You will be directly involved in the development of predictive modeling and business intelligence solutions for clients such as credit lenders, insurance carriers and other financial services institutions
Contribute to projects involving predictive, descriptive and prescriptive analysis leveraging a variety of techniques (such as machine learning, segmentation, logistic regression, survival analysis, principal component analysis, Monte Carlo simulation and sensitivity analysis)
Dig in by extracting data and performing segmentation and statistical analyses on large population datasets (using languages such as R, SAS, SQL and Python on Linux and PC computing platforms)
Deliver analytic insights and recommendations in succinct and compelling presentations for internal and external customers at various levels including an executive audience
Help to cultivate an environment that promotes excellence, innovation and a collegial spirit
Serve as a subject matter expert on the team and act as a mentor and advisor

What Youll Bring:
A graduate degree in the hard sciences with advanced coursework in statistics
4-8 years of professional analytical modeling experience, solving problems relevant to the problems we solve at TransUnion
Expert proficiency with one or more statistical programming languages such as R or SAS; additional experience writing intermediate SQL queries for data extraction preferred
Strong written and verbal communication skills: ability to clearly articulate ideas to both technical and non-technical audiences
Strong analytical, critical thinking and creative problem solving skills
Strong time management skills with the ability to prioritize and contribute to multiple assignments simultaneously

Who We Are:
At TransUnion, we are dedicated to developing ways information can be used to help people make better and smarter decisions. As a trusted provider of global information solutions, our mission is to help people around the world access the opportunities that lead to a higher quality of life. We do this by helping organizations to optimize their risk-based decisions and enabling consumers to understand and manage their personal information. Because when people have access to more complete and multidimensional information, they can make more informed decisions and achieve great things.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.

What Youll Bring:
A graduate degree in the hard sciences with advanced coursework in statistics
4-8 years of professional analytical modeling experience, solving problems relevant to the problems we solve at TransUnion
Design and build out complex machine learning systems
Expert proficiency with one or more statistical programming languages such as Python, R or SAS; additional experience writing intermediate SQL queries for data extraction preferred
Strong written and verbal communication skills: ability to clearly articulate ideas to both technical and non-technical audiences
Strong analytical, critical thinking and creative problem solving skills
Strong time management skills with the ability to prioritize and contribute to multiple assignments simultaneously

Who We Are:
At TransUnion, we are dedicated to developing ways information can be used to help people make better and smarter decisions. As a trusted provider of global information solutions, our mission is to help people around the world access the opportunities that lead to a higher quality of life. We do this by helping organizations to optimize their risk-based decisions and enabling consumers to understand and manage their personal information. Because when people have access to more complete and multidimensional information, they can make more informed decisions and achieve great things.

We are an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability status, protected veteran status or any other characteristic protected by law.

Work Locations|Chicago

60661

Organization|TransUnion Interactive
Schedule|Full-time
Shift|Day Job
Job Type|Standard"
"Software Engineer, Data & Analytics","Guidewire Software, Inc.","Foster City, CA, United States

Are you a strong, distributed systems engineer with interest/experience in data & analytics? Are you interested in learning and working with cutting-edge technologies such as Spark, ElasticSearch, Kafka and AWS? Are you interested in contributing to new Cloud products? Guidewire Data & Analytics engineering has an exciting opportunity for you!

You will play a significant role in the design and development of microservices, applications and tools for Guidewires next generation data & analytics platform in the cloud. The Guidewire data & analytics platform mission is to be best-in-class in enabling our customers to optimize their business with insights and decision-making capabilities. You will work in a highly collaborative environment in agile, cross-functional teams.

What you would do
Design and develop microservices, apps and tools
Contribute to innovative efforts in processing data accurately at scale
Own Continuous Integration (CI) and Continuous Deployment (CD) for your services
Write and maintain automated tests
Own, troubleshoot & resolve code defects
What you would need to succeed
Excellent programming ability in Java and/or Scala
Solid grasp of distributed systems
Solid technical communication skills
Ability to work in a collaborative environment
Desire to learn
BS/MS degree (Computer Science or Math)
2+ years relevant work experience
Bonus
Proficiency in Spark or Hadoop
Experience building, deploying and operating production services on AWS or similar
Working knowledge of Kafka
Working knowledge of ElasticSearch
Startup experience"
Data Engineer,Trimble Inc.,"Position Purpose

This position will be responsible for the analysis, design and continued implementation of TMWs enterprise-wide Business Intelligence needs. The position will also be involved in the analysis, design and architecture of TMWs Business Intelligence platform, and will require participation in the creation and ongoing updates to the Business Intelligence strategy roadmap, developing and documenting strategies for an internal enterprise data warehouse implementation, data visualization, and generation of analytics.

Position Accountabilities
Translate business requirements into the design of the overall enterprise data architecture, data governance strategies and data quality standards
In-depth knowledge of relational database concepts  including design, implementation, programming, day-to-day administration, and support best practices
Designs and implements an enterprise data warehouse infrastructure including data marts, data models/dimensions and metadata repository
Promotes the functionality, scalability, performance, security, and integration requirements necessary to an enterprise business intelligence platform
Develops documents and maintains a formal description of the data and data structures.
Develops entity and attribute descriptions and definitions for the models and ensures that conflicts in descriptions and definitions are resolved
Support implemented BI solutions by: monitoring and tuning queries and data loads, addressing user questions concerning data integrity, monitoring performance and communicating functional and technical issues
Minimum Requirements
Using strong analytical skills to solve and model complex business requirements
Hands on relational and multi-dimensional data modeling, including multiple source systems from databases and flat files, metadata repository development and the use of standard data modeling tools
Supporting data warehouse documentation, installation, implementation, training and support activities
Displaying a sound understanding of BI Best Practices/Methodologies, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques, including data visualization concepts
Collaborating on a team with infrastructure, BI report development and business analyst resources, and clearly communicate solutions to both technical and non-technical team members
Hands-on experience of ETL development for an Enterprise Data Warehouse (EDW) via SQL Server Integration Services (SSIS) or a similar Data Integration toolset
Hands-on experience of BI dashboard/report development, using an enterprise BI tool (Cognos, Tableau, or similar)
Preferred Experience
Bachelor's degree in Engineering, MIS, Computer Science, Mathematics, Statistics or related field OR an equivalent combination of education, experience, knowledge, skills, abilities
Proficient with Kimball multi-dimensional modeling techniques, designing and developing complex information architectures and assessing and evaluating alternative solutions
Experience in planning, developing, and supporting ETL systems to achieve data transformation goals including the design and architecture of operational data stores to support enterprise data integration goals
Experience with Business Intelligence tools and needs
Experience as a data architect involved in implementing company wide data policies, governance procedures, creating metadata repositories and working with data stewards to improve data quality
Strong SQL Server experience required, including SQL Server management; T-SQL and performance tuning techniques
Experience with the Microsoft BI Stack (SSMS, SSIS, SSAS and SSRS) helpful. Some experience in .NET (or equivalent) programming, C# preferred.
Proficient with Microsoft Office Suite
Ability to communicate effectively, orally and in writing
Ability to use sound judgment
Ability to manage time and workload effectively which includes planning, organizing, and prioritizing with attention to details
Excellent organizational and analytical skills
Physical Requirements
Constant and close visual work at desk or computer
Constant sitting and working at desk
Frequent verbal and written communication with team and other business associates by telephone, correspondence, or in person
Frequent lifting of folders, files, binders and other objects weighing up to 20 lbs.
Frequent opening and closing of heavy file drawers
Occasional bending and stooping to pick up boxes and equipment weighing up to 50 lbs.
Frequent walking and standing
Occasional driving of automobiles
Travel Required : 0-10%"
Data Engineer,PACCAR,"Company Information

PACCAR is a Fortune 500 company established in 1905. PACCAR Inc is recognized as a global leader in the commercial vehicle, financial, and customer service fields with internationally recognized brands such as Kenworth, Peterbilt, and DAF trucks. PACCAR is a global technology leader in the design, manufacture and customer support of premium light-, medium- and heavy-duty trucks under the Kenworth, Peterbilt and DAF nameplates and also provides customized financial services, information technology and truck parts related to its principal business.
Whether you want to design the transportation technology of tomorrow, support the staff functions of a dynamic, international leader, or build our excellent products and services, you can develop the career you desire with PACCAR. Get started!

Division Information

PACCARs Information Technology Division (ITD), located in Renton, WA utilizes cutting-edge technology to provide systems development, consulting, voice and data communications services to the entire Corporation, which has high visibility in the technology sector.

Requisition Summary

We are looking for an experienced Data Engineer with an uncanny ability to integrate multiple heterogeneous data sources to build efficient, flexible, and scalable data warehouse and reporting solutions. The ideal candidate is enthusiastic about learning new technologies and implementing solutions using these technologies to empower internal customers and scale the existing platform. The ideal candidate demonstrates solid business and communication skills and ability to work with Research Scientists and business owners across both technical and non-technical teams to develop and define key business questions, then build the data sets that answer those questions. In this role, you will serve as the expert at designing, implementing, and operating stable, scalable, low cost solutions to flow data from production systems into the data warehouse and into end-user facing reporting applications. Above all, you should be excited about bringing large datasets together to answer business questions and drive data driven decision making.
The Data Engineer will design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics and deep learning. Implement data structures using best practices in data modeling, ETL/ELT processes and AWS Redshift, Teradata, Oracle, DB2, and OLAP technologies He/she will be writing scalable queries and tuning performance on queries running over billion of rows of data.
The person in this position should be analytical, have an extremely high level of customer focus and a passion for process improvement. The Data Engineer should be motivated self-starter that can work independently in a fast paced, ambiguous environment and should have excellent business and communication skills to be able to work with business owners to develop and define key business questions.

Job Functions / Responsibilities

Design, implement, and support data warehouse infrastructure using AWS cloud technology, Teradata, and multiple RDBMS engines.
Create ELT/ETL procedures to take data from various operational systems and create a unified dimensional or star schema data model for analytics and reporting.
Support Data Analysts and Research Scientist in analyzing usage data to derive new insights and fuel customer success
Use business intelligence and visualization software (e.g., OBIEE, Quicksight, Tableau Server, etc.) to empower non-technical, internal customers to drive their own analytics and reporting.
Generates scripts for database deployment.
Models physical/application data requirements in a data model tool, such as ERWin data modeler
Teaches data modeling and data design concept to IT and business people.
Provides data model estimates for new projects.
Manages data models and related artifacts in a source safe repository such as TFS.
Provides on-going support for Agile projects.
High degree of knowledge based on prior implementation experience with AWS Glue, Aurora, S3, Redshift, VPC, etc.
Performs expert-level data development and design work in Cloud environments that may include logical data topology design, cloud data architecture analysis and design; including integration with additional 3rd party data sources in multiple cloud platforms.
Contributes to and maintains data standards.
Ensures security is integrated into all data solutions to meet compliance standards.

Qualifications

Bachelors degree in Computer Science or related field.
5+ years relevant experience in data modeling and data architect.
Expertise with SQL and relational database systems.
Knowledge of data warehousing concepts.
Experience in data mining, ETL, etc. and using databases in a business environment with large-scale, complex datasets.
Experience in modeling logical/business data requirements in a data model tool, such as ERWin data modeler.
Experience in generating scripts for physical database deployment.
Ability to teach data modeling and data design concept to IT and business people.
Familiarity with Cloud, NoSQL, and Hadoop data technologies
Familiarity with dimensional and data vault modeling techniques.
Proficiency with ETL tools and techniques.
Knowledge of more than one database platform, such as SQL Server, Oracle and Teradata.
Expertise in standard data model reports.
Must have knowledge of data normalization and denormalization techniques
Excellent interpersonal and organizational skills, including active listening, problem solving when under pressure, and facilitation.
Strong troubleshooting and problem solving skills.

Education

Bachelors degree in Computer Science or related field.

Additional Job Board Information

PACCAR is an Equal Opportunity Employer/Protected Veteran/Disability."
Data Engineer,SparkCognition,"SparkCognition, named top 20 on CNBCs Top 50 Disruptors of 2017, is one of the fasting growing AI companies in the world. Our technology is deployed across dozens of enterprise environments with some of the largest global organizations, covering O&G, utility, manufacturing, finance, government, defense, and aerospace.

Description

We are looking for a versatile Data Engineer enthusiastic about working with the latest technologies to streamline, automate and optimize large scale data solutions. You are a designer, builder, and manager of information. You have a deep understanding of both software development and database technologies allowing you to select and integrate tools, frameworks and database systems into corporate products and other initiatives. You keep abreast of new tools and techniques to get the job done. You are self-motivated and can effectively work both independently or as part of a team.

Essential Duties and Responsibilities

As a Data Engineer you will:
Select, develop, and integrate tools, frameworks, and database technologies supporting highly scalable data processing focusing on reliability, performance, and data quality

Install, configure, maintain databases

Backup/Restore

Maintenance patches

Security Administration

Optimization/Performance tuning

Define and implement data retention policies and disaster recovery systems

Model data for various data stores

Understanding of normalization, warehouse, distributed databases, column store

Understanding of Relational, non-Relational NoSQL, Data Stream and Time Series data

Design, develop and implement ETL/ELT processes

Working with different file formats both on import & export

understanding of Bulk import/export, BCP, data link and virtualization techniques

Knowledge and Ability to use scripting and/or tools (SSIS, Informatica, Kettle, ODI etc)

Data Analysis

Analyze data quality and integrity

Using SQL, PL/SQL, TSQL and other scripting techniques, find matching pattern and data points across distributed system

Identify entities and attributes for facts/dimensions in datastore/data warehouse

Required Skills/Experience

Bachelor's degree in a related field with 7-10 years' relevant work experience

Advanced SQL skills and database experience in Oracle, SQL Server, or Postgres

Advanced experience with scripting languages like Python, JavaScript, PHP, or Perl

Understanding of Data Storage techniques, Data Lake, XML, JSON, Parquet, Column Store, row store, horizontal/vertical partitioned, distributed etc

Ability to multi-task in a fast-paced environment with accelerating priorities

Ability to diagnose performance issues and address them

Preferred Skills/Experience (Any of these is a plus)

Hands on experience with NoSQL database technologies like Mongo, DynamicDB, Hbase

Experience with various messaging systems, such as Kafka or RabbitMQ

Experience working with cloud ecosystem such as GCP, AWS, Azure

US Citizenship required"
Data Engineer,MediQuire,"We are looking for an energetic and highly motivated Data Engineer to help launch customer implementations and analyze our database of patient medical record data.

About MediQuire:
MediQuire is a big data analytics platform for primary care clinics that treat low-income patients in the United States.

We are a Venture Capital funded start-up with an experienced management team. Our product helps primary care clinics analyze Electronic Health Record patient data and provide actionable feedback to clinicians, thereby improving health outcomes and reducing costs for the poorest patients across the US.

You will work closely with and learn from seasoned professionals including a former Bain Capital executive (Harvard Business School), the former senior Truvent product owner, a seasoned sales rep with over 20 years experience. And the best part, you get to work in a sector of healthcare that needs you most: 25 million Medicaid/ Medicare patients around the poverty line, whose health depends on us.

Key Responsibilities:
Launch new customer implementations

Analyze customer patient medical record data

Ongoing product and analytics development

Qualifications and Experience:
Bachelor Degree in computer science preferred

Big Data technologies in the cloud such as Hadoop, Pig, Hive, etc.

Traditional relational DBMS as well as No-SQL

Extract, Transformation and Load (ETL) tools and practices

Data and object modeling

General software engineering principles such as OOP, SOLID, etc.

Quick and eager to learn new technologies

Competitive salary, benefits and equity provided. This position is located in New York City.

Only candidates with US citizenship or Green Card accepted."
Data Engineer,Rakuten Marketing,"Under the general guidance of the Lead Data Engineer, quickly and accurately translate concepts and specifications into workable data solutions and implement effectively; ensure consistent and coherent approach to software development; general data platform engineering, maintenance, and bug fixing.

Description:

 Awareness of priorities of client-led projects and internally generated tasks

 Endeavour (team and self) to respond to Jira ticket requests within agreed timeframes

 Work with team to develop data products where all components communicate effectively

 Contribute to technology stack and identification of standards for performance management

 Data structure development for transactional systems, data warehouses, and virtual data stores

 Design data integration packages

 Develop reporting architectures, query paths, and transformation processes

Requirements:

 Bachelors degree in computer science, math, physical science, or equivalent.

 2+ years experience working for a medium or large technology company

 Strong development skills in any ANSI-SQL variant

 Ability to develop in Java, C++, or other C-type language unaided

 Hands-on experience in a no-SQL data persistence system (graph, document, OO, ...)

 Good understanding of general math concepts

This sounds like you:

 Highly organized, use initiative to identity solutions to complex issues, self-aware, self-motivated

 Excellent verbal and written communication skills, can take requirements and translate into a technical concept

 Must be an action-oriented individual, self-motivated, and love to identify and solve the right problems

 Prefers working with groups in open collaboration

 Able to plan individual work aligned within a group

 Grasps reasons for business decisions as well as impact of decisions on other parts of the company

 Sets a good example, demonstrates good practices, shows good judgement, balances service to others and service to self

 Has a deep understanding of something and can teach others in a way that simplifies the concept

RM uses the following tech:

 PostgreSQL, Hadoop, Cassandra, Aerospike, Redis, LevelDB

 Golang; Java; JavaScript; Python; C; C#

 AWS; Google Cloud; OpenStack

 NSQ, Kafka, Flume, GCP Pub/Sub

 and is always trying out new possibilities"
Data Software Engineer,Medidata Solutions,"Medidata is making a real difference in the lives of patients everywhere by accelerating critical drug and medical device development, enabling life-saving drugs and medical devices to get to market faster. Our products sit at the convergence of the Technology and Life Sciences industries, one of most exciting areas for global innovation.

We invented the Medidata Clinical Cloud which is the primary technology solution powering clinical trials for over 90% of the world's top global pharmaceutical and medical-device companies - from study design and planning through execution, management and reporting. We are publicly traded (MDSO), have 850 customers with retention rates at 100% and a revenue run-rate of over $500M in 2017. Our customers include global pharmaceutical companies, biotech, diagnostic and device firms, leading academic medical centers, technical partners and contract research organizations.

Medidatas solutions have powered over 12,000 clinical trials to date giving us the largest collection of clinical trial data in the world. With this asset, we pioneer innovative, advanced applications and intelligent data analytics, bringing an unmatched level of quality and efficiency to clinical trials enabling treatments to reach waiting patients sooner.

We know that diverse teams win and therefore we are committed to selecting leaders and employees that represent the markets in which we operate. We are still led by our Co-founders, Tarek Sherif and Glen de Vries, and have global operations in US, Europe and Asia with 1600 employees.

Your Commitments:
Development of application, data and deployment routines in a rapidly changing environment.
Create scalable, consumable, performant web applications geared towards an SOA environment.
Build useful, well-designed interactive graphics and experiences.
Suggests/ Identifies appropriate tools/technologies as needed to use for solution implementation.
Your Competencies:
Knowledge of Java, Clojure or any other JVM languages is a must.
Basic understanding of statistics and data analysis techniques.
High level of understanding of the AWS stack with some familiarity in deployments, containerized applications and Docker.
High level of proficiency in Javascript, preferably with knowledge of the React framework.
High level of proficiency in working with data via SQL.
Some familiarity with distributed computing, coordination and/or queuing/ pipelining techniques.
Knowledge/ practice of Good Code Practices (GCP)
Excellent communications, both written and verbal, and able to communicate ideas concisely and clearly to the team and stakeholders in a professional manner.
Your Preferred Competencies:
Advanced knowledge of statistical algorithms, data science methods, or model driven processing.
Experience implementing consumable models, or machine learning application features.
Some basic knowledge of Python / R for data exploration or porting / consumption as needed.
Some experience with big data processing tools such as Spark, Hadoop or EMR.
Basic knowledge of rapid application development or agile processes as well as source / revision control.
Experience working in a collaborative environment that includes working heavily with scientists and statisticians and/ or productization of research from other scientific teams.
Your Education & Experience:
Requires a Bachelors degree and a minimum of 2+ years of related experience in software engineering, data architecture and statistics; or an advanced degree without experience; or equivalent work experience.
Degree in software engineering, computer science, statistics or related field preferred.
Medical or Life Sciences industry experience a plus.
Our Culture: Who we are
We know that creativity doesn't happen on-demand. Developing cutting-edge cloud technology takes great minds and talented people working together in a collaborative environment. That is why we are committed to fostering an innovative, agile company culture. We encourage our teams to come together and experiment with new concepts, research new approaches and test out new technologies. We believe that being part of our team will make a difference in the world.

Our Mission: Powering smarter treatments and healthier people.
Our Vision: To be the most innovative cloud company in Life Sciences.
Our Principles: Integrity, Partnership, Inventiveness, Humility, Nimbleness, Tenacity, Inclusiveness, and Caring.

Our Leadership Drivers:
THINK: Inspires purpose, articulates strategy, and simplifies complexity
TEAM: Communicates effectively, builds relationships and collaborates with others
DO: Plans ahead, scales for growth, ensures accountability
LEARN: Self-aware, values difference, strives to learn
TEACH: Inspires work, coaches others, builds teams
EEO Statement

US:
Medidata Solutions, Inc. is an Equal Opportunity Employer. Medidata Solutions provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, gender identity, national origin, age, disability status, protected veteran status, or any other characteristic protected by the law. Medidata Solutions complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

LI-DM1"
Machine Learning Engineer,Apple,"Maps is about more than just location and directions; it's about modeling the physical world in software. Join the team charged with aggregating, processing, filtering and conflating data from many sources and multiple modalities and presenting a unified, rich, and up to date set of Points of Interest (POIs) to hundreds of millions of Apple Maps users across the world. Were looking for an experienced scientist/engineer who is expected to take projects from initial data mining and research through all stages of prototyping, development and final integration and into the production data that does out the users.

Key Qualifications
5+ years experience working on data science projects
Hands-on experience with NLP, mining of structured, semi-structured, and unstructured data
Intuitive understanding of machine learning algorithms, supervised and unsupervised modeling techniques
Experience working with large, real world data  big, messy, incomplete, full of errors
Experience with Apache Hadoop, Spark, Solr/Lucene, Cassandra and related technologies
Working knowledge of SQL, Hive, Pig, and other query languages
Experience with machine learning tools and libraries such as Scikit-learn, R, Spark, TensorFlow
Intuition about algorithm and system performance and throughput
Experience Ranking entities or attributes a plus
Multimodal learning applications a plus
Architecture and system/pipeline layout experience a plus
Deep learning, computer vision, topic modeling, graph algorithms are pluses
Scala language experience a plus
Attention to detail, data accuracy, and quality of output
Strong interpersonal, written, and verbal communication skills
Ability to effectively function in a fast-paced environment with shifting priorities and simultaneous projects
Full ownership of project and its properly evaluated and deployed operation in production is a must

Description
Take ownership of a project that focuses on a topic such as classification, attribute ranking, filtering, or enriching of entity records or their data
Implement open source and proprietary algorithms for handling and processing the data
Use distributed data processing and analysis

Education
Masters or PhD in Computer Science, Statistics or related field"
Data Engineer,TMW Systems,"This position will be responsible for the analysis, design and continued implementation of TMWs enterprise-wide Business Intelligence needs. The position will also be involved in the analysis, design and architecture of TMWs Business Intelligence platform, and will require participation in the creation and ongoing updates to the Business Intelligence strategy roadmap, developing and documenting strategies for an internal enterprise data warehouse implementation, data visualization, and generation of analytics.

Position Accountabilities
Translate business requirements into the design of the overall enterprise data architecture, data governance strategies and data quality standards
In-depth knowledge of relational database concepts  including design, implementation, programming, day-to-day administration, and support best practices
Designs and implements an enterprise data warehouse infrastructure including data marts, data models/dimensions and metadata repository
Promotes the functionality, scalability, performance, security, and integration requirements necessary to an enterprise business intelligence platform
Develops documents and maintains a formal description of the data and data structures.
Develops entity and attribute descriptions and definitions for the models and ensures that conflicts in descriptions and definitions are resolved
Support implemented BI solutions by: monitoring and tuning queries and data loads, addressing user questions concerning data integrity, monitoring performance and communicating functional and technical issues
Minimum Requirements
Using strong analytical skills to solve and model complex business requirements
Hands on relational and multi-dimensional data modeling, including multiple source systems from databases and flat files, metadata repository development and the use of standard data modeling tools
Supporting data warehouse documentation, installation, implementation, training and support activities
Displaying a sound understanding of BI Best Practices/Methodologies, relational structures, dimensional data modeling, structured query language (SQL) skills, data warehouse and reporting techniques, including data visualization concepts
Collaborating on a team with infrastructure, BI report development and business analyst resources, and clearly communicate solutions to both technical and non-technical team members
Hands-on experience of ETL development for an Enterprise Data Warehouse (EDW) via SQL Server Integration Services (SSIS) or a similar Data Integration toolset
Hands-on experience of BI dashboard/report development, using an enterprise BI tool (Cognos, Tableau, or similar)
Preferred Experience
Bachelor's degree in Engineering, MIS, Computer Science, Mathematics, Statistics or related field OR an equivalent combination of education, experience, knowledge, skills, abilities
Proficient with Kimball multi-dimensional modeling techniques, designing and developing complex information architectures and assessing and evaluating alternative solutions
Experience in planning, developing, and supporting ETL systems to achieve data transformation goals including the design and architecture of operational data stores to support enterprise data integration goals
Experience with Business Intelligence tools and needs
Experience as a data architect involved in implementing company wide data policies, governance procedures, creating metadata repositories and working with data stewards to improve data quality
Strong SQL Server experience required, including SQL Server management; T-SQL and performance tuning techniques
Experience with the Microsoft BI Stack (SSMS, SSIS, SSAS and SSRS) helpful. Some experience in .NET (or equivalent) programming, C# preferred.
Proficient with Microsoft Office Suite
Ability to communicate effectively, orally and in writing
Ability to use sound judgment
Ability to manage time and workload effectively which includes planning, organizing, and prioritizing with attention to details
Excellent organizational and analytical skills
Physical Requirements
Constant and close visual work at desk or computer
Constant sitting and working at desk
Frequent verbal and written communication with team and other business associates by telephone, correspondence, or in person
Frequent lifting of folders, files, binders and other objects weighing up to 20 lbs.
Frequent opening and closing of heavy file drawers
Occasional bending and stooping to pick up boxes and equipment weighing up to 50 lbs.
Frequent walking and standing
Occasional driving of automobiles
Travel Required : 0-10%

Back Apply Now"
"Software Engineer, Data Privacy",Facebook,"(Washington, DC) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Facebook's Data Privacy team is seeking engineers who are passionate about ensuring the proper stewardship of 2 billion people's personal data. You will help build scalable systems and infrastructure to guarantee the correct retention, processing, and deletion of data while helping shape privacy products and features. Have a large impact by making sure Facebook meets both its high internal expectations and the expectations of people who use the service. Responsibilities
Implement robust enforcement mechanisms to safeguard data privacy.
Build infrastructure that tracks products launching across the Facebook suite of apps and ensures privacy is built into those products.
Work collaboratively with privacy and legal teams to understand and mitigate privacy risks.
Minimum Qualifications
Bachelors degree in Computer Science or related field
3+ years of software engineering experience
Demonstrated experience shipping products
Thrives in dynamic, fast-paced environments"
Data Engineer,"OnShift, Inc","As the Data Engineer, you are a member of the Product Development team whose objective is to design, construct, and maintain a highly scalable data management system. You will support our company goals to build a world-class data intelligence system to provide our clients empirically proven outcomes

You will work closely with Data Analysts, Developers, DBAs, and Product Management to determine and design the best data management system to meet our needs. You will define and implement strategies to deal with database integration from multiple sources in multiple formats  both structured and unstructured.

A DAY IN THE LIFE

Provide best-practice guidance to engineering on how to store data for future use

Design and build large-scale data management and reporting systems

Integrate new data management technologies and software engineering tools into existing systems

Integrate separate data sources

Recommend and implement disaster recovery procedures

Recommend ways to improve data reliability, efficiency, and quality

Collaborate with other team members to achieve goals

Automate manual data collection processes

Effectively communicate recommendations to stakeholders, including product, development, and senior leadership

DO YOU HAVE WHAT IT TAKES?

B.S. in Computer Science, Software Engineering, Applied Mathematics with real-world experience

3+ years of experience as a Data Engineer

Overwhelming desire to understand how things work

Tireless desire to make data better and easier to access

Attention to detail

Highly technical

Technical Skills:
Statistics and methods

Database architectures

Hadoop-based technologies for big data

PostgreSQL

Python

R

Kibana / Elastic Search

Google Analytics

Machine/deep learning technologies"
Software Engineer,Adobe,"The Challenge
Join an agile software development team to build a new big data platform. You will design, develop and release new services to power Adobe's Experience Cloud. Specifically, this team enables customers to discover, control and trust their data in the Experience Cloud.
What youll do
Develop services to power a big data platform
Develop a SaaS solution using microservices
Work in an agile environment
Build first generation technology from the ground up.
What you need to succeed
3-6 years software development experience
1-3 years experience in enterprise SaaS
Extensive experience in software development
Moderate experience in functional programming preferred (e.g. Node.js or Scala)
Experience in RESTful APIs preferred
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
Data Engineer,UCLA HEALTH,"Job Duties

The Data Engineer is a key role in technology initiatives to advance health informatics and analytics in the health sciences by advancing the usability, performance, and overall architecture of the enterprise data warehouse. This role develops data processing pipelines to ingest data from multiple data sources into Azure SQL Data warehouse, Azure Data Lake using SSIS, Azure Data Factory. The engineer also works closely with Data scientists to prepare the analytical datasets for Predictive Analytics and Machine Learning. The engineer also contributes to the development of Machine Learning algorithms to support Enterprise Analytics needs.

KEY RESPONSIBILITIES
Design, architect, prototype, and implement solutions to tackle the Big Data and Data Science needs for UCLA Health.
Work with stakeholders in OHIA and the Centers of Excellence to understand data needs and ingest rich data sources such as external claims data feeds, Electronic Health Record data, financial data, operational data, public data sets, and social media feeds.
Research, experiment, and utilize leading Big Data methodologies, such as Hadoop, Spark, and SQL Data Warehouse on the Azure platform.
Architect, implement, and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as Azure and on-premise technology stacks.
Develop and operationalize Predictive Analytics and Machine-Learning algorithms using Azure ML, Python, and R to enhance business and clinical decision-making.
Translate advanced business analytics problems into technical approaches that yield actionable recommendations. Collaborate with others in the OHIA Data Science Lab to communicate results and educate stakeholders through designing and building insightful visualizations, reports, and presentations.
Job Qualifications

Minimum five years of big data experience with multiple programming languages and technologies.
Demonstrated experience designing and delivering solutions utilizing the Microsoft Azure including HDInsight, Azure Data Lake Analytics, Azure SQL Data Warehouse, Azure Data Factory, Streaming Analytics; experience with on premise Microsoft technologies such as SQL Server 2008 (SSIS, SSRS, SSAS) and above.
Strong background in Data warehousing principles, architecture and its implementation in large environments.
Fluency in programming languages such as Python, C# or Java, with the ability to pick up new languages and technologies quickly; understanding of cloud and distributed systems principles and experience with large-scale, big data methods, such as Map Reduce, Hadoop, Spark, Hive and Pig.
Experience developing Machine Learning Algorithms using Azure ML Studio is highly preferred.
Ability to take a Machine Learning model from a laptop machine to deploy it on Production servers at scale. (Using REST APIs, PMML etc.)
Experience developing statistical packages using R and Python is a plus.
Ability to evaluate multiple technologies and platforms and propose the right solution for the problem is highly desired.
Relevant experience with other cloud platforms (such as AWS, Google Cloud, etc.) is nice to have.
Work with team members and clients to assess needs, provide assistance, and resolve problems, using excellent problem-solving skills, verbal/written communication, and the ability to explain technical concepts to Leadership and business audiences.
Bachelor's degree in Computer Science, Computer Engineering, or related field from an accredited college or university."
BI Engineer/Analyst,Instacart,"Instacart is building the best way for people to shop for groceries. Whether on mobile or the web, you can order groceries and have them delivered to your doorstep in minutes. You can choose from a variety of stores like Whole Foods and Costco, as well as local grocers, and you are able to mix items from multiple stores into one order.

Every day, we solve incredibly hard problems to create an experience for our customers that is absolutely magical. Join us!

What Youll Be Doing

You will be joining our nascent but growing data engineering team and you will have a chance to have a large impact at Instacart. You will shaping the way of internal and external facing financial reporting are done. You will be working with our partner engineering as well as finance team to identify key metrics, conduct deep and rigorous analysis, design and implement business critical reports. You will be a trusted member of the team enabling financial decisions by providing reliable access to high quality data. More details on what youll be doing:

Work with finance team and partner engineering team to identify and document metrics definitions
Design and implement reporting platform to ensure data integrity across various reports and dashboards
Partner with Data Engineers to reconcile various financial data sources. Identify and quantify discrepancies
What You Should Already Have

Passion for and track record of leveraging data in finance, preferably in e-commerce environment
Very strong data retrieval skills using SQL, HiveQL, and Python (or other scripting languages)
Experience with analytical tools such as Tableau, Looker, D3.js or other tools
Attention to details and accuracy
Ability to thrive in a dynamic and collaborative environment
Bonus Points

MBA and/or management consulting experience
Data engineering / ETL and data pipeline development experience
What Well Set You Up With

Competitive salary and equity, based on experience
Comprehensive health, dental and vision coverage
A MacBook
Lunch and snacks while you work
A fulfilling, challenging adventure of a work experience"
Big Data Software Engineer - Federal,Sila,"Big Data Software Engineer  Federal Practice
As a Big Data Software Engineer at Sila, you will leverage your experience with Hadoop and software engineering on key technology implementations for our Federal clients. You will develop and implement systems that solve the main challenges of big data: velocity, volume, and variety. Through your understanding of emerging and existing models of big data and analytics, you will translate the mission problems of many diverse departments and agencies into creative technical solutions  making an immediate impact on how our clients process and understand data for today and the future.

WHAT SILA OFFERS:
At Sila you will do more than just code. We want all our employees to be better because of their Sila experience. Beyond the client work  where you will conquer diverse and interesting problems and execute solutions that drive lasting change and transform the ways our clients operate; you will impact Silas growth. As a key player of our big data practice you will dive into some of the latest technologies, honing your skills, and growing professionally.

WHAT TO EXPECT:
Build sustainable big data platforms while identifying gaps and opportunities to improve on existing solutions and processes
Create prototypes and proof of concept solutions as part of R & D efforts across our Federal clients
Play a critical role in designing, developing, and implementing Hadoop-based, big data solutions with advanced software patterns and standards
Collaboratively participate in the entire project lifecycle from design to delivery, taking advantage of flexibility in technology and approach
Proactively research and evaluate emerging and evolving technologies
Represent Sila in a client facing capacity, presenting actionable recommendations to solve challenges across diverse problem sets

SKILLS YOULL NEED:
Prior experience developing on parallel or multi-threaded systems
3+ years of hands-on, large scale, software development and integration experience using Java/JEE technologies
Prior experience using Hadoop ecosystem technologies like: Hadoop, HDFS, Spark, Pig, Hive, Impala, Flume, Sqoop, Zookeeper, Oozie, Hue, Kafka, Scala, Elastisearch, and python
Prior experience with the software stacks of Hadoop vendors like Cloudera and Hortonworks
Exposure to high availability configurations, Hadoop cluster connectivity and tuning, and Hadoop security configurations with a strong understanding of related algorithms and data science
Exposure with search systems (ElasticSearch, Lucene, Solr)
Interpersonal skills developed through previous client-facing implementation projects
Good understanding of relational databases and solid SQL skills
Good understanding of Operating Systems (Unix/Linux), Networks and Systems Administration
Intellectually curious, reliable, and flexible team player
US Citizen with TS SCI with CI Polygraph or higher.
BS/MS in Computer Science or related field
Must be innovative and a self-starter with the ability to manage multiple projects/initiatives on deadline
Travel may be required

ABOUT SILA:
Sila is a North American technology and management consulting firm that provides lasting and substantial business solutions to the worlds leading corporations and U.S. government agencies. We are a values-driven company with a culture that fosters collaboration, creativity, and social responsibility. Sila employees are part of a community of vibrant, high-performing contributors who push each other to achieve the highest standard of excellence. Our team members have extensive opportunities to discover their passions and shape their own career paths, and we continually invest in employees growth through innovative training, mentorship, and professional development programs. Staff are quickly immersed in clients business challenges, work closely with emerging technologies to develop impactful solutions to these challenges, and are exposed to a variety of industries and market offerings.
We are looking for full-time employees to become an integral part of our growing team. Sila offers a range of great benefits including a comprehensive healthcare package, 401K with matching, paid time off, and paid company holidays, as well as other unique benefits that support our staffs active work/life balance.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability or protected veteran status, or any other legally protected basis, in accordance with applicable law."
VIE - IoT and Big Data Engineer - Andover H/F,Schneider Electric,"PLEASE NOTE that only applications submitted in English can be considered by our non-French speaking partners at Schneider Electric. Therefore, it is strongly recommended that you post your resume in English.

Schneider Electric is the global specialist in energy management and automation. With revenues of ~27 billion in FY2015, our 160,000+ employees serve customers in over 100 countries, helping them to manage their energy and process in ways that are safe, reliable, efficient and sustainable. From the simplest of switches to complex operational systems, our technology, software and services improve the way our customers manage and automate their operations. Our connected technologies reshape industries, transform cities and enrich lives.

Missions

1/ Understand and hands on the innovation team activities regarding Big Data and Semantics

2/ Interact with other innovation teams such as EcoBuildings, Partner-Power Solutions and ITB-Data Centers to gather requirements.

3/ Implement new functionalities and operations on the existing innovation projects based on the previously gathered requirements

4/ Deployment of the solution, Test, and performance evaluation

5/ Document the accomplished work
Exigences
The VIE is an international programme for young professionals between the age of 18 and 28 who are European Union nationals. In addition, the candidate cannot come from the country hosting the mission.

Profile and Competencies

Education: bac+5 / bac+8

Technical Field: Computer Science, Computer Engineering

Work experience needed: 6 - 12 months

Requested languages (marked as bilingual, fluent, well known, basic knowledge): well known/fluent In English

Personal abilities or skills: Strong communication and synthesis abilities, proactive, structured, able to work independently

Lieu principal : US-Massachusetts-Andover

Horaire : Temps plein
Date de retrait de la publication : Continu"
Data Engineer,Penske,"This position will be responsible for managing, implementing and supporting data model design implementation on GreenPlum database . He/she will be implementing data summarizations models for analytics on Greenplum /big data environment. Using this model he/she will recommend best practices for data loading, data integration and analytics implementation on Greenplum/ Big data.

Telecommuting is an option

Major Responsibilities:
Model, summarize and integrate data in Big data environment.

Create and implement efficient DW/summary for analytics usage .This should have efficient data integration, transfer and summary strategy on GP database.

Lead/support Big Data Platform initiatives for the enterprise specifically related to data analytics on GP platform.

Make recommendations and give expert advice to management on topics related to: GP Data Platform Architecture, Tools, Technologies and Infrastructure, analytics

Manage expectations of internal and/or external customers. Handle complaints and offer meaningful short and long term resolutions.

Support all the groups with various needs of data analytics.

Create strategy to ingest and presentation of data of Green plum or other big data environment.

Other projects as assigned by the supervisor

Qualifications:
B.A. /B.S. Computer Science, Mathematics, Engineering, or equivalent work experience required

Minimum 5 years work experience as an Green Plum, MPP databases, Big Data and in-memory databases.

Must have comprehensive knowledge of database technologies

Greenplum 4.3.x.x and Unix or Linux platform

Background in relational databases like Oracle/Teradata is helpful.

Experience with Linux scripting is required (Perl and/or Korn shell/Bourne Shell/ Python)

Understanding of basic SQL commands necessary to perform administrative functions.

PostgreSQL is a plus

Knowledge in Greenplum/Postgress Architecture, Configuring Greenplum system

Defining and Managing the database objects and data in Greenplum.

Knowledge in GPLOAD and GPFDIST in Greenplum.

Managing the Schema, Tablespace and Partitioning in Greenplum.

Install, Configure and maintain Greenplum databases

Broad understanding and experience of real-time analytics, NOSQL technologies(e.g. Hbase, Cassandra, MongoDB) is plus

Knowledge in the following is very beneficial:

-Pivotal Big Data Suite  Greenplum, Gemfire, Spring XD and Rabbit MQ

-Environments: Amazon Web Services, Cloud Foundry and vCloud Air

-Big Data Technologies  Hadoop, Kafka, Zookeeper, Hbase, Hive etc. (Nice to have)
Should be able to support Java application is a plus

Gemfire or in- memory solution experience is also a plus.

Ability to work independently and with other database administrators, application administrators, product developers, and product management as required

Experience monitoring database management systems and in- memory solutions as well as evaluating performance to maximize efficiency

Experience with support of high volume, low latency, 24x7 production database environments with strict availability requirements and revenue at risk

Ability to plan work to meet project deadlines, accommodate demands by users, set priorities and escalate issues appropriately

Superior communication skills (verbal, written and presentation), customer service-oriented

Strong problem solving skill set, and the ability to understand new technologies quickly are essential

Ability to work independently and with other database administrators, application administrators, product developers, and product management as required

Regular, predictable, full attendance is an essential function of the job

Willingness to travel as necessary, work the required schedule, work at the specific location required, complete Penske employment application, submit to a background investigation (to include past employment, education, and criminal history) and drug screening are required.

Physical Requirements:

-The physical and mental demands described here are representative of those that must be met by an associate to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

-The associate will be required to: read; communicate verbally and/or in written form; remember and analyze certain information; and remember and understand certain instructions or guidelines.

-While performing the duties of this job, the associate may be required to stand, walk, and sit. The associate is frequently required to use hands to touch, handle, and feel, and to reach with hands and arms. The associate must be able to occasionally lift and/or move up to 25lbs/12kg.

-Specific vision abilities required by this job include close vision, distance vision, peripheral vision, depth perception and the ability to adjust focus.

Penske is an Equal Opportunity Employer, including individuals with disabilities and protected veterans.

About Penske Truck Leasing

Penske Truck Leasing Co., L.P., headquartered in Reading, Pennsylvania, is a partnership of Penske Corporation, Penske Automotive Group, and Mitsui & Co., Ltd. A leading global transportation services provider, Penske operates more than 260,000 vehicles, employs more than 29,000 people, and serves customers from more than 1,000 locations in North America, South America, Europe, Australia, and Asia. Product lines include full-service truck leasing, contract maintenance, commercial and consumer truck rentals, used truck sales, transportation and warehousing management and supply chain management solutions. Visit http://www.GoPenske.com to learn more.

Work Locations: 100 Gundy Drive Reading, PA

Primary Location: United States-Pennsylvania-Reading

Job: Information Technology

Penske (Oracle) Job Name: Information Technology

Req ID: 1708670"
Cloud Data Engineer (SQL Azure),Quisitive,"US Citizens or those authorized to work in the US are encouraged to apply. We are unable to offer sponsorship at this time.

Quisitive is a digital technology consulting firm that leverages the strengths of technology, data insight and digital experience to empower organizations on their journey to digitally transform. Quisitive, in its name and mission, was inspired by envisioning a new approach to the industry, one that began with asking the right questions to break down the barriers to business innovation rather than relying solely upon technology. This belief ensures the solutions we develop treat the underlying causes of our clients issues and lay the foundation for business transformation.

We believe that a one size fits all solution simply doesnt work  we tailor each solution to the unique challenges our clients face, bringing to the table an integrated team of consultants and a custom approach that creates a viable, valuable and feasible solution for our customers. We know innovation lies in the heads and hands of those who can blend the black and white world of technology with the shades of gray in business. We are looking for a Data Engineer that can bring their expertise, experiences and perspective to the Quisitive team to help us build solutions that empower holistic and tangible transformation at every level.

Quisitive has offices in Dallas, TX, and Denver, CO.

Responsibilities:

 Responsible for supporting the Modern data and analytics portfolio by working with customer and internal stakeholders (business clients, business analysts, developers, other architects), defining customer solutions, and delivering engagements that are innovative and exceed business requirements.

 Collaborate with other team members to develop and enhance deliverables

 Continuously improve team processes to ensure information is of the highest quality, contributing to the overall effectiveness of the team

 Stay abreast of architectural/ industry changes; especially in the areas of big data and the cloud

 Ability to blend quantitative and analytical skills, problem-solving and strategy consulting.

 This position will assist with the development of customer-focused solutions based upon market and customer needs. This includes the development of solution and project artifacts for current and new service offerings along with the quality of the delivery to customers.

 Meets with clients to determine needs and/or identify problem areas

 May be responsible for the development of Statement of Work and Proposals which outline professional services required to meet client requirements

Education, Experience and Other Skills

Requirements:

 General Database Administrator skills and knowledge

 Knowledge of and experience with the Azure Data Platform  Azure Data Lake, Data Factory, Data Management Gateway, Azure Storage Options, DocumentDB, Data Lake Analytics, Stream Analytics, EventHubs, Azure SQL.

 Knowledge of tabular OLAP, including PowerPivot and SSAS Tabular mode.

 Knowledge of Microsoft Cortana Intelligence Suite.

 Understanding of how traditional processes apply to the modern world (be able to talk about where big data and NoSQL can replace traditional enterprise data stores, how Azure Data Factory can supplement/replace SSIS, etc).

 Hadoop and Big Data experience.

 Experience with the setup and configuration of Power BI (Power Query, Data Management Gateway, SSAS Tabular Connector)

 Experience implementing Master Data Services and Data Quality Services.

 Degree in Information Technology or Business Administration or equivalent experience.

 Exceptional customer engagement, interpersonal, presentation and overall communication skills.

 Ability to successfully handle multiple work streams across multiple engagements, while maintain composure and professionalism to meet tight deadlines and shifting priorities.

 Quickly learn and adapt to new business and technical concepts.

 Goal-oriented team player committed to quality and detail.

 Innovative thinker who is positive, proactive and readily embraces change.

 Strong understanding of Business Intelligence concepts and best practices, with an understanding of its strategic importance to organizations.

 Superior conceptual and analytical abilities, identifying opportunities for improvement through analysis and creative thinking.

 5+ years experience with a Major BI Technology Sets

 5+ years, recent experience developing solutions in a Microsoft SQL Server environment using SQL, T-SQL stored procedures, T-SQL functions; and/or an Oracle Environment using SQL Plus and PL/SQL Coding Languages as well as familiarity with complex SQL Server scaling concepts (AlwaysOn, Geo-Scaling, Sharding, Large Scale Database Management)

Additional Capabilities a Plus:

 Experience with traditional statistical platforms is a major plus. R, Revolution Analytics (Pre-Acquisition), SAS, JMP, Predixion, Halo BI

 Experience with modern reporting tools is a plus. Tableau, QlikTech, Panorama, Domo

 MongoDB"
Data Engineer,Signet Jewelers,"The Data Engineer reports to the SVP, Retail Analytics
Prepares and processes large data sets (structured and unstructured) through queries, scripts, and ETLs to facilitate accurate reporting and statistical analysis per the requirements of data scientists and analysts.
Design and develop big data solutions using industry standard technologies. Analyze business requirements and develop long-term data warehousing strategies leveraging state of the art big data technology. Work with internal and external customers to overcome technical obstacles, answering questions and proposing solutions pro-actively.
Utilizing big data tools, gather, collect and store data. Complete batch processing or real-time processing to provide in a ready-to-query and analyze format. Map and document data into a standardized set of facts and dimensions. Create aggregates for use in reporting and analytics.
Monitor the system, log and troubleshoot errors, build human-tolerant pipelines and address continuous integration. Perform data audits on client data sets to test domain and range values, patterns and build a plan for doing data quality assessments and clean up.
Follow best practices for testing, capacity planning, documentation, monitoring, alerting and incident response. Ensure compliance of acquisition, storage and analysis of data.
Collaborate with team to build and design core technologies. Demonstrate strong implementation aptitude to translate objectives into a scalable solution to meet the needs of the end customer while meeting deadlines.
QUALIFICATIONS:
Bachelor's degree in a Statistics, Mathematics, Economics, Computer Science or related
3-5 years software engineering experience
Big Data and software engineering experience required
Self-starter with the ability to adapt and learn quickly.
Have a passion for developing and deploying data models.
Experience with deployment of analytics software.
Demonstrated knowledge of cloud computing, including virtualization, hosted services, multi-tenant cloud infrastructures, storage systems and content delivery network.
Use of data blending software such as Alteryx and SAS.
Experience in enterprise data governance."
Big Data Engineer,KPMG,"Known for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today's most important industries. Our growth is driven by delivering real results for our clients. It's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it's no wonder we're consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you're as passionate about your future as we are, join our team.
KPMG is currently seeking a Big Data Engineer, to join our Advanced Analytics Organization.

Responsibilities:
Facilitate the establishment of a secure big data analytics warehouse and an environment
Deliver solutions that fulfill the Business information needs and align with the information vision and strategy
Develop and document data models to facilitate data analytics and solution development
Develop ETL flows to obtain maximum value of the data assets and to facilitate the easy usage of the data assets
Help establish and maintain the data governance processes and mechanisms for data lake and EDW
Understand various data security standards and use secure data governance tools to apply and adhere to the required controls on a per data set basis

Qualifications:
A minimum of 4 years of data modeling & architecting experience, end-to-end development life-cycle support and SDLC processes
Bachelor's degree or higher in computer science or relevant field from an accredited college or university
Established experience of designing and building analytical data warehouses including using ETL and other data wrangling tools
Demonstrated understanding of enterprise data warehouse, big data, cloud, BI & analytics, content management, and data management
Experience with enterprise data warehouses, cloud data warehouses, Big Data environments, and RDBMS's
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an equal opportunity employer. All qualified applicants are considered for employment without regard to race, color, creed, religion, age, sex/gender, national origin, ancestry, citizenship status, marital status, sexual orientation, gender identity or expression, disability, physical or mental handicap unrelated to ability, pregnancy, veteran status, unfavorable discharge from military service, genetic information, personal appearance, family responsibility, matriculation or political affiliation, or other legally protected status. KPMG maintains a drug-free workplace. KPMG will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable local, state or federal law (including San Francisco Ordinance number 131192). No phone calls or agencies please."
Digital Data Engineer,UNITED PARCEL SERVICE,"This is an exciting opportunity to join a growing organization within UPS, leveraging new technologies to help make UPS's marketing efforts more data and analytics driven. The Digital Data Platform team partners with campaign owners, channel owners, media agencies, data scientists, and IT to develop new audiences and deliver them to the activation platforms that support the targeting and personalization of all channels, including ups.com, paid media, and email. In addition, the team ensures that quality performance data is captured and tools are developed to support digital performance reporting and audience insights mining.

The Digital Data Engineer is responsible for the continued development and optimization of the companys Customer Data Platform (CDP) in support of Marketing business objectives. He/She leads the collection, cleaning, and transformation of data from multiple channels into the Customer Data Platform in support of the segmentation, analytics, reporting, and activation needs of marketers and agencies. This position leads development of new data engineering and data activation processes, and oversees the delivery of high quality data to agency media buying platforms, the ups.com personalization platform, and other programmatic activation platforms. He/She interacts regularly with IT on development of new ETL (Extract, Transform, and Load) jobs, maintenance, and troubleshooting. The Digital Data Engineer also provides key input in support of the overall digital marketing data architecture.

Responsibilities and Duties

Designs and develops Extract, Transform, and Load (ETL) processes within Customer Data Platform in support of the segmentation, analytics, and reporting needs of the business

Develops processes to clean and transform data into the processed data store within the Customer Data Platform and build aggregated views in the analytical data store in support of reporting and business intelligence needs of Marketing stakeholders

Develops automated quality assurance processes to monitor quality of inbound data and ensure the continued accuracy of data processing within the platform

Develops and manages data processing jobs to assign customers to marketing segments in the Customer Data Platform using demographic data, behavioral data, and intent signals and works with data scientists to incorporate machine learning outcomes into the overall customer segmentation model

Leads projects with marketing stakeholders, software vendors, media agencies, IT, and data owners to specify and execute new data feeds into the Customer Data Platform from internal repositories, digital marketing applications (web analytics, data warehouse, etc.), and advertising platforms

Knowledge and Skills

Strong technical knowledge of Hadoop and Spark ecosystem, including Hive, HBase, Cassandra, Sqoop, and Oozie

Strong knowledge of data modeling principles and best practices in building analytics repositories in SQL and NoSQL environments

Strong coding skills in SQL, Spark, and Python

Ability to diagnose and troubleshoot complex data quality issues

Ability to translate marketing stakeholder requirements into technical specifications

Experience building and managing data pipelines and repositories in cloud environments such as Microsoft Azure"
Machine Learning Engineer,FactSet Research Systems,"Department: Content Integration Engineering

FactSet is currently seeking a talented and motivated professional to join the Machine Learning and Language Technologies(MLT) Group. The primary responsibility for this position will be the development of systems for natural language analysis, classification tasks, information retrieval, machine translation and processing of financial data.

FactSet's Engineering team is composed of highly talented and motivated individuals, primarily selected from top-tier technical programs around the world. We believe in creating a loose team environment in which everyone can contribute individually, yet at the same time cultivating a community where engineers can depend on each other for help, learning, and development.

Our management approach is to trust and expect the best. We encourage developers to take initiative to complete projects, manage multiple tasks, set individual day-to-day priorities. We also encourage creativity and innovation. In our friendly yet professional atmosphere, FactSet offers an excellent opportunity to grow and be a part of a successful team. An ideal candidate for this position has a good mix of technical and analytical skills for software development, as well as a research-oriented background suitable for developing solutions for language processing tasks

Job Requirements
 Solid foundation in Natural Language Processing or Machine Learning
 MS, or PhD in Computer Science or related discipline
 Strong proficiency in Java or Python
 Familiarity with Open Source ML/NLP toolkits (We like tools such as scikit-learn, Mallet, IPython, Pandas, Weka, Vowpal Wabbit)
 Strong verbal and written communication skills

Familiarity with any of the following is recommended:

 A scripting language like Python, Bash
 Machine Translation
 Knowledge of Database Technologies : SQL, NoSQL
 Financial/Investment/Business knowledge

Benefits

 Work-Life balance
 Free lunch
 Dental, Medical and Vision Plans
 Transit Benefits
 401k match
 Employee Stock Purchase Plan

FactSet Research Systems Inc. is an E-Verify participant and EOE/M/F/D/V Employer which strongly supports diversity in the workforce.

It is the policy of FactSet Research Systems Inc. (""FactSet"") to provide equal employment and advancement opportunities to all qualified employees and applicants for employment regardless of their race, color, religion, sex, age, sexual orientation, gender identity or expression, national origin, physical or mental disability, genetic information, protected veteran status, pregnancy, military or military reserve obligations, or any other class or status protected by law. This policy applies to all policies and procedures related to recruitment, hiring, training, promotion, compensation, benefits, transfer, discharge, and other terms and conditions of employment. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability or protected veteran status."
Data Engineer,Networked Insights,"Who We Are

Networked Insights technology helps marketers from the worlds biggest brands and agencies understand their customers, inform their media spend, invent new products, drive amazing creative campaigns and improve their targeting by providing game-changing insights no other technology company can provide.

Our two primary products - Kairos and Audience.ai - help brands tap into billions of public online and social conversations (classified intelligently using the latest advancements in AI and machine learning) so that they can become more relevant to consumers, and get better results from their campaigns.

What Youll Do

We are looking for a Data Engineer who will help us build and architect databases, data sets and systems to support our companys products. You will have the opportunity to use established and new technologies to help us accomplish these goals.

Responsibilities

You work with relational database technologies (e.g. PostgreSQL, Oracle, MySQL, or SQL Server) and write performant queries on large data sets.

You have worked with moving lots of data from one system to another and transforming it along the way (e.g. JSON to relational data sets). You have developed full load and incremental load ETL processes using tools such as Microsoft SSIS, Pentaho Data Integration, or custom scripts. You have also worked on enhancing existing data sets such as adding additional attributes.

You get things done using established tools and at the same time, enjoy exploring new technology for better/faster ways of processing data.

You are comfortable with the software development workflow: ie., moving and deploying code from development to QA to production.

You have a basic understanding of bottlenecks that may occur such as disk IO, CPU, or memory. You have improved the performance of a database system or query by making either DDL or DML changes or both.

You are familiar with ETL workflows which progress through several different database technologies.

We make heavy use of PostgreSQL for our OLTP databases and even some small OLAP databases.

Occasionally, the ability for database administration tasks such as management of database configuration, replication, access, backups, and so-on.

We use cloud technologies such as BigQuery and Amazon AWS RDS for some of our needs.

We use git, maven, and Jenkins, and Docker. You do not need to be an expert in these, but familiarity helps.

We practice sprints and use JIRA for project management.

What Youll Need

Bachelor's degree in Computer Science or a related field of Engineering, or equivalent combination of education and experience.

Significant background in creating complex functions/stored procedures in SQL

Experience with database modeling tools, database normalization, logical and physical database design and implementation

Familiarity with database internals such as binary logging, indexes, statistics, query planning, transactions, concurrency management, and high-performance loading.

Strong analytical skills

5-8 years of experience in database development with RDBMS (PostgreSQL, MySQL, Oracle, etc.)

ETL operations

Big Data and Agile experience is a plus.

Experience preferred with Linux, Web Services, Postgres, Lucene, ElasticSearch and MapReduce frameworks such as Hadoop and Hive

Experience with Cloud platforms (eg., AWS, GCP, Azure, or similar)

Experience with higher-level languages such as Groovy or Python, particularly for authoring ETL.

Experience authoring webservices and webservice clients.

Bonus points are awarded if you have worked with append-only or distributed technologies.

You are also familiar with what data modeling approaches work well for transactional needs vs. analytical needs.

Who You Are

You work well with others and enjoy knowing the business reasons behind why we develop a new feature or product.

Initiative, results-oriented drive and a solid work ethic requiring minimal direction

Excellent time management and prioritization skills; ability to prioritize and manage competing tasks simultaneously in a fast-paced, time-sensitive environment

Assertive personality with a solid work ethic and initiative, requiring minimal direction

High energy and drive, coupled with resourcefulness, required to succeed in a start-up environment

A ferocious problem solver, capable of becoming productive quickly in an unfamiliar codebase.

What Youll Get

Ownership and challenge in your work

Collaborate with intelligent and highly skilled coworkers

Competitive compensation

Stock and Equity Options

Excellent benefits, including health, dental, vision, disability, 401(k) and life insurance

Generous paid time off policy including paid parental leave

Company Culture that is Committed to Work-Life Balance for each and every employee

Casual office environment with a startup feel  We have a particular fondness for Peanut M&Ms

Fantastic Downtown Location connected to the Merchandise Mart

If you are interested and think you can keep up with our team, then we would love to hear from you!

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Machine Learning Engineer,2U,"2Us data science projects are growing in complexity and require an increased level of technical rigor. We are looking for an engineer dedicated to high-quality, sustainable code to support the machine learning teams advanced analytical research and model development deployed across the company, including sales, marketing, human resources, finance, and operations. Youll be developing and maintaining the data architecture that data scientists and analysts rely on to create predictive modeling and reports, as well as the platform to help maintain, test, and deploy those solutions. Youd be responsible for coordinating data streams across the growing ecosystem of products. This role reports to SVP Data Science, Analytics, and Infrastructure.

Responsibilities Include, But Are Not Limited To
Create and maintain a data lake, with data from many disparate sources
Develop a query engine to query and extract data (e.g. Presto, Impala, etc)
Work with Data Scientists to import necessary data for modeling and coordinate data extraction across different database architectures
Develop reliable processes for the deployment of deep learning architectures across a range of business domains
Organize data structures to be accessible to analysts and reporting frameworks
Collaborate with Data Science Team to identify drivers & trends within our business
Ad hoc analysts and reporting as needed
Design and implement data architecture and systems to support training, testing, and governance of machine learning engines in production, including a centralized platform to test and track data models and a processes for the deployment of deep learning architectures across a range of business domains.

Things That Should Be in Your Background
Degree in statistics, mathematics, computer science, economics, or other quantitative field required; Graduate degree preferred
7+ years of experience working with data engineering and warehouses and big data applications
Experience with relational and distributed database systems and streaming systems
Familiarity with data extraction across different database architectures
Proficiency in Python, data models and architectures
The ability and desire to identify and deploy novel solutions to business problems

Other Attributes That Will Help You in This Role
Strong interpersonal & communication skills
Ability to maintain a fun, casual, professional and productive team atmosphere
Excellent communication skills with the ability to communicate in a courteous, tactful, and concise manner
Ability to work with a diverse team in a fast-paced environment
Enthusiasm and the ability to thrive in an atmosphere of constant change
EEO statement:
2U is an equal opportunity employer that does not discriminate against applicants or employees and ensures equal employment opportunity for all persons regardless of their race, creed, color, religion, sex, sexual orientation, pregnancy, national origin, age, marital status, disability, citizenship, military or veterans status, or any other classifications protected by applicable federal, state or local laws. 2Us equal opportunity policy applies to all terms and conditions of employment, including but not limited to recruiting, hiring, training, promotion, job benefits, pay and dismissal.

Note: The above statements are intended to describe the general nature and level of work performed by individuals assigned to this position, and are not intended to be construed as an exhaustive list of all responsibilities, duties and skills required. All employees may be required to perform duties outside of their normal responsibilities from time to time, as needed.

About 2U (NASDAQ: TWOU)
2U partners with leading colleges and universities to deliver the worlds best online degree programs. Our Platform, a fusion services and technology, enables schools to attract, enroll, educate, support and graduate students around the world. Our company culture is united by our No Back Row philosophy; the idea that when we each lean in and do our part, we are better individually and collectively.

2U Diversity and Inclusion Statement
At 2U, we are committed to creating and sustaining a culture that embodies diverse walks of life, ideas, genders, ages, races, cultures, sexual orientations, abilities and other unique qualities of our employees. We strive to offer a workplace where every employee feels empowered by the ways in which we are different, as well as the ways in which we are the same.

Why Its Great to Work at 2U
2U offers a high-energy work environment thats both challenging and fun. We work hard, but our offices are casual and social places. We wear jeans to work and fuel brainstorming sessions with snacks and seltzer.

Benefits

2U offers a comprehensive benefits package:
Medical, dental, and vision coverage
Life insurance, disability and 401(k)
Unlimited snacks and drinks
Generous paid leave policies including a PTO allowance for your vacation, personal, or sick days
Additional time off benefits include:
time off to volunteer for non-profit organizations
parental leave after 12 months of employment
holidays that include a winter break from Christmas through New Year and more!
Tuition reimbursement program"
DATA ENGINEER,Leesa,"Overview:
Leesa is seeking a Data Engineer to join the Data & Analytics team. You will be the subject matter expert for designing and maintaining our ETL pipelines. You will help shape the architecture for the data warehouse and analytical capabilities for Leesa.

Job Responsibilities:
Support the analysis, design, architecture, and development of our new data warehouse and reporting solution
Write custom code in python or similar language to pull data from APIs, s3, SFTP and other data sources into redshift
Help automate redshift SQL transformations from the data warehouse to support more complex analysis and visualizations
Collaborate with other engineers, data scientists /analysts and stakeholders, taking learning and leadership opportunities that will arise every single day
Creating and ensuring accuracy of data and reports while being a data evangelist
Qualifications:
Strong SQL and python skills
Experience writing & extracting data from web APIs and normalizing into a database format
Experience with both NoSQL data stores such as mongo DB and Relational databases
Knowledge of ERD and database architecture
Passionate about promoting data literacy and training the business on new tools and technology
Bonus Skills:
Knowledge of the AWS/Microsoft Azure
Knowledge of different big data and streaming capabilities / tools
Statistical analysis or machine learning
Experience working with various data sources such as clickstream and marketing data
Strong desire to always be learning
Experience with high growth ecommerce companies
Working Conditions:
Office environment  no special conditions
Flexible nature to manage competing and changing priorities
Some travel to support HR and social impact events and activities
Other Benefits:
Exciting and dynamic work environment fostering a people first mindset focused on giving back
Open & casual office environment  promoting wellness and engagement among team members
Competitive benefits package: health, dental, 401k, paid leave, on site gym, game room and product offerings for employees
Located at the Virginia Beach oceanfront, just steps from the beach with many surrounding amenities!
Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions of this position.

This job description should not be interpreted to be a complete list of all the duties, qualifications and responsibilities performed by the jobholder. To maintain organizational flexibility, management has the discretion to add, drop or change at any time the duties, responsibilities and expectations of this job. This job description does not constitute an offer of employment, continuous employment or an employment contract.

About The Company:
Based in Virginia Beach, Leesa is an innovative, fast-growing e-commerce mattress brand named one of the Top 20 Startups to Watch in 2015 by Forbes. Leesa is challenging the traditional players in the mattress industry with their premium mattress designs that are ordered online and ships compressed in a box to your door. As a proud certified B-Corporation, Leesa measures its success as much by its bottom line as by its impact on the world."
Data Engineer,Return Path,"Who is Return Path?

Return Path is the expert in email deliverability. Every day, our customers trust our data and insights to help them optimize their email marketing. Partnered with best-in-class email service providers, we help marketers take their email programs to the next level by driving more response and increasing revenue.

At Return Path, we know email.

Through the Return Path Data Exchange, weve brought together the worlds most comprehensive source of data from the email ecosystem. We partner with more than 70 providers of mailbox and security solutions, covering 2.5 billion inboxesapproximately 70 percent of the worldwide total. Also feeding into this data platform is our consumer network of more than 2 million consumers, delivering unparalleled insight into user behavior, brand affinity, and consumer preferences.

Once youre an employee here, this is a place where youll be challenged, inspired, rewarded, and transformed.

The Impact Youll Have

The Return Path Data Engineering team is responsible for general database administration, processing and maintenance of ""Big Data"", and the rollout and administration of tools to help employees access data. As a Data Engineer, you will collaborate with fellow engineers, data scientists and data analysts to support their needs and the needs of the business.

While you dont need a fancy degree to work here, you do need to embrace learning and continuous improvement, have a strong technical skill set, and favor simple solutions over complex ones. The people we hire are pragmatic, versatile, compelled by data and self-directed.

What does it take to be successful in this role?

Mid-to-Senior engineer with at least 4 years of professional experience in the software development field.
You have experience with, and a desire to continue exploring large data processing solutions (Spark, MapReduce, Hadoop, Hive, HBase, etc)
You are proficient in some or all of the following programming languages:
Python
Scala
Linux/Unix shell scripting
You have a deep understanding of and experience with relational database systems
SQL
Stored procedure experience
Oracle expertise a plus
You have development experience on Linux/Unix systems
Familiar with Linux/Unix command line
Experience with Linux/Unix utilities (i.e. cron, grep, editors, innovating processes, etc)
Knowledge of cloud data access tools such as Qubole and Snowflake a plus
We use virtualization/cloud technology
Public or private cloud (e.g. Amazon AWS)
Docker / Kubernetes
Exposure to common software development/deployment tools helpful
Revision Control Systems (Git/Github/Gerrit preferred)
Continuous integration (e.g. Jenkins)
You have an analytical and creative mind with a deep understanding of data structures and algorithms
You love working in an agile environment and are self-motivated People First

We are building an extraordinary company that helps people and businesses communicate more reliably, effectively, and securely. We put people first, do the right thing, and succeed together.

We believe that our people are the most important part of our business. We incorporate this philosophy into our practices and we live it every day. This means we have generous traditional benefits and take a lot of time to focus on quality management and leadership development. We also strive to ensure each person in the company is in the right job, knows how his or her job contributes to the overall company mission, and has a clear sense of career growth and direction. We have great employee retention because people love our culture and really feel like they're part of something special.

Do the right thing

The RP experience is a two-way exchange. We expect you to give us your best day in and day out, and you can expect the same from us. In exchange for your talent, your energy, and your unique contributions, we will strive to give you the flexibility to live a balanced, healthy life. This means more than competitive pay and benefitsit means offering the freedom to create your ideal experience inside and outside the office.

For more information visit www.returnpath.com

Return Path is proud to be an equal opportunity employer. We are committed to equal opportunity regardless of race, color, ancestry, religion, gender, gender identity, parental or pregnancy status, national origin, sexual orientation, age, citizenship, marital status, disability, or Veteran status."
Data Engineer,Sense360,"Sense360 is the leader in real-time, 360 insights which are revolutionizing the current state of market research, thats plagued by high-cost, static solutions that are neither timely nor accurate. Through analyzing and combining massive datasets of digital behavioral data with attitudinal surveys, Sense360 creates customized industry solutions that enable continuous measurement and optimization of its clients' business strategies and tactics . Sense360 was founded by successful repeat entrepreneurs and is funded by investors of Pinterest, Uber, HotelTonight, RiotGames and Twilio.

Sense360 has identified a massive market opportunity, and we are looking for a data engineer to help take us to the next level. You will have a large and immediate impact on the company, as you will be the 5th engineer on the team. We are looking for a scrappy engineer with solid CS fundamentals, who focuses on delivering value quickly. As a data engineer you will be responsible for architecting our data infrastructure that processes 1.4 TB data/day in an efficient manner. You will be working closely with our data science team to scale their processing algorithms. You will be working heavily with Apache Spark, AWS, Python and Apache Airflow.

Qualifications

3+ years experience

experience with distributed processing technology: Spark, Storm, Presto, Hadoop, Samza, Flink

experience with Python

experience working in a startup or an extremely strong desire to do so

solid CS fundamentals

excited to contribute in all phases of development; design, coding, testing, deployment and debugging

strong testing fundamentals

bias for delivering value over beautiful solutions

nose for data and data analysis a plus

Our Core Values

1. We are one team - We are one team with a single goal - to build an amazing company. If the company wins, we all win. We build the team by looking for amazing teammates who will elevate the team over individual genius.

2. Always improving - We believe that best people never stop learning and growing and that the only way to do that is to be humble and eager to be better. We crave feedback, we aren't defensive, and we internalize it. We are also equally as comfortable giving constructive and helpful feedback.

3. We are all owners - We empower everyone at the company to own their work. Owners want lots of responsibility, have authority and freedom to make real decisions, surface bad news quickly, and ask for help when they need it. A company of owners also means that ideas come from everywhere, that we all learn from each other, and titles and seniority don't matter.

4. Scrappiness - We are trying to do something transformative while also taking on the biggest and most resourced companies in the world. The only way we can pull off the impossible is to simplify everything to its core, only spend time and money on what truly matters, and work very, very, very hard.

5. We have fun - Building a company is hard work, but is also needs to be incredibly fun and rewarding. We don't just work together; we also make the time to have fun. This includes team outings, dinners, ping pong, halo and clipper games.

Our challenges

We have been solving hard problems from the beginning and we have many more to go!

We have built cutting-edge Android and iOS data collection SDKs. We have licensed our technology to companies with much bigger teams, who failed to build the same SDKs. Our data processing pipeline receives over 1.4 TB of sensor data/day. Cleaning, storing and processing that amount of data in a cost-efficient manner is a great challenge! We have built a complex data modeling pipeline, which evaluates the sensor data through multiple models and then fuses the results to come to the final conclusion. We also need to match our sensor data to a real world place (e.g. McDonalds, Nike store, laundromat). There are many datasets available but all have weaknesses. We are working to smartly fuse them.

If these types of problems excite you, you are the correct person for us!

Willing to relocate or sponsor visas."
Software Engineer (Data),Funding Circle,"Youve been there and done this
7+ years experience in database design and optimization, ETL and data pipelines.
Strong programming skills (Python, Java, Ruby, Scala)
Familiarity with AWS and Apache stacks including EMR, Lambda, EC2, Mesos etc.
Unix development skills including shell scripting and automation.
Experience with streaming platforms (Kafka/RabbitMQ/JMS/etc.).
Exposure to big data/NoSQL systems and the issues that arise from working with large data sets.
a self-starter attitude with an enthusiasm to work in a fast-paced, team-oriented, start-up environment.
experience in financial or other strongly regulated industry where oversight and attention to detail is paramount.

Why Join Us?
Happy employees are productive employees, thats why we offer a hearty benefits package. From subsidized catered lunches, to a competitive salary, equity, and health benefits, weve got you covered! That being said, have you heard about what we're doing?! Our mission is what really motivates us to come to work each day:
We're supporting small business, the engine of economic growth.
We're helping facilitate higher yields for investors and lower interest rates for borrowers.
We can fund loans extremely quickly, all online!
We have a clear competitive advantage globally in areas like domain expertise and regulatory processes.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Funding Circle provides equal employment opportunity to all individuals regardless of their race, age, creed, color, religion, national origin or ancestry, sex, gender, disability, veteran status, genetic information, sexual orientation, gender identity or expression, pregnancy, or any other characteristic protected by state, federal or local law."
Machine Learning Engineer,Adobe,"The Challenge
Machine Learning is critical part of Adobes Cloud offering. Adobe Clouds enable customers to create & manage Digital content, such as assets, composites, 3D, documents etc., and Digital experience and transformations. In Creative Cloud, Creative professionals and novice users alike need to manage lifecycle of their digital assets, libraries, the variety of creative content, and documents they work with every day, from brushes to colors, images, photos, videos, 3D and beyond. In Experience Cloud, it is all about optimizing the Digital Experience and Digital Transformations for Enterprises where digital content rules with mobile playing pivotal role, whereas in Document Cloud it is all about paperless world where offerings provide way for authoring and seamless transfer of content across users & entities. Adobe Cloud also provides the stock image marketplace, Adobe Stock, and the community, Behance, which entails deep machine learning embedding to enable content quality, search, discover, organize, contributor moderation, and more to allow for faster content velocity.
We are building new machine learning platform, called Adobe Sensei, that powers machine learning and AI across Adobe Cloud product lines by enabling the worlds best creative tools, leading cloud services for managing digital assets and digital experiences and the leading marketplace such as Adobe Stock and Adobe Behance. This platform will cater to thousands of applied researchers, millions of users, and billions of content pieces. Become part of this growing team at Adobe and a phenomenal impact in the area of computer vision, user understanding, language understanding, and digital experience optimization. The objective is to make machine learning offerings a world class, leading edge, differentiating product in Adobe Cloud ecosystem. We match the pace, innovation and excitement of a startup, backed by the resources and infrastructure of Adobe!
How can you participate? Were looking for a machine learning engineers, from entry level to senior roles, in the area of content understanding, computer vision, deep learning, user understanding, language understanding, information retrieval, search ranking, campaign and channel optimization, advertising bid optimization, and more. This is an opportunity to make a huge impact in a fast-paced, startup-like environment in great company. Join us!
Responsibilities
Work on machine learning algorithms, web services, distributed systems, data mining, big data, Hadoop, deep learning, recommendations, and more by developing a Machine Platform at Adobe that would power Adobe Clouds. Apply data mining and machine learning to improve content understanding, computer vision, deep learning, language understanding and content ranking & recommendations. Build platform to enable lifecycle of billions of images, documents and other assets in Adobe Cloud. Maintain and optimize machine learning platform, identify new ideas to evolve it, develop new features and benchmark possible solutions. Build machine learning platform using technologies such as REST web services, micro-services, Caffe, Tensorflow, Spark, Elastic, AWS, Kafka, Deep Learning, Matlab, R, and more.
What you need to succeed
BS or MS in Computer Science
Minimum 5-10 years of relevant experience in industry
Experience in machine learning technologies, especially in content understanding
Experience in building distributed systems.
Experience with Spark, big data processing, Hadoop.
Experience with Docker, Containerization, AWS.
Experience in engineering SaaS based software development
Hands on experience with Java, Python, and/or C++.
Exposure to some machine learning frameworks  Scikit, MLPack, TF, SparkMLib, Caffe, 
Hands on experience in building pipeline & lambda architecture
Experience in Web services, micro-services, and REST.
Experience in RDBMS & NOSQL database.
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, sexual orientation, gender identity, disability or veteran status."
Deep Learning Data Search Software Engineer,NVIDIA,"We are now looking for a Deep Learning Data Search Software Engineer:
Join the NVIDIA Autonomous Vehicles dataset team to support the development of state of the art Deep Learning / AI algorithms for our advanced Autonomous driving platform and other AI verticals.
We are seeking a Software engineer to implement a dataset search engine that will provide our Deep Learning teams with the requested datasets instantly. You should have developed expertise in databases, structural data search, scripting languages such as Python, and interacting with web front ends. You demonstrate excellent communication skills and have a drive to excel. This role involves collaborating with a diverse set of teams to implement, maintain and constantly improve the search solution for the larger dataset.
What you'll be doing:
Design, implement and maintain database schema
Design and implement highly efficient query and search of datasets
Dataset mining and performance optimization
What we need to see:
You have a Bachelors Degree in Computer Science or closely related field with 3+ years of relevant experience. Advanced degrees are helpful.
Strong background in crafting, maintaining and dealing with large databases
Experience working with front end web technologies, Rest APIs
You should be comfortable developing in Linux server platforms
Ways to stand out from the crowd:
Your experience working with large datasets and building server-based tools, database search and query is a huge plus
Experience with web application development for distributed systems and dealing with cloud based technologies for searching and querying will also help differentiate you from others applying.
Artificial intelligence, the dream of computer scientists for over half a century, is no longer science fiction. And in the next few years, it will transform every industry. Soon, self-driving cars will reduce congestion and improve road safety. AI travel agents will know your preferences and arrange every detail of your family vacation. And medical instruments will read and understand patient DNA to detect and treat early signs of cancer.
Where engines made us stronger and powered the first industrial revolution, AI will make us smarter and power the next. What will make this intelligent industrial revolution possible? A new computing model  GPU deep learning  that enables computers to learn from data and write software that is too complex for people to code.
NVIDIA is widely considered to be one of the technology worlds most desirable employers. We have some of the most brilliant and talented people in the world working for us. Are you creative and autonomous? Do you love a challenge? If so, we want to hear from you. Come, join our Deep Learning Infrastructure team and help build the real-time, cost-effective computing platform driving our success in this exciting and quickly growing field.
NVIDIA is committed to fostering a diverse work environment and proud to be an equal opportunity employer. As we highly value diversity in our current and future employees, we do not discriminate (including in our hiring and promotion practices) on the basis of race, religion, color, national origin, gender, gender expression , sexual orientation, age, marital status, veteran status, disability status or any other characteristic protected by law. #deeplearning"
Back-End Scala Engineer (Data Analytics),Fractal Industries,"Location: Reston, VA
Reports to: Chief Technology Officer

About Fractal
Fractal Industries applies artificial intelligence to solve complex, real-world problems at scale. Our Human+AI operating system, Fractal | OS, blends capabilities ranging from data handling, analytics, and reporting to advanced algorithms, simulations, and machine learning, enabling decisions that are just-in-time, just-in-place, and just-in-context.
Job Summary
As a Back-End Scala Engineer, you will design and develop elegant and performant code, predominantly in Scala and making extensive use of current tools such as Akka, Spark, and Swagger. You will join a growing multi-disciplinary team of Data Scientists, Engineers, and Solutions Experts with deep domain knowledge working to develop new capabilities and workflows to solve real-world client problems.
Your work will include developing tools to support complex analytics, simulation modeling, deep learning, and artificial intelligence within the Fractal OS SaaS platform.
Responsibilities
Develop well-designed and performant Scala applications and services using scalable technologies like Akka and Akka HTTP for RESTful services
Develop state-of-the-art analytics tools to support diverse tasks ranging from ad hoc analysis to production-grade pipelines and workflows for customer applications
Partner with Data Scientists and Analytics Engineers to improve performance and reliability of advanced algorithms
Ensure high performance and availability of distributed systems and applications
Interact directly with client project team members and operational staff to support live customer deployments
Qualifications
Residence in the greater Washington D.C. area or ability to relocate
2+ years of full-stack development expertise
Expert knowledge of Scala, sbt, and Maven, git
Experience with well-known Scala libraries, including Akka, Spark, Shapeless
Experience with RESTful API development
Effective testing and software quality assurance experience, including performance profiling, static analysis, and dynamic analysis
Experience with agile development within commercial projects or elite non-commercial research
Affinity for learning and applying new technologies and solving new problems
Effective organizational skills with strong attention to detail
Strong communication skills  both written and verbal
Compensation & Benefits
Competitive salary
Incentive stock options
Full range of benefits starting day one, including 401(k) and medical, dental & vision coverage
Flexible Personal Time Off (PTO) plan and 10+ paid holiday days per year
Contact Us
Apply for this role at fractalindustries.com/careers"
Machine Learning Engineer,"Faraday Future, Inc."," Build core data capabilities and services for cloud platform
 Design, develop, and implement data solutions
 Designing a flexible recommender system
 Be an active hands-on team member, collaborating with other developers and architects in developing client solutions

Basic Qualifications:
 Expert knowledge of Java and JVM, C and Linux system
 Solid understanding of data science and machine learning algorithms and their appropriate use.
 Solid understanding of the various aspects of the Data Science / Machine Learning / Big Data / Artificial Intelligence ecosystem
 Experience with Spark
 An advanced degree in computer science, computer engineering, statistics, economics, mathematics, operations research or related field

Perks + Benefits
 Stock options for every employee (we are all shareholders)
 Healthcare, dental, and vision benefits (free for you and discounted for your family)
 401k options (save for your future)
 Engineering orientation and onboarding program (learn, work, and grow)
 Healthy snacks and beverages 24/7 (satisfy your afternoon hunger pangs)
 15 days of annual paid time off, 10 days of paid holidays
 Relocation assistance/reimbursement (well help you start your new life)
 Free parking & carpool reimbursement (save the environment and your wallet)
 Casual dress code & relaxed work environment (your work will speak for itself)
 Frequent social events, happy hours, and team parties (meet your new best friends)
 Soul of Faraday community outreach team (global change starts locally)"
Big Data Engineer,Wargaming America,"Wargaming is looking for a Junior Data Engineer who have a keen interest in working with large amounts of data and optimizing and creating algorithms at scale. This is a fantastic opportunity to be part of a vastly growing and award winning global brand in the gaming industry that is at the cutting edge of PC and console gaming technology.
The Data Engineer will support and perform activities involved in the design and development of ETL/data integration process and programs which include data analysis, data modeling, metadata definition and management, job scheduling, and the development and testing of MapReduce code.
Reports to: Enterprise Data Architect located in Austin, Texas. Occasional international travel required.

Responsibilities :
Write code to ingest data from various sources
Work with game development studios to log data in a consistent and complete manner
Write Data Integration tasks/processes to clean/transform raw data into clear data models
Optimize pipelines for performance
Implement algorithms at scale as needed
Implement new technologies as needed
Have fun playing with loads of data

Requirements :
Bachelors-degree in Computer Science, or Computer Engineering, or applicable work experience
Working experience in Hadoop Ecosystem
2+ year experience in Scala
1+ year experience in Spark
Experience with message brokers, such as Kafka, Flume, or similar technologies
2+ year experience in Python, Java , C++ and related frameworks
English proficiency
Knowledge of Russian language is a plus"
Data Engineer,Brilliant Infotech Inc.,"Location: San Francisco, CA

Contract Duration: 7 months

Visa Status: Valid work authorization.

Position Summary:
We are seeking an experienced Analytics Full Stack Developer / Data Engineer in the New Team to lead moderate to highly complex programming tasks and provide code reviews of various development tasks. You will implement automated unit and integration tests and ensure the practice is followed consistently. You will participate in system and cloud-targeted in fractured design and specifications, program logic flow-charting, data engineering, development, testing, debugging, documentation, and support. Additionally, you will provide analysis of problems and recommends solutions.

Position Responsibilities:
Works on complex problems having broad impact that require in depth analysis and judgment to obtain results or solutions.
Defines and documents business requirements for complex system development or testing.
Analyzes existing applications and systems and formulates logic for new systems, devises logic procedures, logical database design, performs coding and tests/debugs programs.
Represents the company in contacts with other utilities, various industry boards, committees, regulatory agencies, governmental bodies, and other private organizations.
Mentors and provides guidance to less experienced colleagues.
Modifies and configures complex and broad applications/systems as specified in the requirements and/or technical design document.
Develops, implements, and approves guidelines and process documentation.
Designs and deploys new complex Enterprise systems and enhancements to existing systems ensuring compatibility and inter-operability.

Minimum Education/Skills:
Bachelor of Arts or Bachelor of Science in related field or equivalent work experience
5+ years of related work experience
Experience with AWS  S3, EC2, RDS, EMR, CloudFormation, etc.
Strong technical skills and relevant experience with Node.JS server side Javascript
Experience in designing and developing web services using REST and JSON
Experience in designing and developing data pipelines within distributed environments, i.e. Python, Spark, etc.
Experience with utilizing Git as a shared repository, i.e. code check in, branching, etc.
Experience with NoSQL document-based to traditional SQL relational data stores like MongoDB, MySQL, and RedShift and designing suitable data models
Strong Unix/Linux/Shell skills
Hands on development of applications and services

Desired Education / Skills:
Experience in product development
Experience as Developer, full-stack, 5 years
Experience in product management
Competency in building partnerships and working collaboratively with others to meet shared objectives
Competency in anticipating and adopting innovations in business-building digital and technology applications
Competency in relating openly and comfortably with diverse groups of people
Competency in developing and delivering multi-mode communications that convey a clear understanding of the unique needs of different audiences
Knowledge of interface design concepts, methods, best practices, and techniques as applied to software, such as computers, mobile devices, and other electronic devices as needed to perform at the job level"
Data Analytics Engineer,Fractal Industries,"Who We Are
Our management team of experts come from industry, government and education. The rest of us are a cross-section of engineers to architects with credentials and experience in everything from DevOps and building microservices to data analytics, machine-learning and simulation modeling. Weve all come together to take on a critical challenge of building an AI system to help companies navigate cybersecurity within, around and at the borders.
Whether its advanced data analytics, simulation, or deep learning, its more than code and algorithms. Its risk calculations, cyberdefense, protecting global systems and augmentation of the human cunning. Also, its the occasional late-night Red Bull run : )
In the end, were having fun. Want to help us redistribute the future?

Who You Are
You dream of algorithms to harness the mysteries of sprawling oceans of data
You are an expert in Scala, Python and SQL with basic knowledge in Machine Learning or Deep Learning
You are a problem-solver, where some require elementary Math/Statistics, and others, a blend of analytic and simulation modeling
Your ideal day is breakfast with the leading expert on the NASA created automated planning language ANML in the morning to talk over how youll be implementing a near real-time data processing solution for a modeling application. In the afternoon, you have lunch people from Fractals man-made-peril insurance underwriting team. In between, you work with some of the full-stack Scala folks to ingest more sources of real time streaming data for use by machine learning models. By the end of the day, youve learned some cool things about amazing technology and youve contributed in a meaningful way to making us all a little better at what we do.

Think you might be interested?

This is all pretty high-level, obviously, but if it interests you and youre curious to learn more, then we look forward to receiving your application."
Software Developers - Data Analytics Engineer,"Solidyn Solutions, Inc","Overview **Top Secret/SCI Clearance Required**

Solidyn Solutions, Inc., a 2016 & 2017 Denver Post ""Best Places to Work"" award winner, is looking for highly qualified Software Developers to join our team supporting several high speed, large data processing and reporting systems in Aurora, CO. This team develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) working jointly with the customer to define system requirements and the user implementation.

Solidyn Solutions, Inc. values highly qualified talent and supports them through a comprehensive benefits and training package. We differentiate ourselves within the industry by valuing the individual employee, offering them a tailored and unique training package to support their career growth. As a member of a growing small software engineering firm, you will have opportunities for growth that support our Nation on the largest most complicated programs.

Our ideal candidate blends a solid background in Engineering with the ability and desire to make a difference on program. While supporting large teams, we differentiate ourselves by seeking out challenges on program and providing unique solutions to the Prime Contractor and end customer. The candidate must have experience and knowledge with software engineering using object oriented methodologies to analyze and manipulate customer data sets. The candidate will work closely with systems engineers, integrators, testers, and customers to design, develop and support software applications throughout the development and maintenance life cycle.

Responsibilities RESPONSIBILITIES:
Develop data queries and algorithms to meet customer requirements
Develop unique data reporting and display capabilities to support user needs
Ability to design, develop, integrate and maintain new software functionality
Ability to work with previously written code and make modifications as necessary
Enthusiastic and energetic performer able to work in a dynamic, fast paced, and sometimes pressured environment
Team player incorporating a team based success philosophy
Communication of complex and diverse engineering topics

REQUIRED SKILLS:
Prior experience designing and implementing software systems that manage, store, report, and analyze large scale data sets
Development experience in SIGINT data processing using cloud architectures, and cloud analytic development
Minimum 6 years working experience in object-oriented software design and development using C++ and/or Java
Software and system-level debugging experience including the ability to identify problems in established code and make modifications as necessary
Experience with software execution throughout the entire software life cycle (preliminary design through maintenance phases)
Current or Active SSBI and ability to obtain a security clearance at the TS/SCI level
DESIRED SKILLS:
Experience in data ingest and Ozone Widget Framework
Certification in Cloudera data technologies (CCAH, CCP Data Scientist, CCA Spark and Hadoop developer)
Programming experience in MapReduce, Key-Value store technology, Hadoop, Apache Accumulo toolsets
Experience in Distellery/IBM Streams framework, NiagraFiles
Experience with application development involving RDBMS (such as Oracle, mySQL, etc)
Experience adapting agile software development methodologies to large scale or more traditional, requirements based software development efforts
Experience with UML design modeling in modeling tools such as Rational Rose, RSA, or Rhapsody
Experience with customer interaction and end-user engagement
Experience integrating software (modules, components, subsystems)
Experience preparing and presenting status information and technical material

Qualifications
Minimum of 4 years of experience working as a software engineer on Intelligence Community or DoD related data analytics programs
REQUIRED EDUCATION:
Bachelor of Science, BS in a Math, Science or Engineering related field additional education and training through an advanced technical degree or continuous learning is highly desired

Compensation

We offer a challenging work environment in a growing business. At Solidyn you will have the opportunity to work with interesting, motivated, and professional engineers within an organization that strongly supports personal growth and continued development. Solidyn offers a competitive compensation and benefits package including: health, dental and vision insurance, flexible spending accounts, company paid life insurance, long and short term disability insurance, 401K plan with company contribution, tuition reimbursement and a personal training allowance. With a combination of paid time off and paid holidays, Solidyn staff enjoys 5 weeks of paid leave per year with a flexible work schedule.

Solidyn Solutions is an Equal Opportunity Employer, Females/Minority/Veterans/Disabled/Sexual Orientation/Gender Identity are encouraged to apply."
Data Analyst,"Honda R&D Americas, Inc.","Position Core Role:

Data Analysts utilize market data to investigate and identify quality problems on Honda and Acura products in the field. They are able to analyze multiple data sources in detail (warranty claims, dealer information, manufacturing data, geographical, environmental, etc.) to identify trends and problem indicators. Data analysts are able to summarize and evaluate market data, make conclusions and support next step direction in terms of problem analysis.

Essential Function and Responsibilities:

Create and maintain job query criterias in web based application which involves writing complex queries to capture only claims within a specific problem scope. Support and direct analysis engineers with developing data queries for a specifically described problem. Forecast the magnitude and impact of market problems.

Daily Tasks:

Review warranty claims from various sources to identify high-priority claims using engineering judgement. Perform data requests, confirm requests are all encompassing of info. needed. Data trend, comparison and/or forecast analysis. Develop data and reports related to top problems and provide to Market Quality management. Issue formal documentation of problem investigations to Market Quality management.

Required Knowledge, Skills and Abilities:

Ability to be decisive and determine the importance of market problem and its impact to customer satisfaction and future models.

Ability to create and maintain top problem lists to support setting next step direction, priority and to track countermeasure effectiveness.

Ability to develop/program tools to improve department efficiency, eliminate repetitive tasks and implement new warranty analysis tools/techniques.

Ability to explain data investigations and conclusions that were reached to Market Quality management members, specifically critical, high-priority items.

Ability to interpret data and summarize in a logical and effective manner.

Ability to make sound decisions.

Ability to prioritize workload.

Proficient in Microsoft Excel.

Strong communication skills (verbal, written and visual) with the ability to create and present market data reports to support investigations.

Desired Knowledge, Skills and Abilities:

Excel VBA programming skills.

QlikView programming skills.

SQL programming skills.

Required Experience:

Minimum of 3 months of experience in data analytics or related data analysis experience.

Education Requirements:

Bachelors degree in Data Analytics, Computer Science, Mathematics, Statistics, Engineering or equivalent experience.

Honda Factors:

Responsible for team data analytics on automotive products built at every Honda and Acura factory in North America (Accord, CR-V, RDX, NSX, etc.).

Ability to travel approximately time per quarter for approximately 1-3 days.

#LI-BS1"
Software Engineer - Data,Blackbaud,"We are looking for a passionate, versatile Software Engineer who will help create, manage and monitor our applications and infrastructure that power the philanthropic community worldwide. Youll work with other engineering team members in the Analytics division to develop both applications and their infrastructure as software. The ideal candidate is someone with an interest in and ability to work in a cross-functional team in multiple capacities including application development, infrastructure development, deployment, testing and monitoring. Our responsibilities span data ingestion/ETL, Big Data, data analysis, machine learning, web application development, DevOps and more.
We work in an Agile/Scrum environment and believe that teams should have ownership of their solutions from inception to delivery. We value curiosity, creativity, and flexibility over experience with a specific technology stack. Our ideal engineer is forward-looking and detail-oriented, but pragmatic about decision-making; self-directed in their work but collaborative in nature; above all, committed to the teams success. As a Blackbaud Software Engineer, you will directly impact inspiring philanthropic organizations.
What youll be doing:
Designing, building and maintaining the data-oriented applications and infrastructure that allow Blackbaud to deliver analytic capabilities to clients
Building data pipelines, data stores and service-oriented applications that deliver data, insights and predictive analytics
Developing an understanding of what it takes to scale our services and reduce costs in both public and private cloud environments
Contributing to the systems that makes it possible for an agile team to easily operate and maintain these environments
Youll be working with Blackbauds state of the art tech stack including:
Public/private cloud environments: AWS and Azure
Data Layer Technologies including: Hadoop Ecosystem Tools (Hive, Presto, Impala, MapReduce), MS SQL, Oracle, PostgreSQL MySQL, Redshift, DynamoDB
Front End: ASP.net, Single Page Applications
Infrastructure and Managed Cloud Services: EMR, Data Pipeline, Elastic Beanstalk, RDS, HDInsight, Azure Data Factory
Languages: SQL, C#, Java Python, PHP
Scripting: Linux Shell Scripting, PowerShell, Python
What well want you to have:
2-5 years software development experience
Versatility and an interest in multiple areas of software engineering: data processing, data store design, application development, devOps, automated testing, infrastructure as software, etc.
Hands-on experience in some of the technologies listed above, particularly in the area of data engineering.
Understanding of software engineering best practices including: Agile/Scrum methodology, design patterns, test-driven development, and working within cloud environments such as Azure/AWS
BS degree in Computer Science, Computer Engineering, or similar field"
Data Warehouse Engineer II,InComm,"Overview Leveraging deep integrations into retailers point-of-sale systems, InComm provides connectivity to a variety of service providers that allow consumers to conduct everyday business at more than 450,000 points of retail distribution worldwide. Whether those consumers are activating prepaid products, paying bills, enjoying real-time discounts through a membership card, purchasing digital goods in-store or adding funds to an online account, InComm is there to provide unique gift-gifting opportunities, cater to on-the-go shoppers, deliver added value through loyalty programs and serve cash-based consumers. With 186 global patents, InComm is headquartered in Atlanta with a presence in over 30 countries in North and South America, Europe and the Asia-Pacific region. Learn more at www.incomm.com or connect with us on www.twitter.com/incomm, www.facebook.com/incomm, www.linkedin.com/company/incomm or www.incomm.com/blog.

About This Opportunity InComm is currently seeking a Data Warehouse Engineer to be a part of our Enterprise Data Warehouse group. This position reports directly to Manager, EDW Development. The EDW Development group consists of 2 teams, EDW Development and EDW Reporting. The DW Engineer will be a part of EDW Development, a team of 5 who is responsible for collecting data from multiple sources sending data to a centralized DW for the organization.

The current EDW team consists primarily of developers. However, the Data Warehouse Engineer will be dedicated to problem resolution and coordination of changes with other teams, (Network, DBA, Server Systems, etc.). This role is key to the success of this team. The primary responsibilities for this role will include:
Provide critical systems support and timely problem resolution
Heavily involved with the data center migration activities for the BI servers
Coordinate for other team on special projects
ETL files
Transfer files to loadable data warehouse
Work closely with report development team
Extra data from Data Warehouse and put it into a defined mart

The Data Warehouse Engineer should have experience with the following:
ETL (SSIS)
Previous database experience
SQL queries
Data marts
Pentaho experience is a plus but, not required

Responsibilities
Primary responsibility for problem investigation and resolution during normal work hours.
Participation in after hours support rotation
On-going platform maintenance
Coordination with Data Integration, Network, DBA, Server Systems, and Storage Systems for cross-team projects, resource procurement, system maintenance, server migrations or upgrades, database maintenance, and general change management.

Qualifications
Minimum 5, Preferred 7+ years experience in Data warehousing or related position
4-year degree in related field
Extensive knowledge of Microsoft BI Stack (SSIS, SSAS, SSRS)
Extensive knowledge of Microsoft SQL Server and query performance tuning
Extensive knowledge of ETL and best practices
Extensive knowledge of C# and VB as it pertains to Scripting in SSIS
Knowledge of data warehouse modeling and design
Knowledge of creating and managing restfull services for accessing data warehouses
Strong collaboration skills and ability to build strong working relationships across multiple teams to include Data
Integration, Network, DBA, Server Systems, etc.
Strong written and oral communication skills

InComm provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, gender identity or national origin, citizenship, veterans status, age, disability status, genetics or any other category protected by federal, state, or local law.

*This position is eligible for the Employee Referral Bonus Program - Tier 4"
Machine Learning Engineer,Lawrence Berkeley National Laboratory,"The National Energy Research Scientific Computing (NERSC) Center at Berkeley Lab is seeking a passionate Machine Learning Engineer to collaborate with scientists and conduct applied research and development, outreach and training in Machine Learning for Science.

This role will enable Office of Science researchers to benefit from the very latest machine learning and deep learning (ML/DL) techniques, conducted on one of the worlds largest supercomputers. You will gain experience using cutting edge machine learning techniques and Big Data technologies and tools, operating at extreme-scales and work in a collaborative environment with scientists and engineers from a wide variety of backgrounds.

You will be part of a NERSCs Data and Analytics Services group that supports experimental science and advanced analytics. You will also be part of multidisciplinary and cross-institution projects, involving academic and industry partners such as Google, Intel, NVIDIA, and Cray, and renowned academics both in domain sciences as well as in machine-learning and statistics.

What You Will Do:
Collaborating with scientists to develop new applications of machine learning for science - opening door to new science.

Designing new deep learning architectures to improve potential for science discoveries in many fields.

Bringing these techniques and the latest ML/DL frameworks and tools to NERSC users and the some of the worlds largest supercomputers.

Provide expert ML/DL advice and consultancy services to scientists and users of NERSC computing resources.

What is Required:
Requires a Bachelors degree, or equivalent experience and 8 years related experience (or higher degree).

Background in machine learning or statistics, including familiarity with multiple deep learning architectures and technologies.

Some experience, knowledge and/or significant interest in applying multiple machine learning techniques to scientific data.

Possess strong communication and interpersonal skills.

Proven ability to work productively both independently and as part of an interdisciplinary team balancing divergent objectives involving research, code development, supporting software and consulting with scientists.

Additional Desired Qualifications:
A PhD in the above numerical fields.

A proven track record of publications in Deep Learning at machine learning or domain science venues.

Familiarity with multiple hardware backends (e.g. CPU, GPU, FPGA)

Familiarity with performance profiling, benchmarking, optimization and scaling of DL architectures on HPC systems.

The posting shall remain open until the position is filled, however for full consideration, please apply by close of business on January 15, 2018.

Notes:
This is a full time, 2 year, term appointment with the possibility of extension or conversion to Career appointment based upon satisfactory job performance, continuing availability of funds and ongoing operational needs.

Full-time, M-F, exempt (monthly paid) from overtime pay.

Salary is commensurate with experience.

This position may be subject to a background check. Any convictions will be evaluated to determine if they directly relate to the responsibilities and requirements of the position. Having a conviction history will not automatically disqualify an applicant from being considered for employment.

Work will be primarily performed at Lawrence Berkeley National Lab, 1 Cyclotron Road, Berkeley, CA."
Big Data Software Engineer,Altamira Technologies Corporation,"Altamira is a top open source technology company in the national security space. Headquartered in McLean, Virginia, Altamira serves the defense, intelligence and homeland security communities by focusing on creating innovative solutions leveraging common standards in architecture, data and security.

Altamira is looking for a Big Data Software Engineer with an active TS/SCI security clearance and favorable Polygraph, to support a government customer in Reston Virginia.

The selected Big Data Software Engineer will develop and maintain a data ingestion and enterprise search tier capable of supporting thousands of queries a day. The successful candidate will be a member of a dynamic team that develops and maintains multiple back end projects that support the application. The candidate will develop, implement, and maintain complex back end systems. The selected candidate must have the ability to work in a mixed technical environment and maintain productivity when switching between languages and technologies. The candidate must work well in a team environment and have the desire to develop software in an agile fashion.

Job Requirements:
6+ years of experience as a software developer
5+ years of Java development experience
Experience with Hadoop, Lucene, Hive
Familiarity with distributed data processing of large datasets
Perl, Bash and similar scripting languages
Experience with agile software development practices (unit testing, continuous integration, static analysis)
Experience with NoSQL technologies
Understanding of JVM efficiency and focus on the speed of execution
***Requires TS/SCI with Polygraph clearance***

Altamira is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or protected veteran status."
Data Engineer - New Ventures Campus,McKinsey & Company,"Qualifications
Be currently enrolled in an academic program and looking for a full time job in 2018
Ability to structure and analyze data in appropriate framework (e.g., Excel, Access, VBA, SQL)
Passion for data tools and processes to drive business insights
Strong analytical thinking, process management and quality control
Ability to quickly understand and appreciate underlying business context, problems and objectives of analytical projects
Clear communication skills to run well defined analyses and produce reports
Excellent time management skills

Who You'll Work With Youll work with our New Ventures team in one of our North America offices.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for and expands our investment in our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and were hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.Jr. Specialists have a base of expertise in a function or industry. They are staffed on engagements (largely in full time roles) and are expected to leverage expertise to solve some of the most pressing and complex issues at clients.
What You'll Do You'll conduct full life cycle analysis of data sets in consultation with senior colleagues, including implementing data acquisition, cleansing, transformation and upload activities.

You will provide analysis, analytical modeling, and/or visualization of data sets using relevant data tools. You'll identify possible trends and patterns in the data, and communicate relevant findings that help support problem solving, insight generation and decision making.

You'll apply your understanding of client data sets and intended use to effectively capture, validate, cleanse, transform and upload data. You'll perform quality checks to ensure consistency, integrity and robustness of data. You'll proactively identify potential issues (e.g., inconsistencies) with data quality, collection or reporting. You'll support solution delivery team, often remotely, in managing client data.

You'll leverage standard reporting to clearly articulate findings to solution delivery team or client team. You will be responsible for coding and scripting (SQL) for automating data transformation and loading specified by delivery team. You will perform impactful data enrichment, based on understanding of the sector/industry and types of data that are available in the space."
Data Engineer,FocusKPI Inc.,"FocusKPI, Inc. is looking for a Data Engineer to work for our client in San Francisco,, CA. This is a contract position with a negotiable rate.

Duties:
Help define and drive roadmap for delivery of usage analytics to business stakeholders

Work with product and business teams to understand data needs, provide in-depth analysis support and share associated business insights

Perform regular ad-hoc data querying and analysis to better understand customer behaviors including acquisition, engagement, conversion and retention

Gather, synthesize and interpret disparate quantitative information sources, develop meaningful insights and clearly disseminating to key stakeholders

Create and update regular reporting based on a variety of data sources

Understand, monitor, QA, translate, collaborate with business stakeholders to ensure on-going data quality

Skills:
At least 1+ years of Mobile and Web analytics exp.

Experience in a software product company working on SAAS related metrics and dashboards is strongly preferred.

A strong proficiency in querying and manipulating large data sets for analytical purposes using SQL-like languages (Hive / Impala experience preferred)

Advanced Excel and PowerPoint Skills, including use of the PowerPivot add-in

Familiarity with big data platforms such as Hadoop and Amazon Redshift

Python knowledge is strongly preferred.

Experience with data visualization tools such as Tableau

Good attention to detail and ability to QA multiple data sources

Desired experience in developing and implementing a full range of analytic techniques resulting from the application of business analytics, statistical models, segmentation techniques, and optimization techniques

Thank you for applying!

FocusKPI Hiring Team

About FocusKPI:
FocusKPI Inc. is an analytics consulting business providing solutions such as CRM customization, Risk Management, Business Intelligence, Advanced/Operational/Web/Digital Analytics as well as Human Resources for our clients."
Big Data Engineer,Impact Radius,"Who we are...
Impact Radius is marketing technology company that is transforming how advertisers handle media and performance marketing partnerships. Our integrated suite of products enables the worlds largest and most successful digital brands and agencies to maximize their return on global ad spend by providing a single trusted view into the consumer journey. If you are looking to join a team where your opinion is valued, your contributions are noticed, you get to work with fun and talented people solving cutting edge marketing challenges with technology, and advance your career, this is the place for you.
Why this role is different and what you will be doing.
As a Big Data Engineer in this role, you will develop software using the latest open source technologies for our Forensiq Product. Forensiq is a SaaS product that uses algorithms to accurately pinpoint fraudulent activity in online advertising. Using our services, companies make more intelligent media buying decisions, saving millions in advertising cost.
You will collaborate with people in the office and remotely over video chat with other locations. You will be part of an autonomous, agile team called a Squad. Each Squad is made up of members focused on Product, Engineering, and Data Science. We look for individuals like yourself to provide recommendations to the team. We also expect an independent capability in order to investigate and learn new technologies.
Skills we require...
Roughly 3-7 years work experience in software engineering
Solid Java, Scala or similar development foundation
Experience with Hadoop and SQL environments (Spark, HBase a plus)
Wants to participate in all facets of Design-Test-Build-Deploy
Loves Big Data and wants to get into Google Cloud and Spark
Skills we desire...
Curious about Data Science and new Algorithms
Likes automating tests and monitoring data quality
Leverages open source and contributes to it
Showcases code on Github, blog, or website

Our benefits & perks...
Medical, Dental, Vision w/Flexible Spending Accounts
401k Matching
Unlimited PTO, a Results Only Work Environment
Casual Office w/Games, Foosball, Ping-Pong
Weekly Lunch, plus Stocked Kitchen
Flat Org Structure
____________________________________________________________________________________________________________________________
Do you want to JOIN OUR ever-growing TEAM: If yes..... Please click on the Apply button below and fill in your details! Easy!
If you said no, perhaps this is not for you. However, would you still like to join our awesome company? Wed love to hear from you at careers@impactradius.com with the subject line: ""I want to join Impact Radius!"" and we will connect with you shortly.
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Big Data Engineer,IBM,"Job Description

We live in a moment of remarkable change and opportunity. The convergence of data and technology is transforming industries, society and even the workplaceby creating professions that didnt exist before the emergence of data, cloud, social and mobile. IBM Global Business Services is a leader in this worldwide transformation and just the place to define and develop your consulting career. Whether its business consulting, sales, project management or a technical path, youll have the opportunity to make an impact on the world by working to solve some of societys most complex problems with your original thinking and ideas. As an IBMer, youll innovate in pursuit of higher value in everything you do, all while being guided by IBMs purposeto be essential. Essential in your leadership and dedication to building valuable client relationships with groundbreaking work. Essential in uncovering whats possible and helping global clients succeed. Join us as we make the most of these exciting times and discover what you can make of this moment. What will you make with IBM?

As a Big Data Engineer; you will be responsible to design and develop multiple big data utilities that automate various aspects of data acquisition, ingestion, storage, access and transformation of data volume that scale up to petabytes. You will be part of a multi-disciplinary technical team.
Design and build data services to auto create Hive structures, HDFS directory structures and partitions based on source definitions using a configuration and metadata driven framework.
Design and build custom code for audit, balance, and controls, data reconciliation and entity resolution; Create custom UDFs if needed to do complex data transformations.
Create technical specifications, Unit test plan/cases and document unit test results. Perform Integration testing for the end-to-end data pipelines.

BENEFITS
Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.
ibm.com/employment/us/benefits/
ibm.com/press/us/en/pressrelease/50744.wss
CAREER GROWTH
Our goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:
Skill development: helping our employees grow their foundational skills
Finding the dream job at IBM: navigating our company with the potential for many careers by channeling an employees strengths and career aspirations
Diversity of people: Diversity of thought driving collective innovation In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP
With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!
ibm.com/ibm/responsibility/initiatives.html
ibm.com/ibm/responsibility/corporateservicecorps/

Required Technical and Professional Expertise

2-4 years of deep working knowledge in use of open source tools such as; Hive, Sqoop, etc..
2-4 years of coding experience with SQL & Linux Bash/Shell Scripting
2-4 years of coding experience with Java, Mapreduce, Pig and Python.
4 + years of hands on experiences and thorough understanding of Linux scripting.
5+ years of Java development experience.
Familiarity with Big Data concepts, Data Lake development, Business Intelligence and data warehousing development processes and techniques
Familiarity with business intelligence and data warehousing development processes and techniques
Experience working with Cloudera Manager, Navigator, Impala and security tools (Sentry, Gazzang)

Preferred Tech and Prof Experience

SPARK
Kafka
Flume
MYSQL
NoSQL
Casandra
Neo4J
MongoDB
Hortonworks
Big Insights
Cloudera

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Machine Learning Engineer,Capital One,"7900 Westpark Drive (12131), United States of America, Tysons, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Machine Learning Engineer

Machine Learning Engineer  C4ML

Job Description
Are you a high performing software/system engineering-hybrid looking to take part in some of the most cutting edge research and production projects? Do you read constantly about advancements in various applied machine learning architectures and solution white papers? Would you like to take part or drive the creation of publishable advancements in machine learning across various disciplines? You could be a perfect match for a Machine Learning Engineer role at Capital One's Center for Machine Learning (C4ML).

As a Machine Learning Engineer in C4ML, you will contribute to building fast architectures and machine learning solutions to create or improve some of the most interesting use cases in the financial services industry. Our projects revolve around advances in system/model design and cloud based deployment for resilient production systems. Capital One maintains a full stack of technology solutions including streaming big data, state of the art machine learning, micro-service architecture, distributed computation engines, and intuitive visualizations in the cloud. To manage this, we are working with several cutting-edge technologies and are actively developing and contributing to the open source community. We are highly technical with strong backgrounds in our fields to support use cases ranging from cyber threat prevention to sophisticated NLP semantic understanding in an always on 24/7 service architecture. We have the highest executive support for acting as a catalyst of machine learning across Capital One providing our researchers extraordinary diversity in topics.

What you will bring to the role:
Excellent communication skills evidenced by multiple white papers (internal proprietary or externally published).
Demonstrated ability to build full stack systems architected for speed and distributed computing.
Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.
Experience mentoring junior engineers.
Adept at simultaneously working on multiple projects, meeting deadlines, and managing expectations.

What you will do in the role:
Act as an independent consultant to various lines of business to help create or improve projects.
Develop both deployment architecture and scripts for automated system deployment in AWS.
Code new machine learning paradigms, sometimes from first principles, for integration into production systems.
Learn and work with subject matter experts to create large scale deployments using newly researched methodologies.
Construct data staging layers and fast real-time systems to feed machine learning algorithms.
Create white papers, attend conferences, and contribute to open source software.

Basic Qualifications:
Bachelors Degree or Military Experience
At least 2 years of experience designing and building full stack solutions utilizing distributed computing.
At least 2 years of experience integrating with larger code bases or materially important pull requests against larger projects.
At least 2 years of experience working with Python, Scala, or Java.
At least 2 years of experience with leading distributed file systems and multi-node database paradigms.

Preferred Qualifications:
Masters Degree
At least 4 years of experience in designing and building full stack solutions utilizing distributed computing.
At least 6 years of experience integrating with larger code bases.
At least 4 years working with Python, Scala, and Java.
At least 4 years of experience with leading distributed file systems and multi-node database paradigms.
At least 2 years leading teams in code development and balancing feature requests with feasibility constraints.
A history of publications and conference attendance.

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Big Data Engineer - Senior Associate - Consumer Markets,PwC,"PwC/LOS Overview
PwC is a network of firms committed to delivering quality in assurance, tax and advisory services.

We help resolve complex issues for our clients and identify opportunities. Learn more about us at www.pwc.com/us.

At PwC, we develop leaders at all levels. The distinctive leadership framework we call the PwC Professional (http://pwc.to/pwcpro) provides our people with a road map to grow their skills and build their careers. Our approach to ongoing development shapes employees into leaders, no matter the role or job title.

Are you ready to build a career in a rapidly changing world? Developing as a PwC Professional means that you will be ready
to create and capture opportunities to advance your career and fulfill your potential. To learn more, visit us at www.pwc.com/careers.
PwC Advisory helps our clients with their most challenging imperatives from strategy through execution. We combine the breadth of knowledge of over 48,000 global professionals with deep industry knowledge to deliver custom solutions for our clients. We work with the world's largest and most complex companies and understand the unique business issues and opportunities our clients face.

Job Description
Our clients are focused on improving business performance, responding quickly and effectively to crisis situations, and extracting value from transactions. Our growing Consumer Markets industry team provides strategy, management, technology and risk consulting services to help a diverse set of clients around the world anticipate and address their most complex business challenges. This industry vertical team includes Consumer Products, Retail, Travel andTransportation clients.

Our Data and Analytics consultants help clients maximize the value of their data.

This high performing team helps clients define their

information strategy, architecture and governance, get the most value from business intelligence and analytics, and implement enterprise content and data management solutions to enable business insights, reduce cost and complexity, increase trust and integrity, and improve operational effectiveness.

Position/Program Requirements
Minimum Year(s) of Experience: 3

Minimum Degree Required: Bachelor's degree in Economics, Statistics, Operations Research, Computer Science, Information Systems, Engineering or similar quantitative discipline

Knowledge Preferred:
Demonstrates thorough knowledge of, and proven success with a role, executing various aspects of the business or information technology-related client engagements and/or projects within a professional services environment, including knowledge of an industry or functional domain and expertise in at least two of the following areas:
Statistical Analysis and Techniques;
Customer, Product and Pricing Analysis;
Business and Operations Analysis within marketing, operations, or risk analysis using quantitative techniques. Demonstrates significant experience working with analytical models or techniques and the ability to apply such methods to an applicable challenge within the larger field of expertise. This background should include:
Visualization techniques and tools;
Advanced analytics techniques (regression, simulation, etc.);
Applicable sources of external, internal or new data sources to support analytic methods and visualization techniques;
Healthcare, Insurance, Financial Services, Customer an industrial Products and Services, Manufacturing or Telecom Industries.
Skills Preferred:
Demonstrates thorough skills and/or proven success in a role participating in consulting-related projects within a professional services environment that utilize creative thinking skills and individual initiative, including the following:
Analyzing large and complex data sets, including a demonstrated thorough aptitude for conducting quantitative and qualitative analyses;
Collaborating and contributing as a team member: understanding personal and team roles, contributing to a positive working environment by building solid relationships with team members, proactively seeking guidance, clarification and feedback;
Identifying and addressing client needs, building relationships with clients, developing requests for information, demonstrating flexibility in prioritizing and completing tasks, communicating potential conflicts to a supervisor;
Prioritizing and handling multiple tasks, researching and analyzing pertinent client, industry and technical matters, utilizing problem-solving skills, and communicating effectively in written and verbal formats to various audiences (including various levels of management and external clients) in a professional business environment;
Coaching and collaborating with associates who assist with this work, including providing coaching, feedback and guidance on work performance;
Understanding how the application of analytical techniques correlated to business value;
Being able to select the appropriate analytical techniques for the problem at hand"
Software Engineer - Map Data,ALK Technologies,"Software Engineer - Map Data

Were looking for passionate software engineers at all levels of experience, from college graduate to senior-level professional. ALK is an established leader, providing software solutions for the transportation and logistics needs of our customers. Our engineers directly shape our future and have the opportunity to influence products from day one.

This role is important because our map data is at the heart of every single product we release. Keeping our road network data up to date and validating it for quality is an ongoing challenge. Were looking for driven developers who are up to the challenge.

Job Description

Software Engineers at ALK are responsible for designing, implementing, testing, and maintaining our software. This specific position requires maintaining and building software used for analysis, validation and enhancement of ALKs proprietary road network. Most importantly, this position requires a technically strong, analytical engineer, who has desire to 1) automate processes to make data validation and data conflation more efficient, 2) profile and optimize performance of existing processes, 3) take pride in overall product and data quality, 4) work with others, including GIS professionals, and 5) coach junior devs.

Qualifications
BS in Computer Science, or similar degree

Excellent coding, debugging, and problem solving skills

Extensive professional or academic experience using C++ and/or Python

Strong understanding of relational databases and SQL

Keen ability to identify systemic problems/patterns through data analysis

Strong understanding of advanced algorithms, data structures, and data analytics

Professional or academic experience with spatial database technologies like PostgreSQL and PostGIS

Strong understanding of multi-threaded application design and implementation

Familiarity using Windows and Visual Studio for application development

Desire to automate testing in the interest of development efficiency and product quality

Experience with the Quantum GIS (QGIS) plugin development is a plus

Strong verbal and written communication skills

ALK is located on Route 1 in Princeton NJ in a new office beautifully designed to support our recent and expected future growth. ALK offers great benefits such as paid healthcare, 401K, stock options, and fun perks like summer BBQs.

Trimble Navigation Limited is proud to be an Equal Opportunity and Affirmative Action Employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, status as a covered veteran in accordance with applicable federal, state and local laws, or any other protected factor. EOE/M/F/V/D"
Big Data Engineer,FocusKPI Inc.,"FocusKPI, Inc. is looking for a Sr. Data Engineer to work for our client in Sunnyvale, CA. This is a contract position with a negotiable rate.

The Big Data Engineering team is responsible for building and maintaining the state-of-the-art ETL pipelines that makes this data available and accessible to the entire company to make data driven decisions. The team works closely with Data scientists, Product Managers, Executives and other key parts of the business across the globe to understand their data requirements and build appropriate solutions or platforms that meet or exceed those needs.

Responsibilities

----------------

Contributing at a senior-level to the data warehouse design and data preparation by implementing a solid, robust, extensible design that supports key business flows.
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting.
Establishing efficient design and programming patterns for engineers as well as for non-technical individuals
Designing, integrating and documenting technical components for seamless data extraction and analysis on our big data platform.
Ensuring best practices that can be adopted in Big Data stack and share across teams and BUs.
Working in a team environment, interact with multiple groups on a daily basis (very strong communication skills).
Desired skills and experience

-----------------

5+ years of relevant work experience in data
Ability to write, analyze, and debug SQL queries
Working experience with Hadoop projects/infrastructure
Experience with Data Warehouse design, ETL (Extract, Transform, Load), architecting efficient software solutions
Experience in the Big Data space (Hadoop Stack like M/R, HDFS, Pig, Hive, etc.)
Knowledge of data modeling for both data warehousing and Big Data
Experience working extensively in multi-petabyte data environment
Experience in engineering large-scale distributed systems in a production environment
Experience with at least one scripting language (Shell, Python, Perl etc.) a bonus
Experience with an OO programming language like Java a bonus
Thank you for applying!

FocusKPI Hiring Team

About FocusKPI:
FocusKPI Inc. is an analytics consulting business providing solutions such as CRM customization, Risk Management, Business Intelligence, Advanced/Operational/Web/Digital Analytics as well as Human Resources for our clients."
Software Engineer - Map Data,Trimble Inc.,"Software Engineer - Map Data

Were looking for passionate software engineers at all levels of experience, from college graduate to senior-level professional. ALK is an established leader, providing software solutions for the transportation and logistics needs of our customers. Our engineers directly shape our future and have the opportunity to influence products from day one.

This role is important because our map data is at the heart of every single product we release. Keeping our road network data up to date and validating it for quality is an ongoing challenge. Were looking for driven developers who are up to the challenge.

Job Description

Software Engineers at ALK are responsible for designing, implementing, testing, and maintaining our software. This specific position requires maintaining and building software used for analysis, validation and enhancement of ALKs proprietary road network. Most importantly, this position requires a technically strong, analytical engineer, who has desire to 1) automate processes to make data validation and data conflation more efficient, 2) profile and optimize performance of existing processes, 3) take pride in overall product and data quality, 4) work with others, including GIS professionals, and 5) coach junior devs.

Qualifications
BS in Computer Science, or similar degree

Excellent coding, debugging, and problem solving skills

Extensive professional or academic experience using C++ and/or Python

Strong understanding of relational databases and SQL

Keen ability to identify systemic problems/patterns through data analysis

Strong understanding of advanced algorithms, data structures, and data analytics

Professional or academic experience with spatial database technologies like PostgreSQL and PostGIS

Strong understanding of multi-threaded application design and implementation

Familiarity using Windows and Visual Studio for application development

Desire to automate testing in the interest of development efficiency and product quality

Experience with the Quantum GIS (QGIS) plugin development is a plus

Strong verbal and written communication skills

ALK is located on Route 1 in Princeton NJ in a new office beautifully designed to support our recent and expected future growth. ALK offers great benefits such as paid healthcare, 401K, stock options, and fun perks like summer BBQs.

Trimble Navigation Limited is proud to be an Equal Opportunity and Affirmative Action Employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, status as a covered veteran in accordance with applicable federal, state and local laws, or any other protected factor. EOE/M/F/V/D"
Machine Learning Engineer,Temboo,"Do you want to help build tools that can change how the world works? We do, and wed like you to join us. At Temboo , we build software tools that people are using to fundamentally change the world around us. From life sciences and farming, to energy, aviation and smart cities amongst others, Temboo is empowering people to build an amazingly diverse range of physical computing systems at the intersection of hardware, software and human aspiration, and we want you to help us as we grow and scale Temboo. Our customers use Temboo to make everything from small, incremental improvements to transformative shifts in how we live our lives - reducing waste, increasing efficiency and enhancing quality of life. If you want to be involved in the next major wave in technology, while feeling great about how youre spending your time, Temboo is the place for you.

Were looking for a curious, skillful engineer to employ machine learning techniques to develop new features and products at Temboo. You will be responsible for leveraging and advancing existing machine learning systems as you apply them to Temboos increasingly rich sensor data and device activity dataset. You will lead the design, prototyping and productization of machine learning-based features, and take responsibility for introducing other Temboo developers to machine learning projects.

Your specific technical skill-set is less important to us than your all-around intelligence, problem-solving ability, and eagerness for new challenges. It will be your responsibility to identify and introduce new languages and tools to support your machine learning initiatives.

Our team is wickedly smart, passionate about what we do, and committed to making programming faster, easier, and more fun. We are dedicated to defining elegant solutions to big challenges. Help us re-imagine programming for hardware and software developers.

What You'll Be Doing
Identifying opportunities to apply machine learning techniques to develop new Temboo features, particularly those based on sequence mining
Working with other Temboo engineers to implement machine learning algorithms in an efficient manner
Shipping new features based on your research
Continually observing and refining these features in production to improve behavior over time
Taking responsibility for maintaining our data collection, storage and processing infrastructure
What We're Looking For In You
Proven experience with developing machine learning systems in a professional or academic environment
A deep understanding of the mathematical foundations of machine learning algorithms
Strong computer science fundamentals (notably algorithms and data structures, distributed systems and information retrieval)
Experience with relational database technology e.g, MySQL, and distributed data storage and processing systems e.g., Hadoop
Youre a strong programmer, with experience in both scripting languages and strongly typed programming languages
A willingness to learn and use new technologies, strategies, tools, and components
Strong written and verbal skills, with an ability to explain complex concepts to a varied audience
Experience in a startup environment"
Data Engineer,Cervello Inc,"Summary:
You have experience with client projects and in handling vast amounts of data  working on database design and development, data integration and ingestion, designing ETL architectures using a variety of ETL tools and techniques. You are someone with a drive to implement the best possible solutions for clients and work closely with a highly skilled Analytics team. Play a key role on projects from a data engineering perspective, working with our Architects and clients to model the data landscape, obtain data extracts and define secure data exchange approaches.

Plan and execute secure, good practice data integration strategies and approaches

Acquire, ingest, and process data from multiple sources and systems into Big Data platforms

Create and manage data environments in the Cloud

Collaborate with our business analysts and data scientists to map data fields to hypotheses and curate, wrangle, and prepare data for use in their advanced analytical models

Have a strong understanding of Information Security principles to ensure compliant handling and management of client data

This is a fantastic opportunity to be involved in end-to-end data management for cutting edge Advanced Analytics and Data Science

Qualifications:
Experience on client-facing projects, including working in close-knit teams

Experience and interest in Big Data technologies (Hadoop / Spark / NoSQL DBs)

Experience or familiarity with real-time ingestion and streaming frameworks is a plus

Experience and desire to work with open source and branded open source frameworks

Experience working on projects within the cloud ideally AWS or Azure

Experience with NLP, Machine Learning, etc. is a plus

Experience working on lively projects and a consulting setting, often working on different and multiple projects at the same time

Strong development background with experience in at least two scripting, object oriented or functional programming language, etc. SQL, Python, Java, Scala, C#, R

Data Warehousing experience, building operational ETL data pipelines across a number of sources, and constructing relational and dimensional data models

Excellent interpersonal skills when interacting with clients in a clear, timely, and professional manner.

A deep personal motivation to always produce outstanding work for your clients and colleagues

Excel in team collaboration and working with others from diverse skill-sets and backgrounds

Cervello is a dynamic technology company that is focused on business analytics and planning. We take an innovative approach to making complex solutions simple so our clients can focus on running their businesses. Our services and applications enable our clients to gain the benefits of a world-class analytics and planning capability without the headaches."
Data Engineer,Boston Health Economics,"BHEs mission is to improve healthcare through innovative analytics. Our team has built a next-generation analytics platform, Instant Health Data (IHD), to bring researchers together to generate insights into improving population health, quality of care, and managing costs.

Our engineering team is looking for talented individuals who want to bring our platform to the next level, expanding into new markets. In undertaking this challenge, our new engineers will be at the forefront of the latest technologies, learn about large-scale, new and emerging data sources, and help BHE maintain its leadership position.

Our engineers work in a fast-paced, rapid-learning environment with leaders in software development, working with massive data sets, and healthcare data analytics. In our environment, everyone is encouraged to make a difference without experiencing the fixed ways of doing business in larger, bureaucratic organizations.

Responsibilities:
Build new features to improve our analytics platform

Contribute to the design of new functionality

Assist customer support team with analytics and software inquiries

Run ETL scripts and perform quality control tasks

Configure our software platform to use data sets and ensure ETL instructions and documentation are up-to-date

Become an expert in the structure and contents of each data source we work with

Skills & Requirements

Intermediate-advanced SQL (NoSQL experience a plus)

Proficient in at least one general purpose programming language and one scripting language

Economics, statistics, math, computer science or other technical degree

Eager to learn new systems and technologies

Why be a part of BHE's Team?

Leading healthcare data analytics/big data company

Work on a team of talented and pragmatic engineers/researchers

Great mentorship and growth opportunities"
Software Engineer,Apple,"Join a team transforming the way apple delivers its cloud infrastructure. We are looking for a highly motivated developer to help enhance our tooling. The primary responsibilities will be building internal applications, integration with various existing tools, and ensuring quality development.
Key Qualifications
At least 3 years of experience developing tools or building automated frameworks
Proficiency in at least two of Java/Scala/Python/Go/Swift/R
Proficiency in SQL
Experience managing MySQL DDL
Experience working with distributed data pipelines using system like Spark/Flume/Kafka
Proficiency with Django or Rails
Working knowledge of HTML5/JavaScript/CSS
Proficiency in scripting UNIX shells
Experience defining and consuming REST/SOAP APIs
Good understanding of code reviews
Proficiency working with GIT on a distributed team
Must be self directed, analytical and work well in a large cross functional environment
Deals well with Ambiguity and embraces change in a fast moving environment
Strong written and oral communication skills
Description
The Fleet Planning and Delivery team is looking for an experienced Software Engineer to work to work on tools, automation, and testing infrastructure. You will join a team of people responsible for ensuring that our fleet management is as smooth as possible. We are looking for someone who can thrive in a small team while working collaboratively with other cross-functional teams. Work closely with various teams to understand the requirements for the tools. Design, build, and test systems to support new and existing use cases.
Education
BS/MS in Computer Science or Electrical Engineering (or equivalent)
Additional Requirements
Knowledge of data center management
Knowledge of integration with complex data sets"
Data Engineer,Oracle,"Data Engineer-17001H61
Preferred Qualifications

Data Engineer

Job Overview
We are looking for a Data Engineer to join our growing team of analytics experts. This person will assist in expanding our collection of global Internet operation and performance data sources, and in moving our data repository and data processing services into the Oracle cloud. The ideal candidate is very familiar with the foundational building blocks of the Internet (BGP, DNS, TCP/IP, etc.) and is a data wrangler and pipeline builder who enjoys instantiating, maintaining, monitoring, and processing a variety of real-time data feeds. The Data Engineer will support our data scientists on analytics initiatives and in maintaining an efficient and consistent data delivery architecture. The candidate should be self-directed and comfortable supporting the data needs of multiple teams, systems and products. The right person will be excited by the prospect of expanding our existing data sources and procuring entirely new ones, while ensuring their delivery to support our next generation of products and data initiatives.

Responsibilities for the Data Engineer
Maintain, improve and expand our existing data pipeline architecture.
Assemble large, complex data sets that meet business requirements.
Identify, design, and implement internal process improvements: automating manual processes, optimizing data delivery, re-designing infrastructure for greater scalability, etc.
Support our efforts to move data collection and processing to the Oracle cloud.
Work with external parties to expand and maintain our BGP and VM collection infrastructure.
Work with stakeholders across teams to assist with data-related technical issues and support their data infrastructure needs.
Work with our data scientists to add new data sources of utility to the company and to improve the coverage and functionality of existing data sources.
Qualifications for the Data Engineer
Experience working with Internet-related data and protocols.
Systems administration experience in a Linux environment.
Experience building big data sets, pipelines and processing capabilities.
Experience performing root cause analysis on data sets and processes to answer specific business questions and identify opportunities for improvement.
Experience building processes to support data collection, data transformation, and workload management.
Strong organizational skills.
Bachelors degree in Computer Science or related field.
Excellent written and verbal communication skills.
Experience with scripting languages, e.g., bash, Perl, Python, etc.
Experience with operating within and moving to a cloud storage and computing environment a plus.
Experience maintaining Hadoop a plus.
Experience with establishing and maintaining BGP peering sessions a plus.
Position is located in Hanover or Manchester, New Hampshire.

Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will assist in defining and developing software for tasks associated with the developing, debugging or designing of software applications or operating systems. Provide technical leadership to other software developers. Specify, design and implement modest changes to existing software architecture to meet changing needs.

Duties and tasks are varied and complex needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS or MS degree or equivalent experience relevant to functional area. 4 years of software engineering or related experience.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
Product Data Engineer,Tesla,"Tesla is committed to having industry-leading uptime in our deployed fleet of residential and commercial batteries, and we are using the data collected from our connected systems to diagnose problems and improve product performance. The Tesla Energy Product team is looking to hire a Product Data engineer to use analytics techniques to automatically identify and action improvement to the product design, process, or firmware. As data focused engineer, you will enable our worldwide support teams to diagnose and troubleshoot any issue to ensure our customers are delighted using the product.

Responsibilities
Create software to automatically diagnose and report on product field failures to drive necessary actions with manufacturing, design, field service, customer and technical support teams
Oversee the development of fleet analysis and failure identification database-driven tools
Work with the engineering and manufacturing teams to drive design and manufacturing changes to improve product quality in both existing and future products
Define and create scalable automated software solutions to create a worldwide platform to enable action through data and work with the software team to deploy these solutions
Disseminate data visualization to summarize, and find new opportunities for product improvements
Must have requirements
BS in computer science, mechanical or electrical engineering with 2 years or experience or MS degree with a focus on data analytics
Experience with Python, SQL and an understanding of large data structure and querying techniques
Experience applying analytics techniques to data to create models
Strong engineering fundamentals and intuition applied to firmware/software-enabled mechatronic systems
Strong prior experience troubleshooting mechanical, material, electrical, system, firmware, or process failure modes
Excellent oral, written communication skills
Nice to have requirements
Experience using Hive, Spark-SQL
Experience with battery, solar or inverter systems a plus but not required
Work travel is expected to be about 10% in this job. This would include traveling to customer sites, our manufacturing facilities, etc."
Big Data Front End Engineer,Apple,"The Interactive Media Group at Apple is looking for a highly motivated front end engineer. You will be a critical part of a team focused on Big Data analytics applications for media streaming and their underlying networking architecture on iOS and OS X. You must enjoy the excitement of rapid development, tight deadlines, and interacting with other high energy teams. Youll be asked to build user interfaces to present and visualize data sets for troubleshooting and improving how our software behaves in real-world situations. The candidate should posess solid front-end engineering, leadership, and communication skills.
Key Qualifications
Well-versed in fundamental visual and interactive design discipline
Strong communication and presentation skills
Strong command of JavaScript, HTML/HTML5, AJAX, DOM, CSS/CSS3, JSON
Experience with Ruby
Familiar with contemporary JavaScript frameworks (JQuery, Backbone, Underscore, etc.)
Ability to write high-performance, reusable code for UI components
Knowledge of user interface design and proven ability to work with business counter parts to create compelling user experiences
Ability to define new approaches to complex design problems for data presentation
Must be process driven, possess strong prototyping design skills, careful attention to detail and have a solid understanding of user centered design principles
Strong knowledge Git and Linux environment
Description
Provide guidance and leadership to the front-end team
Play hands-on development and design role, and deliver products in a rapid and dynamic environment
Prototype, define, and develop web solutions that meet modern web standards using semantic markup, CSS, and JavaScript
Consider scalability and ease of maintenance in all solutions
Engage business and technical teams for feedback and validation

Education
B.S or M.S in EE/CS with a strong focus in software engineering
Additional Requirements
Experience developing web pages optimized for desktop, mobile devices and tablet devices a plus
Experience with cross-browser compatibility issues and browser degradation strategies a plus
Experience with Ruby Web frameworks (like Sinatra) a plus"
Big Data Engineer,"DataSync Technologies, Inc","Be a part of an award winning company in Northern VA while at the same time keeping America safe. DataSync Technologies is looking for several Big Data Engineers to help support our Customer.

Qualified candidates must be able to develop, maintain, test and evaluate big data solutions within organizations. Candidates must be able to build large-scale data processing systems, be an expert in data warehousing solutions and should be able to work with the latest (NoSQL) database technologies.

ONLY CANDIDATES WITH ACTIVE GOVERNMENT SECURITY CLEARANCES AND APPROPRIATE POLY WILL BE CONSIDERED.

Required

Bachelors or Masters degree in computer science or software engineering;

Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.

Ability to architect highly scalable distributed systems, using different open source tools.

Experience building high-performance algorithms.

Extensive knowledge of different programming or scripting languages such as Java, Linux, C++, PHP, Ruby, Phyton and/or R.

Experience with different (NoSQL or RDBMS) databases such as MongoDB needed.

Experience building data processing systems with Hadoop and Hive using Java or Python

Desired Experience

Excellent oral and written communication skills;

Experience in designing efficient and robust ETL workflows;

AWS experience

________________________

DataSync Technologies, Inc is a woman owned small business providing consulting excellence and real time solutions for customers with complex information technology needs within Intelligence Community. Our cleared consultants bring real world experience with a common sense approach to their jobs whether they are creating complex analytic dashboards, architecting new cloud technology infrastructures, securing sensitive data or streamlining business processes for efficiency.

DataSync is able to attract and retain consultants who desire to work in a small business with entrepreneurial spirit. DataSync has a responsive relationship to our customers because our staff have a directly vested interest in the success of the company.

DataSync is an EEO and Affirmative Action Employer of Female/Minorities/Veterans/Individuals with Disabilities.

Equal Employment Opportunity

All employment decisions shall be made without regard to age, race, creed, color, religion, sex, national origin, ancestry, disability status, veteran status, sexual orientation, gender identity or expression, genetic information, marital status, citizenship status or any other basis as protected by federal, state, or local law

Information about Equal Employment Opportunity (EEO) and Employee Polygraph Act (EPPA) provisions in addition to other Federal labor laws can be found at http://webapps.dol.gov/dolfaq/go-dol-faq.asp?faqid=537

DataSync is committed to providing veteran employment opportunities to our service men and women.

www.datasynctech.com

www.facebook.com/DatasyncTechnologies

www.twitter.com/Jobs at DataSync (@DatasyncJobs)

www.twitter.com/datasynctech

#datasynctech on Instagram"
Database Engineer,Excelitas Technologies Corp.,"Position Summary :

The Database Engineer II position will be part of the Enterprise Applications team in IT, working with global IT members, participate on software development projects and support Excelitas global applications. The ideal candidate will collaborate with a variety of IT teams to leverage reporting, analyses and ETL & Data Warehouse design with a customer service mindset. This position will be based in the Fremont, CA office but will support the global applications such as Data Warehouse, Data Marts and Business Object reporting platform.

This position requires use of information which is subject to the International Traffic in Arms Regulations (ITAR). All applicants must be U.S. persons within the meaning of ITAR. ITAR defines a U.S. person as a U.S. Citizen, U.S. Permanent Resident (i.e. 'Green Card Holder'), Political Asylee, or Refugee.

Essential Functions :
Implement solutions that enhance, streamline and automate the administration/management of Excelitas Data Warehouse application.
Develop functional and technical specifications, test plans and scripts to validate the business functional enhancement needs.
Design, develop, test and implement interfaces across several on premise or cloud applications such as SAP, SFDC, ANAPLAN etc.
Troubleshoot and execute root cause analysis for application issues/bugs.
Provide support for Excelitas Data Marts and Business Object Universes
Collaborate with IT Infrastructure support team

Basic Qualifications :
Strong experience in MS SQL Server Database platform and tools (SSMS, SSIS)
Proficient in developing complex SQL queries and stored procedure.
Proven professional experience in data analysis, reporting and data warehousing.
Must have knowledge of ETL process, Data warehousing concept and Management reporting needs
Proven ability to excel in customer service; anticipate, meet and exceed expectations by solving problems quickly and effectively; making customer issues a priority
Sound understanding of Data Warehouse concept, best practices, relational structures and data modeling techniques.
Strong analytical and problem-solving skills
Excellent verbal and written communication skills
Education/Experience Requirements :
BA/BS Degree or AA degree with equivalent exprience
Minimum of 2 years experience in supporting Data Warehouse application for manufacturing company
Proficient in at least one ETL platform with development experience on implementing Enterprise Class Data Warehouse application.
Experience using MS SQL Server RDBMS, 2016 SQL Server experience is a plus.
Additional Qualifications :
MS SQL Server DBA experience is a big plus
SAP Business Object platform administration and Universe support experience is a plus
Experience on SAP Data Services (BODS) ETL platform is a plus
EEO/Minority/Female/Veterans/Disabled

All qualified applicants will receive consideration for employment without regard to their protected veteran status."
Data Science Engineer,Gogo Commercial Aviation,"Managing the internet serviceforcustomersflying on thousands of flights around the globe every day requires a lot of movingpieces, and the way we manage data in thisenvironmentis a hugely critical piece of the puzzle.
Gogo has spent the last several years modernizing its software development and is in the final stages of moving its stack to the cloud. We have embraced DevOps and developer empowerment with single click deployments to production. Were now embarking on our new phase of development with our move to more event driven designs and systems enabling us to have a data driven mindset.
To help with this transition. We're looking for skilled data engineers who have broad experience and a passion for modern data systems. We are moving away from simply knowing what happened, to being able to predict what is going to happen next. This role will play a critical role in defining anomaly detection algorithms across our organization.
This role will join in our ever-growing data team and help drive and build these data initiatives within Gogo. The engineer will help us set and execute our data engineering strategy and development.
Basic Qualifications
Bachelor's degree in Information Science / Information Technology, Computer Science, Engineering, Mathematics, Physics, or a related field
Strong developer background with passion for data systems
Experience in one or more languages such as Java, Python, R, and Scala
2+ years with one or more big data streaming technologies such as Spark Streaming or Kinesis
Understanding of statistical analysis on streaming systems
Spark ML is a big plus
Understanding of telecom systems and networks and their events is a plus
Preferred Qualifications
2 or more years of experience developing data based software solutions.
Gogos worldwide inflight Wi-Fi services have made internet and video entertainment a regular part of flying. We are a diverse group of technologists, marketers, strategists, and any other function you can think of- all working together in extraordinary harmony. And thats just the beginning.
We connect the aviation industry and its travelers with innovative technology and applications, and we do it all in a high-energy environment that welcomes the next challenge. Be prepared for a dynamic ride with people who are passionate about what theyre building.

Gogois an equal opportunity employer and works in compliance with both federal and state laws. We are committed to the concept regarding Equal Employment opportunity. Qualified candidates will be considered for employment regardless of race, color, religion, age, sex, national origin, marital status, medical condition or disability. The EEO is the law and is available here .

Gogo participates in E-Verify. Details in English and Spanish . Right to Work Statement in English and Spanish ."
Data Visualization and UI/UX Engineer,Apple,"Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.

Key Qualifications
Strong working knowledge of UI/UX initiatives from discovery, research, design, prototyping, requirements definition, and full-scale development
Understanding of UI design intents and interaction design towards delivery of compelling and effective visual communication tools
Proven experience in development of desktop, mobile and web-based User Interfaces for data visualization
Adept at front-end development tools such as HTML/CSS/Javascript and experience with UI frameworks and libraries (D3, REACT, Qt, etc.)
Experience at turning large data sets for Apple products (from function data to cosmetic data) into easy to read interactive visuals
Experience with design of interactive data visuals for the visualization of multi-dimensional data sets and blending with product imagery
A good level of comfort with programming in Java, Obj-C, GO, C++ or Python and strong understanding of object oriented programming principles
Adept at software development for both prototyping and enterprise-grade data dashboards for visualization of manufacturing process and product information
Working knowledge of applied statistics (descriptive and inferential) preferred
Knowledge in processing large data sets from querying (SQL based), collating and organizing towards development of a fully automated data analysis pipeline
A working level understanding of the principles of data science from wrangling, cleaning, collation and querying through various methods such as JS, Shell scripts or SQL
Strong understanding of securing user interfaces though authentication and access control via a variety of methods (open directory, certificates, oAuth* tokens, etc.)
Understanding of the current tools and trends in data visualization
Understanding of software development life-cycle and adept at prototype building from idea through concept and demonstration
Strong working knowledge of the Unix/Linux OS and shell scripting. Understanding of text parsers such as sed/awk is a plus
Experience in applied ML in the domains of anomaly detection, image classifications, and leading indicator predictions is a plus
Experience in AI principles and deep learning from model building, training, tuning and validation is a plus
Experience working within a high-volume manufacturing production environment a plus
Understanding of industrial grade control systems such as PLC, softPLC and IPC a plus
Understanding of industrial engineering and factory physics a plus
Able to communicate effectively in a worldwide environment
Travel to Apples suppliers domestically and internationally with travel up to 15% in a given year
Description
Responsible for ideation, concept development, and scaling of an enterprise data automation system with specific focus on data visualization for the visualization of manufacturing metrics and product related KPIs in the manufacturing of Apple products. Perform research and development on new technologies necessary to solve complex visualization tasks that have a direct impact to the quality and delivery of Apple products.

Lead the development of a compelling enterprise-grade data visualization platform to effectively visualize process and product data
Develop new and or improved data visualization methods through the application of theoretical and practical data visualization methods
Proof-of-concept dashboards and data visuals for a wide range of visualization needs
Perform exploratory visualization on data sets towards the development of a a full-scale system
Assemble large data sets for analysis either through direct SQL-based querying or development of scripts and code-modules to collate distributed and disparate data sources
Lead projects with hands-on analysis and modeling. Choose the right tool from a broad range of statistical/data mining/machine learning techniques and apply them to big data sets to solve business problems and support business decisions
Engage cross-functionally to frame complex, ambiguous problems, and figure out where and how advanced analytics tools can be applied with the biggest impact. Then collect data, build analytical models and make recommendations
Develops software components in Python, Java and/or C/C++/Obj-C towards roll-out of a data automation system (with the visualization platform a core piece of this architecture)
Communicate and collaborate effectively with the multiple, globally distributed cross functional teams
Education
Bachelors in Industrial Engineering or Design with 7+ years of experience, Masters in Data Science with 5+ years or PhD with 3+ years in a data science or analytics research"
Database Engineer,Vistaprint,"We are looking for a Database Engineer to join the Vistaprint Data Team, this team is focused on enabling business partners to make data-driven decisions that deliver value to our customers. This includes enabling analytical insights into our business, driving intelligent product recommendations, and helping to optimize our website experience and functionality. Solving these challenges involves working with technologies on the leading edge of data movement, processing, and warehousing.

Reponsibilities
Support the multi-terabyte Vistaprint data warehouse used across several functions and geographies
Develop procedures to extract-transform-load data from various sources
Troubleshoot data issues
Work directly with Marketing, Analytics, Finance, Customer Support stakeholders

Qualifications:
Data warehouse knowledge using the Kimball approach
Some experience with reporting/analytic/OLAP tools (e.g. SSAS, Tableau)
Expert knowledge in Transact-SQL
Comfortable developing primarily in a Microsoft BI stack (Powershell, VS, C#)
Attention to detail
Ability to research data issues and resolve in a timely manner
Familiarity with Azure, APS, MDX, Hadoop, Spark, Hive is helpful
Work with data movement tools (e.g. SSIS), scheduling/jobs
Data replication concepts
Comfortable working in an Agile / Scrum environment, and interacting with internal clients
Some experience with code management tools (e.g. Git, Subversion)"
Data Engineer,HYLIION,"At Hyliion (www.hyliion.com), you will be revolutionizing the trucking industry by contributing to a solution which combines advanced Software and a state-of-the-art Electro-mechanical design to decrease truck emissions up to 30%.

Position Overview

The Database Engineer will serve as the lead technical resource in the development, implementation and oversight of the database systems. Responsible for software and database development, optimization based on best practices and proactive maintenance to ensure database systems integrity, performance, and availability levels meet business and operational needs.

Responsibilities Include:
Coordinate with Hyliion staff on the ETL processes necessary to successfully incorporate data into our PostgreSQL database

Using SQL, query Hylliion data sources to extract information for internal requests of data

Perform unassisted research towards the goals of the company

Contribute to and lead architecture/design conversations and code reviews

Design and implement features for new and existing products, features, APIs, platforms and frameworks

Responsible for the design, development and administration of transactional and analytical data constructs/structures.

Write complex stored procedures based on business requirements

Analyze legacy stored procedures/processes

Take ownership of database related adhoc, request, incidents, alerts, escalations, research and respond to resolve in a timely manner.

Must be a team player, collaborate with application tams to ensure successful integration testing and delivery.

Write scripts to create databases and database objects

Write scripts to create views, procedures, and functions in support of application development.

Actively contribute to the configuration, layout and performance tuning of the production infrastructure.

Prepare and conduct system and programming tests requiring interfacing of hardware and software.

Required Critical Skills & Qualifications:
3+ yrs of experience with PostgreSQL

Experience with distributed PostgreSQL (GreenPLUM, EnterpriseDB, PostgreXL/XC/X2, etc)

Experience with Java, C, C++ or other OOP language or

Experience with Erlang, Go, Lisp, Scala or other functional language

3 years of database development in PostgreSQL

High-level PL/pgSQL knowledge and experience

Experience with database design

Experience building and maintaining complex mission-critical production database systems

Experience analyzing issues holistically, from the application tier through the database, down to the storage.

Located in, or willingness to re-locate to Austin, TX

Benefits

Help make the world a cleaner place by revolutionizing the trucking industry.

Work with a team that operates at an incredible fast pace in a high-energy environment.

Unlimited vacation.

Competitive salary, equity and benefits including medical, dental & vision."
Machine Learning Engineer Intern (Spring),KAYAK,"Position
KAYAK is looking for people who love working with data - parsing, analyzing, investigating, interpreting, modeling, and extracting knowledge from large volumes of data. Good candidates will have a passion in processing data to help drive better business decisions or to develop new features, in a fast-paced environment where many small and diverse projects are the norm. Responsibilities
Extract data from databases, write scripts to parse, clean, combine, and process them
Apply Machine learning, Natural Language Processing, Image processing and deep learning methods to big data sets
Prepare graphs or visualizations of processed data, spot trends, anomalies Qualifications
SQL and database experience is a must
Strong programming skills in a high level language, preferably Python
Solid Machine Learning background and familiar with statistical tools in Python such as pandas and scikit-learn
Basic knowledge of data structures, algorithms, and statistics
Familiarity with Unix shell scripting
Knowledge of deep learning is a plus
Projects in the field of NLP and Image Processing is a plus About KAYAK
Our mission at KAYAK is to provide the worlds favorite travel planning tools. With every query, KAYAK searches other sites to show travelers the information they need to find the right flights, hotels, rental cars and vacation packages.
And because our team is always on the lookout looking for ways to make travel planning and trip management even easier, we offer a variety of tools and features such as KAYAK Trips, Explore and Price Forecast and are constantly evolving our app, Facebook Messenger and Slack bots, Alexa skill and related A.I. innovations.
In addition to KAYAK, we manage a portfolio of metasearch brands including: momondo, Cheapflights, SWOODOO, checkfelix and Mundi that together process more than two billion consumer queries a year.
KAYAK is an independently managed subsidiary of The Priceline Group."
"Data Integrity Program Analyst, AMP Analytics and Data Products",Apple,"Apple is seeking a proven Data Integrity Program Analyst to join the Apple Media Products Analytics and Data Products team.

Data Integrity Program Analyst plays a critical role in assessing, improving and monitoring data integrity across the data pipeline through building metadata repository & data lineage, defining data quality rules, collecting quality metrics, producing/monitoring data integrity scorecards, defining proposals to improve quality and ensuring proper implementation of data quality during the creation and modification of data.

Key Qualifications
8-10 years of industry experience focused on improving quality of the data in big data environment
Experience building metadata repository, data lineages using data discovery techniques and tools such as Apache Atlas
Advanced knowledge in Data Quality, Data Profiling and Data Integration tools, SQL, Talend, BI tools or other big data analysis software
Demonstrates advanced skills in understanding and correcting data discrepancies, reading and translating data models, data querying, identifying data anomalies and performing root cause analysis
Knowledge of Hadoop related technologies such as HDFS, Azkaban, Oozie, Impala, Hive, and Pig
Knowledge of building dashboards using Tableau, Qlikview or similar tools
Experience in working with ETL applications, data pipelines, and BigData
Experience with Spark or streaming technologies would be a bonus
Ability to work with data engineers and analysts to understand data requirements and translate them into data quality tasks
Description
Builds metadata repository including end to end data lineage. Builds metadata discovery framework to extract metadata from HDFS, Teradata, Vertica to load into metadata repository.
Defines data quality rules, thresholds, and standard metrics/quality-expectations for data elements that support critical business processes.
Partners with engineering teams on the development and implementation of the controls to mitigate data quality risk.
Monitors data quality results, reports and dashboard, and consults on data quality corrective action plans.
Partners with enterprise data and analytic stakeholders to establish metrics for data quality process assessment. Checks and reviews failures from complex compliance assessments.
Partners with QA teams to build and execute test cases to improve data quality and stability of the data pipeline.
Builds a data quality dashboard/scorecard to help monitor the quality of the data pipeline.
Performs root cause analysis of the data quality issues and drives corrective actions.
Education
Bachelor's degree or equivalent work experience in Engineering, Computer Science, Business Information Systems.
Additional Requirements
Apple is an Equal Opportunity Employer that is committed to inclusion and diversity. We also take affirmative action to offer employment and advancement opportunities to all applicants, including minorities, women, protected veterans, and individuals with disabilities. Apple will not discriminate or retaliate against applicants who inquire about, disclose, or discuss their compensation or that of other applicants."
Data Engineer,BSSP,"BSSP is looking for a Data Engineer who is relentlessly curious, passionate about data, able to gather and process raw data, and detail oriented.

This position will be responsible for data collection and data architecture of varied data sets, which will contribute to our data management system. The core role of the engineer will be to support in the collection and normalization of these various data sets.

While not the entire position, the successful candidate will be responsible for systems architecture and administration pertaining to data collection - currently four Linux machines (virtual) and remote services as needed.

For our existing database environment that are responsible for data integrity and performance tuning and as such need to get clean data into the hands of our data scientists, our display environment (Tableau) and our analytics team

They will use their technical skills for regular development work in our data management system and also be responsible for monitoring data hygiene, pulling in new data feeds, and identifying opportunities to integrate systems. The data engineer will comfortable in learning new skills in a fast paced and innovative environment.

CORE RESPONSIBILITIES
Programming and systems administration of our proprietary data management system Explore new and maintain current data sets to enhance our ability to find new insights Identify new data facilitation and storage tools that will create efficiencies in process Maintain data integrity Implementing ETL process Monit oring performance and advising of any necessary infrastructure changes Integrate algorithms into our DMP and DSPs

QUALIFICATIONS
Advanced degree in statistics / advanced math / econometrics preferred Experience manipulating messy data and merging multip le and large complicated data sets Systems Administration experience in a Linux / Unix environment 3-5 years programming experience in Python (pandas preferred), 2+ years Java, and 5+ years Linux command line Experience building predictive and descriptive models A strong ability to efficiently and accurately manage and maintain large data sets. Strong Knowledge of database tools such as SQL/MySQL, OLAP, and MDX. Knowledge of non-relational / big data techniques: MapReduce, Hive / Hadoop, Hbase Knowledge of statistics packages is also preferred: SPSS, Stata, R, or SAS Understanding of SARIMA models and advanced analytics techniques Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O Basic understanding of digital media and/or web analytics tools preferred (DCM, GA, Omniture, Atlas, etc) A curious and creative mindset Strong attention to detail The ability to deliver quickly and accurately Clear and concise written and verbal skills"
Senior Big Data Engineer,Sumeru Solutions,"SPECIALIZED KNOWLEDGE & SKILLS

6+ years of hands on experience and strong and deep knowledge of Java application development
Experience processing large amounts of structured and unstructured data. MapReduce experience is a huge plus.
Experience building and coding applications leveraging Hadoop Components: HDFS, HBase, Hive, Sqoop, Kafka, Storm etc
Experience coding in more than one of the following: Java, MapReduce, Python, Pig Programming, Hadoop Streaming, HiveQL
Experience developing RESTful Web Services
Agile/scrum experience
Experience leading and managing large scale, complex applications with high performance needs
Vendor management experience leveraging staff augmentation and/or outcome based project delivery models; statement of work planning and incremental demand forecasting
Experience managing on-site and off-site staff and demonstrated ability to collaborate and influence others to ensure timely and effective completion of project tasks
Excellent written and verbal communication skills"
Big Data Engineer,ClimaCell,"What its all about
ClimaCell analyzes wireless communications to extract weather data for state-of-the-art weather prediction. Were looking for a Big Data Engineer to build the infrastructure and the core algorithms used by the company. Youll be pioneering a cutting-edge solution to process massive amounts of data in real time, at an order of magnitude higher spatial and temporal resolution than is possible with competing technologies. If you love programming as much as analyzing data, talk to us.

What it takes
Being a backend software engineer with 5 years of relevant experience
Experience in Python development and
Working with AWS and Google Cloud
You prefer UNIX environments
Youre comfortable building from the ground up
Education
Masters degree in Computer Science or EE or equivalent experience
Bonus Points
You love dogs
Expertise in C/C++ development
Expertise in NumPy/SciPy driven analysis (ML ftw!)
Expertise in Machine learning (hands-on experience)
Expertise in Spark driven analysis
Experience with GPU programming
Experience with Git
You like statistics, data modeling
You prefer mint chocolate chip ice cream be green, not white"
Sr. Data Engineer,FocusKPI Inc.,"FocusKPI, Inc. is looking for a Sr. Data Engineer to work for our client in Sunnyvale, CA. This is a contract position with a negotiable rate.

The Big Data Engineering team is responsible for building and maintaining the state-of-the-art ETL pipelines that makes this data available and accessible to the entire company to make data driven decisions. The team works closely with Data scientists, Product Managers, Executives and other key parts of the business across the globe to understand their data requirements and build appropriate solutions or platforms that meet or exceed those needs.

Responsibilities

----------------

Contributing at a senior-level to the data warehouse design and data preparation by implementing a solid, robust, extensible design that supports key business flows.
Performing all of the necessary data transformations to populate data into a warehouse table structure that is optimized for reporting.
Establishing efficient design and programming patterns for engineers as well as for non-technical individuals
Designing, integrating and documenting technical components for seamless data extraction and analysis on our big data platform.
Ensuring best practices that can be adopted in Big Data stack and share across teams and BUs.
Working in a team environment, interact with multiple groups on a daily basis (very strong communication skills).
Desired skills and experience

-----------------

5+ years of relevant work experience in data
Ability to write, analyze, and debug SQL queries
Working experience with Hadoop projects/infrastructure
Experience with Data Warehouse design, ETL (Extract, Transform, Load), architecting efficient software solutions
Experience in the Big Data space (Hadoop Stack like M/R, HDFS, Pig, Hive, etc.)
Knowledge of data modeling for both data warehousing and Big Data
Experience working extensively in multi-petabyte data environment
Experience in engineering large-scale distributed systems in a production environment
Experience with at least one scripting language (Shell, Python, Perl etc.) a bonus
Experience with an OO programming language like Java a bonus
Thank you for applying!

FocusKPI Hiring Team

About FocusKPI:
FocusKPI Inc. is an analytics consulting business providing solutions such as CRM customization, Risk Management, Business Intelligence, Advanced/Operational/Web/Digital Analytics as well as Human Resources for our clients."
Big Data Engineer,Grid Dynamics,"Our customer is one of the world's largest technology companies based in Silicon Valley with operations all over the world. On this project we are working on the bleeding-edge of Big Data technology to develop high performance data analytics platform, which handles petabytes datasets. We are looking for an enthusiastic and technology-proficient Big Data Engineer, who is eager to participate in design and implementation of a top-notch Big Data solution to be deployed at massive scale.

Responsibilities:
Participate in design and development of Big Data analytical applications
Design, support and continuously enhance the project code base, continuous integration pipeline, etc.
Write complex ETL processes and frameworks for analytics and data management
Implement large-scale near real-time streaming data processing pipelines
Work inside the team of industry experts on the cutting edge Big Data technologies to develop solutions for deployment at massive scale

Requirements:
Strong knowledge of Scala
In-depth knowledge of Hadoop and Spark, experience with data mining and stream processing technologies (Kafka, Spark Streaming, Akka Streams)
Understanding of the best practices in data quality and quality engineering
Experience with version control systems, Git in particular
Desire and ability for quick learning of new tools and technologies

What will be a plus:
Knowledge of Unix-based operating systems (bash/ssh/ps/grep etc.)
Experience with Github-based development processes
Experience with JVM build systems (SBT, Maven, Gradle)

What we offer:
Work in the Bay Area with terrific customers on large, innovative projects
High-energy atmosphere of a growing and successful company
A very attractive compensation package with generous benefits (medical, dental, vision and life), 401K and Section 125 pre-tax offerings (POP and FSA plans)

About Us:
Grid Dynamics is the engineering services company known for transformative, mission-critical cloud solutions for retail, finance and technology sectors. We architected some of the busiest e-commerce services on the Internet and have never had an outage during the peak season. Founded in 2006 and headquartered in San Ramon, California with offices throughout the US and Eastern Europe, we focus on big data analytics, omnichannel services, DevOps, and cloud enablement"
Database Engineer,National Oilwell Varco,"Requisition ID

: 1707846
Description

NOV is seeking a qualified Database Engineer candidate for its Corporate Engineering Data group. Our Engineers work in partnership with cross functional teams across the organization to build innovative solutions and data visualizations for the Oil and Gas industry within various segments. Our goal is to solve unstructured and complex business problems. As a Database Engineer, you will have the opportunity to support the data acquisition efforts with our team. The ideal candidate will have a background in Computers with a heavy emphasis on Database management, support, as well as proficiencies in common scripting languages, good interpersonal communication skills and a passion to explore technical trends.

Qualifications

Job Description:
Experienced Database Engineer responsible for installation, maintenance and deployment of Bigdata applications and databases supported by the Corporate Engineering Data Team. Candidates must be flexible and excited about working in a fast-paced team environment; exhibit communication and organizational skills; and be willing and able to focus on both internal and external customer service to meet business needs.
Work on installation, upgrade and maintenance of open source databases likes InfluxDB, MongoDB.
Move data across databases.
Keep the database versions up to date.
Perform the house keeping activities on the databases.
Performance tuning of the databases.
Setting up data replication between different instances of the DBs.
Setup DB backup.
Restore the DB in case of any issues with primary database.
Setup DR database.
Run periodic DR tests.
Review system resources and troubleshoot performance issues.
Document the current state of the system.
Work with the vendor for support issues and close the issues.
Respond to any alerts from these databases.
Maintain the database configurations in source control and document any new changes.
Restart databases during planned outages.
Work with server admins for resolving application issues.

Required Skills:
3+ years experience in InfluxDB and MongoDB.
Experience in setting up InfluxDB cluster.
Experience in setting up MongoDB cluster.
Experience administering MS SQL or Oracle DB.
Good understanding of InfluxDatas TICK stack.
Good understanding of time series databases.
Good understanding of Document databases.
Understanding of a Time series Historian in a plus.
Scripting experience using Shell, Python or PowerShell.
Good understanding of installing and deploying applications on Linux Platform.
Understand, track and debug process flows across multiple interconnected systems.
Good analytical skills.
Good communication skills, both verbal and written.
Ability to learn new systems with limited help from peers.
Job

: Engineering
Schedule

: Full-time
Shift

: Day Job
Job Posting

:"
Software Engineer - Data Engineering,Counsyl,"The Digital Products team at Counsyl builds software tools that span the entire testing process for patients and doctors. These tools include: identifying patients most likely to benefit from our tests, estimating their costs with insurance, tracking progress, and helping to understand the results. For doctors, we surround our tests with online services that improve their efficiency and let them focus on what they do best: providing excellent care. This software touches tens of thousands of real people every single day at critical moments in decision making. Above all, we make it possible for healthcare companies to be both hi-tech and high touch.

Were seeking a talented software engineer whose passion is in obtaining and deriving insight from complex datasets. You will work with Technical Program Managers, Product Managers and Software Engineers to make decisions large and small about product direction and feature development armed with the appropriate data. Our platform generates significant amounts of data that can drive business decisions and shape production direction.

Examples of Recent Work Include:
Using website traffic and user survey data to decide which features on our provider portal benefit which customers and targeting qualitative research by designers.
Improving the performance and scalability of our custom data export tools.
Providing utilization and metric data for sizing key strategic opportunities.
About you:
You love using Python or similar languages to obtain and analyze data quickly.
You understand that deep insight requires deep knowledge of domain models and have significant experience tackling complex data sets.
You routinely use both Structured Query Language (SQL) and programming code to obtain data for analysis.
You have experience developing and implementing custom Extract Transform and Load (ETL) and data export tools.
You have experience helping others use Business Intelligence (BI) tools such as: Looker, Tableau, Spotfire, etc.
You are driven to share knowledge and insights with others, via clear, crisp written and verbal communications.
What you'll do:
Work with Technical Program Managers and Product Managers to understand, find, export, and perform initial analysis on diverse datasets.
Design, develop and maintain custom ETL tools for multiple data sources.
Perform initial analyses and troubleshoot data anomalies.
You will advance these initiatives independently with minimal direction.
Pluses, but not required:
Experience in healthcare or research.
Data visualization skills.
Computer Science background.
Counsyl is a health technology company that offers DNA screening for diseases that can impact men, women and their children. The Counsyl philosophy is simple: screen for diseases where advanced knowledge makes a difference in health outcomes, whether its changing a behavior, pursuing preventative measures, or simply preparing for what lies ahead.

The Counsyl team includes an accomplished group of problem-solvers  top engineers, scientists, and designers  who are taking the lead on building the modern clinical laboratory. Scientific rigor, custom robotics, and software are at the heart of every Counsyl product, resulting in better, faster and more affordable screens that are broadly accessible. Beyond the laboratory, Counsyl offers doctors and patients a technology platform for results delivery in real time, with on-demand access to board-certified genetic counselors.

Counsyl has screened more than 750,000 patients and served more than 7,000 health care professionals in its CLIA-certified, CAP-accredited and NYS CLEP-permitted clinical laboratory. The company has raised a total of $102 million and is privately backed by Pilot Growth Equity, Goldman Sachs Asset Management, Founders Fund, Felicis Ventures, David Drummond, and other high profile investors."
Data Engineer,ManTech International Corporation,"ManTech is seeking a candidate to become a part of a diverse team that provides support to the End Stage Renal Disease (ESRD) value-based purchasing programs administered by the Centers for Medicare & Medicaid Services (CMS). The team provides the full lifecycle development and support necessary to consolidate the following three existing data entry and reporting systems into a single redesigned system:
 The Consolidated Renal Operations in a Web-Enabled Network (CROWNWeb)
 The Renal Management Information System (REMIS)
 The ESRD Quality Incentive Payment (QIP) System

ManTech provides agile requirements gathering and breakdown for sprint story refinement, user experience (UX) design, acceptance test-driven development (ATDD), DevOps, automated testing to include comprehensive regression testing, software release and deployment, documentation, help desk/maintenance support, and knowledge transfer.
ManTech is looking for a Data Engineer for our client location in Owings Mills. MD. The Data Engineer is an accomplished technical leader, proactive customer-focused advocate, a team player with substantial software engineering experience, preferably with some experience within the healthcare industry. The Data Engineer must have hands-on experience with enterprise level software development, integration and implementation with big data. The ideal candidate will have an advanced understanding of Java, data ingestion, data discovery & analysis, data cleansing, data transformation, data visualization, and SQL/data modelling.
The ideal candidate will provide technical design, prototype development, and support to program management in the initiation, design, development, implementation, maintenance and management of EQRS Data Lake. The candidate must demonstrate the ability to evaluate cutting edge technologies and overcome technical challenges in a fast paced environment. The Data Engineer will play a key role of migrating three enterprise applications into a consolidated application which leverages DevOps, cloud computing, and Data Lake / big data technologies.

General Responsibilities:
 Design, develop, implement, and maintain code, information architecture, and conceptual models to support data processing, and flows thru data lake
 Landing Zone - data ingestion of raw data or capture of streaming data
 Discovery Zone  evaluate data quality, transform raw data, and cleanse data
 Enterprise Zone  transform into data model for external consumption by reporting & self-service BI
 Develop data and metadata policies and procedures
 Recommend, design, implement and maintain the various file formats (e.g. XML/XSD, SequenceFiles, Avro files, or Parquet files) for information interchange between application, external systems, and/or data lake.
 Review and evaluate database performance, risk and financial analysis feasibility studies
 Investigate and repair application defects regardless of component, including platform, business logic, data process logic, or database (SQL and data modeling).
 All other duties as assigned or directed

Qualifications:
 Bachelors of Science (or higher) in computer science or related field
 5+ years of systems/application analysis & design experience
 3+ years of data modeling & database administrator experience
 3+ years of experience in designing, building, and using a big data distribution, preferably Cloudera (Hortonworks, or MapR), for
o data ingestion, cleansing, and transformation (e.g. Talend, Scoop)
o data discovery & analysis using querying tools (e.g. Impala, Hive)
o data storage using distributed databases (HBASE, Kudu)
o data streaming (e.g. Kafka, Apache Spark)
o data visualization (e.g. Tableau, Birst, Qlik)
o processing monitoring (e.g. Cloudera manager, Hue)

Skills:
 Demonstrable knowledge with Java, Java map reduce, Apache spark, distributed file systems (HDFS), and concurrent programming
o Spring Framework (XD, Boot, Cloud, Security) experience desirable
 Experience with cluster management technologies such as YARN, Mesos, Kubernetes
 Excellent knowledge of relational databases (PostgreSQL), SQL and ORM technologies
 Preferred experience with ATTD and associated technologies (Fitnesse, DBSlim, Junit)
 Preferred experience with delivering code using Continuous Integration and Continuous Delivery (CI/CD) best practices and DevOps (Jenkins Pipelines, Docker, Groovy, Ansible)

Clearance Requirements:
 U.S. citizen or legal right to work in the United States without sponsorship

Location: Owings Mills. MD

Qualifications
Requires Bachelors degree (in Engineering, Computer Science or related field) or equivalent, and five to seven years of related work experience.

Degrees
See Qualifications

Years of Experience
11-13 years w/High School Diploma
05-07 years w/Bachelors Degree
See Qualifications"
Machine Learning Engineer,Xilinx,"Xilinx is the world's leading provider of All Programmable FPGAs, SoCs and 3D ICs. These industry-leading devices are coupled with a next-generation design environment and IP to serve a broad range of customer needs, from programmable logic to programmable systems integration. Our All Programmable devices underpin today's most advanced electronics. Among the broad range of end markets we serve are:
Aerospace/Defense
Automotive
Broadcast
Consumer
High Performance Computing
Industrial / Scientific / Medical (ISM)
Wired
Wireless

Description

You will be part of an R&D team that develops high-performance low-power FPGA acceleration hardware and software. This position focuses on designing algorithm and infrastructure for high-performance FPGA accelerator for well-known software stacks in the area of Machine Learning.

You will work on projects critical to Xilinx's growth, with opportunities to move among various teams and projects. You are versatile, display leadership qualities and are enthusiastic to tackle new problems across the full-stack as we continue to push technology forward. Most of all, you are driven to find creative solutions where solutions may not exist yet.

Responsibilities

 Design and develop FPGA-accelerated Machine Learning solutions

 Enable FPGA acceleration of open source deep learning frameworks like: Caffe, MxNet, and Tensorflow

 Design and modify machine learning models: reduce computational complexity by model optimization, computation using lower precision arithmetic, data flow reordering for memory bandwidth optimizations

 Work closely with customers to port their deep learning requirements to FPGA

Qualifications
Minimum Qualifications

 MS/Ph.D. degree in Electrical Engineering or Computer Science with 2+ years of industry experience or BA/BS degree in Electrical Engineering or Computer Science with 5+ years of industry experience

 Solid foundation in data structures, computer arithmetic, algorithms and software design with strong analytical and debugging skills

 Good understanding of common families of Machine Learning models and Machine Learning infrastructure

Preferred qualifications

 Experience with implementing machine learning computation framework on GPU, CPU or FPGA

 Experience with developing acceleration application using OpenCL or CUDA

 Experience with internals of one of more frameworks like Caffe, MxNet or Tensorflow

 Solid engineering and coding skills. Ability to write high-performance production quality code. Experience in C++, Python, and other equivalent languages is a plus

 Experience or coursework in FPGA Digital Design or EDA optimization tools

Job Posting : Nov 9, 2017, 9:06:34 PM

Xilinx is an equal opportunity and affirmative action employer. Applicants and employees are treated throughout the employment process without regard to race, color, religion, national origin, citizenship, age, sex, marital status, ancestry, physical or mental disability, veteran status, gender Identity or sexual orientation. The self-identification information requested is not gathered for employment decisions. It is used only for compliance with US Federal laws. Your responses are strictly voluntary, and any information provided will remain confidential. If you choose not to ""self-identify"", you will not be subject to any adverse treatment."
"Software Engineer, Data - Innography",CPA Global Limited,"Ideas change the world. Help us change the way the world manages them.
The company is growing as we expand our software offerings and we are looking for an enthusiastic, driven Senior Software Engineer to join this exciting team. The work is focused on the development and maintenance of our data infrastructure supporting our software applications for the Intellectual Property Market.
The role will enable you to participate in the full systems life cycle. This includes solution design, agile development, test, implementation, and production support. You will help to develop innovative solutions to process large amounts of data. You will be focused on all aspects of the ETL process, including developing novel applications of our cloud computing infrastructure. You will be working with unstructured and semi-structured data, helping transform it into clean and meaningful data warehouses for business applications.
Responsibilities will include:
Write and manage processes used to process large amounts of data
Create innovative solutions to collect, clean, and validate both structured and unstructured information
Develop and implement technical designs and data structures
Drive code review, test, and deployment processes
Work and coordinate development activities with teams in various locations
Required Experience:
Bachelors Degree in Computer Science or equivalent experience
7+ years of experience with ETL and database development
7+ years of experience programming using PHP, Python, Perl for data development
7+ years of experience with SQL databases (eg Oracle, MySQL, MSSQL, etc)
5+ years of experience working in Unix/Linux environment
3+ years of experience using Pentaho Data Integration or similar ETL tool
2+ years of experience with AWS cloud hosting tools (EMR, EC2, S3, RDS, etc)
2+ years working on an Agile development team
2+ years leading projects driving them to completion
Experience working with global teams
Authorization to work in the US
""The long term success of our business relies on our people feeling a part of what we're doing. Together, let's take the right steps to building a strong corporate culture."" Simon Webster CEO.
I f you have any queries please reach out to our recruitment team at this address: recruitment@cpaglobal.com"
Software Engineer (Data Warehouse/Pipeline/ETL),Electronic Arts,"We are EA

And we make games  how cool is that? In fact, we entertain millions of people across the globe with the most amazing and immersive interactive software in the industry. But making games is hard work. Thats why we employ the most creative, passionate people in the industry.

The Challenge Ahead:

The world of infrastructure and operations is changing at a rapid pace, e.g. cloud computing, hybrid cloud, software define networks (SDN), virtualization services, etc. and EA is moving to meet and leverage these new changes to our full advantage. EA's environment is highly distributed and dynamic. It services millions of players every day, worldwide

.

A Software Engineer for EAs Technical Operations team will need the agility to operate in this ever-changing ecosystem. This position will be responsible for driving and implementing long term design and technology choices that will be the cornerstone of tools and services that will make up the future of operations and automation processes. These tools and services will underpin modern video game services such as matchmaking, marketplace, and social features to anti-cheat detection, leaderboards, and server-side physics calculations.

The role is a part of a high performing team in a fast-paced organization, requiring partner and business focus. As well as demonstrated skills tied to leadership, people, communication, analytics and adaptability.

What a Software Engineer does on EAs Tech Ops team:
Build and maintain high-performance, fault-tolerant, scalable distributed software systems
Solve problems at scale (Thousands of servers, hundreds of thousands of gamers, millions of bits of data)
Contribute code on a daily basis
Value adding review of others code
Work with cutting edge data technologies like terraform, relational databases, distributed document solutions, time series data stores, and custom APIs
Active contributor to open-source projects is a plus

The next great EA Software Engineer also needs:
The ability to turn millions of data points in multiple systems into actionable information
Analytical and problem solving skills aligned with enthusiasm, aptitude, attitude and motivation to learn
Bachelor's degree from an accredited education institution in Computer engineering, computer science, information systems or equivalent
5+ years experience in a related professional role
Working knowledge of at least one of the following languages: Go, Python, or Ruby
Strong knowledge of database concepts, database structures, relational data bases and related applications
Strong knowledge of document and/or key/value store such as Cassandra, Memcached, Reddis, RethinkDB, or MongoDB
Working knowledge of distributed source control repository
Any experience with networking is a plus
Excellent communication and presentation skills

Whats in it for you? Glad you asked!

We love to brag about our great perks like comprehensive health and benefit packages, tuition reimbursement, 401k with company match and, of course, free video games. And since we realize it takes world-class people to make world-class games, we offer competitive compensation packages and a culture that thrives off of creativity and individuality. At EA, we live the work hard/play hard credo every day.

Dont Just Play It  Create It!

EA is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, ancestry, pregnancy, age, sexual orientation, marital status, protected veteran status, medical condition or disability, or any other characteristic protected by law."
Data Engineer,Jacobs,"Perform the role of Data Engineer advising wind tunnel Test Managers as needed to ensure test data quality. Data Engineer will serve as a technical expert for quality assurance in the aerospace data that is collected for test customers at the Unitary Plan Wind Tunnel (UPWT). Tasks the Data Engineer will perform include data reduction of wind tunnel balance calibrations, calibrations of wind tunnel facilities, development of tunnel and model corrections, calibration of various test and model instrumentation, prepare data parameter set up checks, assist with training test customers with the Wind Tunnel Standard Data System (SDS) computations, monitor test data collection and test progress by running periodic spot data computation checks and advising the Test Manager, check Test Dependent Equations (TDE) established for the test, understand the details of the Data Reduction programs for each test, verify customer supplied equations, master the data correction methodologies, and understand data uncertainty analysis. The ideal candidate will have a solid aerodynamics background and/or understanding with expertise in experimental techniques and prior experience conducting experiments in aerodynamics testing facilities a major plus. Understanding how test matrixes are developed, prepared and data collection acquired. The ideal candidate will have strong personnel interaction skills, strong written and oral communications skills, programming experience with at least one or more of the following; Excel, Excel VBA, Python, C++, and MatLab. The ideal candidate will have an ability to develop fundamental understanding of a wide variety of customer research requirements in order to guide test approach toward successful conclusion. An emphasis on continual development of technical excellence is expected. Data Engineer will perform forensics to determine what is not right with test data both from a physics and aerodynamic perspective. Investigative work may include CFD comparisons as well as analysis and review of archived test data. Strong understanding of TDE including units, plots, developed from tunnel test conditions, instrumentation and monitoring.
Description of Work Environment: Work in an office setting and industrial field environment around compressors, wind tunnels, cooling water systems, machine shops, and lubrication systems. Field work can be a combination of outside and inside depending on the task. May work close proximity to high and low temperatures, rotating equipment, oil, greases, solvents, and high noise areas.
Equipment and Machines Involved in Work Tasks: Ability to operate personal computer systems, telephones, and other general office equipment. Ability to operate inspection, electrical and mechanical, and complex instrumentation equipment.
Criticality of Attendance: Standard work week of 40 hours, 8 hours per day, M - F, nominally 6:30 am  3:00 pm. Overtime through early start or late departure from shift work may be required. Weekend work may be required from time to time as well as off shift work Swing (2 nd ) and Grave (3 rd ).
Unusual or Special Physical Requirements of Position: Ability to lift a minimum of 40 lbs. Ability to install, operate, troubleshoot, repair, and understand model and test hardware systems or direct others to do the same. Ability to enter confined spaces and ability to move in and around machinery and wind tunnel systems.

Other Essential Functions: Must have the ability to obtain a security clearance if required by client. Must be able to accurately communicate ideas verbally and in writing. Must be able to obtain a NASA badge and maintain access to the workplace facility. Adhere to appropriate safety, quality, environmental, and business ethics practices. Perform other duties as required. Maintain credibility and good customer rapport as required to perform successfully as a Data Engineer

Qualifications
A Bachelor of Science degree in Aerospace or Mechanical Engineering from an accredited engineering program is required. Prior experience conducting experimental investigations in aerospace test facilities is desirable. Position may be filled at a higher level, depending on the applicant's qualifications.

Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. Jacobs is a background screening, drug-free workplace."
Big Data Engineer,United Airlines Inc.,"Seeking Big Data Engineers!

This exciting role will deepen and broaden your skills with leading edge technologies in the data and analytics space. Team members have a chance to make a difference in operational efficiency and help to improve customer experiences through data-driven insights. Lets explore!

Job overview and responsibilities
United Airlines is seeking talented people to join the Data Movement team within the Enterprise Data Analytics organization. EDA organization is responsible for driving data driven insights & innovation to support the data needs for commercial and operational projects with a digital focus. As a Sr. Data Engineer on the team:
You will partner with various teams to define and execute data acquisition, transformation and processing for operational and analytics initiatives that create sustainable revenue and share growth
The role is responsible for designing, developing and operationalizing ETL/ELT programs to support the data needs of cross functional projects
This role will also drive the adoption of data processing and analysis within the Hadoop environment and help cross train other members of the team
Leverage strategic and analytical skills to understand and solve customer and business centric questions
Coordinate and guide cross-functional projects that involve team members across all areas of the enterprise, vendors, external agencies and partners
Leverage data from a variety of sources to develop data marts and insights that provide a comprehensive understanding of the business
Develop and implement innovative solutions leading to automation
Use of Agile methodologies to managie projects
Participate in training and sharing technical knoweledge with team members

Required
BS/BA, in computer science or related STEM field
We are seeking creative, driven, detail oriented individuals who enjoy tackling tough problems with data and insights. Individuals who have a natural curiosity and desire to solve problems are encouraged to apply
2-4 years of experience with Hadoop technologies like Hive, HBASE, Spark, Kafka, Nifi
5-7 years of strong working experience with ETL tools like Ab Initio, SSIS, Talend or Pentaho
3-5 years of experience with relational database systems like MS SQL Server, Oracle, Teradata
Must be legally authorized to work in the United States for any employer without sponsorship
Successful completion of interview required to meet job qualification
Reliable, punctual attendance is an essential function of the position
Preferred
MS in computer science or related STEM field
Experience with on prem & cloud based systems
Data engineering experience with transportation/airline industry
Strong problem solving skills
2 years of hands on software development experience with Python, Java
Strong knowledge in Hadoop space
Equal Opportunity Employer  Minorities/Women/Veterans/Disabled/LGBT

INDUAL
DICEUAL

Equal Opportunity Employer  Minorities/Women/Veterans/Disabled/LGBT"
Big Data Engineer Consultant,Axis Group,"Axis Group is a Business Intelligence consulting services firm headquartered in Berkeley Heights, New Jersey with regional offices in Atlanta, GA and Dallas, TX. Axis provides expertise in the rapidly expanding areas of Business Intelligence, Analysis and Information Management. For over twenty years, Axis Group has delivered hundreds of solutions to satisfied customers.

We are looking for a Big Data Engineer Consultant

Responsibilities

Develop high-volume, low-latency, data-driven solutions on premise and in the cloud utilizing current and next generation technologies to meet evolving business needs
Develop custom batch-oriented and real-time streaming data pipelines (cloud and on-prem), working within the Cloudera ecosystem
Ensure proper data governance policies are followed by implementing or validating data lineage, quality checks, classification, etc.
Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes
Operationalize open source data-ingestion, data-processing, and data-analytics tools for enterprise use
Utilize multiple development languages/tools such as Java, Python, Scala, SQL, Spark, Hive, and Pig to build prototypes and evaluate results for effectiveness and feasibility
Conduct design and code reviews
Analyze and improve efficiency, scalability, and stability of various system resources
Ability to lead change, be bold, and have the ability to innovate and challenge status quo
Contribute to the design and architecture of the project
Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities
Monitoring performance and advising any necessary infrastructure changes
Defining and implementing data retention policiesSkills and Qualifications

BS/MS in computer science or equivalent work experience
5+ years hands on experience
Minimum of 3 implementations in Technical lead role
Solid understanding of Lambda Architecture, along with its advantages and drawbacks

Experience developing software services in Java, Python and SQL
Experience with the entire Software Development Life Cycle (SDLC) and a solid grasp of software engineering fundamentals and their practical application
Ability to quickly identify an opportunity and recommend possible technical solutions
Solid communication skills and demonstrated ability to explain complex technical issues to both technical and non-technical audiences
Understanding of advanced system design and architecture concepts including the different types of NoSQL databases and various SQL databases
Experience developing ETL processing flows using MapReduce, Spark and Hadoop
Experience with Zookeeper and YARN
Experience designing and developing data ingestion and processing/transformation frameworks leveraging tools such as NiFi, Sqoop, Airflow, and Luigi
Demonstrated success working with cross-functional teams
Strong leadership skills and leading teams by example

Basic management of Hadoop cluster
Experience with building stream-processing systems, using solutions such as Storm, Kinesis, Flume or Spark-Streaming
Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala,
Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
Experience with various messaging systems, such as Kafka
Experience with Big Data Client toolkits, such as Mahout, SparkML, or H2O
Familiarity with Hadoop Distributions (Cloudera, MapR, Hortonworks) as well as cloud (AWS, Azure) stackLambda"
Siri Data Engineer,Apple,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile and desktop computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done  just by asking. Join the Siri team at Apple.
Key Qualifications
Through understanding of MapReduce and the Hadoop ecosystem.
Experience with object-oriented design, coding and testing.
Experience with server design (Tomcat, Apache, etc.) desirable.
Experience building high-performance algorithms.
Knowledge of Java and Linux
Optional: Python, MongoDB, Redis

Description
Architect highly scalable distributed data pipelines using Scala/Spark.
Tune Spark job parameters for maximum speed and minimum utilization of cluster resources.
Ingest and retrieve data to/from Solr, Postgres, Hive etc.
Develop prototypes and proof of concepts for new data pipelines
Contribute to extraction of engagement signals from Siri log data.
Education
MS in Computer Science or equivalent"
Siri Data Secrecy Engineer,Apple,"In Siris Operations group, data privacy and data security is held at a very high standard. We are looking for innovative engineers to take on the role in taking data secrecy to the next level and build an automation platform to monitor, track, and block data access using machine learned approaches. This platform will be built as an agnostic solution and will help Siri further protect any sensitive data we collect.
Key Qualifications
5+ years of industry experience working with large-scale systems
Engineering lead on at least three customer-facing features with year-long development cycles
Expert knowledge of one or more object-oriented programming languages (Java, C++)
Ability to leverage several scripting languages (Python, Ruby, Bash, etc.)
Have developed the bring-up or expansion of large-scale backend storage systems
Strong communication skills and experience working on cross-functional projects

Description
The Siri Data Secrecy Engineering team has a challenging effort ahead in order to build a scalable platform to protect sensitive data. The work executed by this team will re-enforce Apples focus on customer privacy and our mission to protect our customers. This project focus will set a new standard of how to detect improper data consuming methods and enable growth of Siris need for protected, yet robust data sets to improve our speech and NL models.

A successful candidate will have experience in system design and architecture as well as a strong background in development.

Design and implement data secrecy platform to be used alongside our big data stack
Define metrics and alerting mechanisms to measure success/failure
Partner with consumer teams to understand business requirements
Utilize machine learning techniques to adapt to usage patterns of our systems
Education
B.S. degree in Computer Science and 3+ years of experience or equivalent
Additional Requirements
(Nice to have items)

Knowledge of core Linux fundamentals
Scripting skills (Python, Bash, Ruby, etc.)
Experience with compute containerization (i.e. - Mesos, Docker, schedulers)"
"Data Engineer, Siri",Apple,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile and desktop computing. Create groundbreaking technology for large scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done  just by asking. Join the Siri team at Apple.

The Siri Metrics Platform team is responsible for building a highly available and scalable system to ingest, process, and analyze billions of events generated daily from devices and our services. This system, along with the data visualization tooling were building, will allow our engineering teams to perform quick exploration with the overarching goal of improving the quality of Siri.

As a Data Engineer on the Siri Metrics Platform team, you will have significant responsibility and influence in improving Siri by using data as the voice of our users. You will develop large scale data pipelines and analytical solutions, interface with our quality initiative leaders to ensure the system is meeting the needs, and iterate as well as innovate based on observations and requirements gathering.

Successful candidates will have strong engineering and communication skills, as well as a belief that data informed processes lead to better products.

Key Qualifications

Key Qualifications:
3+ years of industry experience working with large-scale and high-throughput systems.
Expert knowledge of one or more object-oriented programming languages (Scala, Java, C++).
Ability to leverage several scripting languages (Python, Ruby, Bash, etc.).
Thorough understanding of the Hadoop ecosystem (HBase, HDFS, Hive, MapReduce), Spark, Solr, Kafka.
Description
The Siri Metrics Platform team is in a unique position to align our quality initiatives to a singular platform. This will allow all performance, reliability, accuracy, and usability metrics to utilize the same methodology for dashboard/reporting creation and curation. This platform will also empower our developers to do ad-hoc queries into their problem space in order to troubleshoot or validate the effectiveness of their code from a quality perspective.

A successful candidate will have experience in large-volume data ingestion, processing, and analysis in near real-time.
Design, implement, and manage scalable data models and pipelines leveraged by all Siri teams.
Build analytical solutions to enable data analysts to perform accurate and consistent analysis efficiently.
Develop and contribute back to open source projects supporting the platform.
Ensure the platform can handle all types of robust data exploration in real-time.
Partner with all Siri teams and build features to enhance data analysis.

Education
B.S. degree in Computer Science or 3+ years of programming experience or equivalent."
"Big Data Engineer, Digital Health Team","Samsung Semiconductor, Inc.","Samsung Strategy & Innovation Center
Role / Title : Big Data Engineer , Digital Health Team
Our Mission: The Samsung Strategy & Innovation Center (SSIC) is Samsung's global network of innovators, technologists, investors, and makers. It was launched in 2013 by the President and Chief Strategy Officer of Samsung Electronics with a mission to incubate and build world-class capabilities for existing and new businesses. Our approach is unique and open, we develop and accelerate groundbreaking technologies by working in collaboration with entrepreneurs and other strategic partners, and through our early-stage and growth investment funds (Catalyst Fund and Samsung Ventures), which are committed to discovering solutions for connected devices. We are headquartered in Menlo Park, CA and have offices in San Jose, Tel Aviv, London, Paris and Seoul.
Our Group: Digital Health group is part of Samsungs Strategy and Innovation Center. We are a team of scientists, engineers and builders with the mission of developing and leveraging cutting edge technology to monitor and improve health of individuals .
Location : San Jose, CA
What will you be doing?
Design and develop scalable data pipelines and infrastructure for optimal ingestion, transformation, storage and computation using latest big data technologies. Work with data science team to develop, deliver a nd validate solutions that meet the analytics and machine learning requirements. Provide clear documentation on the data requirements, data architecture, schema and database design.

What are we really looking for?
BS or MS degree in computer science plus 3+ years of experience developing and deploying large-scale data pipelines including ingestion, processing, storage and query. Hands on experience with AWS cloud services such as Lambda, EC2, S3, EMR , DynamoDB and Redshift. Experience with open source big data technologies such as Flume, Kafka, MapReduce, ElasticSearch and Spark . Extensive programming experience in Python. Experience designing and implementing RESTful APIs. Working knowledge of SQL and NoSQL databases. Highly motivated, goal driven, innovative, curious with strong written and verbal skills."
Software Engineer - Data Services,Nest,"As part of the Data Infrastructure team at Nest, you'll be responsible for building out large-scale, distributed systems that process nearly all the data flowing through our service. We work closely with teams across the company who make use of data to drive improvements back into our products and provide helpful insights to our customers. If you're a talented engineer who enjoys challenging problems working with large data systems, we'd like to hear from you!

The work:
Contribute to the design and development of Nest's core big data infrastructure, including data delivery, transformation, and storage systems
Deliver high-performance, reliable, and scalable systems leveraging best-of-breed big data technology
Diagnose problems which can only arise in a complex distributed environment
Support the deployment and operation of our data infrastructure across multiple environments
Collaborate with Engineering, Operations, Analytics, Marketing, and Business teams throughout the organization

Minimum qualifications:
Bachelors degree (or higher) in Computer Science, Mathematics, Statistics, or related field

Preferred qualifications:
Experience building high-quality distributed systems or backend services
Experience working with data processing, loading, and transformation systems (e.g. MapReduce, Spark, Dataflow)
Experience working with large-scale data storage systems (e.g. HDFS, BigTable, Cassandra, BigQuery)
Experience building high quality, large-scale, distributed systems using Java, Scala, C++, or Python
Solid system design, problem solving and data analysis skills
Desire to learn more and fill your knowledge gaps of any of the technologies listed above!"
Data Engineer,Fathom Health,"Were on a mission to understand and structure the worlds medical data, starting by making sense of the terabytes of clinician notes contained within the electronic health records of the worlds largest health systems.

Were seeking exceptional Data Engineers to work on data products that drive the core of our business--a backend expert able to unify data, and build systems that scale from both an operational and an organizational perspective.

As a Data Engineer you will:
Develop data infrastructure to ingest, sanitize and normalize a broad range of medical data, such as electronics health records, journals, established medical ontologies, crowd-sourced labelling and other human inputs.
Build performant and expressive interfaces to the data
Build infrastructure to help us not only scale up data ingest, but large-scale cloud-based machine learning


Were looking for teammates who bring:
Experience building data pipelines from disparate sources
Hands-on experience building and scaling up compute clusters
Excitement about learning how to build and support machine learning pipelines that scale not just computationally, but in ways that are flexible, iterative, and geared for collaboration.
A solid understanding of databases and large-scale data processing frameworks like Hadoop or Spark. Youve not only worked with a variety of technologies, but know how to pick the right tool for the job.
A unique combination of creative and analytic skills capable of designing a system capable of pulling together, training, and testing dozens of data sources under a unified ontology.


Bonus points if you have experience with:
Developing systems to do or support machine learning, including experience working with NLP toolkits like Stanford CoreNLP, OpenNLP, and/or Pythons NLTK.
Expertise with wrangling healthcare data and/or HIPAA.
Experience with managing large-scale data labelling and acquisition, through tools such as through Amazon Turk or DeepDive."
"Data Infrastructure, Software Engineer",WeWork,"If you are passionate about building large scale data processing systems, and are motivated to make an impact in creating a robust and scalable data platform - we would love to talk to you. Data is the only way we make decisions, its the core of our business providing insights into the effectiveness of our physical and digital product & features. You will be a part of an early team that builds the data transport, collection and storage, and exposes services that make data a first-class citizen at WeWork. You will help shape the vision and architecture of WeWork's next generation data infrastructure, making it easy for developers to build data-driven products and features. You will be responsible for developing a reliable infrastructure that scales with the companys incredible growth. Your efforts will allow accessibility to business and user behavior insights, leveraging huge amounts of WeWork data to fuel several teams such as Analytics, Data Science, Sales, Revenue, Product, Growth and many others. We are a set of engineers constantly striving to create an amazing experience for our customers and ourselves, and we believe data brings everything together. We build and operate the platform used by the rest of the company for stream and batch computation serving mechanisms to train ML models. You will be a part of an experienced engineering team and work with passionate leaders on challenging distributed systems problems. We regard culture and trust highly and believe you will add positively to it in your own way Responsibilities
Experience building and operating large scale data infrastructure in production (performance, reliability, monitoring)
Deep understanding of distributed systems concepts and principles (consistency and availability, liveness and safety, durability, reliability, fault-tolerance, consensus algorithms)
Experience bringing open source software to production at scale (Yarn, HDFS, Hive, Spark, Presto, ZooKeeper, Airflow)
Experience designing, implementing and debugging distributed systems that run across thousands of nodes
Hands on experience with Hadoop (or similar) ecosystem - Yarn, Hive, HDFS, Spark, Presto, Parquet, HBase
Experience working with realtime compute and streaming infrastructure - Kafka, Kinesis, Flink, Storm, Beam
Experience configuring, identifying performance bottlenecks and tuning MPP databases
Able to think through long-term impacts of key design decisions and handling failure scenarios
Experience with workflow management (Airflow, Oozie, Azkaban, UC4)
Requirements
Service oriented mindset
MPP database expertise (Redshift a big plus)
Experience owning mission critical service(s)
Experience planning capacity for large scale production systems
Experience working with pub-sub messaging systems (Kafka, Kinesis etc)
Experience contributing to or developing stream compute frameworks (Apache Flink, Storm, Samza, Heron, Beam etc)
Critical Competencies for Success
You do what you love!
You are infinitely curious, but focused on results.
You are a collaborative doer who can work independently to get things done.
You love to learn. Love to mentor and teach others, even better.
You thrive in a dynamic startup environment. You live KISS and embody agile.
You keep up-to-date with latest developments in the field."
Software/Data Engineer - Spark Streaming,Capital One,"114 5th Ave (22114), United States of America, New York, New York

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Software/Data Engineer - Spark Streaming

Were Capital One. Yep, the Whats in your wallet? people. But we are oh so much more than that. We could tell you more, but youre going to check out our web site anyway, right?

Here are some of our favorites:
https://www.youtube.com/watch?v=0E90-ExySb8&t=2s
https://developer.capitalone.com/

Capital One has a history of disrupting, and we are doing it again. Were experimenting, innovating, and delivering really cool stuff to 65 million customers. We love to be curious, to dream, to ask, What if? Oh, and we love to write code.

Your commitment to learn new things is every bit as important to us as what you have already done. Maybe even more so because we dont want to be doing the same thing tomorrow that were doing today. You accept change, want to grow, and evolve into a better member of the team.

What will you work on as a Capital One Software Engineer on our Data team? From highly-scalable micro-services, to back-end systems with sophisticated data pipelines and machine learning capabilities. All on the cloud!

You will drive design, implementation, testing, release in an Agile environment, using modern methodologies, and open source tools. Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, youll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who, love to solve real problems and meet real customer needs.

The Role:
At Capital One, we believe deep understanding of our customers and consumers at large will help us deliver innovative customer experiences, drive differentiation in the marketplace, and simultaneously create business and consumer value.

Enterprise Customer Intelligence is a team of engineers, product managers, data scientists and program managers who are passionate about enabling financial products that are aware and adaptive to consumers needs. We develop APIs, data platforms and tools for Capital One to develop and apply continuous customer understanding and context in all our consumer products and experiences. Enterprise Customer Intelligence is a central component of Capital Ones investment in new data ecosystems and machine learning.

The person we're looking for:
got a wee bit excited about what they read above
has a sense of intellectual curiosity and an addiction to learning
is self-driven, actively looks for ways to contribute, and knows how to get things done
is deliriously customer-focused
values data and truth over ego
has a strong sense of engineering craftsmanship, takes pride in the code they write
believes that good software development includes good testing, documentation, and collaboration
has good communication and reasoning skills, including the ability to make a strong case for technology choices

What the search engines are looking for to help you find this job, as well as the type of environment, technologies, and languages that we work in:
Agile, Spark, Scala, Kafka, AWS (Amazon Web Services), Docker, micro services, Python

Basic Qualifications:
Bachelors Degree or military experience
At least 2 years of experience in Scala, Python or Java
At least 2 years of experience with Unix/Linux systems
At least 2 years of experience building data pipelines
At least 1 year of experience with leading big and fast data technologies like Spark, Scala, Akka, Cassandra, Accumulo, Hbase, Hadoop, HDFS, AVRO, MongoDB, Mesos
Preferred Qualifications:
Master's Degree
2+ years of experience with Spark, Scala, and/or Kafka
2+ years of experience with Spark Streaming, Storm, Flink, or other Stream
Processing technologies

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Agronomic Data Engineer,The Climate Corporation,"Position Overview:
The Climate Corporation is looking for an experienced Data Engineer to join the Fertility Science team within our Science organization. This team directly works with world-class experimental scientists, scientific software engineers, statisticians and domain scientists. The team focuses on creating and improving state-of-the art agronomic models that drive our commercial agricultural decision-support tools. These tools enable growers to maximize fertilizer efficiency and reduce environmentally-taxing fertilizer losses.
Field, remote sensing, sensor, and machine-based data are all leveraged to help maximize the performance of these models. We are looking for a data engineer with a critical and pragmatic mindset. This person will work closely with other data engineers to execute and improve our data ingest pipeline with automated data quality analysis and any associated ETL. In addition, this person will provide provide technical support to data analysts and researchers who are looking to use the experimental data in model development projects. What You Will Do:
Execute our field data ingest pipeline to load experimental data into our internal database systems
Perform quality and exploratory data analyses on loaded experimental data and resolve data quality issues in collaboration with stakeholders
Evaluate the limitations of existing data sources and provide concrete recommendations for future data needs
Develop and maintain software tools to automate data ingest, quality analysis and exploratory data analysis
Support researchers with accessing the data for model development projects
Perform technical consultancy and code reviews for researchers and data analysts Basic Qualifications:
A degree in computer science, data science, statistics, agronomy, crop science, soil science or a related field. Either a BSc with 2 years of demonstrated experience in working with experimental datasets, preferably data gathered in a natural sciences, environmental, or healthcare context, or a Msc with at least 1 year of experience.
Proficient with statistical and numerical software development in either R or python
Deep understanding of current software engineering best practices (agile, test development, code conventions, version control systems, continuous integration/ continuous deployment)

Preferred Qualifications:
Experience in working with agronomic data
Experience in relational databases and writing performant SQL queries
Proficiency in working with statistical or process-based models
Experience in working with Spark
Experience with data APIs, back-end service management, and the AWS ecosystem
Demonstrated oral and written communication skills (e.g., through past presentations, examples of technical writing, consulting etc.)

What We Offer:
Our teams are composed of industry experts, top scientists, and talented engineers. The environment is extremely engaging and fast-paced, with dozens of specialties coming together to provide the best possible products and experiences for our customers.
We provide competitive salaries and some of the best perks in the industry, including:
Superb medical, dental, vision, life, disability benefits, and a 401k matching program
A stocked kitchen with a large assortment of snacks & drinks to get you through the day
Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used
We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hackathons to encourage participation and growth in both community involvement and career development

We also hinge our cultural DNA on these five values:
Inspire one another
Innovate in all we do
Leave a mark on the world
Find the possible in the impossible
Be direct and transparent

Learn more about our team and our mission:
The Climate Corporation - The Technology Behind Making A Difference
https://youtu.be/c5TgbpE9UBI

#LI-TD1"
Big Data Software Engineer,KPMG,"Known for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today's most important industries. Our growth is driven by delivering real results for our clients. It's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it's no wonder we're consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you're as passionate about your future as we are, join our team.
KPMG is currently seeking a Big Data Software Engineer, to join our Advanced Analytics Organization.

Responsibilities:
Lead the development of big data analytics and cloud applications to facilitate the work of data scientists and solution developers
Work with KPMG industry experts to understand the needs and analytics use cases
Architect, develop, build, test, and deploy applications using iterative, and agile-like development processes
Translate advanced business analytics problems into technical approaches that yield actionable recommendations, in diverse domains such as risk management, product development, marketing research, supply chain, and public policy
Architect and develop applications for cloud environments utilizing cloud computing services
Communicate results and educate others through insightful visualizations, presentations, and demonstrations

Qualifications:
Bachelor's degree in Computer Science, Computer Engineering, Engineering, or related fields from an accredited college or university with a minimum of 5 years of relevant experience, Master's degree with a minimum of 2 two years of experience or PhD with a minimum of 1 year of experience
Fluency in several programming languages such as Python, Java, or Javascript, with the ability to pick up new languages and technologies quickly
Knowledge and experience of distributed computing and big data systems (Hadoop, Spark, Hive, Pig, Storm, Kafka, etc) , Relational Databases (RDBMS), SQL and NOSQL data stores.
Experience with Healthcare data and applications
Proficiency with Git, Jira, Confluence and with Linux/Unix environments; proficiency with Agile software development
Strong written and verbal communication skills, ability to work in dynamic team environments, and multi-task effectively
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an equal opportunity employer. All qualified applicants are considered for employment without regard to race, color, creed, religion, age, sex/gender, national origin, ancestry, citizenship status, marital status, sexual orientation, gender identity or expression, disability, physical or mental handicap unrelated to ability, pregnancy, veteran status, unfavorable discharge from military service, genetic information, personal appearance, family responsibility, matriculation or political affiliation, or other legally protected status. KPMG maintains a drug-free workplace. KPMG will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable local, state or federal law (including San Francisco Ordinance number 131192). No phone calls or agencies please."
"Master Software Engineer, Data",Capital One,"McLean 1 (19050), United States of America, McLean, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master Software Engineer, Data

As a Capital One Data Engineer, you'll be part of an Agile team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. You will participate in detailed technical design, development and implementation of applications using existing and emerging technology platforms. Working within an Agile environment, you will provide input into architectural design decisions, develop code to meet story acceptance criteria, and ensure that the applications we build are always available to our customers. You'll have the opportunity to mentor other engineers and develop your technical knowledge and skills to keep your mind and our business on the cutting edge of technology. At Capital One, we have seas of big data and rivers of fast data.

Who you are:
You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule
Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results
You love to learn new technologies and mentor junior engineers to raise the bar on your team
It would be awesome if you have a robust portfolio on Github and / or open source contributions you are proud to share
Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.

The Job:
Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art, next generation Big Data & Fast Data applications
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Open Source frameworks like Apache Spark, Apex, Flink, Nifi, Storm and Kafka on AWS Cloud
Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift
Utilizing Hadoop modules such as YARN & MapReduce, and related Apache projects such as Hive, Hbase, Pig, and Cassandra
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelors Degree or military experience
At least 5 years of professional work experience in data warehousing / analytics
At least 4 years of experience in open source programming languages for large scale data analysis
At least 3 years of Java development for modern data engineering
At least 3 years of data modeling development
At least 1 year of experience working with cloud data capabilities

Preferred Qualifications:
Master's Degree or PhD
5+ years Java development experience
4+ years of experience in Python, Scala, or R for large scale data analysis
4+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)
4+ years of UNIX/Linux experience
2+ years of Agile engineering experience
2+ years of experience with the Hadoop Stack
2+ years of experience with Cloud computing (AWS)
1+ years of experience with supervised machine learning

What we have:
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
A startup mindset with the wallet of a top 10 bank
Monthly innovation challenges dedicated to test driving cutting edge technologies
Your choice of equipment (MacBook/PC, iPhone/Android Device)

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer,Paycom,"Position Summary

This position will be located within (Dev/IT) and work closely with computer scientists, IT and data scientists to deploy and optimize machine learning models in the Paycom production system environment.

Responsibilities
Work closely with IT/Computer Scientists on technical aspects of deploying machine learning models in production.
Work closely with Data Scientists to understand, implement, refine, design, and test deployment of machine learning models in production.
Optimize the environment for production machine learning models to access and handle data more efficiently and ensure scalability.
Designs new processes and builds large, complex data sets needed for machine learning processes.
Serve as SME on machine learning technology and recommend acquisition of appropriate technology for production purposes.
Advise and assist IT/infrastructure on install and configuration of machine learning systems.
Explore, design, and implement a robust production-grade data processing pipeline that can ingest, aggregate, and transform large datasets.
Independently conduct literature search to keep informed of best practices and new methods.
Serve as on-call for production issues related to machine learning processes.

Qualifications

Education
BS degree in Computer Science or related field with 5 years machine learning engineering experience or MS/PhD degree in Computer Science or related field with 3+ years of machine learning engineering experience.

Experience
3+ years hands-on experience deploying production-level machine learning algorithms and productionizing them at scale in a distributed computational environment.
1+ year experience with R. Working knowledge of R required.
Experience working with large, messy real-world data.
Experience with SQL, Ruby, Python, C#, Pig and other query and programming languages.
Experience with machine learning database tools and platforms such as HBase, Mongo, Hive, Cassandra, MySQL, SQL Server, PostgreSQL, Hadoop, Spark.
Experience with machine learning optimization tools and related technologies such as H2O, Theano, mlpack, TensorFlow. Experience with H2O required.
Experience with machine learning platforms for production models such as Apache, Pattern, Shogun.
Skills and Abilities
Strong expertise in computer science fundamentals: data structures, performance complexities, algorithms, and implications of computer architecture on software performance such as I/O and memory tuning.
Working knowledge software engineering fundamentals: version control systems such as Git and Github, workflows, ability to write production-ready code.
Strong knowledge of data architecture and system/pipeline and data processing engines such as Spark and Hadoop.
Working knowledge of R and Rstudio.
Working knowledge of SQL, Pig, Python, and other query languages.
Knowledge of C++, PHP, Java and other languages.
Knowledgeable with machine learning tools and frameworks like Python, Spark, H2O, Theano, mlpack, TensorFlow.
Knowledge of machine learning platforms such as Amazon, IBM Watson, Azure, Google Predict, BigML
Strong trouble-shooting skills.
Knowledge of technical infrastructure.
Knowledge of installation and configuration of machine learning systems/technology.
Strong technical aptitude.
Basic knowledge of statistics, calculus and probability, experimental design, and machine learning techniques to enable conceptual understanding of Data Scientists models.
Has strong critical thinking skills and the ability to relate them to the products of Paycom.
Possesses a combination of creative abilities and business knowledge.
Demonstrates excellent verbal and written communication skills as well as the ability to bridge the gap between data science and business management.
Displays exceptional organizational skills and is detailed oriented

Paycom provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, pregnancy, military and veteran status, age, physical and mental disability, genetic characteristics, or any other considerations made unlawful by applicable state or local laws. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Paycom expressly prohibits any form of workplace harassment based on race, color, religion, sex, national origin, pregnancy, military and veteran status, age, physical and mental disability, or genetic characteristics."
Software Engineer - Maps Data Store,Uber,"About Uber: Were changing the way people think about transportation. Not that long ago we were just an app to request premium black cars in a few metropolitan areas. Now were a part of the logistical fabric of more than 500 cities around the world. Whether its a ride, a sandwich, or a package, we use technology to give people what they want, when they want it.

For the people who drive with Uber, our app represents a flexible new way to earn money. For cities, we help strengthen local economies, improve access to transportation, and make streets safer. And thats just what were doing today. Were thinking about the future, too.

With teams working on autonomous trucking and self-driving cars, were in for the long haul. Were reimagining how people and things move from one place to the next. About the Role: Our team builds large-scale spatial storage systems for map data and also develops algorithms, business logic, and infrastructure for map data. Every day, our systems store, process, and serve billions of objects for thousands of users across the globe.

We are looking for a select few, talented, senior software engineers that will help extend and scale these systems to keep pace with Ubers rapidly expanding business. What Youll Do: Help scale throughput of our geo-replicated, spatial database by 100X!Develop comprehensive algorithms, logic, and infrastructure to ensure map data fits Ubers unique business needs.Enhance a high-impact, geospatial service that serves polygon entities that drive Ubers core day-to-day operations.Collaborate with other team members, design, prototype, implement, test, and deploy algorithms or micro-services.Work with technologies like Cassandra, ZooKeeper, Mesos, docker, HDFS, Spark, Thrift, and AWS. What Youll Need: Bachelor of Science degree (or higher) in computer science, related field or equivalent practical experience.Very strong software engineering skills, supported by 5+ years of industry experience.Deep curiosity to learn new technologies, explore new ideas, and passion to implement them.Strong written, verbal communication skills; ability to influence others within your team and outside your team.Attention to detail, particularly around software engineering fundamentals, testing methodologies, and quality.Strong sense of ownership, passion, and initiative to fix problems and optimize systems that are both inside and outside your area of responsibility.Willingness and optimism to try new ideas; ability to quickly adapt and learn from mistakes and from other team members.Strong work ethic, productivity, and commitment; relentless desire to see projects through to timely completion.A belief that your team can accomplish more together than as individuals; ability to accept constructive feedback for continuous self-improvement; dependability - someone who the team can depend on to deliver results.Ability to understand the big picture and how your technical contributions will positively impact the Uber business. Bonus Points IfYou have experience with any of the following: scalable, distributed systems and services; distributed NoSQL data stores like HBase, Cassandra, HDFS, or BigTable; cloud infrastructure like AWS or Azure; messages queues like Kafka; geo-replication and consistency/availability trade-offs; and service-oriented-architectures.A passion for working on systems related to maps.

About the Team: Our team is comprised of software engineers with diverse backgrounds and experiences."
Associate Data Engineer,IBM,"Job Description

We are in a data science renaissance.

Companies that embrace data science will lead and those who do not will fall behind.

To help IBM's clients lead, we are building an elite team of data science practitioners to help them learn how to succeed with data science. The team will include data engineers, machine learning engineers , operations research / optimization engineers and data journalists.

The team will engage directly in solving real-world data science problems in a wide array of industries around the globe with IBM clients and internally to IBM. The elite team of data professionals will work with other IBMers and client data science teams to solve problems in banking, insurance, health care, manufacturing, oil & gas and automotive industries, to name a few. We will teach our client data science teams how to execute on key responsibilities.

Key Responsibilities :
1. Design and implement optimal data pipeline architectures
Assemble large, complex data sets that meet business requirements
Identify, design, and implement internal process improvements: including process automation, optimizing data delivery, etc.
Design optimal ETL infrastructures from variety of data sources
Incorporate governance processes and tools into the data landscape
Build analytics tools that utilize the data pipeline to provide actionable insights into customer acquisition, operational efficiency and other key business performance metrics.
Work with executive, LOB, design and IT stakeholders on data-related technical issues and infrastructure needs
Keep data separated and secure across national boundaries through replication and failover techniques
Guide and mentor clients to become self-sufficient practitioners
Preferred Work Location: New York

While working across all these industries, you will also get to travel the World as these engagements will require that the team spend several weeks at client sites working on data science problems with a diverse team.

As a member of the team you will have a T-shaped skill set, having a broad knowledge base in Data Science and Industry Solutions in general, but also in-depth expertise in data engineering.

Required Technical and Professional Expertise

At least 1 years experience with relational SQL and NoSQL databases, including DB2, Oracle, Postgres, Cassandra
At least 1 years experience with big data tools: Hadoop, Spark, Kafka, etc.
At least 1 years experience with data pipeline and workflow management tools: InfoSphere, Informatica, etc.
At least 1 years experience with object-oriented/object function scripting languages: Python, Java, C++, Scala, etc.
Working knowledge of the Agile development process
Working knowledge of stream processing, message queuing

Preferred Tech and Prof Experience

Experience working with unstructured datasets
Experience with at least 2 of the following: SQL, Java, Python, Scala or Ruby
Working knowledge of design thinking
Degree in Computer Science, Statistics, Informatics, Information Systems or another quantitative field

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Data Visualization Engineer - Secret Clearance,Deloitte,"Do you want to build your brand by working for a leading consulting firm that drives eminence in the marketplace? Are you interested in leveraging your analytical skills and strategic ideas to improve mission execution? If so, Deloitte could be the place for you! Our Federal Strategy and Operations teams bring deep industry expertise, rigorous analytical capabilities and a pragmatic mindset to help solve our clients most complex business problems. Join our team, and play a key role in helping to design our clients roadmap to the future and help transform the Federal marketplace.

Work youll do

As a Visualization Data Engineer within our Analytics team, you will:
Perform project tasks independently, and may direct the efforts of others
Participate in and/or lead the development of deliverable content that meets the needs of the client and contract
Anticipate client needs and formulate solutions to client issues
Review deliverables for accuracy and quality
Provides coaching to junior staff
Contribute to new business proposals and proposal development
Manages own personal and professional development; seeks opportunities for professional growth and expansion of consulting skills and experiences
Required Skills:
Bachelor's degree in Computer Science, Engineering, Mathematics or other business-related field
At least 5 years of experience engineering data visualizations and interactive experiences using JavaScript
At least 3 years experience building data visualizations D3.js, WebGL, Processing, Three.js, Yarn, NPM, Node.js, jQuery, jSearch
Strong knowledge in Graph Theory and solid understanding with shortest path algorithms such as Dijkstra's and BellmanFord algorithms
At least 2 years of experience with Illustrator and Photoshop
At least 2 years of experience with front-end CSS coding
Strong experiences with D3 multi-dimensional functions such as .map, .slice, .filter, .nest, etc.
Utilize analytics to explore data, and work with senior data analyst to create predictive and prescriptive insights
Strong documentation skills
Secret clearance required
The Team

Transparency, innovation, collaboration, sustainability: these are the hallmark issues shaping Federal government initiatives today. Deloittes Federal practice is passionate about making an impact with lasting change. Carrying out missions in the Federal practice requires fresh thinking and a creative approach. We collaborate with teams from across our organization in order to bring the full breadth of Deloitte, its commercial and public sector expertise, to best support our clients. Our aspiration is to be the premier integrated solutions provider in helping to transform the Federal marketplace.
By offering client-tailored services and deep industry insight, the Analytics team helps clients to seize new growth opportunities, maximize operational effectiveness, elevate their quality of service delivery, and stay ahead of customer demand. Consider a career where youll thrive on helping Federal clients to execute on their strategic visions and improve mission performance.

How youll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe theres always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Deloittes culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloittes impact on the world.

Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youre applying to. Check out recruiting tips from Deloitte professionals.

#IND: PTY"
Big Data Engineer,Equifax,"Big Data Engineer

Global Platform Services, Application Services

Job Description:

Equifax is looking for a Big Data Engineer for its infrastructure team supporting our Big Data services in Alpharetta, GA. The ideal candidate is interested in DevOps, embraces open source software and technology, likes to automate things and immerses themselves into the emerging industry trends and landscape for this space. Responsibilities are to develop and automate Big Data services; adopting new technology; and ensuring modern operations in order to deliver consumer driven Big Data solutions.

The Successful candidate will be team player to be part of a dynamic Big Data product team focused on modernization and automation.

Responsibilities:

 Utilize engineering and programming skills focused on enabling a DevOps model

 Install, configure and maintain physical and cloud servers running Linux

 Deliver repeatable, reliable systems using Chef and domain specific tooling

 Configuration and change management

 Document operations and provide run books

 Participate in and provide feedback on systems engineering and design

 Streamline deployment of infrastructure into consumable services

 Provide input on the future execution of the delivery and management of environments with a strong emphasis on automation, security, scalability and resiliency

Required Experience:

 Bachelor's degree or technical college diploma in a technical field or equivalent experience

 Demonstrable cross-functional knowledge with Systems, Storage and Networking

 Knowledge of Hadoop and the Hadoop ecosystem

 Proficiency in the Linux operating system

 Proficiency in shell scripting

 Proficiency in Java or Python

 Familiarity with configuration automation tools (Chef, Puppet, Ansible, Salt Stack, etc. (We use Chef))

 Familiarity with continuous integration and delivery

 Familiarity with software development methods and techniques

 Strong communication skills

 Absolutely must be a team player (DevOps)

 Boundless passion for technology and a willingness to learn and teach

 Strong work ethic, attention to detail and problem solving skills

Desired Experience:

 DevOps focused engineer with experience in LEAN principles

 Java/Python development skills

 Openstack or other Cloud Management Platform experience

 Experience working within an agile framework

Primary Location:
USA-Atlanta JV White

Function:
Function - IT Operations / Platforms

Schedule:
Full time"
Information Security and Privacy Incident Manager,Google,"There's no such thing as a ""safe system"" - only safer systems. Our Security team works to create and maintain the safest operating environment for Google's users and developers. As a Information Security and Privacy Incident Manager, you help protect network boundaries, keep computer systems and network devices hardened against attacks and provide security services to protect highly sensitive data like passwords and customer information. Security Engineers work hands-on with network equipment and actively monitor our systems for attacks and intrusions. You also work with software engineers to proactively identify and fix security flaws and vulnerabilities.

You use your industry experience to own and drive the resolution of complex security incidents, policy questions and technical security issues.

Google is creating next-generation technologies that will revolutionize how billions of people connect and interact with each other and information. Security and Privacy are critical components to these new solutions.

You are passionate about information security and privacy and want to help keep the world safe. You will be on the front lines of security and privacy incidents from all around the company, setting the tempo and driving resolution. Youll set the bar for excellence in rapid response and protection of user information.

Behind everything our users see online is the architecture built by the Technical Infrastructure team to keep it running. From developing and maintaining our data centers to building the next generation of Google platforms, we make Google's product portfolio possible. We're proud to be our engineers' engineers and love voiding warranties by taking things apart so we can rebuild them. We're always on call to keep our networks up and running, ensuring our users have the best and fastest experience possible.

Responsibilities
Coordinate and drive resolution on a diverse range of incidents.
Analyze root causes, trends and systematic issues.
Improve the security and privacy of Googles products and services.
Develop excellence in Incident Management through tools and infrastructure and preparing teams across Google through exercises and training.
Manage relationships across the company: Legal, Engineering, Communications, etc.

Qualifications Minimum qualifications:
BA/BS degree in Computer Science or equivalent practical experience.
3 years of experience with incident or emergency response coordination.
Technical experience with experience in systems administration, software engineering, digital forensics, security engineering or privacy engineering.

Preferred qualifications:
MS degree in Computer Science.
Experience with Python, Go, C/C++ and/or Java.
Understanding of legal, information security and privacy frameworks, e.g., ISO 27001 and Safe Harbor Frameworks.
Ability to lead teams of people in ambiguous situations through influence and not authority.
Outstanding organizational, prioritization and multitasking skills.
Exceptionally strong communication skills and a data-driven analytical approach toward solving complex challenges."
Big Data - Data Engineer,NCR,"TITLE: Big Data - Data Engineer
LOCATION: Atlanta, Ga
GRADE: 9

Who Youll Work With

NCRs strategy is to transform to a data-driven culture in which large volumes of disparate data drive new revenue opportunities, increase returns on investments, and improve business decisions. You will work in the CDO organization with our Data Lake group. Our team builds the Enterprise Data Lake platform serving analytics for multiple lines of business.
We work more like a small startup company. Our development teams are small and embrace agile principles to quickly generate value to our stakeholders. We combine the latest open source technologies together with traditional enterprise software products.
Our office culture is casual, fun and social, with an emphasis on education and innovation. We have the freedom to try new ideas, experiment and are expected to be constantly learning and growing. There is also a strong emphasis on mentoring others in the group, enabling them to grow and learn.

What Youll Do

 Participate in collaborative software development and implementation of the new Enterprise Data Lake on Hortonworks HDP and HDF distributions.
 Explore and evaluate new ideas and technologies.
 Ensure conceptual and architectural integrity of the platform.
 Work on large-scale, multi-tier big data engagements.
 Gain deep technical expertise in the Hadoop administration.
 Assist in automation of on-premise and cloud (Azure) based Hadoop clusters.
 Provide support, on-going maintenance, and required modifications to multiple Hadoop environments

Qualifications
Required Qualifications

 Bachelors degree or higher in Computer Science or a related field.
 Good understanding of distributed computing and big data architectures.
 Polyglot development: Capable of developing in at least one of the programming language like Java, Scala or Go.
 Knowledge of OOP design and patterns
 Strong knowledge of source code repository like SVN/GIT & build tools like Maven  Proven use of open source frameworks
 DevOps: Appreciates the CI and CD model and always builds to ease consumption and monitoring of the system. Experience with Maven (or Gradle) and Git preferred.
 Familiar with agile development practices
 Personal qualities such as creativity, tenacity, curiosity, and passion for deep technical excellence.

Desired Qualifications

 Experience working with public clouds like Azure, AWS etc.
 1+ year of experience with Big Data technologies such as Spark, Kafka and/or Hadoop
 Experience in working with un-structured or semi-structured data

EEO Statement

Integrated into our shared values is NCR's commitment to diversity. NCR is committed to being a globally inclusive company where all people are treated fairly, recognized for their individuality, promoted based on performance and encouraged to strive to reach their full potential. We believe in understanding and respecting differences among all people. This concept encompasses but is not limited to human differences with regard to race, ethnicity, religion, gender, culture and physical ability. Every individual at NCR has an ongoing responsibility to respect and support a globally diverse environment.

Statement to Third Party Agencies
To ALL recruitment agencies: NCR only accepts resumes from agencies on the NCR preferred supplier list. Please do not forward resumes to our applicant tracking system, NCR employees, or any NCR facility. NCR is not responsible for any fees or charges associated with unsolicited resumes."
Senior Data Engineer,Daimler,"Senior Data Engineer
Description
Embedded in a worldwide network Mercedes-Benz Research & Development North America, Inc. continuously strives to remain at the forefront of successful automotive research and development. MBRDNA is headquartered in Silicon Valley, California, with key areas of Autonomous Driving, Advanced Interaction Design, Digital User Experience, Machine Learning, Costumer Research, and Business Innovation. In Redford, Michigan, the focus is on Powertrain and eDrive technology as well as in Long Beach, where the E-Mobility Group helps to shape the future of the North American market for hydrogen fuel cell and battery electric vehicles. The Testing and Regulatory Affairs Division in Ann Arbor and the Advanced Vehicle Design in Carlsbad complete the competence center.

Together, all the developers, technicians, engineers and designers take on the challenges of creating the next innovation. Theyre inspired by the newest trends, find the best solutions for the customer, develop the latest and greatest technologies and create the next generation of connected, safe, sustainable and luxurious vehicles.

Here at MBRDNA, we are looking for talented, energetic, and committed individuals to join our diverse team. Our employees are the key to our success, and we support each individual in fulfilling his or her potential. We proudly continue the pioneering work initiated by founders Gottlieb Daimler and Carl Benz over 125 years ago.

Role Overview:

With the number of connected Mercedes-Benz vehicles on the road increasing daily, the amount of data being generated and its significance is also increasing rapidly. The Vehicle Infrastructure & Data Analytics team is looking for passionate and versatile Data Engineers to support with the collection, storage, processing, and analysis of large amounts of vehicle data.

This position will play a key role in the development and deployment of an innovative Big Data Platform for vehicle data processing and advanced analytics. As a Senior Data Engineer on the team, you'll get to define and build the data pipelines that combine vehicle data and third-party content and will enable better, faster, data-informed decision-making. As part of a larger global interdisciplinary team, you will also make strategic decisions that influence the platform and data warehouse architecture.

This is a unique opportunity to join an innovative group of engineers creating the next generation big data platform that will be accessed and used by hundreds of people across domains within Mercedes-Benz.
Architect and implement the workflows with varying demands of throughput and latency
Support with the implementation of the data warehouse for the new Big Data Platform
Build and optimize the data pipelines that combine new vehicle data and third-party content
Define and implement data stores based on customer requirements and system requirements
Evaluate performance and take steps to optimize all parts of the big data stack and implementation

Qualifications
Minimum Skills Required:
Bachelor's degree in Computer Science or related technical field, or equivalent experience (Bachelors + 5 yrs / Masters + 3 yrs / PhD)
Experience in one or more major programming languages (e.g. Unix shell scripting, Python, Java, C++, C#, etc.)
Proven record building a large scale data warehouse
Strong background in large-scale query processing and data crunching
Experience in data mining and data modeling with hands-on experience in structuring and spatial decomposition of complex data sets
Experience working with low latency, high performance stream processing systems
Strong expertise with Hadoop, MapReduce, HDFS and SQL engines
A strong voice for data integrity and reporting quality utilizing best practices and industry standards.
Excellent critical thinking, problem solving and analytical skills.
Excellent communication skills, and the ability to work effectively with others
Drivers License

Preferred Qualifications:
Working knowledge of Spring Boot, Spring XD, Pig or Spark is a plus
Experienced with test-driven development based on Mockito, Maven & Junit
Familiarity with different data warehouse solutions including Cassandra, Redis, HBase, Grenplum or Gemfire
Experience with enterprise data platforms including Pivotal's Big Data Suite, Microsoft Azure Analytics or SAP Analytics is a plus

MBRDNA is an equal opportunity employer that offers generous benefits and compensation, work/life balance tools and several methods of recognition and rewards. Our benefits include medical, dental and vision insurance, 401K savings plan, tuition and fitness reimbursement programs and much more."
Senior Data Engineer,Hewlett Packard Enterprise,"Aruba, a Hewlett Packard Enterprise Company, is a leading provider of next-generation networking solutions for the mobile enterprise: http://www.arubanetworks.com/company/about-us . The company designs and delivers Mobility-Defined Networks that empower IT departments and #GenMobile, a new generation of tech-savvy users who rely on their mobile devices for every aspect of work and personal communication. To create a mobility experience that #GenMobile and IT can rely upon, Aruba Mobility-Defined Networks automate infrastructure-wide performance optimization and trigger security actions that used to require manual IT intervention. The results are dramatically improved productivity and lower operational costs.

We are looking for a Senior Data Engineer who is responsible for designing and implementing data pipelines at Big Data scale. Your Responsibilities Implement parsers and validators for new Log sources Implement ETL transformers to reformat and enhance the data Implement ETL correlators to update the data from multiple data sources Work on tools and APIs to visualize the backend data Troubleshoot performance and data related problems Work with the Analytics team in defining the schema for new data sources Our Minimum Requirements for This Role Are 4+ years java and/or Python development experience Experience working with Hadoop or Big Data (HDFS, Parquet, HBASE) Experience working with Large scale databases like Cassandra Experience working with Map Reduce or Spark, ElasticSearch, Kafka Experience working with Databases like Postgres, SQL Education Bachelor's or Master's degree in Computer Science, or equivalent and typically 4-6 years experience. Benefits youll enjoy At Aruba, a Hewlett Packard Enterprise Company, we offer an exciting and fun work culture, driving innovation, collaboration, and growth. We place our customers first, deliver some of the most innovative technologies to the market, and have fun doing it all!

Come join our team and be part of an exciting organization poised for success! Thanks for taking the time to review our job, if you think it is a match to your experience and interests please apply today  we are eager to learn more about you! We have dozens of openings, so encourage your friends to apply as well! #ArubaNetworks #GenMobile #ArubaNetworksJobs #HPE #HewlettPackardEnterprise Please note the above statements describe the general nature and level of work only.

They are not a complete list of all required responsibilities, duties and skills. Other duties may be added, or this description amended at any time."
Data Ingestion Engineer,"Apogee Integration, LLC","Apogee Integration is someone to perform Data Ingestion Engineer to work in Chantilly, VA:
Bachelor's Degree in Computer Science or related field
3+ years focused on ETL using frameworks such as Pentaho, NiFi, or similar
Security Clearance
TS/SCI (Security Clearance must be in JPAS)
US Citizenship Required
About APOGEE INTEGRATIONApplied Intelligence!
For 18 years, our core team of engineering professionals have provided the domain expertise and integrated products that serve as a primary means of analysis for leading decision makers throughout the Intelligence Community. We are honored to be able to serve our customers and are dedicated to providing products and services to ensure their long-term success.
Combining innovation with practical results and a proven commitment to customer care, our full life-cycle expertise covers the continuum of engineering needs:
Aerospace Engineering, Software Engineering, Geospatial Analysis, 3D Interactive Visualization, Network Engineering, Systems Administration, Information Assurance, Systems Engineering, Modeling & Simulation, Space Protection, Enterprise Requirements.
Benefits:
Although clearly focused on The Mission, at APOGEE, we realize that our staff or APOGEE-niuses are our greatest assets. We are committed to a high performance culture and provide an environment that challenges our employees to be exceptional. We provide World Class Health & Wellness, Retirement & Savings, Time Off and Beyond the Basics Benefits.
Health & Wellness
United Healthcare, Guardian Dental, Vision, Life Insurance, Long/Short Term Disability and Health Savings Account.
Retirement & Savings
Apogee wants to ensure that every Apogee-nius can participate and grow a meaningful retirement nest egg without losing the flexibility to save for other goals. To provide this flexibility, the Company provides a 401(k) that enables Apogee to contribute up to 7% of your gross salary to your retirement.
Apogee 401(k)
Apogee offers a 401(k) plan to all our employees. You are eligible to participate in the plan beginning on the first of the month following 3 months of employment. You may contribute up to 20% of your gross wages each pay period on a pre-tax basis. Apogee will match 100% of your contribution to the 401(k) up to 7% of your gross salary. The maximum dollar amount you can contribute for 2017, per the IRS, is $18,000. The Vesting period for Apogee matching contributions is 3 years. The Apogee 401(k) is administered by Fidelity Investments. Fidelity provides Apogee 401(k) plan users a wide selection of investment choices including index funds, managed funds, stocks and bonds tailored for your investments goals and years till retirement. The goal of the Apogee Retirement Plan is to help you maximize annual retirement growth and savings while providing you the flexibility to meet other personal financial goals.
Time Off
26 days off annually from a mixture of fixed, floating and earned personal time off!
Mobility Bonus
Apogee-niuses are on-the-go! Apogee provides our Apogee-niuses up to $1,000 Mobility Bonus for Broadband Internet and Cellphone services. Apogee has an on-line time & expense system and interactive website that is integral for our business processes. To ensure that you stay in touch with the Apogee-nome, your customers, and check on the latest Apogee tweets and texts, cellphone access is key. This bonus is paid out quarterly in the amount of $250.
Professional Development Assistance (PDA) Plan
Apogee provides its Apogee-niuses with PDA toward management-approved training, tuition, continuing education, and conferences. Annual cap is $5,250. Apogee assists by offering software, books and the training you need to excel in your job. Apogee will purchase for you any approved materials that apply to your job.
Apogee Integration is an M/F Disabled and Vet EEO/AA Employer

Reach your peak and become an Apogee-nius! Today!"
Big Data Software Engineer - Data License,Bloomberg,"Job Requisition Number: 63622

We love data, do you?

Financial institutions thrive on data and the Bloomberg Enterprise Content business satisfies their needs. Enterprise Content is a fast growing business that provides clients with rich programmatic access to the best financial data and services in the industry such as Reference, Pricing, Corporate Actions and Regulatory content sets.

Our team is responsible for storing detailed usage metrics on every piece of data taken down by our clients. We just don't stop there - our team develops applications and services to analyze the data and apply various pricing models to be able to bill our clients. Our pricing models involve optimizing usage patterns across multiple access points that our clients use to access financial data and require us to churn through 2 years of data (trillions of data points) in 2 hours.

Let's talk technology. To build a scalable ingestion pipeline for our usage metric data, we have picked Apache Kafka, one of the fastest distributed streaming platforms. Now we need to store that data - we use Hadoop Distributed File System (HDFS) and, for some use cases, HBase/Apache Phoenix. The data isnt very useful unless we can analyze it - thats where Apache Spark & MapReduce come in. We use other tools in the Hortonworks Hadoop distribution like Oozie for scheduling our billing jobs, and are investing in Jupyter Notebooks to give tech-savvy users more flexible and sophisticated ways to analyze our data.

As a part of building new products, our business users have to design new pricing models to offer products to our clients. Time to market is absolutely one of our key metrics. Our goal is to empower users to be able to configure various attributes that make up the pricing models and reduce the time it takes on the engineering side to build them.

These are big goals and were just getting started. Were looking for bright, knowledgeable, driven people to help us make key technical decisions on the architecture, processes, best practices, and direction of this initiative. You will be responsible for the research, development, and stability of this critical platform and its impact on one of the highest revenue businesses within Bloomberg.

We are looking for an experienced Java developer with a strong understanding of big data ecosystem to help drive our challenging vision.

Well trust you to:
Design and develop highly scalable, fault tolerant solutions

Lead from the front in building a configurable, operational independent system while understanding the challenges surrounding the current implementations.

Work effectively in a small, adaptable, agile team focused on business value and operational independence

Incorporate and adapt multiple technologies to help reach our goals of enterprise-wide usage and support for data-in-motion

Understand and enforce the right balance between innovation, experimentation, and delivering completed features to production in a timely manner

Understand the core business and apply the technologies to other use cases that drive the client facing business

Youll need to have:
5+ years of experience programming enterprise solutions in Java/Scala

3+ years of experience in Apache Kafka, Apache Spark, Apache HBase or any other big data technologies

Excellent understanding of algorithms, data structures, object-oriented design and multi-threading

Stellar problem solving & communication skills and the ability to thrive in a highly collaborative and dynamic work environment

Experience with low-latency, high-volume, and high-availability distributed systems to support real time use cases.

If this sounds like you, apply!"
"Internal Data Tools, Business Intelligence Analyst",Apple,"Apple is seeking a proven Business Intelligence Analyst to join the Internal Data Tools team. Youll partner with other business analysts and internal stakeholders to understand reporting requirements, work closely with data engineers to design supporting data models, and deliver dashboards using our internal data reporting platform. You will need to understand how to curate and communicate complex data in a clear dashboard view, often partnering with UI engineers to build new compelling visualizations. As part of a centralized team that supports marketing, operations, product management, engineering, business - amongst other teams - youll act as a thought partner and domain expert on data and help in driving decision making at Apple. This is a full-time position in Apple Media Products and is located in our Cupertino, CA office.

Key Qualifications
2-5 years of industry experience in a Business Intelligence role
Well-versed in SQL languages. Experience with big data frameworks preferred
Well-versed with data visualization and building dashboards, e.g. Tableau or similar
Knowledge of data modeling and database schema design
Excellent communication skills, both written and oral
Excellent project management skills
Description
Wireframe, prototype and build dashboards to support informed decision making on key business initiatives
Partner with reporting analysts and data engineers to design and build supporting data models
Contribute towards centralized documentation
Organize QA and UAT
Define success metrics for projects and review post-launch

Education
Minimum of a Bachelors degree in Economics, Computer Science, Statistics, Mathematics, Engineering or relevant field. Ideally, Masters in related field."
Software Engineer - Data,When I Work,"There are less than a few times in your life when you will have the opportunity to join a company at this stage. When I Work is the leader in Scheduling and Time & Attendance applications. Our portfolio of Software-as-a-Service (SaaS) applications, including our award-winning scheduling app, When I Work, has revolutionized the way that customers manage and schedule their employees. Over 25,000 businesses use When I Work to spend less time on scheduling and attendance and improve team communication - including Uber, Ben & Jerrys, and Virgin America. No matter how you slice it the future of business software is mobile. Consider joining the mobile leader, When I Work, and be a part of a team that is changing the way the world schedules and tracks time for hourly workers.

Who You Are

You are a programmer who is excited by data and its endless possible use cases. Someone that enjoys creating tools and infrastructure to empower your peers. Collaboration and teamwork are a must, but you also have the ability to work independently when needed to get things done. Above all, you are driven to learn and a motivated problem solver who wants to help tackle the new and interesting challenges that we encounter as a fast-growing startup.

What Youll Be Doing

We have just started a reboot of our data infrastructure that transitions us away from a fragmented collection of small data solutions. You will be a key contributor to this transition process, helping to grow and mature our new big data infrastructure as well as create and manage data pipelines and data application tooling that will have significant impact to the company and our users.

Minimum Qualifications

You have strong programming fundamentals

You are comfortable with agile software processes

You have experience with multiple programming languages

You have solid understanding of structured and unstructured data

You have familiarity with cloud computing environments and infrastructure

You are a proponent of DevOps and enthusiastic about DataOps

You practice empathy and kindness, and you look to help others

Our Technology Soup

We use a lot of different technologies to get the job done, and each member of our team brings a their own mix of technology experience. If you have familiarity with even a few of these (or equivalents), you could make a valuable contribution: Python, Go, JavaScript, Git, Gitlab, GitHub, Spark, PrestoDB, MySQL, NoSQL, Terraform, Kubernetes, lots of AWS (EC2, ECS, Lambda, S3, RDS, DynamoDB, Aurora, Redshift, Elasticache, VPC, CloudFront, Athena, EMR, Glue, CloudSearch, Elasticsearch, Kinesis, Data Pipeline)

Whats in it for you:
Passionate coworkers -- youll work with open-minded, enthusiastic people who will constantly challenge and motivate you to go above and beyond to produce great work and grow in your career.

Transparency -- youll experience an open door policy that gives you the unique opportunity of having direct and informal access to our entire team.

Individual ownership -- youll be empowered to take personal ownership of projects that can directly impact the future direction and overall success of our products.

Diversity of thought -- youll become part of a talented team with a wide range of skills, experience, and philosophy.

We are:
A communication and collaboration company used daily by tens of thousands of business locations to schedule hundreds of thousands of hourly workers around the world.

A well-funded ($15M Series B in June 16) 130+ person company growing at a fast rate.

A 3X Best Places to Work

Led by a CEO named to the 2016 Business Journal 40 under 40 and Inc.com 2014 list of 35 Under 35 Coolest Entrepreneurs

The 2013 Minnesota Cup High Tech Division Winner

From our employees:
I work here because the team is awesome. There's no politics, there's no middle management second guessing everything or micro-managing my time, and everyone treats each other with respect.

Theres so much to do to make the app a world-class app, Im anxious to get it there and I believe I know how to do it. I enjoy building things, so this is right up my alley!

What's cool about the work - The fact that I'm legitimately given the latitude to directly contribute to new initiatives and major product changes. I'm not micromanaged. I am empowered to determine what to do to help grow the company and to go do it.

The people who work here are a lot of fun! Even though weve grown to 100 employees its still easy to get to know people especially if you join in on the lunch room shenanigans ;) The company activities we do are always fun too.

Benefits:
Professional development allowance

Paid parental leave

Medical benefits - employee premiums paid 100% by WIW

Dental benefits

Paid vacation and holidays

Flexible work environment

401K Match

Transit Pass or parking stipend

Dynamic and dedicated team

Data/Cell (internet) stipend

Casual dress code

Sound Like a Good Fit?

Check out our core values . If they excite you, wed love to talk! Please submit the following to apply:

Resume (including months/years of employment for each position)

Cover letter

Must already be authorized to work in the United States on a full-time basis for any employer."
Postdoctoral: Vehicle Data Analysis Engineer,National Renewable Energy Laboratory,"Perform analysis and performance characterization of new technologies such as fuel cells, hybrid drivetrains, all-electric drivetrains, autonomous controls and alternative fueled vehicles to identify new novel methods and technology to improve driveline and vehicle efficiency. The successful candidate will work with NRELs Commercial Vehicle Technologies engineering team to obtain and manage large sets of data from in-use vehicle fleets, perform analysis through simulation and modeling, visualize data, as well as publish and present the resulting findings. Position requires extensive interaction with the NREL Commercial Vehicle Technologies engineering team and other NREL vehicle technology research groups, industry managers, vehicle and equipment manufacturers, and other industry representatives as well as extensive data collection, analysis and visualization activities. Job Description Vehicle Data Collection: Work with commercial fleet operators, industry partners, NREL lab testing staff, and in some cases subcontractors to collect vehicle performance, operations, emissions, refueling/charging, and maintenance data using on-board data acquisition systems, telematics as well as off-line data mining techniques as required by project plans and protocols. Work with NREL engineers in Commercial Vehicle Technologies and Fuel Performance groups to coordinate test plans, data collection and analysis of results from NRELs heavy-duty chassis dynamometer to identify new research opportunities. Work with NRELs Fleet DNA data engineers to run quality assurance secure data storage processes, identify and resolve data quality issues from various sources. Vehicle Data Analysis: Develop vehicle system models of commercial vehicles using NRELs FASTSim model, validated with chassis dynamometer and on-road data, to conduct parametric analysis of vehicle design and operational parameters. Analyze collected technical data and fleet information to contribute to technical journals and other publications which will be useful to industry operations managers, vehicle OEMs, advanced power train developers, fuel providers and various research organizations. Apply statistical analysis techniques as necessary to determine statistical significance of results. Use Matlab, Simulink or other math based programs to develop and refine vehicle modeling and simulation techniques and tools to provide additional analysis of advanced vehicle technology. Information Reporting and Dissemination: Author or co-author technical reports, scientific journal articles and technical papers on results of studies and research. Work with NREL, the US Department of Energy (DOE) and other national laboratories to utilize data and information obtained from vehicles. Work with NREL database and publication specialists to make results available over the Internet. Make presentations on results at relevant conferences or workshops. Required Knowledge, Skills and Attributes Knowledge of medium- and heavy-duty commercial vehicles test engineering especially, vehicle testing, CAN-based data acquisition, telematics, and vehicle instrumentation. Solid engineering and technical capabilities in experimental design, on-board data acquisition, data mining and data analysis. Knowledge and experience with commercial vehicle and powertrain technologies including heavy-duty engines, engine control systems, heavy-duty engine after-treatment systems, and emissions regulations. Demonstrated experience with engineering computer modeling tools with ability to use and adapt models and code in Matlab, Simulink, Python, SQL, and Excel to enable data analysis, statistical analysis, and data visualization. Demonstrated exceptional interpersonal and communication (oral and written) skills to enable effective internal communications and external interactions with industry. . Required Education, Experience, and SkillsMust be a recent PhD graduate within the last three years. . Preferred Qualifications . Submission Guidelines Please note that in order to be considered an applicant for any position at NREL you must submit an application form for each position for which you believe you are qualified. Applications are not kept on file for future positions. Please include a cover letter and resume with each position application. . EEO Policy NREL is dedicated to the principles of equal employment opportunity. NREL promotes a work environment that does not discriminate against workers or job applicants and prohibits unlawful discrimination on the basis of race, color, religion, sex, national origin, disability, age, marital status, ancestry, actual or perceived sexual orientation, or veteran status, including special disabled veterans. NREL validates right to work using E-Verify. NREL will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS), with information from each new employees Form I-9 to confirm work authorization."
"Data Engineer, Consumer Hardware",Google,"As a ground-floor member of the data services team within business systems for the Hardware support operations team, you will gather, assess, and manage data about every part of Googles support operations and transform it into beautiful insights, analysis, and reporting. You are deeply comfortable with data, and love to wade into the complex details of data formatting, query massaging, and bulletproofing pipelines. Youre also a self-driven performer who takes a bit of chaos as a challenge, and you thrive on creating structure and value out of complex and ambiguous technical challenges with little guidance. On the flipside, youre a team player with strong communications skills whos eager to understand and work to the nuanced requirements of partners both within and outside hardware.

Google's mission is to organize the world's information and make it universally accessible and useful. Only one thing consistently stands in the way between our users and the world's information - hardware. Our Hardware team researches, designs, and develops new technologies and hardware to make our user's interaction with computing faster, more powerful, and seamless. Whether finding new ways to capture and sense the world around us, advancing form factors, or improving interaction methods, our Hardware team is making people's lives better through technology.

Responsibilities
Work with non-technical business users to understand their analytical needs, document and prioritize requirements, and to help them effectively use the data and analytical tools that the team has developed.
Design, develop, and support data warehouses, dashboards, data pipelines, and reporting tools for operational and business impact data; Design and enforce access control for all the data.
Work with business stakeholders and engineer teams to validate data, identify gaps, and contribute in changes in business processes and data pipelines to fix data gaps.
Develop visualization based on user needs on Google dashboards or Tableau.
Project manage and drive analytical projects end-to-end.

Qualifications Minimum qualifications:
Bachelors degree with an emphasis on quantitative or technical work (Computer Science, Statistics, Mathematics) or equivalent practical experience.
5 years of relevant work experience.
Experience in SQL and relational databases including queries, database definition and schema design. Experience designing data models and data warehouses, writing and maintaining ETLs which operate on structured and unstructured sources.
Experience with one or more programming languages (Python, Java, C++, Ruby, etc.). Experience with fundamental computing concepts including data structures and algorithms (including trees, graphs, file formats, algorithmic complexity).

Preferred qualifications:
Experience in one or more visualization tools such as Tableau. Experience gathering reporting and analytics requirements from stakeholders. Experience designing and executing structured analyses, deriving business insights, and evaluating the impact of business decisions; experience communicating findings to a diverse audience
Experience with Unix or GNU/Linux systems including shell scripting.
Proficiency in a language for statistical computing (R, SAS, Stata)
Ability to prioritize multiple conflicting priorities while driving towards pragmatic decisions/actions
Strong oral and written communication skills, including the ability to communicate complex findings in a structured and clear manner to a non-technical audience."
"Data Engineer, Mid",Assured Consulting Solutions,"Education and Experience Required:
A master's degree in Computer Science, Mathematics, Information Technology or Engineering. A bachelor's degree may be replaced with ten (10) years of Computer Science experience.

Clearance Required:
Must be a U.S. Citizen and possess a current and active TS/SCI with a CI poly granted by the Department of Defense or the Intelligence Community

Job Location:
Washington DC- Joint Base Anacostia Bolling (JBAB)

Background:
A Federal Services information technology (IT) customer requires support of the enterprise IT environment creating and implementation of an enterprise-wide data strategy life-cycle and data model. Under this contract, Assured Consulting Solutions provides direct assistance to the customer in IT services ranging from enterprise architecture to systems integration with a focus on consultant level IT support. The customers IT and Chief Data Officer (CDO) organization requires assistance in data management and exploitation with the goal of delivering the right data, at the right time, in a secure and reliable manner.

Assured Consulting Solutions is seeking a highly-motivated Data Engineer in support of the CDO's Data Management and Librarian Program. The candidate is expected to:

Perform Data Engineering to include data transformation (XSLTs, ETL, etc.) and data ingest
Perform investigations to document as-is data flow in existing systems
Provide software coding in languages such as Python, Groovy

Support the Government in creating and delivering:
Data Source Analysis. To include, an assessment of the accessibility of the data source; the value or quality of the data provided; and, the level of effort required to bring the data into compliance with the goal data model
Data Model. A data model conveying the scope of data required for CDO vision. The data model will also contain a mapping of data source to data model and an assessment of where gaps may exist
Data Life-Cycle Strategy. A notional architecture will be provided that identifies how a Data Life-Cycle would be constructed to meet the needs for aggregating disparate data sources, from both known and future data sources and a broad timeline for the development and approval of supporting life-cycle artifacts

Job Requirements:
Possess experience using Apache NiFi or other data flow tools
Possess an understanding and at least two (2) years of direct experience working in the IC

Preferred Qualifications:
Experience using Collibra Data Governance Center, or similar tools, highly desired"
Web Services Engineer (Python),Apple,"Apple is looking for a talented senior Python Web Developer. We are a team working on services and API's that enable high performance computing and AI applications in the cloud.

Key Qualifications
Minimum 5 years of experience building and maintaining REST services in production environments.
In-depth knowledge Python and at least one Python web framework (Flask preferred).
Expertise in Linux OS and Bash scripting.
Experience with relational databases, ORMs, and data storage technologies (PostgreSQL, SQLAlchemy, Redis, S3, Alembic).
Understanding of cloud computing and hosting technologies (AWS, Heroku, etc.).
Solid understanding of continuous delivery, software testing, and instrumentation.

Description
Were looking for candidates with a deep understanding of RESTful web services, software and API design, and excellent communication skills. Our goal is to build reliable and easy-to-use services for internal customers. You need to have experience designing, developing, maintaining and deploying web services in production. Above all, we value passion for writing good code, architecting good software, designing good tests, and constantly improving in all of these areas.

Education
BS or MS in Engineering or equivalent experience
Additional Requirements
Go programming language
Log analytics in Splunk
Writing command line tools
Instrumenting code
Formally describing and documenting software architecture (e.g. UML)
API documentation and testing tools (Swagger, OpenAPI, Postman, etc.)"
Design Engineer,Intel,"Job Description

Job responsibilities include but are not limited to support power model generation and verification with understanding of design functionality and configuration. Perform power characterization for standard cell library and benchmark designs, develop flows and methodology to support chip and product power roll up and power analysis. Verify design and power intent of digital circuit designsJob responsibilities include but are not limited to support power model generation and verification with understanding of design functionality and configuration. Perform power characterization for standard cell library and benchmark designs, develop flows and methodology to support chip and product power roll up and power analysis. Verify design and power intent of digital circuit designs

Qualifications

You must possess the below minimum qualifications to be initially considered for this position. Experience listed below would be obtained through a combination of your school work/classes/research and/or relevant previous job and/or internship experiences.

Minimum Requirements:
Must have a Master's degree in Electrical Engineering, Computer Science or any related field.
Minimum of 3 months experience
Preferred Qualifications
Minimum 6 months of experience in one or more following areas:
Using TCL/Shell to develop frontend or backend CAD flow.
Using Perl/Python/TCL/Shell to collect data and summarize results.
Using VBA to manipulate data and summarize in excel format.
RTL, Synopsys IC Compiler (ICC), UPF, Spice.
Experience with DDR, PCIe, Ethernet, HBM, ARM, FPGA Inside this Business Group

The Programmable Solutions Group (PSG) was formed from the acquisition of Altera. As part of Intel, PSG will create market-leading programmable logic devices that deliver a wider range of capabilities than customers experience today. Combining Altera's industry-leading FPGA technology and customer support with Intel's world-class semiconductor manufacturing capabilities will enable customers to create the next generation of electronic systems with unmatched performance and power efficiency. PSG takes pride in creating an energetic and dynamic work environment that is driven by ingenuity and innovation. We believe the growth and success of our group is directly linked to the growth and satisfaction of our employees. That is why PSG is committed to a work environment that is flexible and collaborative, and allows our employees to reach their full potential.
Posting Statement. Intel prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status."
"Software Engineer, Data Engineering",IgnitionOne,"About IgnitionOne:
IgnitionOne simplifies life for marketers, providing deeper insights and robust targeting of individuals through the use of proprietary engagement scoring and integrated marketing and advertising solutions. The IgnitionOne DMS is full-featured digital marketing hub which significantly improves performance across all devices and channels. IgnitionOne allows marketers to better understand their customers and activate personalized 1:1 messaging across search, display, social, mobile, email, and website personalization. With a global footprint of over 450 employees in 17 offices across 10 countries, IgnitionOne is one of the largest independent marketing technology companies in the world.

IgnitionOne currently scores over 300 million consumers monthly in 75 countries and powers more than $60 billion in revenue each year for leading brands, including General Motors, CenturyLink, Bridgestone, La Quinta and Fiat, as well as advertising agencies such as 360i, GroupM and iProspect.

The Role:
Does the challenge of dealing with massive datasets (billions of transactions a day) in near real-time using the latest technologies (Spark, Scala, AWS) get you excited? If so, then we want to talk to you. IgnitionOne Data Engineers are using cutting edge technologies to build new data products that aim to change the digital marketing landscape. Youll be the tech leader of a data engineering team that primarily focuses on productionalizing data pipelines that drive our most critical applications. The tech lead position is the glue that makes sure projects get done.

Core Responsibilities:
We are a collaborative and data science/analytics team with diverse backgrounds and experiences; Take a listen first approach but share knowledge and clearly articulate insights and best practices.
Be the liaison between the Scrum Product owner and data engineers translating product requests into Scrum stories.
Build rock solid data pipelines that run 24 hours a day 7 days a week that are easily monitored and maintained by our Dev Ops teams.
Advise implementation teams and data scientists on best practices in data processing.
Clearly articulate your ideas and solutions to both technical and non-technical audiences.
Learn continuously, leveraging IgnitionOne training resources or through self-directed sites.
Qualifications:
Development experience with Big Data/NoSQL platforms, such as Apache Cassandra, DynamoDB, Hortonworks, Redshift or Amazon EMR.
Development experience with traditional relational databases, such as SQL Server, MySQL, or Postgres
Excellent written and verbal communication skills.
Apache Spark development experience using Scala or Python.
Experience with various messaging systems, such as Kafka or Kinesis.
Experience building Dimensional Models.
Implementing ETL processes.
Desired Qualifications:
Experience in Digital Media/Programmatic Advertising a plus.
Previous experience in leading a team of technical staff"
Data Visualization Engineer,Luxoft,"Project Description:
Microsoft Azure provides a comprehensive platform for developers to build, deploy, and scale applications in the cloud. Azure is now being used by over 80% of the Fortune 500 companies and we are adding thousands of new customers per week. New services and features for Azure are being released constantly. With Azure we are building the next major business at Microsoft and it is an exciting time to get involved.

The Azure web team is small and agile team responsible for the Azure.com web site. Azure.com is the front door for new and existing Azure customers. From Azure.com visitors learn and sign-up for Azure, view documentation, calculate pricing, or monitor the status of their services. We operate like a well-funded startup with a constant focus on agility and by helping the Azure business grow customers and revenue. We use the best tools available, release daily, and make data-driven improvements by leveraging A/B tests, user feedback, and analytics.
We are growing our team and looking for several talented developers for Azure.com and some exciting new web properties.

Responsibilities:
1. Creating high quality Power BI reports and dashboards
2. Strong SQL Server and TSQL experience to creating views and functions to support reporting needs
3. Data ingestion and cleansing from a variety of data sources

Skills Required:
BA in Data Science field, preferred
4+ years as a data engineer
4+ years experience using PowerBI"
Azure Data Engineer,Posh Technologies,"Azure Data Engineer

Location: Redmond, WA

Duration: 3-6 Months

Skills Required:
Bachelors degree or higher in a Computer Science or related technical discipline or equivalent experience
Strong experience with Azure; Looking for someone who has experience with cloud features such as data lake, power Bi, containers etc.
Experience on big data technologies like Hadoop
Excellence in Writing SQL queries for data manipulation is a pre-requisite
Solid SQL Server data warehouse and database related Azure services knowledge required"
Big Data Engineer - Quanititative Analysis Development,Universal Consulting Services,"Are you passionate about technology? At UCS, we welcome innovative and cutting-edge thinking. We don't just accept the status-quo, we embrace and encourage your out-of-the-box thinking. We are a pioneering Information Technology firm that prides ourselves on our ability to create lasting, state-of-the-art solutions. We transform the way our customers operate and offer you an opportunity to make a true difference in our Nation. From supporting national defense efforts, to advancing our countries technology infrastructure, you'll be engaged in tackling the challenging issues our Country faces on a daily basis. Help us build groundbreaking solutions and learn what it takes to make your mark on the world. Grow with us and see Intelligence at Work.

UCS is currently seeking a Quantitative Analysis Sr. to implements tools and methods of risk evaluation and monitoring as a member of the Credit Analytics and Reporting team.

Responsibilities include:
 Data Engineering:
o Cleanse, manipulate and analyze large datasets (Structured and Unstructured data - XMLs, JSONs, PDFs) using Hadoop platform.
o Develop PIG, HIVE scripts to filter/map/aggregate data. Scoop to transfer data to and from Hadoop.
o Manage and implement data processes (Data Quality reports)
 Analysis and Modeling:
o Perform R&D and exploratory analysis using statistical techniques and machine learning clustering methods to understand data.
o Develop data profiling, deduping logic, matching logic for analysis
o Big Data languages such as R, Python, Spark for analytics and developing dashboards
 Lead a cooperative effort among members of a project team comprised of IT and Business
 Train and mentor staff on Hadoop, R, Python, Spark and other Big Data languages
 Present ideas and recommendations on Hadoop and other technologies best use to management

Position Requirements
 5+ years of experience in processing large volumes and variety of data (Structured and unstructured data, writing code for parallel processing, XMLS, JSONs, PDFs)
 3+ years of programming experience in at least 2 - R, Python, Spark, Java for data processing and analysis.
 Use of analytical and statistical functions (Standard deviation, decision trees) is preferred.
 Strong SQL experience is a must
 2+ years of experience  using Hadoop platform and performing analysis. Familiarity with Hadoop cluster environment and configurations for resource management for analysis work
 Strong quantitative, analytical, and problem-solving skills
 Detail oriented. Excellent communication skills (verbal and written)
 Must be able to manage multiple priorities and meet deadlines

Location
McLean VA

Full-Time/Part-Time
Full-Time

Exempt/Non-Exempt
Exempt

This position is currently accepting applications."
Data Science Engineer,LendingTree,"COMPANY

LendingTree was founded in 1996 by CEO Doug Lebda to help people comparison shop and get a great deal on the single biggest transaction of their lives: their mortgage. Since then, weve helped over 30 million people in the home buying and refinancing processes while becoming a well-known brand.

The Tree family has expanded to include comparison shopping for all lending categories - mortgage, credit cards, personal loans, auto loans, student loans, and business loans. Our passion is helping consumers comparison shop for lifes biggest financial purchases and make more informed decisions.

LendingTree is publicly traded (TREE) and our stock has delivered impressive performance month after month. If youre looking for an opportunity with a dynamic company that is fanatically pro-consumer and that champions your entrepreneurial spirit, youve come to the right place!

CULTURE

Our clothes are casual and relaxed, but our work ethic is strong. It is our culture for each team member to challenge the status quo, express their opinions, and to stand up, ask for the ball and run with it to meet our aggressive goals. We also have a lot of fun together! Were always looking for the best, brightest, high energy, results-driven Rock Stars to join our team. We reward innovation, creativity and the ability to just GET STUFF DONE.

What youll be doing
Data mining using state of the art methods
Processing, cleansing and verifying the integrity of data used for analysis
Ad-hoc data analysis
Research and develop statistical learning models for data analysis
Collaborate with engineering and business stakeholders to understand company needs and devise possible solutions
Requirements
5+ years of experience with SAS, ETL, data processing, database programming and data analytics
Experience with R or Python
Experience with Big Data technologies such as Hadoop, Apache Spark, Apache Storm, Hydra, etc.
Familiarity with Hive, HBase, Kafka, Nifi, and Spark Streaming
Preferred Experience
Masters degree in computer science, statistics, applied math or related field"
Expert Data Engineer,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities The Data Engineer will work with a team supporting a wide range of activities, including information systems development, integration of scalable solutions using various platforms, and architecting automated and scalable data process monitoring processes.

Qualifications

Required

TS/SCI W POLY

Bachelors degree in computer science, IT or equivalent technical discipline, or approximately 8-10 years work experience in relevant focus areas.

Approximately five years of experience in information systems development, focused on processing large volume, near-real time data feeds to meet data analytics and security requirements.

Demonstrated experience as a SME to prioritize and meet tactical and strategic requirements for frameworks and systems to process data.

Demonstrated experience in information systems development across the IT lifecycle, with a focus on systems to perform high volume and velocity data processing on disparate data types/formats.

Over five years demonstrated experience with delivering integrated and scalable solutions using the LexisNexis, ECL, and Pentaho platforms.

Demonstrated experience with identifying and delivering new, integrated and scalable solutions using current large-scale open-source and GOTS solutions for processing, storing and enabling analytics from high volume and velocity data collections ) on datastores and comprised of billions of records.

Demonstrated experience architecting and delivering automated and scalable data process monitoring and data quality systems and processes.

Desired

Demonstrated expertise delivering solutions to address user requirements and enrich analytic models using common industry tools to perform natural language processing, entity extraction, and data aggregation on human and machine generated data.

Demonstrated work experience delivering solutions applying industry standard Agile methodologies.

Over five years in relevant technologies (current examples include Java, ECL, Pentaho, Oracle, NoSQL, Kafka, FLUME).

Prior experience delivering data processing systems to adhere to Sponsor or IC data handling, tagging, compliance, and security requirements.

Demonstrated work experience in architecting new system solutions using current technologies, as well as transitioning existing systems into modern technologies without negatively impacting operational and compliance requirements.

Demonstrated experience performing data assessment, data engineering, modeling and analytics to enable new methodologies for end user analysts, data scientists, etc.

Experience with data analysis; target needs assessment; systems engineering and integration; data acquisition/distribution/ management/ enhancement; data prioritization; Sigint/Humint Targeting and Analysis."
Data Engineer SME,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities The Data Engineer will work with a team supporting a wide range of activities, including information systems development, integration of scalable solutions using various platforms, and architecting automated and scalable data process monitoring processes.

Qualifications Required

Bachelors degree in computer science, IT or equivalent technical discipline, or approximately 10-12 years work experience in relevant focus areas.

Approximately five years of experience in information systems development, focused on processing large volume, near-real time data feeds to meet data analytics and security requirements.

Demonstrated experience as a SME to prioritize and meet tactical and strategic requirements for frameworks and systems to process data.

Demonstrated experience in information systems development across the IT lifecycle, with a focus on systems to perform high volume and velocity data processing on disparate data types/formats.

Over five years demonstrated experience with delivering integrated and scalable solutions using the LexisNexis, ECL, and Pentaho platforms.

Demonstrated experience with identifying and delivering new, integrated and scalable solutions using current large-scale open-source and GOTS solutions for processing, storing and enabling analytics from high volume and velocity data collections ) on datastores and comprised of billions of records.

Demonstrated experience architecting and delivering automated and scalable data process monitoring and data quality systems and processes.

Desired

Demonstrated expertise delivering solutions to address user requirements and enrich analytic models using common industry tools to perform natural language processing, entity extraction, and data aggregation on human and machine generated data.

Demonstrated work experience delivering solutions applying industry standard Agile methodologies.

Over five years in relevant technologies (current examples include Java, ECL, Pentaho, Oracle, NoSQL, Kafka, FLUME).

Prior experience delivering data processing systems to adhere to Sponsor or IC data handling, tagging, compliance, and security requirements.

Demonstrated work experience in architecting new system solutions using current technologies, as well as transitioning existing systems into modern technologies without negatively impacting operational and compliance requirements.

Demonstrated experience performing data assessment, data engineering, modeling and analytics to enable new methodologies for end user analysts, data scientists, etc.

Experience with data analysis; target needs assessment; systems engineering and integration; data acquisition/distribution/ management/ enhancement; data prioritization; Sigint/Humint Targeting and Analysis."
Data Science Software Engineer,T. Rowe Price,"Our mission as a leading investment management firm is to help our clients achieve their long-term financial goals. We believe our associates are the key to this mission and we are always looking for talented individuals who share our commitment to our clients success.
If youre looking for challenging work experiences and the ability to learn in a collaborative culture, we invite you to explore the opportunities available at T. Rowe Price.
The Global Technology team at T. Rowe Price is playing a key role in helping build the future of financial services, working hand-in-hand with business partners to create client experiences that are changing the way people invest. You will work with smart, talented people across our business.
The Knowledge Services team is looking for an experienced software engineer to join our team in building a suite of next-generation n-tier, native cloud enabled applications. In this role, you will help engineer and develop new capabilities using modern web/cloud technologies used by our Portfolio Managers and Quantitative Analysts.
The ideal candidate is someone who combines ambition with humility and is happy to let their performance do the talking. This is a hands-on, full development lifecycle role which provides an opportunity to solve complex business, logic and technical challenges. As a key contributor throughout the entire development cycle, you will be required to think outside the box and bring cutting edge technology in-house.
Core Capabilities & Attributes:
Exceptional technology skills; recognised by your peers as an expert in your domain
Craftsman-like approach to building software; takes pride in engineering excellence and instils these values within the team and others
Participating in the design of effective engineering processes
Product development expertise
Architecture, craftsmanship & engineering discipline
Domain skills & experience
A proponent of strong collaborative software engineering techniques and methods: agile development, continuous integration, code review or pairing, unit testing, refactoring and related approaches.
Excellent problem-solving and critical-thinking skills; demonstrated ability to employ fact-based decision-making to resolve complex problems, by applying logic analysis, experience and business knowledge
Possess a passion for technology and staying sharp in your craft by keeping on top of new technologies, tools and trends
Ensure and manage excellent customers relationships
Demonstrable passion for technology (e.g. personal projects, open-source involvement) while using their problem solving capabilities to deliver solutions utilizing a top end engineering approach
Responsibilities:
Engineer world-class products with maximum efficiency and agility
Enable improvement of the engineering team through shaping of tools, processes and standards
Interact with Quants and Analysts to understand their workflows and requirements
Collaborate with your engineering manager to enable a fit-for-purpose application portfolio consistent with the target architecture and operating model
Produce comprehensive, usable dataset documentation and metadata
Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers
Ensure excellent customers relationships
Minimum qualifications:
Bachelors or Masters degree in Computer Science, Applied Mathematics, related field, or equivalent practical experience
Demonstrated hands-on software engineering experience
Experience in modern programming languages (C/C++, Java, Python), open-source technologies
Solid Computer Science fundamentals in object-oriented design, data structures, and algorithms
Strong Test-Driven Development and desire to write simple, adaptive and iterative code.
Experience working in a dynamic, fast-paced, Agile team environment
Enthusiasm for learning & results oriented
Strong interpersonal skills; able to establish and maintain a close working relationship with quantitative researchers, analysts, traders and senior business people alike
Preferred qualifications:
Experience building containerized applications and deploying to public or private clouds, such as Amazon Web Services (AWS), Microsoft Azure, or similar providers.
Open source involvement such as a well-curated blog, accepted contribution, or community presence
T. Rowe Price is an Equal Opportunity Employer"
Principal Data Engineer,Aetna,"Principal Data Engineer

Aetnas Data Engineering Team is currently looking for an exceptional Lead Data Engineer to play a pivotal role in in our efforts to re-shape healthcare for America and across the globe. We are developing solutions to improve the quality and affordability of healthcare. What we do will benefit generations to come. Opportunities for this role currently exist in our Hartford, CT; Wellesley, MA or New York, NY offices.

Position Summary: This role will lead key analytics engineering initiatives, including end to end ownership for designing and deploying Big Data products. The ideal candidate will have 10+ years of technical experience with hands-on experience with; Hadoop, developing and deploying cloud native applications. This position is high visibility will need to communicate advanced technical concepts to a broad audience ranging from junior and mid-level engineers to senior levels of management.

Fundamental Components:

 Demonstrates Strong leadership, ability to mentor, effective communication skills.

 Designs and develops complex and large scale data structures and pipelines to organize, collect and standardize data to generate insights and addresses reporting needs.

 Designs and develops native cloud applications leveraging industry best practices

 Uses advanced programming skills in Python, JAVA or any of the major languages to build; robust data pipelines and dynamic systems.

 Uses advanced programming skills to develop restful API services

 Writes complex ETL (Extract / Transform / Load) processes, designs database systems and develops tools for real-time and offline analytic processing.

 Develop frameworks, standards & reference material for architecture and associated products.

 Designs data marts and data models to support Data Science and other internal customers.

 Ability to lead /mentor junior engineers to provide technical advice.

 Applies Aetna systems and products to consult and advise on additional efforts across multiple domains spanning broader enterprise.

 Collaborates with stakeholders to gain a deep understanding of their needs and is able to translate into technical designs/documentation

 Uses in-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines.

 Integrates data from a variety of sources, assuring that they adhere to data quality and accessibility standards.

 Experiments with available tools and advises on new tools in order to determine optimal solution given the problem at hand

Background/Experience:

 10+ years of progressively complex technical experience solving big data problems

 In-depth knowledge of large scale search applications and building high volume data pipelines.

 In-depth knowledge on Hadoop architecture, HDFS commands and experience designing & optimizing queries to build scalable, modular, and efficient data pipelines.

 In-depth knowledge of JAVA and cloud architecture,

 Knowledge of Healthcare is a plus

Education

 Masters degree is a plus

ADDITIONAL JOB INFORMATION
Aetna is about more than just doing a job. This is our opportunity to re-shape healthcare for America and across the globe. We are developing solutions to improve the quality and affordability of healthcare. What we do will benefit generations to come.

We care about each other, our customers and our communities. We are inspired to make a difference, and we are committed to integrity and excellence.

Together we will empower people to live healthier lives.

Aetna is an equal opportunity & affirmative action employer. All qualified applicants will receive consideration for employment regardless of personal characteristics or status. We take affirmative action to recruit, select and develop women, people of color, veterans and individuals with disabilities.

We are a company built on excellence. We have a culture that values growth, achievement and diversity and a workplace where your voice can be heard.

Benefit eligibility may vary by position. Click here to review the benefits associated with this position.

Aetna takes our candidate's data privacy seriously. At no time will any Aetna recruiter or employee request any financial or personal information (Social Security Number, Credit card information for direct deposit, etc.) from you via e-mail. Any requests for information will be discussed prior and will be conducted through a secure website provided by the recruiter. Should you be asked for such information, please notify us immediately."
Hadoop Data Engineer,Rockstar San Diego,"Rockstar San Diego is seeking a Data Engineer with experience in Big Data technologies to join a team focused on building a cutting edge Game Analytics platform and tools to better understand our players and enhance their experience in our games.

The ideal candidate will be skilled in developing complex ingestion and transformation processes with an emphasis on reliability and performance. In collaboration with other data engineers, database administrators and developers, the candidate will empower the team of analysts and data scientists to deliver data driven insights and applications to company stakeholders.
RESPONSIBILITIES
ETL Development  Safely and securely ingesting large volumes of data from high traffic production environments, auxiliary services, and various structured/unstructured data sources into our Hadoop and Vertica ecosystems.
Warehouse Development  Assisting the team in tasks of data modelling on a massive scale, using Big Data tools and platforms to deliver high performance data sets.
Data Quality Management Troubleshooting and correcting quality issues with source data, as well as implementing auditing and alerting procedures to ensure data accuracy.
Operational Support  Assisting in the day to day maintenance of Hadoop and Vertica clusters; assist in troubleshooting and bug fixes for daily processes.
Analysis Support  Collaborating with stakeholders, analysts and data scientists to ensure that high quality data is available to answer any business needs. QUALIFICATIONS
Experience with Hadoop Ecosystem (Map Reduce, Spark, Oozie, Impala, HBase, Scala, etc.).
Experience with Big Data Ecosystems (Kafka, Cassandra, etc.).
2+ years of work experience in a similar position requiring ETL, Data Modeling, and Data Science architectures to be designed and developed.
Deep understanding of data warehouse design and data modelling standards.
Expert in at least one SQL language such as T-SQL or PL/SQL.
Experience in understanding the underlining requirements to translate them into data models.
Experience working in a Linux environment.
Good communication skills.
Dynamic team player.
A passion for technology - we are looking for someone who is keen to leverage their existing skills and seek out new skills and solutions. PREFERRED
Experience with Vertica.
Experience with Python and shell scripting.
Experience with Tableau administration.
Knowledge of the Video Game Industry and video games."
Big Data Analyst / Data Visualization Engineer,Engility,"Engility is seeking qualified Big Data Analysts/Data Visualization Engineers. Positions are available throughout the Northern Virginia area. Duties include but are not limited to:
Transform large amounts of data into compelling visual stories to facilitate customers within the Intelligence Community.
Transform big data and related analytical results into graphical representations that can form a story in a glance. The visualizations will range from line-graph depictions of statistical analyses, or web-enabled interactive graphics driven by SQL and/or NoSQL databases.
Analyze and present information, as well as provide the customer with an understanding of the psychology of visual perception.
Provide foresight on what types of visualization techniques that work best for different types of data, as well as the limitations of certain techniques.
Present data to members of the customer and enable audiences to see threads in a way that allows them to ask better questions, leading to improved strategies, and increased success in various missions.

Typical Duties and Responsibilities:
Must have a current/active TS/SCI with Polygraph.
Experience with SQL and NoSQL databases
Experience with data visualization tools/toolkits (e.g., Pentaho, Tableau, D3, Saiku, ParaView, Protoviz, Maya, 3D Studio, Endeca, Qlikview, etc.)
Demonstrated experience in user experience methods, innovation, ideation, storytelling, prototyping, design frameworks, concept visualization, problem-framing skills and design principles
The ability to design data displays to communicate results simply & accurately
Strong communication skills (both verbal & written)
Experience with any Amazon Web Services (Amazon Simple DB, Amazon S3, Amazon EC2 and/or Amazon SQS)
Experience working with web technologies including: HTML, JavaScript Framework, JQuery, Ajax, Cassandra, Datastax, Fireworks, Adobe Creative Suite (Photoshop & Illustrator)
Experience in developing many different types of visualizations, including visual analytics; real-time visualization for situation awareness; visualizations for interactive data exploration; narrative editorially-guided visualizations; time series analysis methods

Required Qualifications:
Knowledge or experience with PHP, Java, C++, C#, Ruby on Rails"
Data Engineer - Platform,Lenovo,"Lenovos fundamental belief is that life rewards those who Never Stand Still. Every day, every employee at Lenovo is focused on moving forward, rejecting traditional limits, and always seeking a better way. As part of an entrepreneurial team in a growing business, you will help shape the future of Lenovos Cloud Platform and use technology to connect with customers, employees and partners.

As the Data Engineer for Lenovos Cloud Platform, you will own critical pieces of the machine learning and context engine cloud platforms. These systems process millions of data points per day, and you'll be responsible for the data pipeline and its health. You will work closely with data scientists to build the tools and processes necessary to bring research and machine learning models to production as quickly and reliably as possible.

Youll be part of highly dynamic agile software development team and will be involved in the solution design from the conceptual stages through the development cycle and deployments. Be a part of a team working on a high volume, highly available data platform that is critical to the success of the business. Youll be responsible for architecting, developing and extending large and complex systems. Youll own data ingestion services, end to end data pipelines and mapreduce clusters to support development efforts of Lenovo Contextual Engines AI and Cloud platforms.

Responsibilities:
 Design and code solutions on and off databases for ensuring application access to enable data driven decisions.
 Work with product owners to understand desired application capabilities and testing scenarios
 Work within and across Agile teams to design, develop, test, implement, and support technical solutions across a full-stack of development tools and technologies
 Lead the craftsmanship, availability, resilience, and scalability of your solutions on AWS.
 Participate in architecture discussions, influence the roadmap, and take ownership and responsibility over scalability, availability, and performance of the solutions.
 Conduct performance testing and monitoring of production systems
 Review work and assure adherence to best standards and practices
 Recommend ways to improve data reliability, efficiency and quality

Position Requirements
Minimum qualifications:
 Bachelor's degree with minimum 8 years of IT experience
 Minimum 3 years of experience in deployment using Big Data Technologies (such as - MapReduce, Kafka, HBase) in complex large scale environments
 Minimum 3 years of experience in at least 3 of the following: Pig, Sqoop, MapReduce, Kafka, Spark, Java
 At least 3 years experience writing production code with either Java, Scala or Python
 At least 2 year of combined experience with the following technologies: Spark, Scala, Akka, Cassandra, Accumulo, Hbase, Hadoop, HDFS, AVRO, MongoDB, or Mesos
 Experience with databases and data-driven applications
 Experience with replication, backup and restore and disaster recovery

Preferred qualifications:
 MS degree in Computer Science or related software engineering field.
 AWS certifications are a big plus
 Advanced knowledge of Unix/Linux

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class."
Senior Data Engineer,CNA Insurance,"Senior Data Engineer-FIN0001CF
Description

Job Summary
Be a member of an entrepreneurial team within the Property & Casualty Claim department made up of actuarial and data professionals partnering across the Claim organization. Become a CNA Claim data subject matter expert, establish metrics, provide claim data analyses and work directly with Claim professionals in support of Claim business intelligence. The successful candidate will be technically strong and will enhance our analytical capabilities. The position encourages innovation with the broader goal of helping to pioneer a data and analytics culture within Claim. Be a part of the momentum already established and help make a significant impact to the organization.

Essential Duties & Responsibilities
Design, build and own one source of truth claim data marts to facilitate consistency and efficiency in extracting and analyzing claim data across the organization.
Drives accountability for data integrity by developing and executing necessary processes and controls around the flow of data.
Participates in or leads data-related projects
Support Claim initiatives through data analytics by partnering with Claim leaders in identifying opportunities to build and analyze metrics.
Innovate and improve efficiency of managing data to allow for greater speed and accuracy of producing analyses, metrics and insights.
Collaborates with management, technical staff and subject matter advisors to understand business needs/issues, troubleshoot problems, conduct root cause analysis and develop cost effective resolutions for data anomalies.
Participate in data governance initiatives to enhance current systems, ensure development of efficient application systems, influence the development of data policy, and support overall corporate and business goals.
Verifies accuracy of table changes and data transformation processes. Ensures adequacy of test plans and monitors application testing results associated with data transformation processes between applications
Stay up to date on data science trends and developments.
May perform additional duties as assigned.
Reporting Relationship
Data Management Director

Skills, Knowledge & Abilities
Expert SQL skills. Not required, but knowledge of applications such as, Alteryx, Tableau and other business related software a plus.
Strong analytical and problem solving skills and ability to learn programming languages such as R or Python.
Strong verbal and written communication skills including technical topics to non-technical audiences.
Ability to leverage enterprise applications for reporting and process improvement to meet business objectives.
Ability to multitask in a fast-paced dynamic workforce.
Ability to work independently as well as in a team environment.
Possesses some influence management skills.
Readily adapts to change.

Education & Experience
Bachelor's degree in Mathematics, Statistics, or Actuarial Science or equivalent.
Typically a minimum of seven years of related work experience.
LI-PB1
Job

Finance and Accounting
Primary Location

United States-Illinois-Chicago
Organization

CLAIM-Fin&Ops-Services

Job Posting

Nov 27, 2017
Unposting Date

Ongoing"
Internship: Software Engineer 1,Western Digital,":
SanDisk is seeking a talented graduate intern to join the global Software Tools org.. In this role, the intern will be a contributor to the development of engineering tools and services that automate the Firmware Validation workflow, with a particular focus on data collation, analysis, and reporting. She/he will be responsible for researching, architecting, design, prototype and developing Windows desktop tools, WEB tools, and databases used by engineering during product development.

SanDisk offers a highly competitive compensation package and great benefits, which include access to SanDisks onsite fitness center, (Pro-rated) Holiday Pay, employee activities, celebrations and events. SanDisk is an equal opportunity employer.

Requirements:

 Must be a current undergraduate or graduate student, in good standing, working towards BS/MS in Computer Science, Computer Engineering, Electrical Engineering or a related field

 Objected Oriented Design & Implementation

 Experience designing engineering tools that coherently integrate into frameworks that provide a consistent use model

 Software system design

 Strong verbal and written communication skills

 Solid troubleshooting skills

Additional Desired Qualifications/Technical Skills:
Work experience as a software programmer
Projects in data structures, operating systems, cloud computing, big data, in-memory computing and Software Defined Storage

Soft Skills:

The ideal individual must: have proven ability to achieve results in a fast moving and dynamic environment; be self-motivated and self-directed; have demonstrated ability to work well with people; a proven desire to work as a team member, both on the same team and outside of the team; ability to troubleshoot and analyze complex problems; ability to multi-task and meet deadlines; have strong written and verbal communication skills.

SanDisk offers a highly competitive compensation package and great benefits. SanDisk provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, creed, gender, national origin, age, disability, marital or veteran status, sexual orientation, gender identity or expression, or any other legally protected status."
Master Data Engineer,Discover Financial Services,"As a Master Data Engineer, you will provide engineering leadership to create and enhance data solutions enabling seamless integration and flow of data across our data ecosystem. Additionally you will provide senior level technical consulting to peer data engineers during design and development for highly complex and critical data projects. Some of these projects will include designing and developing data ingestion and processing/transformation frameworks leveraging open source tools such as NiFi, Sqoop, Hive, Java, Pig, Python, Spark, etc. Additionally you will work on real time processing solutions using tools such as Apex, Flink, Storm and Kafka. You will deploy application code and analytical models using CI/CD tools and techniques and provide support for deployed data applications and analytical models.

Responsibilities
Develop data driven solutions utilizing current and next generation technologies to meet evolving business needs.
Ability to quickly identify an opportunity and recommend possible technical solutions.
Utilize multiple development languages/tools such as Python, SPARK, HBase, Hive, Microsoft R, Java to build prototypes and evaluate results for effectiveness and feasibility.
Operationalize open source data-analytic tools for enterprise use.
Develop real-time data ingestion and stream-analytic solutions leveraging technologies such as Kafka, Apache Spark, NIFI, Python, HBase and Hadoop.
Custom Data pipeline development (Cloud and locally hosted)
Work heavily within the Hadoop ecosystem and migrate data from Teradata to Hadoop.
Provide support for deployed data applications and analytical models by being a trusted advisor to Data Scientists and other data consumers by identifying data problems and guiding issue resolution with partner Data Engineers and source data providers.
Provide subject matter expertise in the analysis, preparation of specifications and plans for the development of data processes.
Ensure proper data governance policies are followed by implementing or validating Data Lineage, Quality checks, classification, etc.

Skills

Required Skills
Deep understanding of the Hadoop technology stack, preferably the HortonWorks distribution
Experience in migrating ETL processes (not just data) from relational warehouse Databases to Hive
Building custom NiFI processors
Data pipeline development
Experience in developing Python / R applications
Spark application coding in Scala / Python (pySpark)
Deep knowledge and very strong in SQL, and Relational Databases
Strong in Unix / Shell scripting
Experience in creating very efficient HiveQL and SparkQL queries and can educate peers on the topic

Leadership Skills
7+ years of experience of being a lead engineer and able to coach/provide guidance to peer and junior engineers.
Excellent written and verbal communication, presentation and professional speaking skills
Collaborative individual who excels in working within a team and with business partners to identify, develop and deliver innovative data solutions
Influencing skills/ability. Must be able to work with effectively with different levels of management and all business areas.
Ability to demonstrate leadership to managers, and peer level staff.
Ability to build and leverage external relationships.
Decision making abilities while gathering information and then put your decisions into action.
Passionate learner who enjoys education through class room training and self-discovery on a variety of emerging technologies
Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes.

Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes.

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class."
Cloud Data Engineer,Princeton Consulting Group LLC,"A global hedge fund is building a state of the art multi-cloud compute and data platform! Their growing team is searching for a passionate Cloud Data Engineer. This opportunity will be focused on building data pipelines that transform and persist data for various analytics use cases, while ensuring the completeness, consistency, and security of the data. Responsibilities include: -Work with teams to build data catalogs and track lineage-Build solutions for OLTP and OLAP-Design schemas and normalization strategies based on the queries performed-Work closely with software developers, data scientists, and Quantitative analysts-Design and implement highly available, scalable, and encrypted cloud based storage solutions

Skills required: -Building elastically scalable environments that leverage horizontal or vertical scaling-Developing cost optimization using preemptible, spot, or reserved instances-Degree preferred in a STEM or related field
Compensation: 150-200K on base plus bonus/benefits.
Greater NYC candidates only.
Can not sponsor visa's at this time."
Sr. Staff/Staff Engineer - Data Analytics/Machine Learning,Huawei,"Sr. Staff/Staff Engineer - Data Analytics/Machine Learning

Description:

Huawei, North American Research is seeking candidates with strong statistical/machine learning skills for a system engineer and prototype development position as a regular employee. The candidate should be familiar with statistical & machine learning algorithms with strong software skills for prototyping using statistical package and tools such as (distributed/parallel) R, on big data processing platform using MapReduce. This position is based out of Bridgewater, NJ.

Responsibilities:
 Machine learning algorithm assessment, selection and enhancement for the problems being addressed
 Prototyping and validation using statistical and data mining tools, and big data processing platform such as MapReduce.

Requirements
 5 + years of experience in R&D. should be familiar with statistical/machine learning techniques such as feature extraction, clustering, linear regression, logistic regression, forecasting, hypothesis testing, etc.
 An expert with software development experience using big data platform, statistical/data mining tools ( R, MapReduce), etc.
 Candidates should have at least 1 year of tools development experience for a telecom vendor, service provider or a 3rd party analytics tools company
 A plus to be familiar with architectural components involved in the support of 3G/4G Mobile radio access networks/core, OAM, KPI etc.
 Excellent verbal and written communication skill; ability to work well in a team environment.
 PhD in Electrical Engineering, Computer Science or Statistics with strong machine learning background"
"Building Automation Engineer, Data Center Design",Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Facebook is seeking a Building Automation Engineer experienced in the design and construction of Critical Facilities to become part of our Data Center Design Engineering team. Our datacenters are the foundation upon which our software operates. Building and operating reliable and efficient data centers is essential to support the growth of Facebook. The Data Center Design Engineering team collaborates with all key stake-holders to ensure engineering and design of our data centers incorporate considerations from micro-levels (servers and IT equipment design requirements) to macro-levels (mechanical cooling and power distribution options) to ensure maximum efficiency and reliability of our compute infrastructure. This position is full-time and located in Menlo Park, California. The ideal candidate will have strong leadership skills, a proven track record in team management and a strong background in all aspects of Building Automation. A thorough understanding of mechanical systems, electrical systems and IP networking is required. This person shall be detail-oriented, possess strong organizational skills, and be a self-starter that can excel with little direction. Must be willing and available to travel up to 35% of year. Responsibilities
Lead new project control engineering design, quality approval, process control, product evaluation, vendor proposals, evaluate product reliability, automated testing, software, research and development.
Responsible for controls systems strategies, design, specifications, programming, simulation, testing as well as vendor and commissioning oversight.
Lead development and initiate standards and methods for processes, control logic and sequences, testing, evaluation, and integration.
Review BMS change management and provide feedback.
Collaborate with multiple teams across different disciplines.
Drive project schedules while working closely with design and construction to ensure milestones and completion dates are on track.
Travel to datacenter sites for engineering studies, mechanical systems audits, startup testing, and full commissioning, as required.
Travel to various equipment and bench testing as well as remotely located design working sessions.
Respond on an as-needed basis to emergencies.
Minimum Qualifications
6+ years of experience with control system design, development and management.
Work simultaneously on multiple projects, in a team or independently.
Previous experience with managing projects and project teams.
Organizational and communication skills.
Certifications in various control systems.
Knowledge of thermodynamics and psychometrics.
Understanding of electrical systems and schematics. Low voltage and high voltage.
Understanding of BACnet protocol to include IP, ETH and MS/TP.
Understanding of Modbus protocol to include TCP, and RTU.
Ability to establish working relationships and resolve interpersonal conflicts.
Knowledge of Networking Server/Client Workstations.
AutoCAD and VISIO experience.
Preferred Qualifications
Knowledge of BAS DDC and PLC programming language.
N+ certification.
Previous Datacenter/Critical Facilities experience.
IT and server hardware background.
DCIM understanding and working knowledge.
Schneider Electric PLC/Unity Pro experience.
Citect SCADA understanding and working knowledge.
OPC UA understanding and working knowledge."
Software Development Engineer (Data Flow),Tune,"We are looking for a Software Development Engineer to join Tunes small, successful HasOffers Dataflow team. One of the most established engineering groups at TUNE, Dataflow builds and maintains the systems which handle all event measurement, aggregation, data warehousing, and reporting functionality at HasOffers. These high-volume, high-velocity, high-throughput systems are interesting and challenging, and we're looking for the right person to help us retain our top spot in an ever evolving market.

Our system handles tens of billions of events per month. The ideal candidate has a strong history of working on large, distributed systems that deliver predictable performance worldwide, scaling with minimal pain. Have an interesting project on Github? We'd love to see it.

Learn about what it's like to be an engineer at TUNE and check out our annual internal development retreat, Geeks in the Woods .

You'll be responsible for...
Building solutions for consuming and reporting on huge data sets.
Solving hard concurrency and distributed programming problems.
Writing elegant, performant code in a highly available, distributed environment.
Analyzing and optimizing existing code with an eye towards scalability and robustness.
Helping shape our future data management and data warehousing architecture.
Participating in our on-call rotation, along with the rest of our engineers. Though our engineers are occasionally on-call, we're committed to both a healthy work-life balance and to addressing technical debt to keep our nights and weekends worry-free.
What you'll need...
Computer Science or relevant math/science academic background (or truly great experience).
Industry proven software engineering experience (over 3 years of professional experience) with a focus on parallel processing, distributing systems, and large sets of complex data.
Expertise working with programming languages such as Go, Python, Java, PHP or C++.
Ability to actively contribute to scalable solutions on top of platforms such as Kafka, Spark, and AWS. Experience with Hadoop, other cloud platforms welcomed.
Must be comfortable with backend programming as well as working in a Linux/Unix environment.
Extensive knowledge of common design patterns, and how and when to apply them.
Demonstrated ability to communicate and work professionally with teams.


Nice to have:
Expertise or interest in at least one functional language.
What sets TUNE apart...
100% of employees' premiums covered for medical, vision and dental
Paid parental leave: 12 weeks for primary caregiver, 2 weeks for secondary caregiver
4 weeks PTO (increases annually)
401k
Equity (varies by position)
100% commuter transit coverage
TUNE Cares is our philanthropic program supporting education, the environment, diversity, and other important community causes through life enriching experiences
The TUNE House program is designed to promote a collaborative environment for women aligned in their effort to be innovative and extraordinary leaders in the tech industry
A top-notch culinary team serving light breakfast and full lunch.
Dog-friendly offices
Opportunity to strongly impact company strategy and growth
Fun, creative and focused teams committed to learning and problem-solving through collaboration
10 reasons why you should work for TUNE
Endless opportunity for advancement and development in a fast-growth environment.
TUNE is on a mission to make mobile marketing better for everyone. The TUNE Marketing Console empowers marketers to measure their mobile marketing campaigns, gather insight, and engage their users through one, integrated solution. Accuracy, transparency, and efficiency are critical in marketing which is why the TUNE also integrates with and builds technologies (like HasOffers ) that power marketers agencies, ad networks, and other partners.

As the most adopted measurement and attribution platform for the top 100 grossing apps across iOS and Android, TUNE is trusted by brands like Expedia, Sephora, Starbucks, and Amazon, ad platforms like Doubleclick, and AOL, and martech like Google Analytics, Adobe Marketing Cloud, BlueKai and many more.

As the leading performance marketing platform, HasOffers by TUNE  gives thousands of businesses around the world the ability to track and manage their own publisher relationships. The fully-customizable software solution also enables clients to manage campaigns, creatives, invoices, payouts, and more at a detailed levelin real time. For more information, visit www.hasoffers.com.

Headquartered in Seattle, Washington, with over 350 employees in ten offices worldwide, TUNEs solutions are globally recognized as the most innovative, reliable, and best supported in mobile marketing.

Qualified applicants will receive consideration for employment and will not be discriminated against on basis of disability, veteran status, sexual orientation, gender identity, or other legally protected status."
Data Engineer - Hadoop/Spark,Studio Entertainment,"Movies Anywhere seeks a Data Engineer to join a team of seasoned, dedicated technologists solving a range of interesting problems in innovative ways in an exciting and dynamic industry. We are looking for a self-starting engineer who wants to shape the next generation of video consumption applications. Were a casual shop that values passion, community involvement and code that stands out. If you are interested, wed love to hear from you.

The Data Engineer will work in a small team of multi-disciplined technologists developing insights to drive the Business, Marketing and Finance decisions for our next-generation video delivery and consumption platform. We expect you to be up to date on the happening in the data community, passionate about what you do, and connected to the open source community. You will participate in overall system design, developing multi-tiered data solutions emphasizing reuse and good design patterns.

This position is a legal entity of The Walt Disney Studios, an equal opportunity employer.

BS in Computer Science or related field with 2+ years of experience
Kubernetes knowledge
Apache Spark MLlib
Apache Spark GraphX
R
Amazon AWS or other cloud services
Jenkins
Real passion for coding (If you have a Github profile, thats awesome! We would love to check it out!)
Understanding of distributed systems and distributed computation
Working knowledge in at least 2 of: Scala, Java, Python, or Go-Lang
Working knowledge of data Apache Spark ecosystem technologies like Spark, Kafka, Hive, Presto, Oozie, Pig, Hue, Zeppelin
Demonstrated working knowledge of data modeling
Unit, Integration, and Load testing
Developing REST APIs
Git
Ant, Maven, SBT, and/or Gradle
Unix/Linux
Docker containers building and deployment
Excellent communication and collaboration skills
Ability to prioritize tasks, requirements, and complexity
Design, build and implement Hadoop/Spark batch jobs
Build and optimize performance of Spark, Kafka, Cassandra, ELK, and whatever else makes sense for real-time pipelines
Design and architect high quality data-lake, data-warehouse, and data-marts data models
Enable and implement Data Science workflows and advanced machine learning algorithms
Tune and improve search relevancy using ElasticSearch
Build and maintain data pipelines orchestration
Develop and cultivate expertise in current and new technologies and tools
Collaborate with other software engineers and cross-functional teams
Share new ideas with a larger community of highly experienced technologists
Ability to prioritize tasks, requirements, and complexity This position is a legal entity of The Walt Disney Studios, an equal opportunity employer.
500131"
Master Data Engineer,Capital One,"Plano 6 (31066), United States of America, Plano, Texas

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Master Data Engineer

Master Data Engineer position with Capital One Services, LLC in Plano, TX; Multiple positions available:

Lead overall technical design, development, modification, and implementation of software and/or data applications using existing and emerging technology platforms.
Analyze internal user needs and desired results and develop data application solutions with responsibility for the delivery of software and/or data applications.
Coordinate the work of project teams along multiple workstreams.
Develop complex applications using Core Java, Spring, GoLang, Docker, AWS, Chef, Shell Scripts, Javascript, and Git.
Write, design, code, test, implement, and debug programs adhering to corporate requirements and standards.
Perform application testing and validation, programming, and documentation.
Document design decisions and develop software components, adhering to existing process guidelines.
Coordinate software installation and monitor implementation process.
Monitor system performance trends and identify potential issues.
Own change management process for assigned configuration items.
Present complex IT concepts and project metrics to both technical and non-technical audiences.
Identify opportunities for improvement.
This position requires a Masters degree or foreign equivalent in Computer Science, Engineering, Computer Information Systems, or a related technical field and 2 years of experience in job offered or application development. In lieu of a Masters degree or foreign equivalent in Computer Science, Engineering, Computer Information Systems, or a related technical field and 2 years of experience in job offered or application development will accept a Bachelors degree or foreign equivalent in the stated field and 5 years of progressive post-baccalaureate experience as stated. Said or other experience must include at least six months of experience in: presentation of complex IT concepts and project metrics to both technical and non-technical audiences; Core Java; Spring; GoLang; Docker; AWS; Chef; Shell Scripts; Javascript; and Git. All listed qualifications may or may not be acquired concurrently. Must pass companys assessment.

#LI-DNI"
Software Engineer - Data/Back End (TS Only),CCRi,"The Role:

Do you like solving complicated software design puzzles with code that is clean and elegant? CCRi is looking for a brilliant software engineer with a grounding in computer science and software architecture. This is a technical position focused on development. May contain NP-Hard problems. This position requires a TS level clearance.

Our engineers take cutting edge research and build reliable products out of it. You'll help build massively distributed machine-learning systems, like graph-based embedding frameworks or GPU-clustered neural networks. These tools will be used to attack some of today's most pressing problems like relief planning, image processing, and force protection.

The impact youll have:

Responsibilities include, but are not limited to, the following:
Build scalable, efficient and high-performance pipelines/ workflows on Linux browsers that are capable of processing large amounts of batch and real-time data.
Develop or expand existing reporting modules that retrieve high concurrency report content from databases.
Multidisciplinary work supporting real time streams, ETL pipelines, data warehouses and reporting services.
Use Big Data technologies such as Kafka, Data lake on AWS S3, Spark and related technologies to store, move and query data.
Partner with team members to build and release features using CI tools like Git, Jenkins and Maven.
Follow coding best practices  Unit testing, design/code reviews, code coverage, documentation, etc.
Performance analysis and capacity planning for every release.
Work effectively as part of an Agile team.
Bring new and innovative solutions to the table to resolve challenging software issues as they may develop throughout the product life cycle.
What were looking for:

Successful candidates will thrive in a fast-paced environment and demonstrate a record of achievement:
Bachelors or Masters degree in computer science or related field.
7-10 years experience in designing and developing Java/Scala solutions.
Strong experience with SQL and Relational databases.
Experience working with the Agile with Scrum methodology.
Experience with large volume data processing.
Wed also love to see:
Experience with Amazon cloud computing infrastructure (MySQL RDS, Dynamo dB, AWS pipelines, etc.).
Knowledge in MemSQL DB.
Knowledge in Hadoop and big data technologies.
Knowledge in stream processing technologies such as Apache Kafka, Apache Spark.
Familiarity with JSON-RPC.
Requirements
US citizenship required. We do not offer sponsorships
Active TS Clearance required
Bachelors Degree in a technical field
If you apply to this position and are not a US citizen AND do not have an active TS clearance, you will be automatically rejected. This position is for our Charlottesville office only--we do not offer remote and/or contract positions.

In compliance with federal law, all persons hired will be required to verify identity and status as a US citizen, and to complete the required employment eligibility verification document form upon hire. Failure to do so can and will result in dismissal.

Benefits
Casual Work Environment
Intellectually Challenging Work
Health Insurance
Short Term Disability Insurance
Generous Defined Benefit Retirement
Very Flexible Vacation Policy
Want to know more? Check out our recruitment video: https://www.youtube.com/watch?v=W_b2EY1tlRM
Commonwealth Computer Research, Inc. does not discriminate on the basis of race, sex, color, religion, age, national origin, marital status, disability, veteran status, genetic information, sexual orientation, gender identity or any other reason prohibited by law in provision of employment opportunities and benefits."
Software Engineer Data Science Tools,Cloudera,"Job Description:
Cloudera is looking for an experienced software engineer to join the Data Science Tools team and help lead development of Clouderas next-generation machine learning and data science platform. You will be responsible for helping design, build, and deliver a platform that accelerates the work of data scientists and data engineers tackling some of the worlds toughest challenges. This full-stack role is focused on end-user web application development and requires close collaboration backend infrastructure engineers, data scientists, designers, and product management. The role offers significant opportunities for team leadership and product ownership.
Engineering Requirements and Skills
5+ years of general software development experience.
2+ years leading development of a user-facing product.
3+ years experience with JavaScript/Node.js for backend and frontend development.
Expert-level knowledge of modern web application development.
Significant experience designing and building API-first services.
Fluency with one or more client-side JavaScript frameworks (e.g. AngularJS, React).
Strong knowledge of SQL and relational database concepts.
Experience with microservices and container orchestration (Docker, Kubernetes).
Experience with public cloud cloud providers (AWS, GCE, Azure) a plus.
Interest in data science and machine learning.
Self-driven and motivated, with a strong sense of ownership and craftsmanship.
Strong written and verbal communication skills and collaborative mindset.
Experience leading and mentoring development teams and a plus."
Engineer II - Meta Data Team,DISH Network,"DISH is a Fortune 200 company with more than $15 billion in annual revenue that continues to redefine the communications industry. Our legacy is innovation and a willingness to challenge the status quo, including reinventing ourselves. We disrupted the pay-TV industry in the mid-90s with the launch of the DISH satellite TV service, taking on some of the largest U.S. corporations in the process, and grew to be the fourth-largest pay-TV provider. We are doing it again with the first live, internet-delivered TV service  Sling TV  that bucks traditional pay-TV norms and gives consumers a truly new way to access and watch television.

Now we have our sights set on upending the wireless industry and unseating the entrenched incumbent carriers.

We are driven by curiosity, pride, adventure, and a desire to win  its in our DNA. Were looking for people with boundless energy, intelligence, and an overwhelming need to achieve to join our team as we embark on the next chapter of our story.

Opportunity is here. We are DISH.
A successful Software Engineer II will have a Bachelor degree in Computer Science, Software Engineering, Computer Engineering, or Electrical Engineering.

Qualifications: Python experience Experience with the following: MySQL, JavaScript, SDLC, SCRUM Agile Development Ability to multi-task in a changing environment with fluctuating priorities and deadlines Ability to work well individually and as part of the team
1-2 years of Software Engineering experience developing applications under Linux.
Dish Technologies, in Englewood, CO, is seeking a Software Engineer II , who will be maintaining and developing backend applications for the Meta DataTeam.

Primary responsibilities: Design, build, create, and maintain python ETL scripts/services applied to various meta data providers for consumption by dish client services. Implementing Python scripts Developing code based on requirements documents, including development of use cases and test plans Developing strategies and procedures to help the team work more efficiently and effectively Participating in code reviews, brainstorming meetings, and cross training sessions"
Big Data Engineer,Symphony Talent,"Description

Big Data Engineer

Location: New York, New York

About Symphony Talent:
At Symphony Talent, we connect an employers brand with best-fit talent through our integrated cloud-based suite of solutions. Symphony Talents leadership, staff and investors are dedicated to paving the future for forward-thinking organizations focused on sourcing, nurturing and hiring the best-fit talent for great brands.

At Symphony Talent, we respect relationships with one another just as we respect our relationships with our clients, partners and supporters. Integrity is ingrained in our core values and we are committed to an open and honest work culture. Our teams of creative thinkers are inspired to make a unique impact in this often complex market landscape. We are committed to building an organization fueled by the likes and minds of those passionate for this industry.

Symphony Talent is using data to revolutionize and disrupt the online recruitment marketing space. Our Data Management Platform (DMP) is now ingesting thousands of GB's of data from our online marketing sources:

Display Advertising

Social Advertising

Email marketing

Online Job Postings

Career Web Sites

Amongst many other things, Symphony Talent will be using this data to produce the following:

Multi source attribution analytics

Predictive Analytics

Client facing Analytics - via our SaaS portal

Integration with external vendors and clients (Google, Facebook, Twitter, Indeed etc)

About the Role:
We are looking for a ""generalist"" engineer, that will primarily be responsible for collecting, storing, processing, and analyzing huge sets of data. You will also be responsible for integrating the resulting output of the data processing to our internal Analytics Team, our customer facing SaaS analytics platform as well as 3rd party vendors and clients. Over the course of time, you will be the go-to person for all things data related.

Requirements

Responsibilities

Implementing complex, robust ETL pipelines (python, bash, AWS, Hadoop, Redshift)

Writing/Using API's for internal/external data integration and ingestion (Python, java, Node.js)

Building monitoring services

Writing, testing and debugging online tracking tags (Javascript)

Contributing to our client facing SaaS analytics platform (JasperReports Server)

Contributing to our continuous development framework (Github, Jenkins, AWS)

Skills and Qualifications

3+ years of, working with large complex sets of data

Highly proficient in SQL (This is a mandatory requirement)

Competent, working experience with at least 1 of the following languages:

Python

Nodes.js

Javascript

Java

The entire platform runs on AWS (specifically AWS Linux). While its not mandatory to have hands on AWS experience, having solid working knowledge and experience of the *nix OS is mandatory (Shell scripts).

The following hands on experience will be highly desirable:
Hadoop (Hive, Spark, UDF's)

Managing infrastructure in AWS

Building and scaling Machine Learning frameworks

BI tool (JasperReports Server, Tableau, Qlik etc)

Symphony Talent Perks Include:
Competitive compensation

Great benefits package, including a 401(k) plan and unlimited PTO

Open, collaborative culture and flexible work hours

Fun, conveniently located office in Midtown

If this sounds like an exciting next step for your career, wed love to hear from you!

Only candidates with proper permits to work in the United States can be considered. Symphony Talent is an equal opportunity employer M/F/Disability/Veterans and committed to a drug-free workplace."
"Big Data Engineer - Hadoop, Hive, Spark",CVS Health,"Job Description As a Big Data Engineer, you will provide technical expertise and aptitude to Hadoop technologies as they relate to the development of analytics at CVS Health. You will be responsible for the planning and execution of big data analytics, predictive analytics and machine learning initiatives. You will help lead the plan, build, and run states within the Enterprise Analytics Team and act in a lead role driving user story analysis. By creating optimization and stability to the platforms, you will play a key role in the architecture design and data modeling of the platform and analytic applications.

You will engage in solving and supporting real business issues with your Hadoop distributed systems and Open Source framework knowledge. You will perform detailed analysis of business problems and technical environments and use this data in designing the solution and maintaining data architecture. You will focus on creating strategy, researching emerging technology, and applying technology to enable business solutions within the organization. #dce Other responsibilities include: -Designing and developing software applications, testing, and building automation tools.

Designing efficient and robust Hadoop solutions for performance improvement and end-user experiences. -Working in a Hadoop ecosystem implementation/administration, installing software patches along with system upgrades and configuration. -Conducting performance tuning of Hadoop clusters while monitoring and managing Hadoop cluster job performance, capacity forecasting, and security. -Defining compute (Storage & CPU) estimations formula for ELT & Data consumption workloads from reporting tools and ad-hoc users.
Analyzing Big Data Analytic technologies and applications in both business intelligence analysis and new service offerings, adopting and implementing these insights and standard methodologies. Required Qualifications 8+ years experience supporting various enterprise platforms, performance tuning and application performance optimizations including: -5+ years working in Linux servers and platform optimization. -4+ years experience with architecture and implementation of large and highly complex projects. -4+ years with Hadoop Ecosystems, Hive, or Spark.
Preferred Qualifications-Healthcare Domain knowledge. -Understanding of Machine Learning and Artificial Intelligence advanced analytics. -Experience in Data Warehouse and BI Analytics. -Experience with R, Python, Scala, Java/C++, orSQL/RDBM.

Experience with Ambari, Pyspark, Hortonworks Data Platform, Big Data, Isilon, or Kafka. -Experience leading Big Data platform operations. EducationBachelor's Degree.Business OverviewCVS Health, through our unmatched breadth of service offerings, is transforming the delivery of health care services in the U.S. We are an innovative, fast-growing company guided by values that focus on teamwork, integrity and respect for our colleagues and customers.
What are we looking for in our colleagues? We seek fresh ideas, new perspectives, a diversity of experiences, and a dedication to service that will help us better meet the needs of the many people and businesses that rely on us each day. As the nations largest pharmacy health care provider, we offer a wide range of exciting and fulfilling career opportunities across our three business units  MinuteClinic, pharmacy benefit management (PBM) and retail pharmacy. Our energetic and service-oriented colleagues work hard every day to make a positive difference in the lives of our customers."
Software Engineer - Data Services (multiple),Resonate,"Software Engineer  Data Services (multiple positions)
Reston, VA
At Resonate we are working hard to disrupt the marketing and advertising landscape forever. We are replacing the slow, incomplete and siloed marketing research and insight tools of the past with modern technology and machine learning to provide a more accurate, comprehensive and real-time view of the US consumers with integrations across the ecosystem.
We have an excellent engineering culture that focuses on results, values collaboration and loves solving hard problems. We are a team of voracious learners who believe that technology is a journey, not a destination and we actively support ongoing education and experimentation.
If using cutting-edge technologies and transforming an industry sounds interesting to you, then we should talk.
About the Position
As a data services software engineer, you will be working as part of a skilled, collaborative team to jointly design and implement high visibility data services. This is an ideal job if you are an engineer with 3-10 years of experience who wants to be part of an intensely skilled team, values total ownership of your work, and cant imagine a day without coding.
If you are a skilled developer, with professional experience with JVM, Java, MR and distributed systems we want to speak to you! We're looking for a creative, focused, technically curious individual who enjoys both design as well as working hands-on with the code.
Key Responsibilities
Design and develop components of big data processing, as part of an agile/scrum team
Help plan and implement methods to scale our product further
Code features all the way from the application through the Linux OS/shell
Qualifications & Experience Requirements
JVM, Java applications
Distributed System Development for large scale applications
Experience with Cloud Technologies
Experience developing optimum strategies for distributing data over a cluster.
Experience integrating with relational database and NoSQL stores
Preparation of documentation of infrastructure and operational procedures.
Professional experience with NoSQL solutions to solve real-world scaling problems
Any MapReduce/ NoSQL is required, experience specifically with Spark is a strong plus
A detail-oriented and highly communicative personality, with the ability to work independently and as part of a team
Experience with agile methodologies and short release cycles
Strong attention to detail, good work ethic, ability to work on multiple projects simultaneously, and good communication skills
Desired Qualifications & Experience Requirements
Experience with Spring Framework,
Apache Geode, Spark, Kafka
Experience working and implementing machine learning algorithms in the big data environment
Experience in digital media, online advertising, or reporting/analytical applications
Experience working on systems that handle high volumes of data (hundreds of TB)
Experience implementing applications that use extensive statistical or optimization technology
Experience implementing machine learning
Experience with large scale SQL databases is a strong plus
Educational Requirements
Technical Bachelors Degree required, e.g. Comp Sci, Engineering, Math

About Resonate:
Resonate offers the largest proprietary understanding of US consumers. We help B2C marketers understand and engage the right people with the right message with precision and scale. Our SaaS solutions and managed media services accelerate the ability to understand, activate, and measure customers and prospects in real-time, driving successful always on strategy and execution.
Resonate offers a unique opportunity to people who want to grow and shape the future of the industry. Weve been named a Top Workplace by both The Washington Post and VA Business Magazine multiple times over the years. Weve also been recognized for our consistent growth as part of the Inc. 5000 list five years in a row and by industry-leaders for our game-changing approach to understanding people.
Resonate is privately held and backed by Revolution Growth, Greycroft Partners, and iNovia Capital. Resonate is headquartered in Reston, VA with offices in New York City, and Washington, D.C. Be a part of the team that changes the industry!

More Information:
Find out more about our story at www.resonate.com.
Resonate offers a competitive compensation and benefits package."
Senior Data Infrastructure Engineer,Apple,"In this role, youll be working very closely with a small team of engineers and statisticians to design, build, and maintain systems that enable rapid analysis of large datasets.
Key Qualifications
Distributed systems concepts (CAP, Fallacies of Distributed Computing, etc)
Unstructured storage (distributed filesystems, blob storage)
Structured / indexed storage (column stores, faceted search)
Distributed processing models (map/reduce, RPC services)
Distributed schedulers and resource allocators (e.g., Apache Mesos and YARN)
Description
Were seeking candidates who are confident in the systems they build, but humble and cognizant of the limitations of their software and infrastructure. This role also requires great communication skills, as youll contribute to functional specs and design documents to describe the systems you build to coworkers, other teams, and those who join after you. You might ship bugs  but we hope you also employ strategies to reduce risk through thoughtful design, unit + integration tests, stress tests, CI, instrumentation, and monitoring.
Education
BS/MS CS or equivalent experience.
Additional Requirements
We believe that effective systems design should be sympathetic to the underlying hardware. We hope you have some experience with:

Storage fundamentals (disk types and drive layouts, random and sequential IO, compaction)
Compute fundamentals (concurrency models, distributed single-threaded versus coordinated multithreaded scheduling, synchronous and asynchronous IO)
Networking fundamentals (data locality, datacenter network layouts, multi-datacenter systems design)

You might have specific experience with Apache projects like:
HDFS or other distributed file systems
HBase, Cassandra, or similar distributed databases
Kafka or another distributed replicated log
ZooKeeper or a similar coordination system
Mesos, YARN, or other resource allocation and scheduling systems

We also hope you have experience with (or strong interest in learning) modern Java and Python.

We dont expect youre an expert in everything described above, but we do hope you have strong experience in a few areas, and enough curiosity + desire to become skilled in the rest. If this position sounds like a good match for your interests and skills, please consider applying. Youve found a unique team."
Software Engineer  Data,Plaid,"Plaid is lowering the barrier to entry for developers building financial applications. A big part of this entails solving the complex data problems at the core of financial services. The data engineering team is charged with building infrastructure that can handle the millions of users and billions of transactions that touch the Plaid platform. Our job is to better understand behavioral patterns, evaluate risk, and enable developers to move money and data quickly and safely. We work mostly in Scala and Pythonbut we also push production code in Go and Node.js. We're heavy users of Mesos, Docker, and Spark.

What excites us
Production experience building out data systems that make it a breeze to analyze terabytes of data
Experience in Scala, Hadoop, or other data frameworks
A good understanding of core ML principles and a solid understanding of statistics and CS theory
We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Software Engineer (Data Infrastructure),Opendoor,"About Opendoor

At Opendoor, were on a mission to make it simple to buy and sell homes. The traditional process is broken, with an average home taking over 90 days to sell and costing thousands of dollars. We make buying and selling a home stress-free and instant. Weve built an exceptional team, have raised over $300 million from top-notch investors and are growing fast, buying and selling more than $100 million of homes per month.

As a Software Engineer on Data, you will play a critical role in the transformation of the real estate industry by creating high fidelity structured data at scale. Attaining the source of truth for housing data over time is notoriously difficult, and you will need to combine multiple structured data sources with creative unstructured sources to achieve this goal.

Your responsibilities may include:
Using advanced geo data techniques to compute geographical properties of a home
Use image classification techniques to extract information out of photos
Creating a source of truth for real estate data that powers consumer usage as well as ML models
Building Spark ETL pipelines to clean, transform, and resolve data from multiple sources
Applying analysis and data mining techniques to understand data quality
Becoming a domain expert in real-estate through quantitative data analysis as well as interfacing with real estate gurus in local markets
Designing anomaly detection systems to monitor data change and quality loss
Were looking for teammates who have:
Experience building and productionizing end-to-end data pipelines
Ability to write high performance production quality code
Experience in Python, Go, Java, Scala or other equivalent languages
Good understanding of Big Data processing tools, such as Spark, Hadoop, Storm, Kafka
Analytical skills to understand data distributions and anomaly detection
Bonus points:
Experience working with geodata
You love delighting customers with honest, transparent products and experiences
Want to learn more about the work we are doing ? Check out our blog:
https://medium.com/opendoor-labs

Opendoor values Openness

Our team celebrates our diverse backgrounds, being open about who we are and what we do allows us to be better. Individuals seeking employment at Opendoor are considered without regards to race, color, religion, national origin, age, sex, marital status, ancestry, physical or mental disability, veteran status, sexual orientation, gender identity or other protected status under all applicable laws, regulations, and ordinances.

This position is for employment at will and not for any definite time period."
Software Engineer - Data,Wish,"At Wish, data is central to the company and our users. For merchants, we offer data tools and dashboards that display performance metrics. For users, we offer metadata around products, such as ratings and shipping estimates, that allow them to make an informed purchase decision. Internally, we use data to help guide internal decisions.

We're looking for data engineers to help build data pipelines that power product features, and our internal data warehouse.

What You'll Do:
Build systems that track metadata around merchants, and uses this data to determine rewards and infractions
Create merchant facing dashboards that show sales and customer service performance
Modeling and understanding key processes that happen on the Wish Platform
Build ETLs to our Redshift data warehouse
Build alert and monitoring into our job workflow engine
Who you are
CS/Math/Stats degree from a top university
Strong CS fundamentals
Fluency in SQL
Experience with NoSQL (MongoDB/Cassandra)
Experience in big data systems (MPP/Hadoop stack/Spark/Druid)
Experience with dimensional data warehouses
Experience with streaming data infrastructure (Kinesis/Kafka)
Experience in one of these subject areas: data visualization, statistical/machine learning modeling, analytics & reporting
Company Description:

Wish , its a digital mall, right in your pocket! Using our cutting edge recommendation system, we create a personalized shopping experience for each of our users. Its a digital mall, right in your pocket!

Wish was launched in November 2011 and already has over 200 million registered users, making it one of the top 5 mobile shopping applications.

Learn more about us:
- Recode
- TechCrunch"
Database Engineer,i360,"i360, the leading data and technology provider for the pro-free market public policy and advocacy community, supports organizations that promote free enterprise and smaller government. i360 is a dynamic workplace sitting on the leading edge of public policy, technology, and business, and is seeking team members who are as excited about their cause as they are about building the next generation of political technology.

i360 is seeking a Database Engineer to join its data operations team. With your technical expertise, you will design, implement and improve processes, procedures and automation for all database-centric areas. You will maintain our relational database systems and NoSQL systems for performance and reliability. You are responsible for building tools and scripts to monitor, troubleshoot and automate our systems. You propose test plans and interface with other teams, developers and application owners to arrive at optimal solutions. Successful candidates will solve problems unique in scale and concept in the pursuit of new and original features. So, bring your ingenious mind, great team spirit and excellent communication skills to this great opportunity at i360.

To be considered, candidates must be creative, passionate about process improvement, and must possess strong technical ability in the following areas:
Understanding of Cloud technologies such as architecting, developing or maintaining cloud solutions in cloud environments (e.g. Google Cloud Platform or Amazon Web Services).
Knowledgeable on NoSQL and/or RDBMS platforms (SQL Server, Cassandra, Couchbase, MongoDB).

A Day In The Life Typically Includes:
Provide assistance and work with Senior Database Engineers in building and testing SQL and NoSQL databases.
Assist in determining business requirements and planning and deploying test and production systems.
Perform scalability, latency, and availability tests on a regular basis.
Interface with other teams to resolve problems with application/database systems.
Perform code review and QA data imported by various processes.
Investigate, analyze, correct and document reported data defects.
Create and maintain technical specification documentation.

What You Will Need:

Basic Qualifications:
BA/BS degree in Computer Science, Computer Engineering or Statistics or higher

What Will Put You Ahead?

Preferred Qualifications:
Familiarity with Hadoop, Spark, Kafka, and/or Elastic Search

Working knowledge of UNIX/Linux platforms

Working knowledge of Application development using C#/.NET framework"
Big Data Engineer,Clarivate Analytics,"Summary:
We are looking for a high energy and innovative individual with a passion for software development to join our team. The individual selected for this position will have an active role in the development or migration of large-scale, enterprise-wide initiatives to build a platform or custom applications that will be used for the acquisition, transformation, entity extraction, mining of content on behalf of business units across Clarivate Analytics. This individual will contribute to solutions across the entire architecture stack in building cloud native applications using the current technologies including Java, Spark, Hive, Hadoop, JSON, XML, Micro Services, PostgreSQL, GreenPlum and NoSQL databases. This role will have the opportunity to work across a global footprint and with different business units.
Key Responsibilities
Technical
This individual will work closely with users, technical leads, project managers, and other cross-functional development teams to design and deliver software solutions. This includes writing and designing software, making modifications to existing software and integrating applications within the core pathway.
Expertise in Big Data design and development.
Experience working with all aspects of distributed processing, including Hadoop, Hive, Apache Spark and Cassandra.
Active coding in key technologies is required for this development position and includes: Java, Spark, Git, Linux, AWS, PostgreSQL, and GreenPlum.
Develops cloud native applications, including systems integration.
The individual will also be responsible for applying working knowledge of OO design patterns to develop enterprise level and cloud native solutions.
This individual must develop software using disciplined software development processes, adhering to team/company standards and software best practice guidelines, and corporate policies.
Quality
Delivering quality of the software to our QA team is an important function of this position and this individual will be responsible for creating and executing unit tests on the code that they write for their applications.
Project/Team
This team works in an Agile/Scrum development environment.
Qualifications
Candidates must have a bachelors degree in Computer Science or related field.
Experience
8+ proven experience in a Big Data environment
Demonstrable experience in OO design and development role on a medium to large, enterprise-scale software program.
Hands-on leadership of Big Data project design and implementation.
Candidates must be able to demonstrate experience in the following:
Java design and micro services development
Working with RDMS and NoSQL databases.
Web application servers (such as Tomcat)
Oracle / PostgreSQL / MySQL / GreenPlum
Linux, AWS
Xpath, parsing and related XML processing tools"
"Data Engineer - Agriculture Commodity Research Engine (ACRE), New Ventures",McKinsey & Company,"Qualifications
Bachelor's degree in Computer Science, Geography, Geospatial Science, or related subject
Strong command of English language (both verbal and written)
Strong programming skills
Experience working with geospatial data, especially within PostGIS
Experience with SQL, PostgreSQL, UNIX shell scripting, Python
Experienced with data modeling, design patterns, building highly scalable and secured solutions
Knowledge of agile software development process and familiarity with performance metric tools
Knowledge of web application development technologies such as Ruby on Rails, Java, UNIX, HTML, CSS, Perl, or PHP a plus
Demonstrate enthusiasm, influence and innovation
Strong analytical and problem-solving skills paired with the ability to develop creative and efficient solutions; tolerance in dealing with bad quality data

Who You'll Work With Youll work with our Agriculture Commodity Research Engine (ACRE) team in Brussels. ACRE is part of McKinseys New Ventures.

The ACRE team is part of the Agriculture Practice and it applies advanced analytics and big data techniques to global agricultural markets, driving insights at the micro and macro levels. Our Agriculture Practice advises agribusiness, consumer food, government, and investor clients on strategic and operations issues, helping support industry-shaping decisions that impact the future of global food production.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for  and expands our investment in  our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and were hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.

What You'll Do You will work side-by-side with experienced agribusiness professionals and geospatial data scientists to innovate the industry from farm to cloud.

You will deploy creative agricultural analytics in cloud computer settings and participate in building scalable and deployable analytical solutions. You will sharpen your programming and statistical abilities while quickly acquiring knowledge in a variety of topics of key import to our agribusiness clientele.

As a member of client engagement teams, you will leverage your creativity and problem solving skills to tackle clients company-level issues. These issues could include predicting future market price trends, determining the value of new technologies, incorporating market and crop data to predict trade flows, or optimizing agricultural operations in developing economies. You will programmatize data ingestion and munging capabilities to streamline advanced insight into these issues and learn to effectively communicate them to executive audiences.

Along the way, you will receive best-in-class training in structuring business problems and serving as a client adviser. You will be responsible for providing conceptual insights, problem structuring, modeling, and other analytic support to client engagement teams. In addition, you will help codify the knowledge that you build into interactive data tools, with the help of our data sciences teams, by designing new interfaces to deliver faster, more impactful insights to our clients related to the knowledge area that you will champion.

You will develop and continuously deepen your expertise in agriculture through this role, specializing in agriculture and developing your problem-solving skills as well as teamwork and leadership skills. You will have opportunities to work closely with and learn from our senior agriculture practitioners, and work directly with industry players that are shaping the future of food production."
Software Engineer/Data Infrastructure,Apple,"Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.
Key Qualifications
8+ years of experience in software engineering.
Hands-on experience in software development methodologies and working knowledge in one or more programming languages (Java, C/C++, Python, Go).
Strong knowledge of algorithms, data structures and distributed systems
Demonstrated track record of creating and building instrumentation for processing/assessing/tracking network performance.
Impressive track record of being able to deliver on complex initiatives, solid project management skills and attention to detail.
Flexibility and comfort working on a dynamic, fast-growing effort with minimal documentation and process.
Quick learner. Aptitude to deal with ambiguity, and enthusiasm to help solve difficult issues.
Exceptional communication skills, for collaborating across many participating teams.

Nice to have, but not required:
Familiarity with the Networking protocols (BGP, MPLS)
Network monitoring protocols (Netflow, SNMP etc)
Big data pipeline (Kafka, Spark)
Distributed database (ElasticSearch)
Description
Implement network measurement and analytics system for analysis of network performance, failures, errors, health. The position requires building systems to ingest and analyze data from switchers/routers, designing models to summarize data, building complex queries and statistical algorithms to build insights and create dashboard using modern visualization tools.
Education
Master/PhD in Engineering or Computer Science."
Data Engineer Consultant,Slalom Consulting,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of whats possibletogether.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to 4,000+ employees. We were named one of Fortunes 100 Best Companies to Work For in 2017 and are regularly recognized by our employees as a best place to work. You can find us in 25 cities across the U.S., U.K., and Canada.

Job Title: Data Engineer Consultant

As a Data Engineer for Slalom Consulting, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure using core data warehousing tools, Hadoop, Spark, Event Stream platforms, and other big data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.

Qualifications:

 Bachelors degree in Computer Engineering, Computer Science or related discipline

 Minimum 5-7 years relevant experience

 Understanding of distributed systems and architecture design trade-offs

 Ability to solution for big and small data problems

 Understand different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)

 4+ years of experience working with SQL

 Experience with setting up and operating ETL pipelines using Python or Scala or SQL

 2 years of experience on AWS or similar Cloud platform

 2-3 years of experience working with big data technologies such as Hadoop, MapReduce, Apache Spark preferably on the cloud

 Experience working with relational databases

 Strong analytical problem-solving ability

 Good presentation skills

 Good written and verbal communication skills

 Self-starter with the ability to work independently or as part of a project team

 Capability to conduct performance analysis, troubleshooting and remediation

Nice to have:

 Experience working with streaming technologies such a Kafka, Kinesis

 Experience working with MPP databases such as Teradata, Netezza, Redshift

 Experience with NoSQL Databases

 Exposure to ETL tools such as Informatica, Talend etc.

 Development experience with reporting tools such as Tableau, Qlik, Looker

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Mcity Data Engineer,University of Michigan,"How to Apply A cover letter is required for consideration for this position and should be attached as the first page of your resume. The cover letter should address your specific interest in the position and outline skills and experience that directly relate to this position.

Job Summary Mcity is a public-private partnership formed at U-M to transform global mobility by dramatically improving transportation safety, sustainability, and accessibility. Mcity draws on the broad expertise of U-M and its partners to address the host of legal, political, social, regulatory, economic, urban planning, and business issues that must be addressed to devise and implement practical systems that take advantage of emerging technologies. Mcity also provides testing at our one-of-a-kind proving ground, simulating the complexities of an urban environment, and through on-road vehicle deployments in Ann Arbor and Southeast Michigan, working with our partners including the U-M Transportation Research Institute.

We are looking for a seasoned data engineer to work on all layers of the data architecture for Mcity. If you enjoy building simple, intuitive data products for non-technical audiences, and have hands-on experience with multi-tenant, multi-terabyte production deployments, consider joining us.

Responsibilities* Mcity runs several laboratories and C/AV (Connected / Automated Vehicle) research programs. As part of this we collect, store, classify, and assist with the analysis of a wide variety of data, working closely with our industry members and university researchers.

Depending on your skills and interests, you will have the opportunity to learn, build and scale all the components of our data architecture, including:
Batch data ingestion
Streaming data ingestion
Master data layer (metadata + distributed file system)
Analytics layer (data warehousing for reporting and ad hoc query)
Integrations layer (for downstream processing and consumption)
SPECIFIC RESPONSIBILITIES

As a member of the Mcity engineering team, your typical week will include:
Functioning as the technical project manager, coordinating work between various U-M resources and Mcity staff
Designing and building new data projects, and improving existing data projects, usually in collaboration with the lab director and other engineers
Quiet time for coding, query, analysis
Our daily team stand-up (we are a small personable team)
Discussion of requirements with the team and story creation for them
Interaction with customers  this includes industry members as well as the U-M research community
A chance to learn something new and share your knowledge with others, serving in a mentorship capacity for student and junior team members
Required Qualifications*
Bachelors degree in engineering, with at least 4 years of relevant experience building similar types of data systems
Excellent communication skills
Experience with high-volume, sensor-based data collection, processing, warehousing, and analysis
Experience with RDBMS, front-ends for ad-hoc querying and data exploration ?
Desired Qualifications*
Knowledge of connected and/or automated vehicle technologies
Knowledge of vehicle data acquisition systems
Background Screening The University of Michigan conducts background checks on all job candidates upon acceptance of a contingent offer and may use a third party administrator to conduct background checks. Background checks will be performed in compliance with the Fair Credit Reporting Act.

Application Deadline Job openings are posted for a minimum of seven calendar days. This job may be removed from posting boards and filled anytime after the minimum posting period has ended."
Database Engineer,WisdomTree,"The Database Engineer role, as a member of the Technology team, will report to the Data Architect with the goal of supporting the firms progress towards simplifying and streamlining how it consumes, manages, and distributes information across its growing data universe. Central to these efforts is a three-pronged approach to database stewardship and governance:
Fielding and satisfying the inflow of Ad-Hoc requests of varying size, scope and frequency that originate from all functional areas of the firm
Developing and implementing sustainable data-driven solutions relevant to projects at the department level
Implementing a full-stack approach to each solution with the entire application life-cycle in mind (work Upstream, think Downstream)
Responsibilities
Satisfy daily ad-hoc requests out of the databases by writing and optimizing T-SQL to ensure quick turnaround
Facilitate the smooth transition of Research projects from dev to production using SSIS, Control-M and VRS
Liaise with the Research team to build solutions slated for daily or monthly automation.
Coordinate with Operations and Marketing across divisions to implement website data updates and enhancements
Facilitate timely delivery of monthly and quarterly reporting suite (VRS)
Assume various DBA responsibilities to serve an expanding user base (permissions, backups, transactional log jams)
Act as point person for nightly entity batch (Fundamentals, Benchmarks, Performance, Lipper)
Continue to build and deploy new SSIS-base d solutions devoted to automating daily business operations
Skills
3-5 years of experience; financial services experience preferred
Bachelor's Degree required
Proven experience in T-SQL, Python, Mi crosoft Azure, SSIS, and MS SQL Server 2014
Experience with Panda, NumPy , Git, ETL, R , and MongoDB a plus
Coachability and receptiveness to direction
Familiarity with unstructured databases
Positive and open-minded approach to problem-solving
Love of learning and a desire for constant self-improvement
Ability to navigate ambiguity"
Software Engineer - Data Mining - NLP,Yelp,"Messaging is the next frontier in Yelps mission to connect everyone in the world with great local businesses. Were building the experiences and infrastructure that enable users to get quotes from local contractors, request their haircut appointment next Tuesday, or book a birthday event for someone special. Yelpers love being able to contact several businesses at once straight from the app, and business owners are thrilled to hear from them.

Our Messaging Intelligence engineers strive to optimize the business recommendations that strike the ideal balance between user interest, advertiser return on investment, and income for Yelp. They banter about Bandits, know their way down a Gradient, and arent too Nave to kick back in our Bay(es) Area offices. We love new ideas and collaboration: dont come here with any Prior prejudices or you will Maximize your Likelihood of getting your Posterior kicked.

Yelp Messaging has seen explosive growth in the last twelve months, and we want you to help us make Yelp the worlds most intelligent messaging platform! We are an agile team that creates an amazing experience for millions of Yelpers. We are passionate engineers, sure, but we are also fire spinners, roller girls, pilots, bakers, cartwheelers, and makers (of burrito bombers and 3D-printed trophies). Join our world-class team and be one of the personalities that make the Yelp culture awesome.

What You Will Do:
Work with the team to come up with creative ideas for new products and signals, then carry those ideas all the way to launch.
Explore data to measure the impact of your changes and come up with ideas for how to maximize that impact.
Work closely with infrastructure engineers to make sure that your algorithms will effectively scale to large data sets and to ensure that the underlying system provides the flexibility needed for your ideas.
Mentor other engineers who are ramping up.
Continue to learn, grow, and help us tackle brand new challenges.
Be a key part of the larger Machine Learning community at Yelp.
We Are Looking For:
A minimum of 2-years of industry experience in a machine learning, NLP, statistics, or related field.
A passion for big data and creative ideas for what to do with it.
The algorithms and data structures experience to make your ideas workable.
The coding experience to turn those ideas into reality. We use Python. You dont need to be an expert, but experience is a plus and we will expect you to learn it on the job.
Minimum BA/BS degree in Computer Science, Math, or related field.
If you don't have at least two years of experience in a similar role please take a look at our College Engineering roles instead!
A real passion to make the business-to-consumer messaging experience THE BEST!


Pluses:
Industry experience with search or ads ranking, online auctions or auction bidding strategies, text analysis or auto complete, messaging or chatbots.
You quickly pick up new technologies, tools and platforms.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

*LI-MS1"
Software Engineer - Data Mining (Ad Autobidding Team),Yelp,"Our Ads engineers build data systems that pick a billion ads to show every month. They banter about Bandits, know their way down a Gradient, and arent too Nave to kick back in our Bay(es) Area offices. We love new ideas and collaboration: dont come here with any Prior prejudices or you will Maximize your Likelihood of getting your Posterior kicked.

Our Autobidding teams mission: find every last efficiency in our ad market. A coffee shop in the small town doesnt have much competition? Lets find a way to fit more clicks into their budget. Competition among movers heating up in the big city? Time to figure out which niche works best for each challenger. We do that millions of times per month as the competitive landscape shifts.

Our teams eat data for breakfast, chew it up, and spit out useful products. Saddle up and get on board!

What You Will Do:
Work with the team to come up with creative ideas for new approaches and signals, then carry those ideas all the way to launch.
Explore data to measure the impact of your changes and come up with ideas for how to maximize that impact.
Work closely with infrastructure engineers to make sure that your algorithms will effectively scale to large data sets and to ensure that their systems provide the flexibility needed for your ideas.
We Are Looking For:
3+ years industry experience in machine learning, statistics, or related fields.
A passion for big data and creative ideas for what to do with it.
The algorithms and data structures experience to make your ideas workable.
The coding experience to turn those ideas into reality. We use Java and Python. You dont need to be an expert, but experience is a plus and we will expect you to learn them on the job.
Minimum BA/BS degree in Computer Science, Math, or related degree.
A love of delighting people with local knowledge.
Pluses:
Experience with online auctions, auction bidding strategies, operations research, control theory, distributed algorithms, or mathematical optimization.
Experience working with Hadoop, Spark, or mrjob.
Masters degree, PhD, or additional years of experience.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.

*LI-MS1"
"Data Engineer @ Cyence in San Mateo, CA","Guidewire Software, Inc.","About Cyence
Guidewires Cyence Risk Analytics products help the property & casualty (P&C) industry to model new and evolving risks such as cyber. By combining internet-scale data listening, adaptive machine learning, and insurance risk modeling, Cyence Risk Analytics provides insights that help P&C customers face new risks, take advantage of new opportunities, and develop new products. To learn more about Cyence Risk Analytics, please visit www.cyence.net .

Responsibilities
Develop software to perform large-scale data processing and analytics
Build Cloud-based infrastructure to automate and scale data processing tasks
Contribute to the design and architecture of innovative solutions to difficult problems
Actively participate in a fast-paced team-based Agile environment including daily standups and sprint planning meetings

Requirements
Bachelors or Masters Degree in Computer Science or equivalent
Passion for solving difficult problems involving data and mathematics
3+ years experience in a data engineering or other equivalent role
Willingness to learn new technologies, be nimble and adaptable
Excellent programming skills in Java (or other OO languages) coupled with an understanding of software design patterns and good engineering practices
Solid proficiency in one or more scripting languages such as Python or Perl
Expert knowledge of databases and SQL, including tuning and schema design
Familiarity with open source data processing frameworks such as Hadoop (MapReduce, Hive, Pig), Apache Spark, NoSQL (MongoDB, Cassandra, HBase)

Preferred
Experience with ETL (hand-coded or using commercial/open source tools)
Experience with data mining and machine learning
Experience with statistical languages such as R and MATLAB
Experience with Cloud infrastructure such as AWS or Azure"
Big Data Engineer,Coso IT,"You will help lead the plan, build, and run states within the Enterprise Analytics Team and act in a lead role driving user story analysis. By creating optimization and stability to the platforms, you will play a key role in the architecture design and data modeling of the platform and analytic applications. You will engage in solving and supporting real business issues with your Hadoop distributed systems and Open Source framework knowledge.

You will perform detailed analysis of business problems and technical environments and use this data in designing the solution and maintaining data architecture. You will focus on creating strategy, researching emerging technology, and applying technology to enable business solutions within the organization.

Other responsibilities of the Big Data Engineer include:
Designing and developing software applications, testing, and building automation tools.
Designing efficient and robust Hadoop solutions for performance improvement and end-user experiences.
Working in a Hadoop ecosystem implementation/administration, installing software patches along with system upgrades and configuration.
Conducting performance tuning of Hadoop clusters while monitoring and managing Hadoop cluster job performance, capacity forecasting, and security.
Defining compute (Storage & CPU) estimations formula for ELT & Data consumption workloads from reporting tools and Ad-hoc users.
Analyzing Big Data Analytic technologies and applications in both business intelligence analysis and new service offerings, adopting and implementing these insights and standard methodologies.
Other responsibilities of the Big Data Engineer include:

Required Qualifications:
8+ years' experience with Big Data platforms and tools and Hadoop implementation experience including the following:
5+ years' experience in the transformation and delivery in Hadoop ecosystem (Hadoop, Pig, Hive, Flume, Ozie, Avro, YARN, Kafka, Storm and Apache Ni-Fi).
5+ years'experience in architecture and implementation of large and highly complex projects using Hortonworks (Hadoop Distributed File System) with Isilon commodity hardware.
5+ years' experience in Unix shell scripting.
3+ years' experience using different Open Source tools to architect highly scalable distributed systems.
Preferred Qualifications:
Experience with Hadoop Ecosystems and Isilon Storage.
Understanding of Machine Learning and Artificial Intelligence advanced analytics.
Experience in Data Warehouse and BI Analytics.
Experience with Spark, R, Python, Scala, Java/C++, and SQL/RDBM."
Software Engineer  File Systems (New College Grad),DELL,"Why Work at Dell?
Endless challenges and rewards. Opportunities on six continents. A team of colleagues fueled by collaboration. All this, and a company deeply committed to integrity and responsibility.
Dell is a collective of customer-obsessed, industry-leading visionaries. At our core is a commitment to diversity, sustainability and our communities. We believe in working together to build a brighter future, celebrating the entrepreneurial spirit that lives inside us all, and marrying innovation with action. Dell is proud to offer unparalleled growth and development opportunities for our team members. We believe that technology is essential for driving human progress, and were committed to providing that technology to people and organizations everywhere, so they can transform the way they work and live.

We are offering Software Engineering opportunities in Santa Clara. We offer relocation and competitive pay!

PRINCIPAL DUTIES AND RESPONSIBILITIES :
We are looking for candidates to develop Dells cloud data protection products. The engineer will help take a product through multiple phases of the product Life Cycle from translating requirements into viable products with test plans and test automation.

Design, implement, and execute test suites for large and complex modules within the File System areas.
Verify the product for physical as well as cloud and virtualization environments.
Deliver test strategy and test plans.
Participate in all phases of the development cycle.
Serve as a customer advocate, and represent customer point of view, use cases, etc.
Interact with customers to help understand requirements as well as troubleshoot and solve complex problems.
Interface with other functional areas such as Development, Support, and Product Management.
Develop and review automation suite for continuous and regression testing.

The ideal candidate demonstrates willingness to learn, takes initiative, is curious and innovative, possesses broad technical skills, can budget time well, has the ability to troubleshoot issues, and is a team player.

SKILLS AND EXPERIENCE:
Broad knowledge of Linux/Unix operating system technology and experience with contemporary software engineering principles.
Extensive experience in software testing in systems area, especially storage or file systems.
Excellent programming skills in scripting languages such as perl/python/ruby.
Knowledge of Cloud Storage and/or virtualization is a big plus.
Possess strong storage product, technology, industry knowledge.
Knowledge of File Systems, Log structured File Systems, storage systems, a plus.
Excellent problem solving skills, and ability to debug and triage issues.
Understanding of backup, data protection, disaster recovery products
Knowledge of performance analysis tools and techniques for multi-threaded systems.
Strong problem solving, troubleshooting, diagnostic skills
Strong analytical skills with the ability to present analysis in a consumable format (based on different audience)
Strong communication skills are required for interaction with team members, senior members of the staff and other organizations.

Education:
Candidate should be working towards a Master or Bachelors in Computer Science or equivalent

Dell is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Dell are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Dell will not tolerate discrimination or harassment based on any of these characteristics. Learn more about Diversity and Inclusion at Dell here .

LIPriority"
Cloud Data Engineer,Slalom Consulting,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of whats possibletogether. Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 4,500 employees. We were named one of Fortunes 100 Best Companies to Work For in 2017 and are regularly recognized by our employees as a best place to work. You can find us in 25 cities across the U.S., U.K., and Canada.

Job Title: Cloud Data Engineer

We are hiring a Cloud Engineer to join our Information Management and Analytics practice. As a Cloud Data Engineer on our team, you will analyze, design and architect cloud based solutions to address our clients needs for infrastructure-as-a-service, platform-as-a-service, and software-as-a-service. We are looking for sharp, disciplined, and self-motivated individuals who have a passion for utilizing the cloud solutions from Amazon Web Services, Microsoft Azure, and Google Cloud Platform to solve real business problems for our customers.

Responsibilities:
Work as part of a team, to design and develop cloud data solutions
Gather technical requirements, assess client capabilities and analyze findings to provide appropriate cloud solution recommendations and adoption strategy
Define Cloud Data strategies, including designing multi-phased implementation roadmaps
Lead analysis, architecture, design, and development of data warehouse and business intelligence solutions
Serve as a technology leader designing and developing cloud and hybrid data architecture solutions as part of large scale cloud transformations
Research, analyze, recommend and select technical approaches for solving difficult and challenging development and integration problems
Learn and adopt new tools and techniques to increase performance, automation, and scalability
Mentor other Slalom team members
Assist business development teams with pre-sales activities and RFPs
Understand business goals and drivers and translate those into an appropriate technical solution
Provide technical direction and oversight to cloud implementation teams
Qualifications:
3+ years experience with data integration, data modeling, and data schema design
B.S. in Computer Science, related fields or commensurate work experience
Strong SQL experience
Strong experience in building large-scale batch and real-time data pipelines with data integration tools and data processing frameworks
Knowledge of at least one scripting or programming language such as Python or Java
Understanding of REST APIs, JSON, and XML
Deep product knowledge and understanding of Microsoft Azure, AWS, and Google Cloud
Strong analytical problem-solving ability
Self-starter with the ability to work independently or as part of a project team
Capability to perform Performance analysis, troubleshooting, and remediation
Understanding of cloud ecosystem and leading edge cloud emerging technologies
Preferred:
2+ years architecting and implementing Google Cloud, Microsoft Azure, or Amazon Web Services infrastructure
Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Operations Research / Data Science Engineer,IBM,"Job Description
We are in a data science renaissance.

Companies that embrace data science will lead and those who do not will fall behind.

To help IBM's clients lead, we are building an elite team of data science practitioners to help them learn how to succeed with data science. The team will include data engineers, machine learning engineers , operations research / optimization engineers and data journalists.

The team will engage directly in solving real-world data science problems in a wide array of industries around the globe with IBM clients and internally to IBM. The elite team of data scientist will work with other IBMers and client data science teams to solve problems in banking, insurance, health care, manufacturing, oil & gas and automotive industries, to name a few. We will teach the data scientists and sometimes people who desire to be data scientist to:

Key Responsibilities:
1. Identify a use case
2. Break that use case down into discrete MVPs (minimal viable product)
3. Work in code notebooks
4. Build & validate models
5. Deploy models via APIs into applications or workflows
6. Monitor & retrain models
7. Use code repositories to version and share code/notebooks
8. Visualize the output of their data story in a way that is consumable by all
9. Build visual scenario comparison prototypes for ""before"" vs ""after"" KPI comparison, specifically for optimization-based decision-making applications
10. Communicate effectively with line-of-business end-users to discover pain points and use cases, lead project definitions, and convey the business value of the project
11. Guide and mentor clients to become self-sufficient data science practitioners

Preferred Work Locations: New York and San Francisco

While working across all these industries, you will also get to travel the World as these engagements will require that the team spend several weeks at client sites working on data science problems with a diverse team.

As a member of the team you will have a T-shaped skill set, having a broad knowledge base in Data Science and Industry Solutions in general, but also in- depth expertise in Operations Research / Decision Optimization.

Required Technical and Professional Expertise

At least 3 years experience  applying mathematical modeling and/or constraint programming to range of industry problems
At least 2 years experience programming skills in Python
At least 2 years experience ability to consume data via APIs
At least 1 year experience - applying predictive models as input into decision optimization problems
At least 2 years experience - building Monte-Carlo simulation/optimization for ""what-if"" scenario analysis
At least 2 years experience - building visual scenario comparison prototypes for ""before"" vs ""after"" KPI comparison, specifically for optimization-based decision-making applications
At least 2 years experience working as part of a multi-disciplinary team to build and deploy line-of-business solutions

Preferred Tech and Prof Experience

At least 1 year experience CPLEX

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
"Software/Data Engineer, Apple Media Products Data Engineering",Apple,"Apple Media Products is the team behind the App Store, Apple Music, iTunes, and many other high profile products on iPhone, Mac and AppleTV. Our Data Engineering team is looking for talented, performance-savvy, engineers to build out the big data platform and services which power many of these customer features  existing and new. This is your opportunity to help engineer highly visible global-scale systems with petabytes of data, supporting hundreds of millions of users.
Key Qualifications
Significant experience in designing, implementing and supporting highly scalable data systems and services in Java and/or Scala
Experience in Hadoop/Spark, noSQL stores, and traditional relational DBs preferred

Experience of any of the following is an advantage:
Distributed Computing technologies in particular Hadoop MapReduce, Spark / Spark-SQL, YARN/MR2
Building and running large-scale data pipelines, including distributed messaging such as Kafka, data ingest to/from multiple sources to feed batch and near-realtime/streaming compute components
Data-modeling and data-architecture optimized for big data patterns (efficient storage and query on HDFS)
Knowledgable about distributed storage and network resources, at the level of hosts, clusters and DCs, to troubleshoot and prevent performance issues.
Experience with low-latency NoSQL datastores and traditional relational databases is a plus.
Description
The candidates responsibilities include designing and implementing features that rely on processing and serving very large datasets, so an awareness on scalability is required. One of the concrete feature is to help surface popular contents across all our media stores, including, but not limited to, our App Stores, Apple Music or TV App. And not only the computed datasets need to be serviced to our customers, but it also need to be shared across many teams in our organization, such as Search team, Recommendation team, Reporting team, etc.

Because the candidate will have to interact with other groups on an ongoing basis, good verbal and written communication skills are important to this position. You will also enjoy the benefits of working in a fast growing business where you are encouraged to Think Different to solve very interesting technical challenges and where your efforts play a key role in the success of Apples business.

Candidate should have development and implementation experience of large scale mission critical applications. Engineer innovative solutions while playing a hands-on development role to deliver products in a rapid and dynamic environment. Leadership is important for this position as the candidate will be looked to for technical guidance and best practices.
Education
Bachelors/equivalent, or greater, in Computer Science or related discipline"
Cybersecurity Engineer - Data Loss Prevention,Illumina,"Illumina requires a CyberSecurity Engineer who will be responsible for all aspects of a new Data Loss Prevention (DLP) and Cloud Access Security Broker (CASB) program. This position will participate in requirements gathering, product selection, deployment, and support of a DLP / CASB solution that meets Illumina's business requirements.

Responsibilities:
While DLP / CASB will be the focus for this role, responsibilities include but are not limited to:
Meeting with business stakeholders to ensure business requirements are being accounted for.
Providing technical and non-technical advice in the selection & support of a DLP product.
Assisting in planning, management, and execution of a DLP program.
Researching and tracking current security threats.
Creating and updating elements of security governance (policies, procedures, standards).
Responding to incidents, performing forensic investigations, and assisting with eDiscovery tasks.
Additional responsibilities include working in a team environment to design and implement secure architectures for the enterprise. Position will provide leadership and expertise to develop security strategies, designs, and security solutions.

Qualifications:
Experience with developing & implementing DLP / CASB programs
Knowledge of best practice security frameworks (CyberSecurity framework / ISO / NIST)
Practical experience with deployment and operation of common information security solutions, including but not limited to:
Splunk
Google GRR
Cylance
Palo Alto firewalls
Additional responsibilities include working in a team environment to design and implement secure architectures for the enterprise. Position will provide leadership and expertise to develop security strategies, designs, and security solutions.

Requirements:
Bachelor's Degree in Computer Science or engineering preferred
7+ years of experience with large / mid-size Enterprise IT
7+ years of experience with large / mid-size Information Security programs
Possession of or working toward the following professional qualifications: CISSP, or SANS certifications
Strong communication and interpersonal skills
Illumina believes that everyone has the ability to make an impact, and we are proud to be an equal opportunity employer committed to providing employment opportunity regardless of sex, race, creed, color, gender, religion, marital status, domestic partner status, age, national origin or ancestry, physical or mental disability, medical condition, sexual orientation, pregnancy, military or veteran status, citizenship status, and genetic information. If you have a disability or special need that requires accommodation, please contact us at 858-246-8959. To learn more, visit: https://www.dol.gov/ofccp/regs/compliance/posters/pdf/eeopost.pdf"
Data Science Engineer,Salesforce,"Data Science @ Salesforce

Locations we are hiring in: San Francisco and Palo Alto *relocation provided

The role and your impact:

Salesforce is the #1 in CRM in the world, and best known as the Customer Success Platform. As an industry leader in Sales, Service, and Marketing, we have some of the most unique data sets in the world. With our customers we are trailblazing new ways to bring AI to their customers. Whether its Service Technicians triaging their most urgent support tickets, or helping Retailers build deeper, more personalized relationships with each customer, or helping sales teams prioritize opportunities and win more deals.

Salesforce is looking for data scientists with hands on experience transforming unique data into amazing products. At scale.

A typical day for you might include some of the following:

Develop machine learning software and models that can generalize across hundreds of thousands of Salesforce customers, but can automatically adapt to each of their individual features.

Brainstorm data product ideas with the data scientists that built some of the most exciting data products that hundreds of millions people use every day.

Work with fellow engineers to build out other parts of the infrastructure, effectively communicating your needs and understanding theirs.

Meet with external customers and understanding their businesses and their challenges.

Work with internal product teams to transform and sprinkle machine learning fairy dust on their products.

Build intelligence into our services to make them run smarter.

Participate in meal conversations with your team members about really important topics, such as: Should the cuteness of panda bears be a factor in their survivability? Is love a decision tree or a regression model? How far ahead would society be today if we had 12 fingers instead of 10?

What we care about:

You have industry experience with writing code (e.g. Java, Spark, Python, Scala, R) and taking ML models to production. Preference for 3+ years of industry experience (without PhD); at least 2-3 years of industry experience with PhD. This is not an entry level / new college graduate role.

You have to love data - this is what we do. We are looking for people who are excited about different and unique data sets, and all the ways that they could be used in order to improve user experience.

Qualifications:
You share our values: Trust, Innovation, Equality, and Giving Back

2+ years of experience in distributed systems and architecture

Can write scalable code using Java, Scala, Python, or other relevant programming language

Proven knowledge with applied statistics and probability/ predictability data outputs

Keep up to date with cutting-edge machine learning/ big data methods and techniques

You are passionate about automating everything and applying scientific methods to solve business and engineering requirements

Shift quickly from deep thinking to implementing in production.

Can thrive in team environments; using agile methodology and interacting with Product Leaders, Scientists and Engineers to solve technologys greatest challenges

Learn quickly in a fast-paced, dynamic team environment

Completed a Bachelors and/or Masters degree related to Computer Science, Statistics or related major studying AI (artificial intelligence), ML (machine learning), NLP (natural language processing), Data Science or Mathematics

Preferred Qualifications:
ETL and DE concepts

Experience with experimentation, labs, case studies

Taken a product from idea to launch to scale.

Salesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. The company was founded on three disruptive ideas: a new technology model in cloud computing, a pay-as-you-go business model, and a new integrated corporate philanthropy model. These founding principles have taken our company to great heights, including being named one of Forbess Worlds Most Innovative Company six years in a row and one of Fortunes 100 Best Companies to Work For nine years in a row. We are the fastest growing of the top 10 enterprise software companies, and this level of growth equals incredible opportunities to grow a career at Salesforce. Together, with our whole Ohana (Hawaiian for ""family"") made up of our employees, customers, partners and communities, we are working to improve the state of the world.

*LI-Y"
C++ Software Engineer Working With Capital Markets Data,MayStreet LLC,"Were MayStreet, based in the Flatiron district of Manhattan. Were building the next generation of capital markets technology. Global capital markets are an ocean of fast moving, interrelated and complex data. Historically its been nearly impossible for all but a select few firms to make use of much of this data. MayStreet solves this problem by managing collection, storage and API access to uniquely high quality data sets. Where you fit in - were looking for outstanding C++ developers to grow our platforms functionality across asset classes, data sets and business verticals. Developers on our team have the rare opportunity to work on challenging and important problems as they transform high-quality raw data into actionable insight.

MayStreet developers can expect to:
Write extremely high quality, reusable and reliable C++ code

Use code to solve challenging problems that affect a wide range of capital markets players

Be a part of a team and solve problems in a highly collaborative environment

Have exposure to adjacent business functions including sales, product development and client support

Requirements:
A strong work ethic - have the tenacity it takes to complete things!

Excellent problem solving, coding and design skills

A personal and professional need to be part of a fast-moving, dynamic, expanding firm

Currently strong C/C++ language understanding or the desire to become an expert

Who is MayStreet?

Were a financial technology firm that builds next-gen capital markets software for buyside and sellside capital markets firms, exchanges and top tier banks. Our software empowers these firms to thrive in the incredibly fast-paced environment of today's highly-distributed, highly-regulated, highly volatile global capital markets. Our unique perspective on market trends has been cited numerous times in the Wall Street Journal and featured inTabbFORUM, Markets Media. eFinancialCareers, MarketVoice, Enterprise Tech and Waters Technology. In November 2016, MayStreet was invited to exhibit at the Innovators Pavilion, hosted by the Futures Industry Association (FIA) at its annual Expo in Chicago, featuring 18 companies founded in the last five years that have developed innovative products and services that can benefit the derivatives markets. Founded in 2012, MayStreet just concluded its third consecutive year of 100% YOY revenue growth.

Our team of developers are highly-valued at the highest level of the firm, as the companys founders are both longtime developers, having designed their software from the ground up with a clear, well-maintained code base that is easy for good developers to program and further develop. With software engineering at its core, MayStreet recognizes and the incredible importance of top engineering talent  and rewards its developers accordingly."
Data Engineer- Data Engineering and Emerging Technology,Takeda Pharmaceutical,"As
a Data Engineer, you will be tasked with creating an ecosystem to have the
right data, to ask the right question, at the right time.
Apply advanced techniques to complex
problems in R&D and other organizations.
Work
directly with the Data Science in R&D at Takeda along with other advanced
analytics organizations across the company.
Apply advance techniques in structured,
partly structured and unstructured data across different partner organizations.
I mplement solutions for both
big data and difficult to structure data sets.
Maintain up-to-data
knowledge on modern data technologies, explores new platforms and beta tooling.
I ndependently use own
judgement to identify data requirements and influences the design.
I nfluence new computer science
platforms to design, analyze and implement complex and new data driven
solutions that impact the company.
Provide leadership to
complex data analysis, uses and explores data, languages, tools and software to
best construct data for predictive modelling, tests the model, trains data to
deploy the modelling within a complex R&D, Medical, Mathematical
environment and a large complexity of IT systems and data. Qualifications Required:
Bachelor
Degree in Computer Science or equivalent
2+
years experience or relevant project / coursework
Up-to-date
specialized knowledge of data wrangling, manipulation and management of
technologies.
Ability
to manipulate voluminous data with different degree of structuring across
disparate sources to build and communicate actionable insights for internal or
external parties.
Possesses
strong communication skills to portray information.
Ability
to work in an agile environment with high quality deliverables.
Experience
with two of the following languages:
Java, Scala, or Python
Understanding
of Web Services as well as JSON formats
Working
knowledge of SQL and Relational Databases
Experience
with one of the following NoSQL datastores (Cassandra, MongoDB, Neo4J, )
Experience
with concepts of Hadoop and Spark Desired:
Additional
Languages: Chef, R, Javascript
Experience
with Multiple NoSQL datastores (Cassandra, MongoDB, Neo4J, )
Experience
with data formats including Parquet, ORC or AVRO
Understanding of AWS (S3,
EC2, Redshift, EMR, Athena)
Experience with a Rapid UI
tools: EX: Tableau Schedule Full-time"
Database Engineer,Burst Inc.,"Skills/Experience Desired:
Expertise in developing in, and managing, a leading RDBMS such as Oracle, PostgreSQL, MySQL

Expertise in writing and troubleshooting complex SQL queries, and stored procedures

Experience with data warehousing a big plus

Ability to troubleshoot, tune, and profile database performance

Familiarity with NoSQL: Cassandra, Mongo, Couch, Redis

Exposure to AWS a big plus

Experience in two or more of these desired: Java, Python, Linux, REST

Bachelor's in CS or Engineering preferred

Four to eight years of experience"
Machine Learning Engineer - Intern,FactSet Research Systems,"FactSet is a financial data and software company headquartered in Norwalk, CT with offices in 35 locations worldwide. As a global provider of financial information and analytics, FactSet helps the worlds best investment professionals outperform. More than 87,500 users stay ahead of global market trends, access extensive company and industry intelligence, and monitor performance with FactSets desktop analytics, mobile applications, and comprehensive data feeds. As of February 2017, annual subscription value reached $1.19 billion and headcount passed 8,500.

FactSet was ranked #89 on FORTUNEs 100 Best Places to Work list in 2016 and has consistently been recognized as a great workplace by leading publications.

Role/Department Description:
FactSet is currently seeking talented students to work with our Machine Learning and Language Technologies(MLT) Group. As an intern, you will be working alongside a team of engineers responsible for the development of systems for natural language analysis, classification tasks, information retrieval, machine translation and processing of financial data.

An ideal candidate for this position has a good mix of technical and analytical skills for software development, as well as a research-oriented background suitable for developing solutions for language processing tasks. We are looking for dedicated, self-motivated individuals capable of working independently as well as in small teams.

Job Requirements
 Coursework in Natural Language Processing or Machine Learning
 Working towards a BS, MS, or PhD degree in CS/Statistics/Mathematics or related discipline
 Proficiency in Java or Python
 Familiarity with Open Source ML/NLP toolkits:
 We like tools such as scikit-learn, Mallet, IPython, Pandas, Weka, Vowpal Wabbit.
 Strong verbal and written communication skills

Useful to have:

 A scripting language like Python, Bash
 Machine Translation
 Distributed Systems
 Knowledge of Database Technologies : SQL, NoSQL

To find out more about opportunities at FactSet, visit us at www.factset.com/careers, www.facebook.com/factset, or www.twitter.com/factset.

The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractors legal duty to furnish information.

FactSet Research Systems Inc. is an E-Verify participant and EOE/M/F/D/V Employer which strongly supports diversity in the workforce."
Software Engineer  Big Data/IoT,Select Source Solutions,"Sensor technology, the Internet of Things, and Big Data analytics are some of the hottest areas in tech right now. At Savi Technology we offer you the ability to work on ALL of these, with three critical differentiators. First, you get to work with a fantastic tech stack. Second, you get to use this technology to solve very interesting, real-world problems. Third, you get to see some of the largest companies in the world use your solutions everydayon billions of dollars of thingsto realize millions of dollars of value.

Our client is hiring experienced engineers excited by the prospect of using streaming analytics and machine learning to enable our customers to change their entire operations of their enterprises using insights and predictions from sensor and IoT data. We are looking for engineers with experience in analytics and machine learning as well as engineers capable of working with streaming analytics and complex event processing. We have created a DAG-based architecture using Kafka, Spark, Hadoop, Cassandra and Solr that combines data ingestion, transformation, finite state processing, aggregation, indexing, and immutable storage to essentially create a Google-like infrastructure for sensor data (imagine building the equivalent of Google Analytics and IFTTT for the industrial IoT). We tackle fun challenges like ingestion of data in real-timewith any need of an APIand self-healing, exactly-once processing at thousands of transactions per node per second (we recently converted from Storm and MapReduce to Spark--Streaming and batch.)

We work in small, multi-disciplined teams of product managers, hardware engineers, data engineers, data scientists, application engineers, and devops professionals. You will see your work in production in days or weeksnot months. You will get the opportunity and flexibility to explore a wide range of technologies and challenges (you can expand into work in everything from front end tech to data engineering to firmware programming).

If you believe its all about the data and are excited by the combination of a can do startup culture and the customer base and financials of an established company, and want to use the Internet of Things to build solutions to tackle big real-world challenges, then our client is right for you.

Qualifications:
BS/BA in computer science, computer engineering, or related degree
5+ years experience developing large-scale, distributed data platforms and data processing solutions for PaaS / DaaS platforms, Internet-scale companies, government agencies, etc.
Experience working in an agile development environment
You should also have significant experience in several of the following areas:

Data ingestion using distributed queuing technologies such as RabbitMQ, AMQP, Kafka, or ZeroMQ,
Streaming analytics and complex event processing using Storm, Spark, IBM InfoStreams, or similar technologies
Proven experience using MapReduce, Spark, other ETL/ELT technologies to process TBs of data daily, across hundreds (or even thousands) of continuously running jobs
Deep understanding of how to design high-performant data models for multiple NoSQL data stores (file stores, wide column databases, key-value stores, etc.)
Hands-on experience with Hive, Impala, Presto, and similar tools for SQL-like exploration of large-scale data sets
Even better, you also:
Enjoy designing interactive analytics solutions
Are comfortable with the flexibility of high-agile environments
Prefer using Continuous Integration and Deployment to reduce manual work
Are interested blurring the line between Software Engineering & Data Science
Have experience in geospatial processing at scale.
Bonus points for sharing your Github account with your resume."
Data Visualization/BI Engineer,KPMG,"Known for being a great place to work and build a career, KPMG provides audit, tax and advisory services for organizations in today's most important industries. Our growth is driven by delivering real results for our clients. It's also enabled by our culture, which encourages individual development, embraces an inclusive environment, rewards innovative excellence and supports our communities. With qualities like those, it's no wonder we're consistently ranked among the best companies to work for by Fortune Magazine, Consulting Magazine, Working Mother Magazine, Diversity Inc. and others. If you're as passionate about your future as we are, join our team.
KPMG is currently seeking a Data Visualization/BI Engineer in Technology Enablement for our Management Consulting practice.

Responsibilities:
Deliver client analytics projects leveraging and championing Reporting and Visualization applications as part of team of industry/solution professionals by working with the appropriate technical resources to transform client problems into technical problems
Utilize a hypothesis-driven problem-solving approach to design, construct, and rapidly test/iterate exploratory analyses that will reveal insight and opportunities for the client
Build analytics solutions using variety of big data technologies and analytics tools as required by the client, and create robust documentation of solutions and underlying design decisions and operating procedures
Support project leaders in tracking and communicating project risks, budget, rates, and launch/closeout activities, including the administration of work papers and collaboration sites
Proactively broaden and deepen client relationships by working with varying levels of client team members

Qualifications:
A minimum of one year of data analytics experience with an internal strategy/analytics group, or similar environment
Bachelor's degree in a technical field from an accredited college or university (master's or MBA degree preferred) with expertise in programming languages and a working knowledge of topics such as statistics, machine learning, operations research, decision science, and cloud computing
Excellent verbal/written communication skills, including communicating technical issues to non-technical audiences
Proficiency with sophisticated analytics tools, programming languages or visualization platforms
Strong analytical ability, judgment and problem analysis techniques with attention to detail
Interpersonal skills with the ability to work effectively in a cross functional team
KPMG LLP (the U.S. member firm of KPMG International) offers a comprehensive compensation and benefits package. KPMG is an equal opportunity employer. All qualified applicants are considered for employment without regard to race, color, creed, religion, age, sex/gender, national origin, ancestry, citizenship status, marital status, sexual orientation, gender identity or expression, disability, physical or mental handicap unrelated to ability, pregnancy, veteran status, unfavorable discharge from military service, genetic information, personal appearance, family responsibility, matriculation or political affiliation, or other legally protected status. KPMG maintains a drug-free workplace. KPMG will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of applicable local, state or federal law (including San Francisco Ordinance number 131192). No phone calls or agencies please."
Applied Data Science Product Engineer,Esri,"The product engineer for applied data science will perform a wide variety of statistical and analytical techniques using the ArcGIS API for Python and other Web GIS tools along with other open source or proprietary analytic tools and capabilities commonly employed across national security related problems. All work performed in this role is from the perspective of a customer-user in the role of a data scientist working on national security challenges and problems. From this perspective, the overarching goal of the Product Engineer is to create the right ArcGIS products and experiences to meet the analytic needs across the national security community. This work will include performing, documenting and teaching a wide variety of technical tasks, developing best practices, creating tutorials and documentation, identifying new technical requirements, and improving the analysis tools and APIs.

Responsibilities
Participate in the design, release and successful adoption of the Python API for ArcGIS across multiple industries and a particular focus on national security interests.
Develop sample integrations, tutorials and documents for working with Jupyter Notebooks and the ArcGIS System
Leverage open source as well as proprietary modules using Python
Perform bug fixes and maintenance tasks for relevant and related products
Write samples and guides using Jupyter Notebooks, SDK guides and blog posts
Design, test, release, and support ArcGIS software for geoprocessing and analysis to enhance overall product quality and applicability for supporting data science workflows and needs
Participate in the design of new functionality for future releases
Identify workflow, data sources and enhancements for existing analytic methods and techniques
Evangelize data science community to our software community through various venues such as user documentation, educational materials, social media and online content

Requirements
Ability to apply analytical rigor and statistical methods to data and be comfortable manipulating and analyzing complex, high-volume, and high-dimensionality data from varying sources.
Incumbents are effective communicators capable of independently driving issues to resolution and communicating insights to non-technical audiences.
Strong background in statistical theory and application
Experience with case analysis, requirement gathering, and documenting to enhance existing software
Experience with designing, documenting, and executing functional test plans
Knowledge of mocking up user experience modifications to support new software features
Hands-on experience with geoprocessing and Python scripting
Experience as a user with an RDBMS, such as Oracle, SQL Server, etc.
Bachelor's or master's in GIS, statistics, geoscience, or related field, depending on position level"
"Big Data Software Engineer, Mid",Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years. Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Big Data Software Engineer, Mid
Key Role:
Support government and commercial consulting engagements centered on client adoption of Big Data capabilities. Assess, design, and implement forward looking enterprise scale Big Data solutions that target growth opportunities for future capabilities without sacrificing immediate and short-term client expressed objectives. Research into new opportunities and expanding capabilities and BAH branding in the Big Data stack, specifically targeting machine learning and KPI and regression analysis. Apply DevOps best practices, tools, and techniques to create custom modules and applications using programming and development expertise, including Java, Python, shell scripting, and regular expressions. This position may require the ability to travel.
Basic Qualifications:
-1+ years of experience with performing systems administration in Windows, Linux, or VMware environments, including performing installation and configuration, monitoring system performance and availability, performing upgrades, using scripting languages to automate tasks and manipulate data, and troubleshooting
-Experience in developing with a third-generation programming language
-Knowledge of enterprise logging, including application logging and regular expressions
-Knowledge of predictive modeling techniques
-Ability to travel
-Ability to obtain a security clearance
Additional Qualifications:
-Experience with designing, implementing, configuring, and operating Splunk
-Experience with Python or Java
-Experience with Hadoop ecosystem and tools, including Storm, Spark, Accumulo or HBase, or variant NoSQL db
-Experience with statistical software, including SAS, MatLab, and R
-Experience with enterprise scale operations and maintenance environments
-Experience with working in a large enterprise environment and integrating solutions in a multi-vendor environment
-Knowledge of federal information security policies, standards, procedures, directives, frameworks, federal security authorizations, assessment, and risk management processes for enterprise systems
-BA or BS degree in CS, IT, or a related field
-Splunk Architect, Admin, and Splunk Power User Certifications
-CISSP, Security+, or a related Certification
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet.
SIG2017"
Big Data Engineer,Kogentix,"If you have experience with big data solutions, you have a wealth of opportunities to grow your career. How about an environment with extraordinarily talented peers? How about a company offering a range of both big data products and professional services to expand your skills? How about an organization working on cutting edge projects that many of the biggest companies simply can't execute on their own? Are you looking to join a startup where you can directly contribute to the establishment of one the next great high tech companies? Then Kogentix, now hiring Junior-to-Mid-Level Big Data Engineers, may just be a perfect fit.

Responsibilities
Develops distributed applications to solve large scale processing problems, utilizing various languages like Java, Scala , Shell etc.
Implements, troubleshoots, and optimizes solutions based on modern big data technologies like Hadoop, Spark, Elastic Search, Storm, Kafka, etc. in both an on premise and cloud deployment model
Implements data architecture, including data ingress in batch and real time from a broad variety of external systems; data transformations to prepare data for analytics processing, and data egress for availability of analytics results to visualization systems, applications, or external data stores
Supports documentation, change control, and QA processes consistent with enterprise requirements
Establishes strong teamwork with client technical resources, and effectively communicates project status, technical issue options and resolution, and operational requirements to client stakeholders
Qualifications
Overall 2-3 years experience with minimum of 1 year on Hadoop or a closely related technology
Very strong server-side Java experience, especially in an open source, data-intensive, distributed environments
Expert in the Hadoop Framework & java programming (i.e. Spark, MapReduce, Pig, Hive, Kafka, Storm, etc.) including performance tuning
Implemented complex projects dealing with the considerable data size (TB/ PB) and with high complexity
Good understanding of algorithms, data structure, and performance optimization techniques
Experience with agile development methodologies like Scrum
Self motivated, and has the ability to drive technical discussions.
Organized, detail oriented, able to work both independently and in a team
Excellent problem solver, analytical thinker and quick learner.
Strong verbal and written communication skills
Broad understanding of most of the following, with depth of expertise and experience in at least 1:
o Hadoop security (Kerberos, Ranger, Knox)
o Amazon EMR and related technologies (e.g. DynamoDB, Kinesis, S3)
o Data mining, statistical modeling techniques and quantitative analyses
o Data Architecture, Master Data Management and Governance
o Kafka
o Search capabilities such as Elastic Search
o NoSQL DB such as Cassandra and MongoDB
Certifications a plus: Amazon, Cloudera, Spark
Masters / Bachelor of Computer science with focus on distributed computing"
Big Data Engineer,NCR,"NCR Corporation (NYSE: NCR) is the global leader in consumer transaction technologies, turning everyday interactions with businesses into exceptional experiences. With its software, hardware, and portfolio of services, NCR enables nearly 700 million transactions daily across retail, financial, travel, hospitality, telecom and technology, and small business. NCR solutions run the everyday transactions that make your life easier. NCR is headquartered in Duluth, Georgia, with approximately 29,000 employees and does business in 180 countries.

POSITION SUMMARY & KEY AREAS OF RESPONSIBILITY:
NCRs strategy is to transform to a data-driven culture in which large volumes of disparate data drive new revenue opportunities, increase returns on investments, and improve business decisions. You will work in the CDO organization with our Data Lake group. Our team builds the Enterprise Data Lake platform serving analytics for multiple lines of business.
We work more like a small startup company. Our development teams are small and embrace agile principles to quickly generate value to our stakeholders. We combine the latest open source technologies together with traditional enterprise software products. Our office culture is casual, fun and social, with an emphasis on education and innovation. We have the freedom to try new ideas, experiment and are expected to be constantly learning and growing. There is also a strong emphasis on mentoring others in the group, enabling them to grow and learn.

You will:
Participate in collaborative software development and implementation of the new Enterprise Data Lake on Hortonworks HDP and HDF distributions.
Explore and evaluate new ideas and technologies.
Ensure conceptual and architectural integrity of the platform.
Work on large-scale, multi-tier big data engagements.
Gain deep technical expertise in the Hadoop administration.
Assist in automation of on-premise and cloud (Azure) based Hadoop clusters.
Provide support, on-going maintenance, and required modifications to multiple Hadoop environments.
Qualifications
BASIC QUALIFICATIONS:
Bachelors degree or higher in Computer Science or a related field.
Good understanding of distributed computing and big data architectures.
Polyglot development: Capable of developing in at least one of the programming language like Java, Scala or Go.
Knowledge of OOP design and patterns
Strong knowledge of source code repository like SVN/GIT & build tools like Maven
Proven use of open source frameworks
DevOps: Appreciates the CI and CD model and always builds to ease consumption and monitoring of the system. Experience with Maven (or Gradle) and Git preferred.
Familiar with agile development practices
Personal qualities such as creativity, tenacity, curiosity, and passion for deep technical excellence.
PREFERRED QUALIFICATIONS:
Experience working with public clouds like Azure, AWS etc.
1+ year of experience with Big Data technologies such as Spark, Kafka and/or Hadoop
Experience in working with un-structured or semi-structured data
EEO Statement
Integrated into our shared values is NCR's commitment to diversity. NCR is committed to being a globally inclusive company where all people are treated fairly, recognized for their individuality, promoted based on performance and encouraged to strive to reach their full potential. We believe in understanding and respecting differences among all people. This concept encompasses but is not limited to human differences with regard to race, ethnicity, religion, gender, culture and physical ability. Every individual at NCR has an ongoing responsibility to respect and support a globally diverse environment.

Statement to Third Party Agencies
To ALL recruitment agencies: NCR only accepts resumes from agencies on the NCR preferred supplier list. Please do not forward resumes to our applicant tracking system, NCR employees, or any NCR facility. NCR is not responsible for any fees or charges associated with unsolicited resumes."
Big Data Engineer (English-speaking),RTL Nederland,"Contract - Full Time

RTL Nederland

Your challenge:
For further information lease click this

link

."
Sr. Data Engineer,VMware,"VMWare Data Team is looking for a Senior Data Engineer to help build on Next generation NearRealTime BI Platform based on SAP HANA and Hadoop. You will be responsible for building and enhancing the solutions on the existing platform based on the business needs in partnering with fellow Developers and Business groups. Basic Qualifications: 4+ years of experience as a Business Intelligence Engineer in a leading company handling large Datasets Detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ELT and reporting/analytic tools.

Expertise in writing basic and advanced SQL queries. Experience working with Informatica, SAP SDI Expertise in SAP HANA, Hive/Hadoop/Hawq Experience with relational and star schema modeling concepts. Working knowledge of BI Reporting tools like BOBJ and Tableau is a plus. Experience in Python Scripting Preferred Qualifications: Experience in gathering requirements and formulating business metrics for reporting. Ability to understand new data sets quickly. Familiarity with Amazon Web Services (AWS) tools (i.e.

S3, Redshift, EC2, SNS, SQS, SES, Dynamo DB) is a plus Excellent critical and analytical thinking Strong analytical and troubleshooting skills Excellent verbal and written communication skills"
Senior Data Engineer,The Zebra,"The Zebra -- recently named one of Austins Best Places to Work by the Austin Business Journal -- is expanding our engineering team. We're looking for someone with sharp technical chops, lots of experience, and great mentorship skills to build upon and grow our existing engineering efforts.

The Zebra is seeking a Senior Data Engineer for our dynamic start-up! This person will be responsible for helping us collect, connect, centralize, and curate our data. You will immediately contribute to our goal of using our rich datasets to make smarter business decisions. You should love releasing well-tested code multiple times a day, be ready to take ownership and be thrilled about having broad responsibilities within a fast-growing company in a massive industry.

What You'll Do:
Have a meaningful role in the design and development of a reliable, effective and scalable data platform
Design and implement fast and scalable data pipelines
Mentor other engineers and analysts on data processing, data access and manipulation techniques
Assist and guide the company in solving business problems via data
Our Match Made in Heaven:
5+ years of professional programming experience
Experience with Snowflake or Amazon Redshift
Experience with Hadoop, Spark, Flink, and/or related technologies
Familiarity with event streams and stream processing
Comfort with the Linux/Unix command line and shell scripting
Deep understanding of one or more of the following languages: Python, Java, Scala
Experience That Would Impress The Pants Off Us:
2+ years experience building large-scale, distributed, high-volume systems
Experience building ML pipelines
Exceptional experience with relational and NoSQL databases
A passion for problem solving and performance tuning with strong analytical capabilities
A desire to follow and advocate for exceptional software engineering processes
Significant experience with AWS (Amazon Web Services)
Advanced degree in Computer Science
A Github profile or public code portfolio
Perks:
Competitive compensation
Health, dental, & vision benefits
Working with a small team of passionate, collaborative and engaged people
Stock options
Monthly home maid service
Feel good perk (for legal things that make you feel good!)
Work hard / Play hard culture"
Dev Ops Engineer - Big Data,Riversand Technologies,"Riversand Technologies is a Master Data Management (MDM) visionary and a Product Information Management (PIM) leader. We are a team of passionate people who are rethinking the way MDM and PIM work. We recently raised $35 million in series A funding and we are on a trajectory for an accelerated product innovation and growth over the next two years. If you are a Dev Ops Engineer looking to advance your career in Data Management and Analytics, then now is the best time to join Riversand. Our solutions power enterprises worldwide, in a variety of industries including Retail, Manufacturing, Distribution, Energy, Healthcare, and Food Services.

To be successful in this role, you must be a proactive leader, with excellent analytical and problem-solving skills and a track record of taking complex concepts and implementing practical solutions. If you work efficiently in cross-functional teams and can communicate well with both the technical and non-technical staff, please dont hesitate to apply below for this position.

Here are some of your responsibilities. We are interested in knowing what else you can add to this:

You will be responsible for build, deployment, monitoring, and troubleshooting of the MDMCenter infrastructure and technology components using Docker and Big Data technologies

You are comfortable deploying cloud technologies using IaaS and PaaS components in AWS or Azure

You are delivery focused and can deliver on time and with expected quality

You will be part of the initiative to automate the entire CI/CD process

You will be part of the initiative to auto-scale the deployment

You will develop software in collaboration with other developers and product management

If what you read so far excites you about joining us, then we would like you to be already equipped with the following qualifications:

Bachelors/Masters degree in Computer Science/ Engineering or a related field (or equivalent experience)

Worked on any one or more technologies like Elastic Search, Apache Solr, Apache Kafka, Apache Storm, Apache Hbase, Apache Hadoop, Apache Spark and Apache Cassandra

Minimum of 3 years' experience automating CI/CD processes

Knowledge of Shell Scripts, Linux, Windows and Networking

Knowledge of Blue Green and Canary deployment styles

Agile Development Methodology (Scrum process preferable)

Need to be a team player while having the capacity to work independently

Strong analytical and logical skills including troubleshooting

Prior experience working in a software company deploying a shrink-wrap or SaaS product a plus

Open Source contributions a plus

Knowledge of OO Languages like Java a plus

Whats in it for you?

We foster a Collaborative work environment. You will enjoy learning with other creative and analytical minds

We provide an opportunity for you to experiment and fail fast

We want to make sure you get a competitive compensation and benefits

Riversand client roster features high-profile enterprises which will provide you with industry-specific insights into data management and analysis

Beyond work, we compete at local 5ks, 10ks and have fun at various sporting events"
Data Science Engineer,HCA Corporate,"About HCA
Nashville-based HCA is one of the nation's leading providers of healthcare services, operating 171 locally managed hospitals and 119 freestanding surgery centers in 20 states and the United Kingdom. With its founding in 1968, HCA created a new model for hospital care in the United States, using combined resources to strengthen hospitals, deliver patient-focused care and improve the practice of medicine. HCA has conducted a number of clinical studies, including one that demonstrated that full-term delivery is healthier than early elective delivery of babies and another that identified a clinical protocol that can reduce bloodstream infections in ICU patients by 44 percent. HCA is a learning healthcare system that uses its more than 27 million annual patient encounters to advance science, improve patient care and save lives.

About CSG

The Data Science (DS) team is responsible for the expansion of business processes and performance related to HCA's Clinical Services Group (CSG) strategy. The Data Science Engineer is a subject-matter specialist who actively participates in the development and implementation of Data Science requirements as they relate to the clinical, administrative and financial process and variations in the CSG Data Science program. The Analytics Engineer will support data management, storage and efficient computation for analysis.

Duties and Responsibilities

This engineer delivers on development commitments from start to finish for all aspects of the Data Science organization within Clinical Services Group (CSG). This technically focused position is responsible for designing, developing, testing and deploying our Data Science solutions across the Enterprise. This engineer can quickly learn and maintain existing solutions as well as new development. They will provide key problem resolutions for production systems as needed. They have an in depth understanding of the services provided by Clinical Services Group, Data Science and can develop relationships throughout the organization to assist in accomplishing its goals for the company. This engineer strategically designs, constructs, and implements software in a software development environment. This includes selecting, gathering requirements for, designing, and implementing solutions for consumers throughout the enterprise. This engineer is a highly motivated self-starter and is committed to delivering high quality solutions within agreed upon timelines.

Qualifications

3+ years of experience in most of the following:

We are looking for experts in these areas. If you dont have experience in some of these, you are able to work collaboratively on a cross-functional team that builds Data Science signal delivery, data pipeline, and DevOps infrastructure.
Experience with Hadoop, Spark, Kafka, Cassandra (i.e. distributed compute and storage)

Extensive expertise/experience in data analysis, modeling and visualization required

Extensive expertise/experience in data acquisition, data cleansing and parsing required.

Mastery of multiple business intelligence platforms (e.g., Tableau, Qlik, MicroStrategy) required.

User Interface design and development (e.g. JavaScript, Elixir, Phoenix, and other web stack technologies)

Mastery of Teradata SQL required; extensive experience in other ANSI SQL languages (e.g., MSSQL, Oracle, Postgres) required.

Experience with container-based platforms such as Docker, Kubernetes, OpenShift, Mesosphere, Rancher, and CoreOS

Extensive Experience with container monitoring applications such as monitoring solutions such as SysDig, Data Dog, AppDynamics, New Relic, Nagios, and Zabbix

SQL experience / database interrogation techniques

ETL experience required

Linux command line skill is required

Mastery of one or more formal development languages (e.g., Python, R, JavaScript, Ruby, Scala, or Clojure) required

Extensive expertise/experience in the areas of data structures, warehousing, and profiling

Experience using a distributed version control system (DVCS; e.g., GitHub, TFS) a plus.

Scrum, Agile, Lean Product Development, Domain Driven Design
Excellent communication skills, both written and verbal
Experience and knowledge with Service Oriented Architecture (SOA)
Healthcare experience, preferable
Exposure to the fundamentals of Enterprise Architecture (preferred)

Education and Experience
Bachelors degree required; Masters degree preferred.
Experience in healthcare preferred.
7+ years of relevant work experience required."
Data Science Engineer,Adobe,"The Challenge:
Building on top of Adobe Experience Cloud data platform, we are responsible for AI/ML innovations that drive platform excellency. We will be building platform intelligent services encompassing data insights, anomaly detection, forecasting, digital marketing content and consumer understanding, causal analysis and AI-powered user experience. We are looking for a passionate data engineer who will build advanced data wrangling pipelines, extract data insights, and provide data intelligence as a service to our platform core services as well as other intelligent services.
What youll do
Design and develop robust, scalable solutions for collecting, wrangling, cleansing and analyzing large data sets in the cloud-based production environment;
Implement and maintain the data pipelines, structures and reports that are used by teams across the Platform;
Build data expertise on real-world marketing and commerce data in various digital marketing domains;
Own data quality throughout all stages of acquisition and processing, including data collection, ETL/wrangling, ground truth generation and normalization;
Design and develop new framework and automation tools to enable teams to consume and understand data faster;
Interface with internal data science, engineering and data consumer teams to understand the data needs
What you need to succeed
Master or Bachelor with equivalent exeprience in Computer Science or equivalent technical fields;
Expert coding skills across a number of languages: Scala, Python, R, Java
Strong SQL (Postgres, Hive, MySQL, etc) and No-SQL (MongoDB, HBase, etc.) skills, including writing complex queries and performance tuning;
3+ years of experience working with large data sets using open source technologies such as Spark, Hadoop, Kafka on one of the major cloud vendors such as AWS, Azure and Google Cloud;
Big plus: experience working with digital marketing solutions such as Google Analytics, Adobe Analytics;
Big plus: experience developing and productizing real-world AI/ML applications such as prediction, personalization, recommendation, content understanding and NLP;
Big plus: experience in custom or structured (ie. Informatica/Trifacta/Talend) ETL design, implementation and maintenance;
Plus: experience working with visualization tools such as Tableau, PowerBI;
Plus: experience working with data warehouse tools;
Strong verbal and written communication skills
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
Data Visualization Engineer,Lux Research Inc,"What we do:
At Lux Research youll find new and uniquely interesting problems to solve. We help uncover new technologies and strategies that help world-class companies find long-term growth. You will analyze data from uncommon sources, create visualizations, and ultimately design tools to uncover insights on emerging technologies like nanomaterials, hyperloops, artificial cells, and space exploration. Youll participate in planning our future products by helping capture issues and breaking down stories into achievable tasks, deploying your solutions in a cloud environment for our customers. We work on making each other better engineers through pair programming, code review, and tech talks. We make great solutions as a team.

What youll do:
Design and build custom visualizations, data-rich user interfaces, and modular visualization components.

Develop, deploy and support cloud-based tools for innovation data analytics.

Programming and engineering throughout the entire stack of our projects, with a focus on data analytics, visualization, and UX.

Establish and maintain usability standards and visual best practices.

Work with counterparts in other parts of our software development team, and more broadly with Lux Researchs other departments.

What we look for:
Experience developing dynamic-browser based applications using JavaScript and REST APIs.

Bachelor of Computer Science or similar relevant field

A passion for data visualization and a keen design sense.

Proficiency with static and dynamic web visualization, like D3.js or similar.

Experience with CSS, JQuery, and a framework like Vue or React.

Ruby on Rails or Python experience

Familiarity with AWS.

Ability to rapidly prototype new concepts and iterate on feedback from various stakeholders.

Great communication and organizational skills are essential, including the ability to meet deadlines in an agile, fast-paced, and collaborative workplace.

About Lux Research

Lux Research is a leading provider of intelligence services, helping clients drive growth through technology innovation. With quality data derived from primary research, fact-based analysis, and opinions that challenge traditional thinking, Lux clients are empowered to make better, informed decisions today to ensure future success. A pioneer in the research industry, Lux is uniquely positioned with technical expertise in addition to business insights, augmented by advanced analytics.

Analysis you trust. Opinions you rely on. Make better decisions, faster. For more information visit www.luxresearchinc.com, connect on LinkedIn, or follow @LuxResearch."
"AVP, Big Data Engineer (L10)",Synchrony Financial,"Job Description:
Role Summary/Purpose:
We are looking for a Data Engineer to design and develop consumer-centric low latency analytic applications leveraging Big Data technologies for our Enterprise Data Lake initiative.
Essential Responsibilities:
Design and develop cutting edge Analytic applications leveraging Big Data technologies: Hadoop, NoSQL, and In-memory Data Grids.
Build automation of deployment and configuration using open source frameworks
Act as the subject matter expert for Big Data platforms and technologies
Work across IT teams to ensure code quality, performance and scalability of deployed data products
Perform other duties and/or special projects as assigned
Qualifications/Requirements:
Bachelor's degree in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics)
Solid knowledge of scripting, SQL, and Linux
Familiarity with traditional BI Solution Architecture encompassing - ETL, CEP, DW, BI Reporting (preferably in a Unix/Linux, Oracle environment)
Experience in engineering large scale data infrastructures
Experience in deployment of BI & Analytics solutions using Big Data Technologies (such as - MapReduce, Kafka, HBase) in complex large scale environments preferably (20Tb+)
Experience in at least 3 of the following: Pig, Sqoop, MapReduce, Kafka, Spark, Java or Python
Familiar with Hortonworks, Cloudera, Zookeeper, and Kafka is a plus
Desired Characteristics:
Willingness to learn new technologies quickly and have excellent technical research, troubleshooting, and analytical skills (finding and implementing solutions in an always changing environment plays a big role in this position)
Engaging personality with experience collaborating across teams of internal and external technical staff, business analysts, software support and operations staff.
Evidence of exceptional ability
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the selection process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
If currently a Synchrony Financial Employee, you must have been in your current position for at least 6 months (Level 4  7) or 24 months (level 8 or greater), have at least a ""consistently meets expectations"" performance rating and have the approval of your manager to post (or the approval of your manager and HR to apply if you don't meet the time-in-job or performance requirement
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am  5pm Monday to Friday, Central Standard Time.
Grade/Level: 10
Job Family Group:
Information Technology"
Big Data Engineer,Lenovo,"Functional Area: Information Technology Facility: Corporate Office Relocation Provided: No Education Required: Bachelors Degree Experience Required: 3 - 5 Years Travel Percent: 0 Position Description Do you like working with Big Data? The WW Data Platform and Analytics team is hiring BI Engineers to work closely with product managers, and functional teams to build effective data products that will scale with our rapidly growing business to drive product decisions, improve customer experience, and increase overall operational efficiency. As a key member of the Data Platform and Analytics team, you will be at the center of the action and have an immediate impact on projects that are highly strategic and highly visible to Senior Leadership. In this role, you will work with product teams to build machine learning and advanced analytics products using the Hadoop ecosystem to help derive insights and value. Position Requirements Job Requirements:  Bachelors or Masters Degree in a technology related field (e.g. Engineering, Computer Science, etc.).  2+ years industry experience developing machine learning models at scale from inception to business impact.  Experience with Big Data ML toolkits, such as TensorFlow, Mahout, SparkML, or H2O.  Strong programming skill (Python, R, or Scala preferred).  Proficiency with Hadoop, MapReduce, HDFS.  Data visualization (Tableau, Qliksense, experience a plus).  Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala.  Experience with NoSQL databases, such as HBase, Cassandra, MongoDB. Preferred Qualifications:  Experience with one or more of the following: Natural Language Processing, sentiment analysis, pattern recognition, recommendation systems, forecasting/prediction analysis, customer behavior analysis or similar. Motorola Mobility is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, national origin, disability, veteran status, and other protected characteristics. We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class. ---"
Big Data Engineer (L09),Synchrony Financial,"Job Description:
Role Summary/Purpose:
We are looking for a Data Engineer to be part of our scrum teams and perform functional & system testing for Hadoop & Java/Spring applications for our Enterprise Data Lake initiative.
This is high visibility fast paced key initiative will integrate data across internal and external sources, provide analytical insights and integrate with our critical systems.
Essential Responsibilities:
Assist in the development and automation of functional, system, integration test plans, test scripts and the reporting of defects and testing results.
Assist in the data pipeline testing processes
Participate in the agile development process
Document and communicate issues and bugs relative to data standards
Assist in the maintenance of an integration and regression testing framework
Provide support in the reviewal of technical documentation for artifacts delivered
Work with team members to achieve business results in a fast paced and quickly changing environment
Pair up with experienced data engineers to develop cutting edge Analytic applications leveraging Big Data technologies: Hadoop, NoSQL, and In-memory Data Grids
Perform other duties and/or projects as assigned
Qualifications/Requirements:
Bachelor's degree in a quantitative field (such as Engineering, Computer Science, Statistics, Econometrics) and a minimum of 2 years of experience
Hands-on experience writing complex sql queries, extracting and importing large amounts of data
Ability to write abstracted, reusable code components
Programming experience in at least one of the following languages: Java or Python
Desired Characteristics:
Analytical mindset
Performance testing experience
Experience in testing Pig, Sqoop, Spark, HBaseon Hadoop & J2EE/Eclipse, Spring Boot applications
Familiar with Hortonworks, Cloudera, Zookeeper, Oozie, and Kafka is a plus
Willingness to learn new technologies quickly
Superior oral, and written communication skills, as well as the willingness to collaborate across teams of internal and external technical staff, business analysts, software support and operations staff.
Strong business acumen including a broad understanding of Synchrony Financial business processes and practices
Demonstrated ability to work effectively in a team environment
Eligibility Requirements:
You must be 18 years or older
You must have a high school diploma or equivalent
You must be willing to take a drug test, submit to a background investigation and submit fingerprints as part of the selection process
You must be able to satisfy the requirements of Section 19 of the Federal Deposit Insurance Act.
If currently a Synchrony Financial Employee, you must have been in your current position for at least 6 months (Level 4  7) or 24 months (level 8 or greater), have at least a ""consistently meets expectations"" performance rating and have the approval of your manager to post (or the approval of your manager and HR to apply if you don't meet the time-in-job or performance requirement
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status.
Reasonable Accommodation Notice:
Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job or to perform your job. Examples of reasonable accommodation include making a change to the application process or work procedures, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment.
If you need special accommodations, please call our Career Support Line so that we can discuss your specific situation. We can be reached at 1-866-301-5627. Representatives are available from 8am  5pm Monday to Friday, Central Standard Time.
Grade/Level: 09
Job Family Group:
Information Technology"
Full Stack Data Engineer,IBM,"Job Description
IBM Global Business Services (GBS) is a team of business, strategy and technology consultants enabling enterprises to make smarter decisions and providing unparalleled client and consumer experiences in cognitive, data analytics, cloud technology and mobile app development. With global reach, outcome-focused methodologies and deep industry expertise, IBM GBS empowers clients to digitally reinvent their business and get the competitive edge in the cognitive era in over 170 countries.

Bottom line? We outthink ordinary. Discover what you can do at IBM.

We are seeking a full-stack data engineer to join the team of IBMers currently supporting the Office of the Under Secretary of Defense for Intelligence (OUSDI), in the Warfighter Support Directorate, ISR Operations Division. The broader project team members have backgrounds in operations research, geospatial analysis, data extraction, and consulting, with deep expertise in military intelligence, surveillance, and reconnaissance (ISR) systems and employment. The Full-Stack Data Engineer will provide full-time support in the areas of database design, data manipulations, tailored script / algorithm development, and data visualization. The role is located in the Washington DC area (primarily Reston, Virginia).
Candidates will need the following: Must possess the ability to communicate clearly, concisely, and with technical accuracy in both oral and written modes. Must be able to work effectively under time constraints and potentially changing priorities, while maintaining a high level of attention to detail. Must be able to work in a collaborative, team environment, including on government site where availability of advanced development/database tools may be limited
Bachelors Degree in Engineering, Mathematics, Operations Research, or Computer Science, or related technical field
Duties include: Interface with the client, stakeholders, military personnel, and data providers to ensure that the team has maximal buy-in for its activities and can identify and collect data it needs (25%) Maintain and develop clients prototype tool for ISR data enrichment, aggregation, and visualization conduct data stewardship, incorporate new datasets, and support pilot programs to enable access/use by select outside users (75%) Security Clearance Required: ACTIVE TS/SCI

BENEFITS
Health Insurance. Paid time off. Corporate Holidays. Sick leave. Family planning. Financial Guidance. Competitive 401K. Training and Learning. We continue to expand our benefits and programs, offering some of the best support, guidance and coverage for a diverse employee population.

CAREER GROWTH
Our goal is to be essential to the world, which starts with our people. Company wide we kicked off an internal talent strategy program called Go Organic. At our core, we are committed to believing and investing in our workforce through:
Skill development: helping our employees grow their foundational skills
Finding the dream job at IBM: navigating our company with the potential for many careers by channeling an employees strengths and career aspirations
Diversity of people: Diversity of thought driving collective innovation In 2015, Go Organic filled approximately 50% of our open positions with internal talent that were promoted into the role.

CORPORATE CITIZENSHIP
With an employee population of 375,000 in over 170 countries, amazingly we connect, collaborate, and care. IBMers drive a corporate culture of shared responsibility. We love grand challenges and everyday improvements for our company and for the world. We care about each other, our clients, and the communities we live, work, and play in!

http://www.ibm.com/ibm/responsibility/initiatives.html
http://www.ibm.com/ibm/responsibility/corporateservicecorps

(2252) PSHJ

Required Technical and Professional Expertise

4+ years of data engineering and/or programming experience Programming skills in Javascript, Python, SQL/NoSQL in a Windows desktop operating environment Database design and maintenance, including designing new tables and relationships, proper database normalization, and tuning using database indices ETL (Extract, Transform, Load) development, including merging and normalizing related data sources Data visualization skills using Javascript and other tools/languages Technical skills in MVC frameworks, Web API design, natural language processing (NLP)

Preferred Tech and Prof Experience

N/A

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
"Data Engineer, Mid",Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years. Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Data Engineer, Mid
Key Role:
Perform data engineering activities, including working with data storage systems, ETL tools, custom script writing, data ingestion, and data visualization. Help to design, build, and maintain databases and scalable Big Data and NoSQL data stores. Perform data modeling and analytical exploration and examination of data from multiple sources of record, including discovering patterns and identifying potential business rules. Build ETL pipelines to clean data, implement a variety of transformations, and ingest data in multiple end systems. Work with and integrate multiple types of data, including unstructured and structured using SQL and API's. Deploy and support built data system as it goes operational. Work in a Scrum-based Agile team operating in fast-paced environments.
Basic Qualifications:
-2+ years of experience in working with relational databases, including MySQL or PostgreSQL
-2+ years of experience with Big data systems, including HDFS, Hive, MongoDB, Elasticsearch, or Hbase
-2+ years of experience with object-oriented programming, including Java, C#, Scala, Python, or PHP
-2+ years of experience with ETL tools, including Pentaho, Talend, or NiFi
-1+ years of experience with using Cloud services, including AWS
-Ability to obtain a security clearance
-BS degree
Additional Qualifications:
-Experience with a NoSQL database
-Experience with Agile software development
-Possession of excellent oral and written communication skills
-BS degree in CS, Computer Information Systems, Information Systems, or a related field
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet.
SIG2017"
Cloud Data Warehouse Engineer,Slalom Consulting,"Cloud Data Warehouse Engineer

As a Cloud Data Warehouse Engineer for Slalom Consulting, you'll work in small teams to deliver innovative solutions on Amazon Web Services, Azure and Google Cloud using core data warehousing tools, Hadoop, Spark, Event Stream platforms, and other big data related technologies. In addition to building the next generation of data platforms, you'll be working with some of the most forward-thinking organizations in data and analytics.

Who are you?
Youre a smart, collaborative person who is passionate about technology and driven to get things done.
Youre not afraid to be bring your authentic self to work.
You embrace a continuous learner mentality.
What technologies will you be using?

Everything. Its about using the right technologies to solve problems and playing with new technologies to figure out how to apply them intelligently. We work with technologies across the board.

Why do we work here?

Each of us came to Slalom because we wanted something different. We wanted to make a difference, we wanted autonomy to own and drive our future while working with some of the best companies in Washington DC, Northern Virginia, and Maryland, leveraging the coolest technologies. At Slalom, we found our people.

What does our recruitment process look like?

Our process is highly personalized. Some candidates complete their process in one week, others can take several weeks or even months. Deciding to take a new job is a big decision, so regardless how long or short the process may be for you, the most important thing is that you find your dream job.

Qualifications:
Bachelors degree in Computer Engineering, Computer Science or related discipline
3-7+ years relevant experience
Understand different types of storage (filesystem, relation, MPP, NoSQL) and working with various kinds of data (structured, unstructured, metrics, logs, etc.)
4+ years of experience working with SQL
Experience with setting up and operating data pipelines using Python or SQL
2+ years of experience working on AWS, GCP or Azure
Experience working with relational databases
Strong analytical problem-solving ability
Great presentation skills
Great written and verbal communication skills
Self-starter with the ability to work independently or as part of a project team
Capability to conduct performance analysis, troubleshooting and remediation
Experience working with data warehouses such as Redshift, BigQuery and Snowflake
Exposure to open source and cloud specific data pipeline tools such as Airflow, Glue and Dataflow

Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Software Engineer- Data Connect,Patientco,"OVERVIEW

We believe patients and health systems deserve better than the status quo.

At Patientco, we build easy-to-use payment technology that treats patients like consumers and empowers healthcare providers to dramatically improve the way they communicate. Together, patients are happier, health systems are more efficient and doctors can focus on what theyre supposed to do - take care of their patients!

As a Software Engineer- Data Connect at Patientco, you will do what you were born to do - write code that delights your users! You will be working internally to ensure that our systems work with our clients in such a way that builds and expands our ability to help them. This means considering how both healthcare patients and providers use your products and how your peers will interact with your code. You should have a knack for making tools powerful without compromising their ease of use . You should enjoy variety (and we arent just referring to the food and drinks we keep stocked in our kitchen), as you should expect to move up and down the software stack.

Our Application

Patientco is a web-based healthcare payments platform built primarily with object-oriented PHP and Javascript. We move quickly, deploying code many times a week, but we also spend time carefully thinking through the edge cases of our products.

Our Engineering family is responsible for the entire lifecycle of data integration. You will be charged to participate throughout the implementation life cycle of multiple projects simultaneously, from project initiation through post go-live support. This involves communicating with Implementations and Account Management to take in the feed from healthcare providers.

You will be the transformer of data that has been ingested and informed to generate bill of statements allowing ease of mind and use for our patients payment process. The Software Engineer- Data Connect will be responsible for upgrades, change requests, and overall improvements while introducing new clients and features onto the platform.

Candidates will have:
Extensive familiarity with object-oriented software systems based in PHP, Java, .Net or Ruby

Production and development experience with MySQL

Have strong experience in building APIs

Supports customers throughout the lifecycle of a deployment, from initial engagement to post-live support and maintenance.

Experience with container based deployment using Kubernetes and/or Docker

Good understanding of Data Structures and database concepts

Configure and modify to ensure integrations into the client software according to client workflow processes.

Be a communicator! Have opinions about good and bad products and the ability to justify them, we want to know what you think

Be an advocate of the agile/lean software development process

Be a computer scientist  it is required that you degree in Computer Science, Computer Engineering or have equivalent experience

Be a cultural fit - ethical, collaborative, hard-working, creative, FUN, and willing to push your boss out of the raft

WE VALUE

We are a team at Patientco and thats not just some corporate mumbo jumbo. We expect a ton out of everyone here but thats what makes it great. The whole is far greater than the sum of its parts. We value excellent communication and collaboration skills, creative problem solving, empathy, open mindedness, extreme attention to detail, a healthy dose of grit and a good sense of humor.

SWEET PERKS

We take care of our own at Patientco. Some of the highlights include our infamous Free Food Fridays, casual dress code, no vacation tracking, monthly social events, and of course a full benefits package including health/dental/vision/401k. Bottom line: its a great place to work!

Learn more about Patientco at patientcolife.com !"
Software Engineer / Big Data Platform,"23andMe, Inc.","Join 23andMe and solve cool and challenging engineering problems implementing a large scale data store, building data processing pipelines in the cloud, and powering novel research and analytical capabilities. Work within a collaborative environment of small fast-paced development teams that have a mix of veteran engineers and recent graduates. Come join us in our mission to be the world's trusted source of personal genetic information!

Who we are
Since 2006, 23andMes mission has been to help people access, understand, and benefit from the human genome. We are a group of passionate individuals pushing the boundaries of whats possible to help turn genetic insight into better health and personal understanding.

What you'll do
Design and implement new infrastructure, storage features, and frameworks
Build data storage plan, distributed database schema, and optimized caching in order to support data analysis pipelines for statistical phasing, imputation, & association analyses
Participate in product development process
Write unit and functional tests
Solve performance and scalability problems
What you'll bring

B.S. in Computer Science or a related field
Expert knowledge of Java/Python/SQL and strong experience in building applications using HBase
Working knowledge of C/C++ and/or Java/Scala.
Experience engineering high volume data and scientific dataflows
Experience working with cloud platforms such as AWS, Google Cloud Platform, etc.
Experience with distributed databases and large scale data processing
Experience solving problems with Hadoop and related technologies (HDFS, MapReduce, HBase, Spark, etc.)
Interest in bioinformatics
Thrive with minimum supervision to deliver well-designed high-quality code on time

About Us
23andMe, Inc. is the leading consumer genetics and research company. Our mission is to help people access, understand and benefit from the human genome. The company was named by MIT Technology Review to its 50 Smartest Companies, 2017 list, and named one of Fast Companys 25 Brands That Matter Now, 2017. 23andMe has millions of customers worldwide, with ~85 percent of customers consented to participate in research. 23andMe is located in Mountain View, CA. More information is available at www.23andMe.com .

At 23andMe we value a diverse, inclusive work force and we provide equal employment opportunity for all applicants and employees. All qualified applicants for employment will be considered without regard to an individuals race, color, sex, gender identity, gender expression, religion, age, national origin or ancestry, citizenship, physical or mental disability, medical condition, family care status, marital status, domestic partner status, sexual orientation, genetic information, military or veteran status, or any other basis protected by federal, state or local laws. 23andMe will reasonably accommodate qualified individuals with disabilities to the extent required by applicable law.

Please note: 23andMe does not accept agency resumes and we are not responsible for any fees related to unsolicited resumes. Thank you."
Senior Data Engineer,Shippo,"Shippo lowers the barriers to shipping for businesses around the world. As free and fast shipping becomes the norm, better access to shipping is a competitive advantage for businesses. Through Shippo, ecommerce businesses, marketplaces, and platforms are able to connect to multiple shipping carriers around the world from one API and dashboard. Businesses can get shipping rates, print labels, automate international documents, track shipments, and facilitate returns. Internally, we think of Shippo as the building blocks of shipping. Shippos are a diverse set of individuals. We look for culture and skills add in every new person. Join us to build the foundations of something great, roll up your sleeves and get important work done everyday. Founded in 2013, we are a proud team based out of San Francisco. Shippos investors include Bessemer Venture Partners, Union Square Ventures, SoftTechVC, VersionOne Ventures, FundersClub and others.

As a Senior Data Engineer, you will be responsible for building systems to collect and process events of massive scale to gain operational and business insight into the performance and optimization of shipping services. The Senior Data Engineer will work closely with product, engineering, and business leads in generating customer-facing and internal dashboards, ad hoc reports, and models to provide insights and affect platform behavior. This will also include building and maintaining the infrastructure to collect and transform raw data.

RESPONSIBILITIES

Design and implement large scale data systems, including data pipelines, data warehouse, internal analytics, and customer-facing analytics solutions.
Integrate data from various data stores to ensure consistency and availability of data insights.
Articulate and present findings and recommendations at different levels, with a clear bias towards impactful learning and results
Drive the usability and impact of the data projects in ad hoc analysis and real-time and batch processing
REQUIREMENTS

3+ years working experience as a data engineer
Experience with RDBMS, such as PostgreSQL or MySQL, and NoSQL and columnar data stores; experience with implementing ETL process
Experience with Big Data frameworks such as Hadoop, MapReduce and associated tools, building stream-processing systems, using solutions such as Kinesis Stream or Spark-Streaming
Experience with a scripting language such as Python, Ruby or Perl
Experience with statistical analysis and a data visualization package such as R, Mathematica, Stata, Tableau, etc.
Excellent written, oral communication, and presentation skills
BS or MS in Computer Science or related technical discipline or equivalent job experience
Preferred Experience

Experience with cloud environments and devops tools; working experience with AWS and its associated products
Experience with machine learning infrastructure such as Tensorflow or MXNet
BENEFITS

Benefits: medical, dental, vision (90% covered by the company, incl. dependents)
Take-as-much-as-you-need vacation policy + flexible work hours, remote working possible
Free lunch / drinks / snacks
Fun team events outside of work hours - happy hours, escape the room adventures, hikes, and more!"
Data Engineer,Sidecar,"OVERVIEW:
Were looking for a Data Engineer to join our rapidly growing Data Engineering team.

As a Data Engineer, you will be exposed to all facets of the business and be encouraged to learn and develop skills both within and beyond your role. Youll be working in one of the most collaborative offices in Center City Philly, developing and working with machine learning technology, surrounded by passionate, high-performing individuals.

The Data Engineer candidate is an intelligent, motivated person that enjoys building scalable systems for solving data-intensive problems. Theyre comfortable both designing and implementing these systems across every area of the business, collaborating not only with fellow engineers but also analysts and stakeholders in other departments.

BASIC QUALIFICATIONS AND EXPERIENCE:
Experience with Go or Python

Relational databases (MySQL, PostgreSQL, AWS RDS)

Analytical data warehouses (Amazon Redshift)

Big data frameworks (Elastic MapReduce, Hive, Spark)

CI/CD with Travis or Jenkins

Production deployment with Docker and Kubernetes

Designing scalable backend RESTful microservices

Profiling backend services and databases and improving their performance

Amazon Web Services (AWS)

Agile methodologies

PREFERRED QUALIFICATIONS AND EXPERIENCE:
PySpark

Hadoop ecosystem

DevOps knowledge

Machine Learning/algorithms

Experience in startup environments

Experience in the advertising/ad tech industry

RESPONSIBILITIES:
As a Data Engineer, you will empower our users by ensuring they have access to more of the latest data. To accomplish this, you will:

Scale our technology in order to serve more customers with more featureful products

Identify bottlenecks and implement performance improvements, such as choosing Redshift sort and distribution keys and re-architecting systems for better performance

Support our data scientists by scaling EMR jobs

Instrument our data pipeline to ensure that we meet our SLAs for availability

Implement the companys data strategy across departments so that the whole business is realizing the most value from our data

Integrate with a wide array of data sources, including advertising platforms and SaaS tools

Make data available to multiple systems, including advertising partners, our products UI, and BI tools for our CSM, Sales, and Marketing team

Provide expertise in data engineering and model best practices so that all our production systems are scalable and easy to maintain

Support application development by working on cross-functional project-based teams

Evaluate data technologies, weighing tradeoffs to find the best tools to solve our problems

Pay off tech debt by updating systems to use the latest technologies and best practices

WHO WE ARE:
Sidecar built a technology from the ground up thats dedicated to one mission: Tackle the challenges retailers face in data-driven e-commerce channels. Based on advanced machine learning algorithms and deep data science expertise, our technology digests and analyzes massive, real-time datasets, allowing it to spot trends and continually improve campaigns for Google Shopping, Facebook dynamic ads, and Bing Shopping.

These channels are changing every dayand have become too complex and competitive to manage manually or with outdated technologies. They require automated, sophisticated, and ever-evolving technology to stay ahead. Thats what Sidecar is all about. Were proud to work with retailers of all sizes and verticals in the U.S. and abroad, including Under Armour, Crate & Barrel, and Boscovs, to name a few.

As a company, were a group of energetic people who work hard to make a lasting impact in the rapidly evolving world of e-commerce marketing. And our work is paying offwe are quickly becoming a tech leader and the go-to partner for e-commerce marketing pros.

Our open-floor work environment and Slack-enabled team encourages collaboration, creativity, and growth as individuals and as a company. Many of us come from e-commerce, statistical, or start-up backgrounds, but we certainly welcome talent with outside perspective and experience. We believe in regular company social events inspire friendships and strengthen our bond.

OUR ENGINEERING TEAM:
Our engineering team is instrumental in realizing our vision for our technology and the future of e-commerce marketing. You'll have the opportunity to shape your career path at Sidecar in ways that many other companies cannot offer. Our project-based team structure ensures that no one gets stuck doing the same thing for too long. Well add you to projects that challenge you to grow in a particular technical focus area, such as data processing, client reporting and user interfaces, or site reliability. Opportunities to take on a leadership role come up with every new project. And if you're unsure how you'd like to advance, we'll set you up to get a breadth of experience and see where youd like to spend more time.

BENEFITS:
Competitive salary, based on experience

Medical, dental, and vision coverage

Company-paid disability and life insurance

A company 401(k) plan

Unlimited PTO (our policy is simple, respect your team, and take time off when you need it)

Flexible schedule

Choose your flavor of device: Mac, Windows, Linux

Pre-tax deductions of public transportation costs for your commute

Discounted gym membership

Team and social events"
Engineering Leadership - Data Engineering,Riot Games,"Riot Games was established in 2006 by entrepreneurial gamers who believe that player-focused game development can result in great games. In 2009, Riot released its debut title League of Legends to critical and player acclaim. As the most played PC game in the world, over 100 million play every month. Players form the foundation of our community and its for them that we continue to evolve and improve the League of Legends experience.

Were looking for humble but ambitious, razor-sharp professionals who can teach us a thing or two. We promise to return the favor. Like us, you take play seriously; youre passionate about games. We embrace those who see things differently, arent afraid to experiment, and who have a healthy disregard for constraints.

That's where you come in.

As a Senior Engineering Leader for our Big Data team, you'll help lead product engineering for Riot's Insights data ecosystem. Leading senior technical staff as well as other engineering leaders within the group. You'll help set and drive the engineering vision and leadership for a multi-team group of engineers and data professionals in an innovative and fast-paced environment. You'll be expected to have both deep technical skills in data architecture, big data as well as leadership and organizational skills.

----------------------

Required skills:
----------------------

Bachelor's Degree in Computer Science or equivalent work experience
10+ years of relevant software development experience
5+ years leading and/or managing large, fast-paced and dynamic data teams
Experience guiding a portfolio of products from conception to delivery
A proven track record of designing and managing successful high-capacity, large-scale, data ecosystem projects/architectures
Capable of technical deep-dives into code, networking, operating systems, and storage yet verbally and cognitively agile enough to hold their own in a strategy discussion
Outstanding communication skills, both written and oral
--------

You are:
--------

Product-oriented: you're oriented around building products for players and internal partners, and you know there's no point building tech in a vacuum
Technical strategic thinking: you approach technical problems from multiple angles, exhibiting a strong base of data engineering and/or machine learning knowledge; you have expertise in big data and/or machine learning platforms
A collaborative leader: you're able to lead a team through an ambiguous problem space (Big Data and Data Science are constantly changing); you also have a good feel for when to take a backseat and when to lead the charge.
A proficient manager: you have a deep understanding how to build organizations to deliver on technical product vision
Empathetic: You're armed with deep customer (technical and non-technical) empathy
---------

You will:
---------

Develop engineering vision and roadmap for products and teams within the big data ecosystem
Lead software engineers, data scientists and data analysts in order to deliver on product vision
Provide the mentoring and guidance to create an environment for Riot engineers and analysts to continually reach new heights within their career
Work closely with stakeholders to understand and define the value delivered by each part of the ecosystem
Dont forget to include a resume and cover letter. We receive a lot of applications, but well notice a fun, well-written intro that shows us you take play seriously.

Additional insight into applying to Riot Engineering is available on our discipline page ( https://www.riotgames.com/engineering-careers ).

#LI-CT1"
Senior Data Engineer,BNY Mellon,"Summary:
BNY Mellon Investment Management is one of the world's leading investment management organizations and one of the top U.S. wealth managers. Our business encompasses BNY Mellon's affiliated investment management firms, wealth management organization, and global distribution companies. Our goal is to build and deliver investment and wealth management strategies and solutions to meet our clients needs.

Drawing on deep expertise, we collaborate with our clients to tailor our best ideas and resources to meet their specific requirements. Through our global network we have developed a significant understanding of local requirements. We pride ourselves on providing dedicated service through our teams.

With extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments planned giving programs, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management can help our clients reach their goals.

The Role:
Reporting to the Head of Investment Management (IM) Data Solutions, the Senior Data Engineer will lead, design and architect solutions to complex data integration, process re-engineering, data science, and automation problems. This is a unique opportunity to join the IM Data Solutions team, a small, multi-disciplinary team that is transforming the way information is created, used and communicated within BNY Mellon IM. The group works directly with BNY Mellons IM boutiques on strategic projects, creating valuable tools and insights for decision makers across portfolio management, sales, marketing, and other key areas of the business. The Data Solutions team capitalizes on opportunities across business lines using the best datasets, tools, and practices, like machine learning, econometrics, agile development, cloud technology, and alternative data sources like satellite imagery, web data, and proprietary custody data.

Responsibilities:
Lead data engineering projects to support IM boutiques

Understand business requirements at a deep level, not just technical aspects of project

Build or implement data pipelines, databases, visualizations, and other data tools (hands-on)

Contribute to team in a wide range of technical areas by instituting new practices and staying abreast of the latest technical developments

Take initiative to lead/contribute to overall team efforts in software development, data science, and technical consulting

Work closely with Data Scientists to enable faster insight-generation from complex datasets Qualifications

Basic Qualifications:
BA/BS Degree

6+ years of data engineering experience

SQL expertise

ETL & database expertise

Python competence

Microsoft Excel proficiency

Preferred Qualifications:
Advanced Degrees or CFA

NoSQL (e.g. MongoDB)

Python expertise

Experience using Microsoft Azure

Experience with machine learning algorithms

Professional experience working in financial services and/or asset management environments
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments & safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.

As one of the world's leading investment management organizations and one of the top U.S. wealth managers, BNY Mellon Investment Management combines agility, insight and scale to create and deliver strategies and solutions to address our clients' needs. Encompassing BNY Mellon's investment management firms, wealth management organization and global distribution teams, we draw on deep expertise to collaborate with clients and tailor our best ideas and resources to meet their specific needs. We pride ourselves on providing dedicated service through our network of global professionals who have a deep understanding of local requirements.

With our extensive experience in anticipating and responding to the investment and financial needs of the world's governments, pension plan sponsors, corporations, foundations, endowments, advisors, intermediaries, individuals and families, and family offices, BNY Mellon Investment Management is dedicated to helping clients reach their goals.

BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.
Minorities/Females/Individuals With Disabilities/Protected Veterans.

Primary Location: United States-New York-New York
Internal Jobcode: 33645
Job: Asset Management
Organization: IM Infrastructure-HR14826
Requisition Number: 1716329"
Senior Data Engineer,Oracle,"Senior Data Engineer-170013P3
Preferred Qualifications

About Moat
We help brands and media companies push the envelope on privacy-friendly models of advertising, help them make sound product decisions that involve running fewer ads, help make their ads load faster, fight botnets, and strive to de-fund fake news. We are a New York City-headquartered startup with a mission to help the world's top publishers and marketers make smarter decisions through real-time data. Our analytics and intelligence software is used by The New York Times, Cond Nast, Kellogg's, P&G, Spotify, and hundreds of other major platforms and advertisers, and we have unique measurement integrations into Facebook, Instagram, Twitter, Snapchat, and YouTube.

About the Opportunity
Design and implement systems to process and aggregate terabytes of streaming data in real time.
Build micro-services and APIs that facilitate data processing and expose Moat data to our clients
Classify and analyze data using both automated and crowd-sourced approaches.
Build predictive models for fraud detection and ad performance optimization
Write applications for real-time ad decisioning that run on geographically distributed clusters

About You
BS, MS, or PhD in computer science, engineering or other related field
4+ years experience with data-intensive backend programming
You have strong understanding of statistics
Depth experience in one or more of the following areas--Data science and analysis, distributed data processing, complex web-app design, and / or cloud infrastructure design (AWS)
You understand experimental design, and can build for measurement and interpretation of results
You write clean, well-structured, production-quality code in Python
Strong Bash, and Linux systems skills
You have built and deployed large-scale distributed data processing pipelines
Ability to design and deploy lightweight web services that are scalable and fault-tolerant
Familiarity with various data processing and storage technologies including Postgres / Vertica / Redshift, Dynamo DB, Redis, Kinesis, and MapReduce
Passion for extracting insights from large volumes of data

About the Team
We are passionate, excited, and thrive in a fast paced culture.
We love sharing our knowledge and learning about new technologies.
We are the type of people who take risks when looking for novel and creative solutions to complex problems.
We care about solving big-picture, systemic problemslooking beyond the surface to understand root causes so that we can build complete and long-term solutions.

Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will take an active role in the definition and evolution of standard practices and procedures. You will be responsible for defining and developing software for tasks associated with the developing, designing and debugging of software applications or operating systems.

Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. Leading contributor individually and as a team member, providing direction and mentoring to others. BS or MS degree or equivalent experience relevant to functional area. 7 years of software engineering or related experience.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
"Software Engineer, Data",Energy Efficient Equity,"About Energy Efficient Equity

Energy Efficient Equity (E3) provides financing to residential and commercial property owners for use on their energy efficient remodels, installations and upgrades. For example, we finance water efficiency products, solar products, HVAC upgrades and a lot more. To provide this financing we do so through an investment vehicle known as Property Assessed Clean Energy (PACE).

Were building out the data, infrastructure, tools, and applications to support our business and having fun doing it. You'll help design and develop the applications we use to take loan applications, approve them, pay contractors, issue bonds and much more.

Were just getting started at E3 and youll have the opportunity to help define the engineering culture and technology we use.

Things you might work on

Define and develop ETLs and data applications for use in our products

Design and build a data warehouse that enables the rest of the company to build self-service dashboards and reports.

Work with product engineers to design performant database schemas

Relevant Experience to us

Excellent knowledge of Python and Python data tools (eg: pandas, jupyter, scikit-learn)

You have a real passion for implementing ETL Pipelines using Distributed Databases (Elasticsearch, S3, Hadoop, Redshift, BigQuery)

Deep understanding of relational databases (PostgreSQL)

You have strong systems fundamentals

You believe in automation, testing and instrumentation and have demonstrated it throughout your career

You are an excellent communicator with desire to share and learn

5+ years of full time industry experience

Degree in Computer Science or related field

Experience in Finance or Real Estate industries

Benefits

Competitive Compensation

Paid Time Off

Medical, Dental & Vision Insurance

401K

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Database Engineer,Tableau,"What you'll be doing....

You will play a key role on the Database Team in developing and operating Tableaus engineering databases. Tableau is all about enabling our customers to easily pose open ended questions to their data, irrespective of which one of the 100+ supported platforms house the data. The Database Team is responsible for providing well-documented, stable and consistent suite of databases for standard testing. Building on this core of standard testing databases, the Database Team provides database related expertise to Continuous Integration and Delivery, Sustaining Engineering, Technical Support and Quality Assurance teams. Finally, the Database Team is also responsible for smooth operation of the databases supporting the CI/CD pipeline running on premise and in the cloud, including monitoring, availability, performance optimization, replication and backup/recovery.

This role requires a grounding in relational database skills with exposure two or more SQL databases such as Postgres, MySQL, SQL Server etc. Exposure to one or more NoSQL, bug data or multidimensional stores such as SSAS, Hadoop, SAP Hana, SAP BW, etc. is also valuable. Though technically titled as a DBE position, this is a DevOps role that includes database development and operational aspects. Development aspects include continuous integration, automation, change control, scalable and performant designs. Operational aspect include monitoring and diagnosis of database-related issues, and defining database management and maintenance processes. Many tasks on the Database Team have a clearly defined goal, but the paths that can be taken are often unknown or highly variable. This allows for a good deal of self-directed investigation and the opportunity to present the team with optional paths and recommendations, ideal for the self-starter who enjoys jumping in the deep end.

Some of the things you'll be doing include...
Installing, upgrading, maintaining and documenting new and existing databases
Scripting standard content loading for each database type using Python, SSIS or ETL tools such as Talend
Creating, maintaining and deploying OS only, OS + Database and specialized VM templates (on-prem and AWS)
Assist in refining automation around OS and database updates, monthly refreshes of standard testing database VMs and self-service VMs
Creating database and virtual environments for Connectivity team to support new/existing data connector development and testing effort.
Providing database related support to Continuous Integration and Delivery, Technical Support, Sustaining Engineering and Consulting teams.
Who you are...
Domain: Deep understanding of MySQL or PostgreSQL database concepts and processes
Knowledgeable in the following:
Knowledge and Understanding of both Windows and Linux operating systems
Knowledge and Familiarity with using virtual machines and infrastructure (VMWare or OpenStack experience a plus)
Knowledge and Understanding of scripting using PowerShell or Python
Knowledge and Understanding of AWS (Both deployment and maintenance DBs in AWS)
Expertise in... designing database for performance, availability and scalability, as well as diagnosing database performance issues
Experience:
3+ years of experience working as a database developer, data engineer, DevOps engineer, automation engineer or related roles.
Good understanding of SAP BW is a plus
Experience with multiple database vendors, virtual infrastructure, and enterprise systems, supporting development teams and driving for continuous improvements to their support and testing systems.
Wide experience with various databases of many vendors and types.
Understanding and experience with multidimensional data sources (Microsoft Analysis Services and Oracle Essbase a plus)
Education: BS in Computer Science, similarly technical degree or equivalent directly applicable experience.
Tableau Software is a company on a mission. We help people see and understand their data. After a highly successful IPO in 2013, Tableau has become a market-defining company in the business intelligence industry. Our culture is casual and high-energy. We are passionate about our product and our mission and we are loyal to each other and our company. We value work/life balance, efficiency, simplicity, freakishly friendly customer service, and making a difference in the world!

Tableau Software is an Equal Opportunity Employer."
Senior Data Engineer,"Analytical Mechanics Associates, Inc.","AMA is seeking several Data Engineer for our Dallas, TX location!

Analytical Mechanics Associates, Inc., (AMA) is proud of our customer relationships, our diverse and dynamic work environment, and our employees' career satisfaction. AMA is a small business specializing in aerospace engineering, business analytics, information technology, and visualization solutions. AMA combines the best of engineering and mathematics capabilities with the latest in information technologies and visualization to build creative solutions. Headquartered in Hampton, VA, AMA has operations in Huntsville, AL; Dallas and Houston, TX; Denver, CO; Santa Clara and Mountain View, CA; New Orleans, LA; and Hancock County, MS. We offer a competitive salary and attractive benefits package. Our website describes our corporate values and how they apply to the professional success and growth of our employees.

Job Description AMA is actively seeking several Data Engineers for our Dallas, TX location. Project duration is 6-12 months with possible extensions. Projected hours are 40 per week. Ideal candidates would display the following skills:

Who you are:
You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule
Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results
You love to learn new technologies and mentor junior engineers to raise the bar on your team
It would be awesome if you have a robust portfolio on Github and/or open source contributions you are proud to share
Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques in web design
You use, or are passionate about learning Spark, Scala, Akka, Kafka, Cassandra, Accumulo, HBase, Hadoop, HDFS, AVRO, MongoDB, and Mesos  and anything else that will help you solve big, messy problems
Essential Skills/Experience:
Hands on experience in data engineering (
>1 yr), especially in some/all aspects of designing, building and monitoring ETL pipelines (e.g., using Lambda, Kafka, S3, EMR/EC2, Kinesis) Data warehouses (e.g., on Redshift or any other MPP) Data lakes (e.g., using S3/HDFS & Spark)
Strong background in software development with expertise in Python or Java
UNIX/LINUX
Preferred Qualifications:
Masters Degree
Understanding of software security and threat models, and experience building secure applications
Good understanding of data-governance concepts
Experience administrating AWS
Experienced with Stream Data Platforms such as Apache Kafka Streams or Apache Storm Agile
Familiarity with Agile engineering practices"
Data Engineer,GoHealth,"Job Description

GoHealth is looking for a Data Engineer responsible for the design, development, and maintenance of a data warehouse for the company. We are seeking candidates who have extensive experience in data modeling and warehouse builds that meet the complex requirements of various business units. The Data Engineer will own the development of infrastructure necessary for centralized reporting, data science, predictive analytics, and machine learning.

Responsibilities:
Work in conjunction with a Warehouse Architect to design, build, and deploy a data warehouse in the cloud
Create automated data pipelines to the data warehouse from a variety of sources including MySQL, AWS, CouchDB, JSON, XML, and flat files
Ensure quality, validity and consistency of all data points, metrics and KPIs
Perform all required validation testing and adhere to development standards and processes
Collaborate with users making reports, dashboards, and running statistical analysis to ensure data is transformed into a format that can be easily utilized by them
Create and maintain documentation and metadata for the data warehouse and data pipelines
Ensure compliance with regulatory requirements for data transmission, storage, and data access
Identify inefficiencies and rigid processes, and design solutions increasing automation and scalability
Ability to work with the rest of the BI Team to cross-train and provide support for other BI tasks like ETL Development, Cube Maintenance, Data Analytics and Requirements Gathering Qualifications

Bachelors Degree in computer science or equivalent experience required
5+ years experience in database development, preferably on cloud-based infrastructure
Experience developing data lakes and other data storage and processing solutions with tools such as SQL Server, MySQL, Postgres, Cassandra, Hbase, Solr, Spark and/or equivalent
Experience supporting Advanced Analytics/Data Mining/Data Science/Neural Networks (including integration tools like Python, R, TensorFlow, etc)
Expert understanding of data warehousing concepts and dimensional data modeling
Advanced ability in SQL 2012 and above, experience in MySQL preferred but not required
Proven experience extracting data from a variety of data sources (MySQL, Excel, Access, SharePoint, Azure, Text files) both on-premise and in the cloud
Experience with source control tools like Git, BitBucket/SourceTree, SVN or TFS
Strong analytical and problem solving ability with strong attention to detail and accuracy
Ability to handle multiple tasks and adapt to a constantly changing environment
Self-starter with the ability to work independently and take initiative
Excellent written and oral communication skills, with the ability to articulate complex processes to individuals of varying technical abilities

Additional Information

Open vacation policy
401k match program
Medical, life, dental, and vision benefits
Flexible spending accounts
Subsidized gym memberships
Commuter and transit benefits
Professional growth opportunities
Casual dress code
Generous employee referral bonuses
Happy hours, ping-pong tournaments, and more company-sponsored events
GoHealth is an Equal Opportunity Employer"
Senior Data Engineer,"InfoSpace, Inc.","Picture yourself adding your creativity toward game-changing innovation. Our businesses operate at the intersection of tax preparation and financial planning, and our competitive advantage lies in their unique combination. With award-winning products and services through our TaxAct and HD Vest brands, Blucora is on the forefront of financial tech, turning taxes into opportunity for our consumers, small business customers and tax professionals. We have a culture of creating market disruption; a place where big ideas, and those who think them, can flourish. If you are interested in being part of a team that dreams large, talks straight, and helps millions of customers every year build their financial future, then we should talk!

Our Data Platform team is seeking an experienced Data Engineer with software engineering and system administration skills to join our team. Our data platform utilizes a polyglot persistence pattern using SQL Server, MongoDB, ElasticStack, Redis, Hadoop and other technologies to deliver a suite of data services to support our production systems and provide analytical capabilities to our analysts and data scientists.

In this role, you will be supporting multiple components of our data platform by: working with product owners and business partners to understand the data requirements for new projects/initiatives; collaborating with technical teams to develop data solutions to support business initiatives; building and maintaining the data architecture to store the data; and finally to administer and support the data platform to ensure performance targets and system SLAs are maintained.

Major responsibilities:
Design, implementation and delivery of complete analytic solutions in AWS
Own the data architecture and infrastructure
Architect, build and maintain high performing ETL processes, including data quality and testing
Define and build technical/data architecture for data warehouse, data marts and big data solutions (including data and dimensional modeling)
Develop analytics with a mind toward accuracy, scalability and high performance
Provide technical guidance and thought leadership to other members of the team
Establish lean data governance to drive data standards and data quality. Be an evangelist in the company for data-informed thinking and decision-making.
Education and Experience:
Experience implementing and operating solutions with one or more No-SQL technologies in a production environment.
Experience in architecting and building data warehouse systems and BI systems including ETL.
Expert knowledge in modern distributed architectures and compute / data analytics / storage technologies on AWS
Deep knowledge of RDBMS (SQL Server, MySQL, etc.) and NoSQL databases such as MongoDB, DynamoDB, Cassandra
Advanced knowledge of a programming language such as Java/Python/Scala
Outstanding analytical skills, excellent team player and delivery mindset.
Experience in performance troubleshooting, SQL optimization, and benchmarking.
Strong architectural experience in context of deploying cloud-based data solutions.
Good communication (oral and written) and interpersonal skills.
Enthusiastic attention to detail.
Demonstrated intellectual curiosity and ability to acquire new skills quickly
Ideal candidate will also have:
Experience with AWS Redshift,
Experience with Alteryx, Datameer & other data analytics toolsets
Experience with data visualization tools such as Tableau, QlikView
Experience in agile development methodologies.
Knowledge of statistical concepts and their application in reporting and data mart applications.
Demonstrated ability to champion projects from beginning through implementation, meet deadlines, handle multiple priorities and perform job responsibilities accurately with minimal supervision
We offer a competitive salary, outstanding benefits package that includes medical, dental, vision. Life insurance, paid vacation and sick days, paid holidays, tuition reimbursement, and 401(k) with company match.

Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, or disability."
Software Data Engineer - Productivity Applications,GoDaddy,"We are looking for a Data Engineer that will help us discover knowledge hidden in vast amounts of data, and help us make smarter decisions to deliver great products. Your primary focus will be the collecting, storing, processing, and analyzing of huge sets of data. The primary focus will be on choosing optimal solutions to use for these purposes, then maintaining, implementing, and monitoring those solutions. You will also be responsible for integrating them with the architecture used across the company. This product group is an important contributor to the companys revenue, customer growth and technical innovation.

Responsibilities:
Selecting and integrating any Big Data tools and frameworks required to provide requested capabilities

Enhancing data collection procedures to include information that is relevant for building analytic systems

Processing, cleansing, and verifying the integrity of data used for analysis

Doing ad-hoc analysis and presenting results in a clear manner

Requirements:
Bachelors Degree in Computer Science or a related field
2-5 years of experience with Hadoop, MySQL and MSSQL databases
Proficiency in using query languages such as SQL, Hive, and Pig

Experience with Spark

Experience with integration of data from multiple data sources

Demonstrable experience with common data science toolkits such as R, Pandas, NumPy. A high level of proficiency in Pandas is desired

Great communication skills

Data-oriented personality

Experience with data visualization tools such as D3.js, GGplot, etc

Experience building RESTful APIs to provide other internal teams with access to our data is highly desirable

Applied statistics skills, such as distributions, statistical testing, regression, a big plus

About GoDaddy

GoDaddy powers the world's largest cloud platform dedicated to small, independent ventures. With nearly 17 million customers worldwide and over 72 million domain names under management, GoDaddy is the place people come to name their idea, build a professional website, attract customers and manage their work. Our mission is to give our customers the tools, insights and the people to transform their ideas and personal initiative into success. To learn more about the company visit www.GoDaddy.com .

GoDaddy is proud to be an equal opportunity employer. We will not discriminate against any applicant or employee on the basis of age, race, color, creed, religion, sex, sexual orientation, gender, gender identity or expression, medical condition, national origin, ancestry, citizenship, marital status or civil partnership/union status, physical or mental disability, pregnancy, childbirth, genetic information, military and veteran status, or any other basis prohibited by applicable federal, state or local law. GoDaddy will consider for employment qualified applicants with criminal histories in a manner consistent with local and federal requirements.

If you need assistance completing an application for a position with us, please reach out to our Recruiting Team at Recruiting@godaddy.com"
Threat Data Engineer,Carbon Black,"Boulder, CO

Why Carbon Black?

At Carbon Black, youll have the chance to make an impact in the ever-evolving cybersecurity space. Our advanced technology tackles even the toughest challenges and stays ahead of the latest threats. If you want to join an agile company thats building bleeding edge technology in the cloud, Carbon Black is the place for you. Driven by passionate people who are dedicated to making the world safer, its no wonder weve been named a Top Place to Work by the Boston Globe for four consecutive years. Join us!

Why You Matter

Were looking for a Threat Data Engineer who will own the end-to-end ETL process from numerous Carbon Black cloud systems into our backend Analytics environment. This individual will be responsible for building, maintaining, monitoring and validating this infrastructure, which forms the backbone of our Predictive Security Cloud ecosystem, which in turn powers all Carbon Black products.

What Youll Do

Bring a devops mentality, taking responsibility for the ETL / stream processing for our backend analytic environment from end-to-end

Ensure data quality and availability for ongoing analytics projects through stable ETL processes

Build out resilient and scalable components for our stream processing machinery

Design, implement and validate the monitoring and alerting infrastructure as necessary to ensure uninterrupted and performant data flow

What Youll Bring

B.S. in Computer Science or equivalent experience

2-8 years of experience working with a production AWS cloud infrastructure

Strong Python skills

Excellent troubleshooting and performance optimization skills

Apache Spark experience a plus

Security Experience a plus

Who We Are

Carbon Black is the leading provider of next-generation endpoint security. Carbon Blacks Next-Generation Antivirus (NGAV) solution, Cb Defense, leverages breakthrough prevention technology, Streaming Prevention, to instantly see and stop cyberattacks before they execute. Cb Defense uniquely combines breakthrough prevention with market-leading detection and response into a single, lightweight agent delivered through the cloud. With more than 13 million endpoints under management, Carbon Black has more than 3,600 customers, including 30 of the Fortune 100. These customers use Carbon Black to replace legacy antivirus, lock down critical systems, hunt threats, and protect their endpoints from the most advanced cyberattacks, including non-malware attacks.

Carbon Black, Inc. is an EEO/AA employer. Carbon Black is an inclusive employer that believes in workplace equality, supports diversity, creates a welcoming environment, and respects the unique qualities each individual brings to the company."
Big Data Engineer,HP,"HP is a proven leader in personal systems and printing, delivering innovations that empower people to create, interact, and inspire like never before. We leverage our strong financial position to extend our leadership in traditional markets and invest in exciting new technologies. HP has an impressive portfolio and strong innovation pipeline across areas such as:  blended reality technology - our unique Sprout by HP will change the way people do things  3D printing  multi-function printing  Ink in the office  tablets, phablets, notebooks  mobile workstations Were looking for visionaries who are ready to make an impact on the way the world works. At HP, the futures yours to create!

Applies basic foundation of a function's principles, theories and concepts to assignments of limited scope. Uses professional concepts and theoretical knowledge acquired through specialized training, education or previous experience. Develops expertise and practical knowledge of applications within business environment. Acts as team member by providing information, analysis and recommendations in support of team efforts.

Exercises independent judgment within defined parameters. Responsibilities: Participate in the design, implementation, and deployment of successful large-scale systems and services in support of our fulfillment operations and the businesses they support. Participate in the definition of secure, scalable, and low-latency services and efficient physical processes. Work in expert cross-functional teams delivering on demanding projects.

Functionally decompose complex problems into simple, straight-forward solutions. Understand system interdependencies and limitations. Share knowledge in performance, scalability, enterprise system architecture, and engineering best practices. Education and Experience Required: Bachelor's or Master's degree in Computer Science, Information Systems, or equivalent.

Typically 0-2 years experience. Knowledge and Skills: Experience or understanding of software applications design tools and languages. Good analytical and problem solving skills. Working knowledge of Big data technologies - like SCALA/SPARK.

Experience on Amazon Web Services is appreciated. Understanding of design for software applications running on multiple platform types. Understanding of basic testing, coding, and debugging procedures. Good written and verbal communication skills; mastery in English and local language."
Data Science Engineer,Capital One,"Wright Brothers (51050), United States of America, Seattle, Washington

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Science Engineer

apital One (yes, the whats in your wallet? company!) is rethinking the way the world approaches personal investing . Were experimenting, innovating, and delivering breakthrough experiences for 65 million customers. We love to be curious, to dream, and ask What if? Oh, and we love to write code .

As a Capital One Investing software engineer, you'll work on everything from customer-facing web and mobile applications, to highly-available, highly-scalable micro-services, to back-end systems with sophisticated data pipelines. All on the cloud!
You will drive design, implementation, testing, and release in an agile environment, using modern methodologies and open source tools. Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, youll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who love to solve real problems and meet real customer needs

The person we're looking for:
has a sense of intellectual curiosity and a burning desire to learn
is self-driven, actively looks for ways to contribute, and knows how to get things done
is deliriously customer-focused
values data and truth over ego
has a strong sense of engineering craftsmanship, takes pride in the code they write
believes that good software development includes good testing, good documentation, and good collaboration
has great communication and reasoning skills, including the ability to make a strong case for technology choices

You might notice that theres no mention of specific languages or technologies. Thats because your commitment to learn new things is every bit as important to us as what you have already done. Maybe even more so because we dont want to be doing the same thing tomorrow that were doing today. You accept change, want to grow, and evolve into a better member of the team. But just in case, here are some buzzword-worthy tools and technologies we currently use, so the search engines can help you find us: Machine Learning, Data Science, Hadoop, MongoDB, Zookeeper, Cassandra, AWS, Docker, micro-services, Go, Java, Scala, Clojure, C#

Basic Qualifications:
at least 5 years of experience in software development
at least 3 years of Web Services software development
at least 1 year of setting up a CI/CD pipeline

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer - Consultant,Clarity Insights,"Do YOU love working with Data in machine learning, real-time analytics, or Big Data in a Cloud environment? Do you find that you want to choose other technologies but work for a vendor that limits the options that you can recommend? We are looking for a Data Expert to help us help our clients evaluate and implement big data and advanced analytics solutions in the Cloud. At Clarity Insights, we only provide Data & Analytics expertise. Its all weve ever done and we love specializing in it and working on hard problems.

Clarity Insights is the largest independent professional services firm focused exclusively data analytics solutions. Our Data Engineering practice has one vision: to drive better business outcomes through data and analytics. We provide data analytics advisory and solution delivery consulting services. We believe that nothing sells like great delivery, and are committed to our people and client delivery excellence. With continuing, aggressive growth plans for the next five years, Clarity is seeking outstanding data analytics leaders to successfully qualify, propose, close and lead client solution delivery.

As a Data Engineer for Clarity Insights you will not only be building data pipelines to efficiently and reliably move data across systems, but also building the next generation of DW/Big data solutions to enable us to take full advantage of this data. In this role, your work will broadly influence our clients data consumers and analysts. You will get the opportunity to work with focused and scaled objectives at high profiled clients that have some of the most challenging problems to take on.

Responsibilities
Hands-on self-directed engineer who enjoys working in collaborative teams.
Develops highly scalable, end to end process to consume, integrate and analyze large volume, complex data from sources such as Hive, Flume or Kafka.
Integrate datasets and flows using a variety of open source and best-in-class proprietary software.
Profile and analyze complex and large datasets.
Collaborate and validate implementation with other technical team members.
Coordinates and leads internal meetings.


Basic Qualifications
Bachelors Degree and 3+ years of work experience
Experience building processes around data transformation, data structures, metadata, dependency and workload management with object-oriented scripting languages such as Python or other object-oriented/object-function languages (such as Java, C++ or Scala) is required.
Strong SQL experience analyzing, transforming and integrating high volume, complex data sources with considerations for accuracy and efficient performance.
Experience transforming data out of and into Hadoop/Hive is preferred.
Fluency in Linux development and common development-related configuration tasks.
Demonstrated experience implementing custom ETL solutions in Data Warehousing environments.
Expertise creating efficient data structures with considerations for distribution, segmentation, colocation, etc., for ELT and analysis access paths; understanding of data management concepts such as 3NF, Dimensional, Data Vault, NoSQL/Key-value and their applications for data management and analysis.
Ability to analyze high volume data against business requirements to identify deliverables, gaps and inconsistencies.
Passion to build business driven, data solutions regardless of technology.
Excellent communication skills with the ability to identify and communicate data driven insights and technical approach.
Ability to contribute independently and self-manage delivery in a collaborative environment.
Candidates must be comfortable with a national travel model to the client locations weekly (M-TH each week is typical).
Why Clarity Insights, Why Consulting and Why Now?
We dont try to be everything to everybody all the time. We specialize in Data & Analytics and will remain a platform and tool agnostic company so you can grow technically throughout your career. It sounds ridiculous but you actually need to be 100% technical and 100% business with strategy because we dont hire non-communicative robots who have a one size fits all approach. We often could speak to a CFO or Head of Underwriting about a business or finance problem, and based on the need for real-time and affordable scalability, we can outline  in terms they can understand  why they should think about a cloud solution for big data and analytics. If we step across to the DevOps lab, we can easily pick up on a conversation about Apache committers, OSF, full stack development, microservices, containers and much more. Were not trying to boil the ocean. We want to grow from 400 people to 1700 people in 4 years to take the market an accessible set of consulting skills. We have a lot of fun together. Most people join here and stay here for the people. That said, we are popular so we do rack up some travel miles. I have a million things to share about why that may change and why that must stay the same for now, but heres a suggestion  if you LOVE Cloud Computing for Big Data and/or Advanced Analytics, Machine Learning, etc we should talk.

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.

GLDR"
Virtualization/Data Protection Engineer,ScaleMatrix,"Job Information ScaleMatrix is currently seeking a Virtualization/Data Protection Engineer to support continued development of its internal and client-facing systems. ScaleMatrix is a growing company with a dynamic and exciting work environment. This role requires a resource with excellent communication skills and an ability to learn about and understand a broad range of technical issues. This person must be able to work as part of a small, focused team. The engineering team at ScaleMatrix utilizes compliance-oriented methodologies. Teamwork, the ability to function calmly under pressure, and flexibility are crucial attributes to bring to this position.

Responsibility This Virtualization/Data Protection Engineer will primarily be involved in projects delivering virtualization and data protection, utilizing a variety of virtualization, backup, and disaster recovery products, within the datacenter environment. Tasking will include deploying and managing numerous production and development VMware vSphere deployments across multiple hosts and VMs.
It will also include deploying individual and multi-tenant disaster recovery and cloud backup solutions powered by Zerto, Veeam, and CTERA.
This engineer will work closely with clients to troubleshoot guest OS issues in Windows and Linux desktop and server products.

Qualification
 Minimum two to four years of experience in a datacenter or similar environment.
 Experience with back-office solutions and IT architecture, as well as experience with cloud-based offerings.
 Excellent communication and interpersonal skills, including the ability to communicate technical concepts in a simple and straightforward manner.
Education Requirements
 Proficiency with VMware ESXi and vSphere.
 Proficiency with installation and configuration of physical servers.
 Proficiency with troubleshooting and repair of physical servers.
 Proficiency with HyperV environments.
 Familiarity with AD, DNS, Powershell, Barracuda Firewalls, Exchange, Nagios, Zabbix, FTP, SMTP, DHCP, and other products.
 Basic proficiency in Linux system administration.
 Basic proficiency in Windows system administration.
 Experience with a broad section of the VMware product catalog including ESXi, vCloud Director, vSAN, vShield Manager, NSX, Chargeback Manager, and vCloud Usage Meter.
 Experience with PowerCLI.
 Experience with MS SQL.
 Basic IT certifications"
"Data Engineer, Programmatic",360i,"Overview

360i is an award-winning agency that helps brands capitalize on change. As a highly strategic creative and media partner with a deep understanding of how people discover brands and share stories, we dont just react to changes in the marketplace  we predict and define them. By bringing together some of the smartest, most curious people with expertise in data, creativity, media, strategy, search, and social, we help our clients achieve their objectives and drive their business forward. And our hustle doesnt go unnoticed. Weve been recognized in Advertising Age's Agency A-List issue seven years in a row, named one of Fast Companys Most Innovative Companies, and ranked amongst the best Lead Agencies and Search Agencies by Forrester Research. Thats some range. We work with brands including HBO, National Geographic, DSW, Mondelez and Capital One. To learn more about us, visit www.360i.com.

The Data Engineer will work directly with 360is Programmatic Buying Group to improve performance and strengthen the groups val prop through advanced analytics. The ideal candidate is someone who lives and breathes curiosity and enjoys figuring out solutions without prescriptive instructions. This person wants to be a part of growing a programmatic practice and help take things to another level by establishing tighter analytics rigor around all decision-making. S/he must have a passion for leading projects and building data infrastructure. S/he will have the opportunity to architect and direct the development of robust intelligence systems and predictive modeling solutions. S/he must be comfortable with working in a cross-office environment. Business acumen and the ability to effectively communicate analytical work to non-analytical audiences are essential.
Responsibilities

Apply advanced data mining techniques and predictive modeling to determine which tactic(s), audience(s), and inventory (site & placements) to target
Innate understanding of data storage, data hygiene, and data maintenance. Build scalable analytics systems for clients across the Programmatic Buying Group
Work with clients and agency teams to discover context and define requirements
Design systems to make data accessible and useful
Build large scale data warehousing solutions
Develop and manage data pipelines and integrations
Understand and implement automated business rules and analyses. Ability to write scripts or algorithms to help automate reporting and programming feeds via Python, Java, or Scala
Qualifications

2+ years working experience with Bachelor's Degree in a quantitative field
Ability to multi-task and communicate effectively across teams in a deadline-driven atmosphere
Possess strong written and verbal skills with the ability to create presentations for explaining and defending your audience modeling approach and performance outcomes to non-analytics people
A sense of mission. The ability to receive direction, take ownership, and figure out the best solution
using a combination of critical thinking and initiative.
Intuitive competence. The skills to work under uncertainty and produce good work even when the
landscape is shifting and you haven't been given enough to work with.
Top level focus. The ability to keep high-level direction under consideration even when you're deep,
deep in the weeds.
Able to dependably complete projects. Also requires taking responsibility for and learning from failure.
Business intelligence: SQL databases, ETL processes, big data processing
Business tools: Powerpoint, Excel
Cloud infrastructure: AWS or Google Cloud
DevOps tools: Kanban boards, Git, Jira
Marketing data: display, search, social
Scientific Python libraries: Pandas, Scikit, Statsmodels
Web Data visualization: Commercial dashboarding tools, Vega, D3.js, Altair, Flask"
Data Engineer,Amsted Industries,"Amsted Rail is a globally integrated manufacturer. Railroads, as well as car builders, owners, and leasing companies rely on us to provide components and innovative systems for freight cars and locomotives, repair and service and location and condition monitoring. From the big picture to the finest details, Amsted Rail is continuously improving the heavy haul landscape.

We have an immediate opening for the position Data Engineer in Overland Park, Kansas. We are accepting applications for candidates in Overland Park, Kansas or St. Louis, Missouri.

Position Description:

The suitable candidate will function as the lead in projects related to all aspects of collecting and preparing data for analytical or operational uses. The Data Engineer responsibilities include building data pipelines to pull together information from different source systems; integrating, consolidating and cleansing data; and structuring it for use in individual analytics applications as well as assisting with the overall data architecture. The candidate will work with domain experts, data scientists, analyst and engineers who are analyzing and visualizing the data to drive efficiencies and business decisions.

Position Responsibilities:
The Data Engineer will have exposure to all business units, plants, processes, and products in order to work with data scientists, business analyst and other analytics community members.
Supporting and providing data in a ready-to-use form to data scientists who are looking to run queries and algorithms against the information for predictive analytics, machine learning and data mining purposes.
Develops, constructs, tests, and maintains architectures such as databases and large scale processing systems. Responsible for compiling and installing database systems, writing complex queries, scaling to multiple machines, DB administration, and putting disaster recovery systems into place.
The ability to build a robust, fault-tolerant data pipeline that cleans, transforms, and aggregates unorganized and messy data into databases or data sources.
Work with business units and departments to deliver data aggregations to executives, business analysts and other end users for more basic types of analysis to aid in ongoing operations.
Recommending new data streams and ensuring data integrity
Wrestle with problems associated with database integration and messy, unstructured data sets. Their ultimate aim is to provide clean, usable data to whomever may require it.
Identifying new projects and opportunities, and the associated data requirements
Professional skills:
Microsoft SQL Server Database management and query language.
Data Modeling including management of both structured and unstructured data sets -- must be versed in different approaches to data architecture and applications including traditional Data warehousing concepts and Data Lakes.
Skilled in extract, transform and load tools (Data Services, Informatica, or SSIS) and REST- oriented APIs for creating and managing data integration jobs, and providing data analysts and business users with simplified access to prepared data sets.
Practical understanding of statistics, predictive analytics, machine learning, and artificial intelligence algorithms concepts which may include C#, Java, Python, Ruby, Scala and SQL.
Ability to understand a variety of applications and domains (embedded sensors, plant processes, shop floor, ERP,etc.)
Excellent written and verbal communication skils
Must be able to work with all relevant technical and non-technical parties to ensure that quality results are obtained.

Position Requirements:
B.S in Computer Science, Engineering or related disciplines with experience in data engineer and database administration.
Willingness to travel as much as 30% of the time.
Desire to work in a dynamic environment and have a continuous improvement and development mindset
Through competitive wages, outstanding benefits, and internal advancement opportunities, you will have the chance to create a career of which you can be proud. If you are open to relocation, there are also vast advancement opportunities within the Amsted Corporation.

We offer a full range of benefits including medical, dental, vision, 401(k) with employer matching and an Employee Stock Ownership Plan. We are 100% employee owned.

All interested applicants please apply online at www.amstedrail.com."
Software Engineer,stepcase,"You Are an Engineer Who:
Get excited by product ideas and features
Likes to use your engineering skills to influence people
Comfortable working on multiple projects at once
Cares more about the final product quality than about the technology
Not afraid to fail, learn and retry with different ideas
Loves solving problems, not only on the code level
Can work independently and work as a team
What You Will Do:
100% Remote because engineers don't like wasting time in traffic
Transform Lifehack from a content-driven product to a combined content and software solution product
Develop a product that can impact a lot of people around the world by making their time more valuable
Manage and build system architecture on AWS to scale and run our website.
Design, document and structure our codebase to enable other remote engineers / teams to contribute
Code a lot
Qualifications
Youve at least 2 years professional experience as a full stack engineer with devops experience.
Industry experience in frontend web applications and backend (React & PHP)
Industry experience in managing web services in cloud computing environment (AWS)
Experience with data science and how to effectively gather and report metrics
Fluent in written and verbal English
Our Main Tech Stack:
React, PHP, NodeJS, and HTML5/CSS
RDBMS, Message Queue, Caching (Memcached, Redis) and Cloud Computing (AWS)"
Database Engineer,Deloitte,"Do you enjoy cutting edge technology and solving difficult problems? Are you interested in working across a variety of areas from requirements to architecture, testing, deployment and beyond? If so, Deloitte could be the place for you! Harnessing the power of emerging technologies requires you to overcome complex systems integration challenges, both within your own organizations walls, and with your external partners, suppliers, and clients. Join our Systems Integration team and bring focus to the toughest problems in the Federal community through the design, development, and implementation of software solutions. A career at Deloitte will put you on the frontline, supporting Federal agencies tasked with the most critical technical challenges .

Work youll do

As a Senior Consultant in our practice you will:
Provide Technology Solution Development and Integration across the SDLC including requirements, functional specs, design, custom development, integration, testing, and deployment
Perform project tasks independently, and may direct the efforts of others
Participate in or lead the development of client deliverable content
Anticipate client needs and formulate solutions to client issues
Review deliverables for accuracy and quality
Provide coaching to junior staff
Contribute to new business and proposal development
Manage own personal and professional development, seeking opportunities for expansion of consulting skills and experiences
A Database Engineer/Modeler will assist in the planning and engineering of an organizations databases.
Evaluate existing database design to determine necessary updates and integration requirements of new design, and to ensure final solutions meet organizational needs.
Design, develop and deliver / implement data solutions to include: architecture design, prototyping of concepts to proof of concept, development of standards, design and development of test plans, code and module design, development and testing, data solution debugging, design and implementation of a solution that follows efficient design techniques and development that meets and exceeds the intent of the design of the data solution.
Effectively manage day-to-day tasks / activities in coordination with a team of developers to effectively meet the deliverables and schedule of a data solution component within a larger application project.
Lend support to various business and technology teams as necessary during design, development and delivery to ensure solid, scalable, robust solutions.
Support and maintain data and database systems to meet business delivery specifications and needs.

The team

Transparency, innovation, collaboration, sustainability: these are the hallmark issues shaping Federal government initiatives today. Deloittes Federal practice is passionate about making an impact with lasting change. Carrying out missions in the Federal practice requires fresh thinking and a creative approach. We collaborate with teams from across our organization in order to bring the full breadth of Deloitte, its commercial and public sector expertise, to best support our clients. Our aspiration is to be the premier integrated solutions provider in helping to transform the Federal marketplace.
Our Federal SI team manages the complexity inherent with technology change, from requirements planning to architecture, testing to deployment, and beyond. We also offer a complete range of industry-leading services, including systems development, solution and platform integration, and program management, functional, and testing services.

Qualifications

Required:
Bachelor's degree and at least 3 years of technology consulting or other relevant industry experience
Experience working independently with minimal supervision and guidance
Strong problem solving and troubleshooting skills with experience exercising mature judgment
Proven experience effectively prioritizing workload to meet deadlines and work objectives
Demonstrated ability to write clearly, succinctly, and in a manner that appeals to a wide audience
Proficiency in word processing, spreadsheet, and presentation creation tools, as well as Internet research tools Strong understanding of SDLC methodologies (Agile, SCRUM, RUP, other)
Proven experience effectively prioritizing workload to meet deadlines and work objectives
Experience in implementing large-scale custom development and/or systems integration projects in one or more phases of the SDLC
Experience working with business users to gather requirements, writing functional and technical specifications and communicating technical requirements
Must be able to obtain and maintain TS SCI with Polygraph clearance for this role
Requires comprehensive knowledge of database technologies and must be able to conceptualize and design DB solutions.
Preferred:
Degree in computer science, information systems, or other technology-related field
Ability to translate requirements into design specifications and experience in developing design documents
How youll grow
At Deloitte, our professional development plan focuses on helping people at every level of their career to identify and use their strengths to do their best work every day. From entry-level employees to senior leaders, we believe theres always room to learn. We offer opportunities to help sharpen skills in addition to hands-on experience in the global, fast-changing business world. From on-the-job learning experiences to formal development programs at Deloitte University, our professionals have a variety of opportunities to continue to grow throughout their career. Explore Deloitte University, The Leadership Center.

Benefits
At Deloitte, we know that great people make a great organization. We value our people and offer employees a broad range of benefits. Learn more about what working at Deloitte can mean for you.

Deloittes culture
Our positive and supportive culture encourages our people to do their best work every day. We celebrate individuals by recognizing their uniqueness and offering them the flexibility to make daily choices that can help them to be healthy, centered, confident, and aware. We offer well-being programs and are continuously looking for new ways to maintain a culture where our people excel and lead healthy, happy lives. Learn more about Life at Deloitte.

Corporate citizenship
Deloitte is led by a purpose: to make an impact that matters. This purpose defines who we are and extends to relationships with our clients, our people and our communities. We believe that business has the power to inspire and transform. We focus on education, giving, skill-based volunteerism, and leadership to help drive positive social impact in our communities. Learn more about Deloittes impact on the world.

Recruiter tips
We want job seekers exploring opportunities at Deloitte to feel prepared and confident. To help you with your interview, we suggest that you do your research: know some background about the organization and the business area youre applying to. Check out recruiting tips from Deloitte professionals."
"Software Engineer, Data Engineering",Juntos,"Named Best New Product for the Underserved by the Center of Financial Services Innovation and the global winner of the G20 Innovation Award for Financial Inclusion, JUNTOS builds mobile personal financial management tools for the financially underserved across four continents. Our team combines high technology with empathy-driven design to increase financial inclusion around the world.

Software Engineer, Data Engineering

At JUNTOS, being an engineer is about more than solving interesting problems, its about applying the power of exceptional engineering to truly meaningful global challenges.
Data is central to JUNTOS ability to carry out its mission. Data Engineering ensures that the company has the right data, in the right form, when and where its needed. We maintain a balance of rigor and agility as the company scales its operations. As software engineer, you build simple and effective solutions by drawing creatively from your broad set of skills and tools.
If youve always dreamed of using your skills to make an impact, wed love to talk to you.

Essential to the position:
 Minimum 2 years of industry experience
 Excellent communication skills, whether in prose or data visualization
 Diligence in coordinating changes to our systems with their stakeholders
 Literacy in statistical concepts like power, error, and bias
 Proficiency with SQL database design and management (we use PostgreSQL)

Preferred qualifications:
 Literacy in R, Ruby, and Rails (much of our existing infrastructure is written with these)
 Experience with, or an enthusiasm to learn, Python, Spark, and surrounding ecosystem

Compensation: $100-$130k salary, plus equity

Juntos is venture backed by Aligned Partners and Omidyar Network and headquartered in San Carlos, CA. To apply, send a CV and a cover letter explaining why this position & Juntos are appealing to you to join.the.team [at] juntosglobal.com. Background check will be required upon employment."
Senior Data Engineer,Intuit,"Intuits Small Business Group (SBG) develops and brings to market the products that small businesses and accountants depend on every day for their success. These products include accounting products (QuickBooks), payments, payroll and others.

The SBG Data Engineering team is looking for a Senior Data Engineer with a winning track record in Big Data and Distributed Systems. Were using data in groundbreaking ways to uncover customer insights, personalize customer experiences, and provide a unified customer view across all SBG products.

Responsibilities:
Work in the Small Business Group (SBG) Data Engineering team. The team has twenty-seven engineers working on Data Warehousing, Personalization, Risk, Fraud, Graph Database Solutions, Business Intelligence and more

Design and develop big data and real-time analytics solutions using industry standard technologies

Develop web services that make big data available in real-time for in-product applications

Work with data architects to ensure that Big Data solutions are aligned with company-wide technology directions

Lead fast moving development teams using agile methodologies

Lead by example, demonstrating best practices for unit testing, CI/CD, performance testing, capacity planning, documentation, monitoring, alerting, and incident response

Communicate progress across organizations and levels from individual contributor to senior executive. Identify and clarify the critical few issues that need action and drive appropriate decisions and actions. Communicate results clearly and in actionable form

Serve as technical go to person for our core technologies  Hadoop, Spark, AWS, Vertica, Tableau, Cassandra, Graph Databases and others

Demonstrate strong implementation aptitude to translate objectives into a scalable solution to meet the needs of the end customer while meeting deadlines

Demonstrate commitment to your professional development by attending conferences, taking classes, giving technical presentations, and participating in developer communities inside and outside of Intuit

Qualifications

BS in Computer Science. MS Preferred

Strong CS fundamentals including data structures, algorithms and distributed systems

Strong database fundamentals including SQL, performance and schema design

Strong programming skills in Java, C++, Python, Ruby or similar

6+ years of hands-on software engineering experience

6+ years of experience integrating technical processes and business outcomes  specifically: data and process analysis, data quality metrics/monitoring, data architecture, developing policies/standards & supporting processes

Strong project leadership experience, including 5 or more years leading multiple complex software development projects using agile methodologies

Experience in people management or interest in people management is a plus

Experience with Hadoop, Hive, HBase, Spark, Kafka, Storm, Druid, Cassandra, Columnar Databases and Graph Databases

Experience with varios offerings from AWS, including S3, EMR, Redshift, Data Pipeline, Athena and Kinesis

History of contributing to open source projects is a plus

3+ years DevOps experience including configuration, optimization, backup, high reliability, monitoring and systems version control

Track record working with data from multiple sources  willingness to dig-in and understand the data and to leverage creative thinking and problem-solving

Excellent interpersonal and communication skills, including business writing and presentations. Ability to communicate objectives, plans, status and results clearly, focusing on critical few key points. Demonstrated ability to work in a matrix environment, ability to influence at all levels, and build strong relationships

Knowledge of enacting service level agreements and the appropriate escalation and communication plans to maintain them"
"Hardware Analytics, Intern",Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Facebook is seeking a Hardware Analytics, Intern to join our Release to Production Engineering team. Come learn about our cutting edge power and hardware architecture, which has one of the most efficient infrastructures in the industry. Our servers and datacenters are the foundation upon which our rapidly scaling infrastructure efficiently operates and upon which our innovative services are delivered. Responsibilities
Interface with internal hardware, software engineers and operations teams to understand system architectures and failure modes
Proactively create experiments and data visualizations to detect and diagnose hardware health issues, focusing on systemic solutions
Develop data pipelines and discover insights to answer relationship between hardware and data center parameters to server failures
Troubleshoot, diagnose and root cause of system failures and isolate the components /failure scenarios while working with stakeholders internally and externally
Share insights with stakeholders and software teams to develop architectures to handle server failures based on hardware health data
Minimum Qualifications
Pursuing a degree in engineering, statistics, math, or related field
Programming experience
Scripting experience
Knowledge of Linux
Troubleshooting and data tooling skills (data analysis, big data, building analytical models, visualizations)
Communication Skills
Ability to obtain and maintain work authorization in the United States in 2018
Preferred Qualifications
Pursuing a doctoral degree in engineering, statistics, math, or related field
Past experience or coursework related to computer & systems architecture, statistics, machine learning, networking etc."
Senior Data Engineer,Epi Source,"POSITION DESCRIPTION:
Episource leverages data to provide patient insights and drive interventions. As thought leaders and subject matter experts, were constantly striving to identify actionable insights. We service the $3.0 trillion healthcare space and our clients represent leading organizations throughout the United States.
Episource is at the forefront of the ever-evolving healthcare marketplace. We help clients paint an accurate picture of patient health profiles by analyzing medical records, claims, and clinical data sets.
Weve grown significantly to support the regulatory changes over the last 10 years, and are poised to continue our expansion as the leader in the marketplace.

RESPONSIBILITIES:
Build innovative and market changing technology in the healthcare analytics space.
Assist Software Engineering team with understanding product requirements, strategy, implementing technical architecture, creating prototypes & management of the full product lifecycle from ideation to rollout.
Plan deployment of complex enterprise-level projects on scalability and reliability
Drive product development and set up code reviews, peer reviews and implement best practices
Drive architecture based on business use-cases to promote performance, security, and reliability.
Perform hands-on development and deployments. Mentor software engineering team on technical best-practices, understanding complex business requirements and cross-training.
Basic Requirements
Bachelors degree or greater in Computer Science
Minimum of 8+ years of experience in information technology projects with experience in designing and developing at least a few enterprise level software products end-to-end
Deep technical experience with  Relational Data Models, PostgresSQL, SQL Server, T-SQL, SSIS, Tableau, NoSQL, MongoDB, Hadoop, Dimensional Data Models, Data Warehousing, ETL, Data Lake Architecture, AWS, Github, Jenkins, Python
Agile/scrum practitioner
Solid analytical and conceptual skills
Ability to design effective software frameworks
Perform other duties as requested or required
Strong team oriented interpersonal and communication skills
Mentoring software engineering team.
Preferred Qualifications
Technical experience with  Real-time data streaming, Spark, Luigi, Airflow, Kafka, ElasticSearch, Redshift, DynamoDB, Docker
Leveling up on  Machine Learning, Data Analytics, R, SciKit-Learn, TensorFlow
Experience with UNIX/LINUX
Prior healthcare industry experience
HIPAA/PHI compliance experience
Benefits
Competitive salary
Flexible work schedule
Collaborative work environment
Medical/dental/vision
401K matching
Weekly catered lunches
Snacks
Game nights
Computer setup of your choice"
Senior Data Engineer,Drizly,"Drizly Inc., Boston, MA seeks Senior Data Engineers to help it become the best place to shop for alcohol, period.

Responsibilities
- Identify and design data requirements and access patterns for big data initiatives.
- Identify and integrate data processing and reporting frameworks using Hadoop and MapReduce.
- Implement near real-time ETL process to handle hundreds of data sources using Spark and Kafka technology.
- Monitor performance and advise of any necessary infrastructure changes to help scale the ETL process.
- Define and design data retention policies leveraging data stores such as Vertica, Hive or Redshift.
- Setup ETL jobs using workflow systems including Azkaban and Oozie.
- Educate team on new technologies and best practices in interacting with data platform.
- Own the entire ETL process to ingest data and help design and support different data products with real time data.
- Assist as required on data extraction for Data Scientists and other internal or external parties.

This position is full-time and is in Boston, MA. We are not hiring for remote positions at this time.

Requires Bachelors (or educ. equiv.) Degree in Computer Science or Information Technology and five (5) yrs. (post-degree, progressive) experience in Job Offered. Alternatively, will accept Masters (or educ. equiv.) Degree in Computer Science or Information Technology and three (3) yrs. experience in Job Offered. At least 2 yrs. experience must have included working with batch-processing and tools in Hadoop technology stack including MapReduce, Pig, Hiveand HDFS; building realtime systems with Storm or Spark data transformation pipelines; large data store experience, including HBase, HDFS, Vertica, Redshift; data modeling and performance tuning; experience with message queues/brokers such as Kestrel, Kinesis or Kafka and working with workflow systems including Azkaban and Oozie."
Database Engineer,Wish,"The role of the Database Engineer is to operate, monitor, and scale the database. You will help solve performance problems on our platform and service. You will be working with other engineering teams on our main application with a special focus on performance of application database at scale. Eventually, you will automate everything you do.

What youll be doing:
Build and organize observability for core applications and their databases.
Identify and address performance issues in our product with a focus on database.
Own performance roadmap deliverables.
Build tools to help you diagnose performance bottlenecks.
Maintain end to end responsibilities of the distributed data platform.
Deliver multi-cloud, loosely-coupled, distributed, highly available platform.
Process large datasets in near real time for mission critical data.
Who you are:
3+ years experience with relational and non-relational databases. Postgres, MongoDB, and Cassandra preferred.
Skill with one or more scripting languages. Python or Go preferred.
Exposure to and experience dealing with large scale systems.
A love for automation and projects to prove it."
Data Implementation Engineer,"Aristotle, Inc.","We are currently looking for a Data Implementation Engineer interested in an agile and fast-paced client focused environment. Our data implementation engineers design, develop, and deploy ETL and ad-hoc SQL projects.

The primary focus will be creating client successes by managing client data for our suite of political software and converting new client data to work with our software. Working with sales representative and account managers, you are the expert resource for how a clients data should be incorporated into the applications. The position also includes working with product designers and developers to suggest appropriate development paths for software enhancements and new features.

Qualifications:
Bachelors degree in Computer Science/IT/related field or equivalent work experience
2+ years of Microsoft SQL Server (2008/2012/2014/2016), Transact-SQL, Stored Procedures, normalized data structures
2+ years of experience with (SSIS) Packages and SSIS Date Processing Automation
Proficient at numerous record matching techniques with large datasets
Ability to research complex requirements and offer complete technical solutions
Ability to explain technical solutions to clients and non-technical team members
Ability to estimate work time requirements, schedule appropriately, and deliver in a timely manner
Strong critical thinking skills
Strong ability to research and solve problems in both team and independent environments
The ideal candidate will possess some of the following:
Experience with political/campaign data, specifically fundraising, issues and grassroots activities, federal or state campaign finance activities and reporting
Experience with financial databases
Familiarity with database administration
MCSA: SQL Server or MCP on track to MCSA: SQL Server
Knowledge of de-duping records, NCOA, and CAS
Please click here to Apply .

Share on Facebook Share

Share on Twitter Tweet

Share on LinkedIn Share

Send email Mail

Print Print"
Protocol and Data Stack Software Engineer,Intel,"Job Description

This position will be responsible for the design, development, integration and testing of CDMA Protocol stack software, including cdma2000, EVDO, EHRPD modules as well as the design, development, Radio Link protocols, MAC layers, IP layer, and interface with host/AP

Additional responsibilities include:
Design, development, integration and testing of Global Multimode
interworking of CDMA with 3gpp systems, including LTE, WCDMA, GERAN, TD-CDMA.
Maintenance and bug fixing of existing protocol and data stack software, for customer projects and internal lab testings.
Provide customer support especially for real time embedded software related issues on customer platforms
Work with customer during customer lab integration, interoperability testing and carrier certification testing
Design and implement major components and/or subsystems of Intel's wireless communication application
Consult with hardware engineers and other engineering staff to evaluate interface between hardware and software, and operational and performance requirements of overall system
Analyze software requirements to determine feasibility of design within time and cost constraints
Modify software code as needed to meet design specifications and resolve related problems in the implementation of new software

Qualifications

Minimum Bachelor's or Master's Degree in Engineering, Computer Science or related field with 5-10 years working experience in the wireless industry
Experience with CDMA 1X, EVDO, EHRPD, LTE, UTRAN, GSM, TDS protocol stack
CDMA data stack, including Radio Link protocols, MAC layer protocols experience desired
IP protocols (PPP, IP, TCP, UDP, FTP, HTTP, ROHC, TLS) highly desired

Additional Qualifications:
Ability to learn new things quickly-Ability to troubleshoot problems efficiently
Ability to work effectively in a team
Ability to initiate new ideas and creative solutions
Ability to provide technical leadership Inside this Business Group

Communication & Devices Group: The wireless revolution at Intel! We are one team - passionate engineers and technologists from diverse industry backgrounds working together to realize a world of connected computing. We are bringing the best ideas from the brightest minds to deliver future mobile experiences into the market. We are on the journey towards making Intel a wireless leader with exciting products for the Internet of Things, 5G and an opportunity to change the world with your work.
Posting Statement. Intel prohibits discrimination based on race, color, religion, gender, national origin, age, disability, veteran status, marital status, pregnancy, gender expression or identity, sexual orientation or any other legally protected status."
Data Engineer / Scientist,Adwerx,"We're looking for someone to join the analytics team at Adwerx. Ideal candidates will have a passion for numbers and data discovery, deep analytical capabilities, a collaborative work-style and strong strategic judgment. Youll get to help drive data-based decision making at Adwerx. If you like to discover the ""truth"" through data, this role's for you.

Responsibilities:
Generate reports on key product metrics.

Conduct analysis for major projects.

Be an expert on Adwerxs reporting systems and data flows (currently MixPanel, MySQL, Redshift, and Tableau), including troubleshooting of data issues and implementation of new data streams.

Help other users find and understand the data they need.

Help Adwerx find new ways of measuring and understanding the behavior of our users.

Advise and train product, business development, marketing, etc. on metric definition/ implementation requirements for projects and testing methodology (what to measure, how to structure a test, etc.).

Requirements:
1-3 years of related experience in online reporting/analytics.

Proven analytics ability/ experience dealing with complex analytical projects and large data sets.

Ability to pull data directly from large databases and write complex queries in SQL.

Demonstrated ability managing data and analytical projects cross- functionally to achieve results.

Experience with database systems such as MySQL, SQL Server, Amazon Redshift, Google BigQuery or similar tools.

Experience with Python.

Strong business sense with the ability to do detailed work while remembering the big picture.

Demonstrated ability to learn new technologies.

Strong written and verbal communication skills to communicate findings.

Experience with Microsoft Office, especially Excel - Pivot Tables, Vlookup, etc.

A team player who enjoys working in a fast-paced environment.

College degree mandatory. Prefer candidates with an advanced degree in a quantitative field such as CS, Engineering, Statistics, Mathematics, Economics, or Physics.

Nice to have:
Experience with one or more web reporting packages (MixPanel, Google Analytics, Facebook Insights, Coremetrics, Adobe Omniture, etc.).

Experience with R.

Familiarity with Business Intelligence and Data Warehousing concepts.

Experience with Data Visualization tools such as Tableau, Qlik, Tibco, Periscope.io, or similar.

Knowledge of at least one scripting language (Perl, Python, Ruby, etc.).

Knowledge of statistical techniques such as clustering, and logistic regression.

Note: Candidates will be screened on their ability to write SQL queries and use advanced functionality in Excel"
Natural Language Processing Data Engineer,Apple,"Apple's Natural Language Processing Group is responsible for a wide range of text-driven technologies, including auto-correction, spellchecking, predictive typing, Chinese & Japanese input, tokenization, tagging and other solutions that are at the intersection of machine learning and NLP. Our multi-faceted team delivers intelligent and highly performant technologies driven by natural language text data. We are looking for a passionate engineer to harness the potential of our big data to develop better machine learning models and user experiences on all Apple platforms.
Key Qualifications
Extensive experience in designing performant and scalable distributed systems with Spark and the Hadoop ecosystem
Proficiency in writing high performance/memory efficient code
Java, Python, and Linux fluency
5+ years experience in software development
Strong communication skills & collaborative mindset
Experience in NLP, speech recognition or machine learning is a plus

Description
The Natural Language Processing group is looking for a data engineer to design scalable and performant distributed text processing pipelines. You will be a key member of a team that is responsible for delivering the natural language intelligence behind keyboard input on Apples platforms, and will work closely with software engineers and machine learning experts to extract insights from data. The ideal candidate will be technically proficient, passionate about distributed systems and technologies and self-motivated, and will have a strong focus on details and quality.
Education
BS in Computer Science or an equivalent field"
"Siri Data Engineer, Language Technologies",Apple,"Play a part in the next revolution in human-computer interaction. Contribute to a product that is redefining mobile computing. Create groundbreaking technology for large-scale systems, spoken language, big data, and artificial intelligence. And work with the people who created the intelligent assistant that helps millions of people get things done  just by asking. Join the Siri team!
Key Qualifications
Strong data analytical skills
Past experience with gradings tasks and tools, e.g. for transcribing speech audio, labeling tasks, language translation, etc.
Excellent coding skills in one or more of following languages  Python, Java, Shell scripts, C/C++
Experienced in big data processing, tools development and/or web crawling
Great communication skills
Knowledge in the field of language technologies/NLP, such as speech recognition, machine translation, text mining, parsing, etc.
Multilingual speaker a plus

Description
We are looking for highly motivated engineers to fulfill our data needs and drive the product quality to the next level. You will be working closely with a group of talented researchers and engineers on the cutting edge machine learning technologies. Your responsibilities include data creation, data management, processing pipeline and tools development. The role also involves the collaborations with other cross functional teams on data related projects.
Education
BS or MS in Computer Science or equivalent experience"
Senior Data Engineer,Claritas,"Primary Responsibilities
Work with Data Product Managers and the Data Sciences organization to clarify requirements, specifications and methodology
Lead and mentor junior developers including peer code review and tutoring on higher level code quality constructs
Interpret complex mathematical formulas and other specifications provided by Data Science and transform raw data accordingly
Analyze and discover data trends and linkages that highlight errors or areas of potential opportunity for client utilization. Additionally, ensure that data warehouse standards are being maintained
Self-test and correct defects related to code developed. Correct security vulnerabilities
Skills, Education and Experience:

Required:
Bachelors degree in Computer Science or Engineering
Minimum of 3-7 years of experience in SQL, PLSQL
Experience with Oracle, PLSQL, Java, JavaScript/Dojo, Python, R
Experience in Unix scripting (Korn or Bourne)
Solid understanding of database design/modeling
Strong knowledge in Performance Tuning
Ability to apply analytical and problem-solving skills to solve complex design and coding problems
Knowledge of code repositories for maintaining code in development, test and production
Ability to apply software design and testing principles
Experience with Data Warehouse
Understand metadata and metadata management
Ability to manage multiple priorities and meet deadlines
Ability to work effectively as a team member, across project teams, and independently
Excellent interpersonal skills, as well as excellent written and verbal communication skills
Highly preferred:
ETL/ELT experience
Experience with ER/Studio
Experience with Big Data  Hive, MapReduce, Pig, Spark or other NoSQL technologies"
Data Engineer,"Partner's Consulting, Inc.","The Data Engineer position is responsible for gathering requirements, preparing the Functional Requirements and mapping documents, Technical Design, Development, Unit Testing of ETL (Extract, Transform, and Load) Interfaces for Databases, Data Warehouses, and Data Stores. This position requires working in the Data team to design, develop, test, document, and implement solutions utilizing the Application development standard processes. This position will require working with various systems and databases to move data effectively and efficiently across different platforms.

Required Skills:
A Bachelors degree in Computer Science or Mathematics or Business Administration or equivalent work experience.
5 years of hands-on experience in Data migration using ETLs in a MS SQL Database/ Data Warehouse environment.
Deep knowledge in requirements gathering process.
Deep technology understanding in MS SQL Database design and ETL development in a complex environment, and the MS SQL Business Intelligence stack (SSIS, SSRS, SSAS).
Extensive experience in writing complex SQL (Structured Query Language) statements.
Through understanding of data integration and ETL methodologies and best practices.
Project leadership experience. Experience working with/ managing an Onshore/Offshore team.
Ability to interact effectively with all levels of the organization.
Background in data modeling and database implementations.
Excellent analytical, written and verbal communication skills.
Excellent teamwork.
Excellent troubleshooting skills.
Ability to learn new or existing systems quickly.
Ability to work independently on multiple projects.
Accountable for adherence to program and project management standards.
Accountable to manage budget.
Experience working in or with health payer industry and knowledge on health data is preferred.

Partner's Consulting is a reputable, Nationally Certified Women Owned Business Enterprise Information Technology Consulting and Recruiting firm based in the Philadelphia area. We have built personal relationships with our clients over the years, and work directly with hiring managers on all our open positions. Our core values are focused on the highest level of professional dedication to our client's requirements coupled with our desire to partner for joint success. If you have any questions or concerns about the posting above, please contact us at recruiter@partners-consulting.com

559528"
"Software Engineer, UI / Data Visualization",Cloudera,"170512 Job Description:
We are looking for an innovative individual with extensive web UI development experience, and a good taste for clean, elegant, and fast interfaces. Experience with data and information visualization is a strong plus.

We look for ""The Startup Spark"", a desire to create new things, dive in wherever there's a need, and learn new things. You must be self-motivated, innovative, and proactive and enjoy working in a team environment and be eager to make an impact as an individual and have a passion for writing super optimized clean code.

Responsibilities:
Make Hadoop easy to use
Create and deliver new UI features and upgrades for our products
Design, code, and implement clean and elegant user interfaces
Build strong relationships and collaborate with UX designers, other developers and testers, as well as, Product Management, Field Engineering, and external partners

Qualifications:
8+ years of experience in developing web UI applications using JavaScript
Experience in coding standards-compliant semantic markup in HTML/CSS
Experience with JavaScript frameworks such as Angular, jQuery, React, Knockout.js, etc.
Server-side JavaScript tooling, Node.js, npm, grunt

Pluses:
Data Visualization using D3
Experience building Enterprise applications
Dynamic stylesheet language such as Less, Sass
Test driven development; unit tests (Jasmine, Mocha, Karma)

Why Cloudera?

Amazing people - We are a fun and smart team, including many of the top luminaries in Hadoop and related open source communities. We frequently interact with the research community, collaborate with engineers at other top companies and host cutting edge researchers for tech talks.
Innovative work - Cloudera pushes the frontier of big data and distributed computing, as our track record shows. We test and deploy our code on clusters with hundreds of nodes, terabytes of RAM, and petabytes of storage. We work on high-profile open source projects, interacting daily with engineers at other exciting companies, speaking at meet-ups, etc.
Great culture - Transparent and open meritocracy. Everybody is always thinking of better ways to do things, and coming up with ideas that make a difference. We build our culture to be the best workplace in our careers.
Experience - be part of the exhilarating ride at a fast-growing pre-IPO startup
Perks - 30 monitors, top of the line laptops, free lunches, happy hours, etc."
SOW - Data Engineering software development and benchmarking/testing,Intellipro Group Inc.,"1. Setup a data analytics, monitoring and presentation framework with open source components such

as Kafka, Zookeeper, Flink, Beam, InfluxDB/TSDB, Prometheus and Grafana, etc. over VMs or

Kubernetes and Docker Containers in cloud environment. Implement stitching code to connect

theses open source components together and make them work.

2. Implement interface between the framework and a data source simulator or a selected testing

equipment based on data formats and protocols provided.

3. Integrate Prometheus exporters, push gateways and Prometheus.

4. Implement data transformation on the collected data according to the requirement by selected Client

algorithms.

5. Implement interface between the framework and an external Client algorithm module to send

structured data to and receive output from the Client algorithm module based on the data formats and

protocol provided.

6. Implement selected machine learning algorithms in Python/Java as integrated part of Flink Client

library.

7. Write test plan and discuss with others engineers about the test plan and project related issues.

8. Design automated testing/benchmarking processes

9. Collect valid operational data including labeled data generation

10. Perform data analytics benchmarking/testing.

11. Analyze test/benchmarking results and write reports"
Big Data Engineer,"Resolvit, LLC","RESOLVIT

Bringing Solutions That Make Business Better

Join Resolvit as a Data Engineer and be part of a creative, forward-thinking team. Our success at deploying skilled, highly knowledgeable experts has landed us on the Inc. 5000 list of Americas fastest-growing companies four times  and were just getting started.

As the Data Engineer , you will develop the enterprises Apache Kafka distributed messaging, streaming, and integration ecosystem. You will provide subject matter expertise in the deployment of Apache Kafka and Spark Streaming. You will provide physical deployment across multiple environments; optimization and tuning; standards and patterns development; and user guide development and training overviews for supporting teams. Additionally, you will:
Troubleshooting and implement best practices methodology for development teams, including process automation and new application onboarding
Design monitoring solutions and baseline statistics reporting to support the implementation
Contribute to technology strategy and engineering roadmaps around Big Data and execute strategic engineering proof of concepts
Develop monitoring strategies for infrastructure, platforms, and applications aligning with enterprise strategy and overall industry trends
Champion the appropriate use of open source and commercial technology based upon industry trends and innovative concepts
What Youll Need to be Successful:
Strong Kafka experience
Experience implementing, testing, and deploying medium-to-large data pipelines
Experience with stream processing using Kafka
Familiarity with automated unit and integration testing
Experience collaborating in small teams spanning engineers, product designers, and data scientists
A champion of good engineering practices
Knowledge of Big Data technologies such as Hadoop, EMR, Spark, Impala, and Kafka
Great Additional Skills:
Spark experience
Benefits:

At Resolvit, youll be given the support you need to grow in your consulting career. In addition to opportunities for advancement, we offer:
Medical, dental, and vision insurance
Life insurance coverage
Long-term and short-term disability coverage
401(k) retirement plan with matching
Professional support from our National Business Services Manager
We currently have more than 100 open career opportunities across the country, so be sure to mention the appropriate Job Code with any correspondence!

About Resolvit:

Resolvit is an international technology consulting firm with industry-leading customers in the financial services, high tech, manufacturing, retail, life sciences, and government sectors. Through its partnerships, Resolvit delivers highly impactful, innovative solutions across five core areas: Infrastructure Modernization, Application Development Services, Enterprise Data Management & Analytics, Knowledge & Content Management, and Strategic Staffing."
Big Data Engineer,UnitedHealth Group,"The health solutions marketplace is hungry for new ideas, innovative products and software that drives elevated performance for the business and the customer. Optum, part of the UnitedHealth Group family of businesses, is feeding incredible solutions to that marketplace every day by bringing out the best in our software engineering teams. We serve customers across the health system. Not only do we have more of them every day, we also have more technology, greater data resources and far broader expertise than any competitor anywhere. Were out to change the way our businesses and consumers engage with technology. If youre in, youll be challenged like never before. Its time to join this history making.

The complexity of this role can be challenging. We deal with huge amounts of data across our entire enterprise. Everything you work on will have implications that impact others. Well provide the most up-to-date technology, but youll need to provide creativity, critical thinking and clarity on an everyday basis.

Watch this video to learn more about Optum Technology Careers

Primary Responsibilities:
Perform all phases of software engineering including requirements analysis, application design, code development and testing
Design and implement product features in collaboration with business and IT stakeholders
Design reusable components, frameworks and libraries
Work very closely with architecture groups and drive solutions
Participate in an Agile/Scrum methodology to deliver high-quality software releases every 2 weeks through Sprints
Design and develop innovative solutions to meet the needs of the business
Review code and provide feedback relative to best practices and improving performance
Troubleshoot production support issues post-deployment and come up with solutions as required
Mentor and guide other software engineers within the team
Demonstrate collaborative skills working within a project team of diverse skills
Bring strong communication skills including oral, written and presentation skills plus creativity, and problem solving skills to a challenging environment

Required Qualifications:
8 + years of experience in large development initiatives
6+ years of experience in developing high volume Java JEE/.NET & database applications
2+ years of experience with complex shell scripting using Unix & Python
1+ years of experience in developing distributed computing systems applications using Hadoop, Hbase, Hive, Java/MapReduce, Spark, Scala, Storm, Kafka, Flume, Sqoop & Pig
3+ years of agile experience
Bachelors degree or equivalent experience

Preferred Qualifications:
Experience in troubleshooting performance issues
Experience with multiple file structures and programming languages
Thorough understanding of technological infrastructure and how it relates to projects
Experience working in a DevOps model using Agile
Experience with CICD with Jenkins pipelines, OpenShift, Gradle, GitHub and Docker
Any experience building RESTful APIs
Any experience with AWS
Test Automation experience
Excellent teamwork and collaboration skills
Healthcare domain background
Technology Careers with Optum. Information and technology have amazing power to transform the health care industry and improve people's lives. This is where it's happening. This is where you'll help solve the problems that have never been solved. We're freeing information so it can be used safely and securely wherever it's needed. We're creating the very best ideas that can most easily be put into action to help our clients improve the quality of care and lower costs for millions. This is where the best and the brightest work together to make positive change a reality. This is the place to do your life's best work.(sm)

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.

Job"
R&D Data Analysis Engineer,Dexcom,"About Dexcom:
Founded in 1999, Dexcom, Inc. provides continuous glucose monitoring technology to help patients and their clinicians better manage diabetes. Since our inception, we have focused on better outcomes for patients, caregivers, and clinicians by delivering solutions for people with diabetes - while empowering our community to take control of diabetes .

Summary:
You will be an essential part of a Data Analysis team that supports continuous improvement of Dexcoms CGM products. Drive root cause analysis through interactions with various departments including R&D, QA, Tech Support and Patient Support. Develop metrics to track and evaluate system performance and reliability from field data. Support CAPAs and NCMRs, work with Complaints, Technical Support, and Patient Education groups to evaluate current product safety and effectiveness and propose improvements. Other responsibilities may be assigned periodically.

Essential Duties and Responsibilities:
Support data analysis activities for continuous glucose monitoring (CGM) system performance and reliability

Generate insights from field data

Support product development activities

Interact with cross-functional groups outside of R&D to address field issues

Required Qualifications:
Typically requires a Bachelors degree in Engineering or another quantitative field such as Physics or Mathematics

1+ years of experience in the medical device field

5+ years of related experience

Data analysis experience

Knowledge of statistical methods as applied to time series data

Working knowledge probability and statistics

Understanding of CAPAs and NCMRs

Ability to use one or more scientific programming languages (MATLAB, JMP, R, etc.)

Preferred Qualifications:
Knowledge of diabetes and CGM

Experience working hands-on with large-scale data sets

Advanced degree in an analytical or technical field (e.g. bioinformatics, applied mathematics, statistics, physics, computer science, operations research)

#LI-SM1

Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
Data Integration Engineer,ALQIMI,"Key focus areas: Big Data, Enterprise Analytics
Duration: Multi-year opportunity
What you will be doing!
The Data Integration Engineer will have experience with application integration solutions and configure, deploy and enhance enterprise cloud applications. The ideal candidate will have a passion for acquiring, analyzing and transforming data to generate insight. The role is very hands-on and requires a structured mindset and solid implementation skills.

Key responsibilities:
Engage directly with our customer to participate in design and development of data integration/transformation solution according to functional requirements
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Participate in the development of documentation, technical procedures and user support guides
Work with Big Data environments (Elastic Search, HADOOP in later phase) to import and export data
What you will need
5+ years of experience with system/data integration, development or implementation of enterprise and/or cloud software
Demonstrated knowledge of JavaScript
Experience with Unix-based operating systems
Solid understanding of concepts of cloud computing (AWS)
Working knowledge of Agile Software development methodology
Strong organizational and troubleshooting skills with attention to detail
Strong analytical ability, judgment and problem analysis techniques
Interpersonal skills with the ability to work effectively in a cross functional team
Clearance
TS/SCI with CI Polygraph
Education
Bachelor's degree in Computer Science, IT Management, Computer Engineering.
Who are we?
Headquartered in Rockville, MD, ALQIMI has developed an outstanding track record of designing, integrating, and managing complex technological solutions for customers around the world. Our IT services group is our founding heritage and services the intelligence and commercial communities with technologically sophisticated services including cybersecurity, big data and software development. We take pride in using our demonstrated ability to build profitable businesses, subject matter expertise and ecosystems to incubate new ideas. We have established new businesses in the data analytics, military facilities construction management and energy industries and intend to incubate additional opportunities.
What makes ALQIMI unique is that we offer challenging projects, excellent benefits, work/life balance, and a fair and ethical executive team.
ALQIMI provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, sexual orientation, national origin, marital status, age, disability (including disability due to pregnancy) or genetics, protected veteran status, or any other characteristic protected by law. ALQIMI complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment.
More information about ALQIMI can be found at www.alqimi.com."
Data Science Engineer,UnitedHealth Group,"We're changing the way health care works for the better. That means consulting with our members, partnering with our physicians, and delivering drugs in the most efficient and effective way. Join us and start doing your life's best work. (sm)

Primary Responsibilities:
Extract data from internal systems and work closely with the modeling teams to prepare data for analysis
Assist with further processing of the data to aid data scientists during the development process
Translate business requirements into analytic implementation code for use within internal systems and models
Develop reports and analysis to inform management on model performance, and/or tools to facilitate understanding of model results
Model/Analytics Implementation. Deploy and maintain model performance monitoring processes. Follow standard SDLC processes and agile Scrum program management principles. Become one of the Data science/Predictive Analytics team experts on internal servers and databases
Be a positive team player, comfortable working in a team environment setting for project deliverables and delivering High quality results as well meeting project deadlines
Develop and/or maintain a library of commonly used macros/modules and programs for data extraction, transformations, analytic model scoring execution, monitoring and performances etc.
Support existing Analytics processing in a tier-production environment prioritizing SLA returns for claims processing of Statistical Models

Required Qualifications:
Bachelors Degree in Engineering, Data Science, (bio) statistics, applied statistics, applied mathematics, economics, or similar quantitative fields of study
5+ years of program development and production deployment experience with SAS (Unix), SAS EG & SAS Tools
3+ years of program development and production deployment experience with SQL, PL/SQL for Oracle or SQL Server
1+ years of software development experience with R or Python
2+ years of software development and engineering experience on SAS Unix/Grid environments
Knowledge and process experience using SDLC for Advanced Analytics implementations and support for predictive analytics models
Microsoft Office Products experience

Preferred Qualifications:
Masters Degree in Engineering, Data Science, (bio) statistics, applied statistics, applied mathematics, economics, or similar quantitative fields of study
2+ years predictive modeling/Machine learning implementation experience in R, Python using Hadoop, Oracle 11g/12c and MS SQL Server 2016/2017
1+ years of software development experience with Spark, Hadoop, HBASE, HIVE or H20
Claim Data Analytics Integration with SAS/R/Python or other relevant ETL development experience
Development experience in Data sourcing from Netezza, Teradata or Hadoop clusters
Agile Project Management experiences on Analytic development and deployment
XML domain knowledge
Knowledge of ICD-10 coding of Medical claims
Agile methodology analytics development experienceCareers with Optum. Here's the idea. We built an entire organization around one giant objective; make the health system work better for everyone. So when it comes to how we use the world's large accumulation of health-related information, or guide health and lifestyle choices or manage pharmacy benefits for millions, our first goal is to leap beyond the status quo and uncover new ways to serve. Optum, part of the UnitedHealth Group family of businesses, brings together some of the greatest minds and most advanced ideas on where health care has to go in order to reach its fullest potential. For you, that means working on high performance teams against sophisticated challenges that matter. Optum, incredible ideas in one incredible company and a singular opportunity to do your life's best work.(sm)

Diversity creates a healthier atmosphere: UnitedHealth Group is an Equal Employment Opportunity/Affirmative Action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, age, national origin, protected veteran status, disability status, sexual orientation, gender identity or expression, marital status, genetic information, or any other characteristic protected by law.UnitedHealth Group is a drug-free workplace. Candidates are required to pass a drug test before beginning employment.

Job"
Data Engineer Analyst Expert,EXPERIAN,"Description Experian Services is looking for a Data Engineer Analyst Expertto join their Data Science Solutions (DSS) Data Intelligence team. This is a unique opportunity to get involved witha variety of information management, analysis projects and documentationprojects. The Analyst will be conducting analyses and producing reports basedon data generated from a number of different sources and data formats, bothinternal and external to the company. In addition to information and datareporting projects, the analyst will also be heavily involved in documentationand quality assurance work; operational improvement; documenting processes; developingand maintaining reference and technical guides.

This position will alsointeract on a consistent basis with other business/functional analysts, ETLanalysts, programmer/analysts, database/data warehouse administrators, and datastewards/functional owners. The candidate will analyze data requirements, studycomplex source data, interpret data, create data models and variables, andcould also assist to determine the best methods to extract and handle largedatasets. The ideal candidate should be able to work well under directsupervision as well as independently by executing a variety of establishedprocedures to accomplish assigned tasks. He/she will use basic and someadvanced analytical, information management, programming, technical andstatistical skills to solve a variety of problems.

Key Responsibilities: Coordinates QA tasks of project deliverables. Analyzes and confirms the integrity of source data to be evaluated. Generates reports following quality control procedures to documentanalysis and model findings. Participate in the design sessions.

Collaborate in the planning, design, development, and deployment of new functionality, and enhancements. Develop scripting tools and scripting strategies for deploying code in languages such as SAS, SQL or C to handle large data repositories based on technical specifications. Understand pseudocode and collaborate in the elaboration of complex data structures and programs. Create documentation, specifications, diagrams, and charts to provide direction to business and other IT teams.

Maintain technical support documentation and training documents. Perform other related duties as assigned. Collaborates with internal and external clients to provide projectssupport. Manages data and departmental document repository ensuring up-to-dateversions and correct document storage.

Provides support for project management including internal and externalclient projects. Translate complex business requirements into technical solutions. Perform data analysis and data modeling to create source to target mappings. Meet with decision makers, systems owners, and end users to define business, financial, and operations requirements and goals, and identify and resolve data related issues.

Assist in setting up for presentations, summarizing meetings and creating related documentation. Knowledge, Experience & Qualifications Minimum Job Requirements and Qualifications: 4 years degree in Statistics, Economics, Information Management,Engineer, Computer Science, Information Technology or another related discipline. 2+ years of work experience preferably within theFinancialServices. Ability to interpret data and to communicate it in both technical anduser-friendly language.

Proven problem solving and analytical skills. Excellent written and oral communication skills. Must be able tocommunicate and write in English. Proven experience working withUNIX/LINUXis preferred.

Advanced knowledge and experience in Perl or Python scriptingprogramming is preferred. Advanced Excel skills, especially in developing Excel Macros. Proficient in PC desktop applications, including MS Office Suite. Proven experience working withSAS and SAS programming ispreferred.

Proven experience working with SQL, PostgreSQL, PL/SQL and/or PL/pgSQLscripting is preferred. Experience in developing best practices and excellent logical,analytical, and creative problem-solving skills. Highly self-motivated and directed. Ability to effectively prioritize and execute tasks in a high-pressureenvironment.

Experience working in a team-oriented, collaborative environment. Candidates with the following skill sets andqualifications will be given special consideration: Masters degree is a plus. Advanced knowledge of SAS and SAS programming is a plus. Good knowledge/understanding of statistical theory and methods is aplus.

Knowledge and experience in many Analytic Platforms is a plus. Experience working with data science tools like R, RStudio and Python isa plus. Proven experience working with Databases likeOracle, MySQL, SQL, PostgreSQL,Sybase, Netezza or DB2is a plus. Advanced Knowledge of UNIX Bash programming is a plus.

Advanced knowledge and experience of C programming is a plus."
Big Data Engineer (m/f) C967,BFFT Fahrzeugtechnik GmbH,"Key Responsibilities
Help design and implement key components for highly scalable, distributed data collection and analysis system built for handling petabytes of data in the cloud
Analyze and support requirements from machine learning specialists, focusing on the most important topics in the rapidly evolving automotive industry
Help move architecture and implementation through the automotive development pipeline, from research to deployment in millions of cars on the road
Work with engineers from other divisions contributing to this analytics system, contributing to best practices in backend infrastructure and distributed computing topics
Professional Qualities and Skills
Hands-on development with key technologies including Scala, Spark, and other relevant distributed computing languages, frameworks, and libraries
Experience with distributed databases, such as Cassandra, and familiarity with the key issues affecting their performance and reliability
Experience using high-throughput, distributed message queueing systems such as Kafka
Familiarity with operational technologies, including Docker (required), Chef, Puppet, ZooKeeper, Terraform, and Ansible (preferred)
An ability to periodically deploy systems to cloud environments such as AWS
Mastery of key development tools such as GIT, and familiarity with collaboration tools such as Jira and Confluence or similar tools
Overall, 3+ years of industry experience developing backend components with a recent focus on big data systems
Bachelors degree or equivalent required"
Big Data Engineer,Research Innovations Inc,"Our team is currently seeking high quality engineers for a variety of positions in the Northern Virginia area. Our projects include developing complex, web client server applications; developing mobile device applications and prototypes; developing Big Data solutions; architecting and prototyping complex distributed command and control network applications. These projects require experience in modern web technologies and software development approaches.

If you are a sharp, experienced engineer with demonstrated big data capabilities in deploying and implementing Hadoop, Accumulo (or other big table solutions), data extract, transform, and load technologies, and the development and implementation of the analytics that act on this data we want to hear from you. Joining RII not only provides unique challenges and opportunities, it also directly and positively impacts many of our Defense and Homeland Security end users.

WHAT YOU WILL BE DOING
 Establish and maintain a big data architecture in support of several customer projects
 Be the engineering point-person for big data, ingest, and analytical capabilities and technologies
 Maintain awareness of the current and emerging capabilities in big data and analytics technologies and how these apply to our customers
 Develop captivating solutions by collaborating with customers and development team
 Document use cases, solutions, & recommendations to customers

WHAT YOU HAVE DONE
 BS in Computer Science, equivalent degree, or previous work experience
 Worked with both SQL and NoSQL databases and data models, including document and graph databases, distributed indexing systems (Elasticsearch or SOLR) and the BigTable family of columnar stores (HBase, Cassandra, Accumulo)
 Developed scalable ingest/ETL pipelines using Hadoop ecosystem technologies (such as HDFS, MapReduce, and Spark) and distributed messaging systems (such as Kafka or AMQP solutions
 Experience in Linux administration, runtime configuration management (Puppet), and automated deployment (Ansible)
 Experience in Java, Python, and Ruby
 Strong understanding in Linux operating systems, distributed systems, and databases (NoSQL and relational)
 Excellent understanding of ETL and data analytics platforms
 Experience understanding and decomposing system level requirements into discrete and measurable tasks

EVEN BETTER
 Experience with Agile Methodologies
 Strong knowledge of the installation, configuration, and maintenance cloud computing and big data infrastructure to include Hadoop, Accumulo, Kafka, Spark, Elasticsearch, Puppet, Ansible, Lucene, and related technologies
 Experience with continuous integration and continuous deployment using Atlassian products
 Experience with Atlassian products
 Designed, developed, and deployed machine learning based capabilities
 Knowledge of cloud computing infrastructure such as: Amazon Web Services EC2 & Elastic MapReduce
 Experience creating big data solutions using public, private, and hybrid cloud approaches
 Experience with integration methodologies and tools for big data applications and services
 Experience with the Red Disk environment
 Experience with data quality and data profiling tools

US Citizenship required. Candidates must be clearable to Secret, desired TS/SCI.
Active clearance not required to apply.

Research Innovations, Inc. is an equal opportunity employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national, origin, disability status, protected veteran status, or any other characteristic protected by law."
Software Engineer,Microsoft,"Do you want to work on massive scale, distributed workloads that are deployed in datacenters around the world? Do you want to contribute to critical solutions for Azure? The Azure Batch and High-Performance Computing team provides large scale distributed job scheduling and resource management as a platform service. We continuously schedule millions of tasks and manage thousands of machines. We deal with large scale data ingress and egress. We are looking for a developer to join our team and help design and build our high-scale distributed system. We are building a batch scheduling service that gives a simple API to users to schedule distributed Jobs/Tasks at any scale (small jobs or running a job on thousands of machines). The service handles all the intricacies related to scale and reliability.

As part of the core team, you will be dealing with reliable scheduling at scale, allocation of thousands of resources, work graphs, data encryption and transfer, multi-tenancy, security and other aspects of distributed computing. You will contribute to the design and implementation of the Azure Batch Service. We are a small team with a passion for continuous delivery with quality. Apart from the technical qualifications listed below, we are looking for very strong, motivated, result-oriented and collaborative people who can be independent and get things done in a team setting.

Basic Qualifications:
Minimum of a technical degree in computer science
Preferred Qualifications:
C++, C#, C OR Java - Solid CS fundamentals, fluent in multi-threaded programming, strong inclination for architecting at scale
Excellent technical design, problem solving and debugging skills
Ability to plan, schedule and deliver quality software
Experience related to work/Task scheduling on other scheduling frameworks will be a plus
Ability to meet Microsoft, customer and/or government security screening requirements are required for this role. These requirements include but are not limited to the following specialized security screenings: Microsoft Cloud Background Check: This position will be required to pass the Microsoft Cloud background check upon hire/transfer and every two years thereafter.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to askstaff@microsoft.com."
Senior Data Engineer,Epic Games,"For over 25 years, Epic Games has been making award winning games and game engine technology that empowers others to make visually stunning games and 3D content that brings environments to life like never before. Epics award-winning Unreal Engine technology not only provides game developers the ability to build high-fidelity, interactive experiences for PC, console, mobile, and VR, it is also a tool being embraced by content creators across a variety of industries such as media and entertainment, automotive, and architectural design. As we continue to build our Engine technology and develop remarkable games, we strive to build teams of world-class talent.

The Senior Data Engineer will be responsible for helping to build Epics data collection systems, processing pipelines, and data warehouses. This role will be responsible for helping to architect, build, and maintain an optimized and highly available data pipeline for deeper analysis and reporting by a team of analysts.

The person in this role will be responsible for the following:
Assisting in the architecture, design and implementation of big data platforms operating on Amazon Web Services
Performing performance tuning tasks on large compute clusters
Working with both batch and near-real-time data sources
Creating SaaS applications
Streamlining maintenance tasks as they pertain to the platform
The ideal candidate will have a mix of the qualifications below:
Skilled in managing multiple Hadoop clusters via the command line

Past experience building or integrating open source applications (e.g.Lipstick, Inviso, Genie) into Hadoop environments

Knowledge of Apache Spark, Apache Hive and Apache Pig

Expertise in one or more programming languages (Java or Python)

Familiarity with basic Database administration tasks on a major RDBMS platform such as Oracle

Experience with enterprise scheduler software (control-m, Automic/UC4, Tivoli workload scheduler, etc)

Expertise using SQL

Experience working with complex data types (JSON, XML)

Comfort with software development methodology around unit testing, performance tuning, integration testing, etc.

Experience working with Amazon Web Services, specifically Elastic Map Reduce and EC2

This is going to be Epic!

#LI2"
"Data Pipeline Engineer, Data Science",System1,"System1 is hiring a Data Pipeline Engineer to be embedded within our Data Science team! We are looking for candidates that can bring an experienced background in handling data pipeline engineering problems with a specific interest in providing data for modeling purposes. You will be an individual contributor and build data infrastructure to directly enable optimizations that operate in real-time on a significant amount of daily revenue involving hundreds of millions of data points per day. You should be comfortable working in a highly cross-coordinated team environment where you will collaborate daily with our Engineering, Account Management, and Product Teams. System1 is an organization centered around data and data products. We firmly believe data should inform and guide every quantitative decision process. We hire people who can utilize their broad set of technical skills to address technical challenges, as well as mentor junior team members on industry specific challenges and best practices. At System1, our Data Science team is responsible for optimizing the business across the following departments: engineering, business, product, ETL, serving, analytics, and operations.
Responsibilities
Make data architecture decisions for key revenue-generating projects
Be a key part of developing core optimization architecture
Work directly with business stakeholders to drive optimization products forward
Work with large scale data sets
Qualifications
Bachelors degree in Computer Science or another quantitative discipline required
3 years of relevant experience working hands-on in industry, Python strongly preferred
Interest in production data science solutions using Python and R
Extensive work experience with various database technologies such as Redshift, Athena, Snowflake, Spark, DynamoDB, and Redis
Adtech experience is preferred but not required
Perks
Free Uber/Lyft to and from work every day!
Collegial and collaborative team with highly intelligent and motivated coworkers
Cross-team lunches and demos to foster learning
Unlimited Paid Sick Time, Competitive PTO and Benefits package
Catered meals and fully stocked kitchen
Weekly happy hour at various bars, restaurants, and venues across Los Angeles
Weekly fitness class with private trainer: high intensity training, yoga, beach volleyball, beach soccer, ultimate frisbee
Competitive Relocation Package
Company parties and outings: Skyzone indoor skydiving, Medieval Times, Karaoke, etc."
Big Data Software Engineer,Mitre Corporation,"Big Data Software Engineer - (00047518) Description
Do you believe in the missions of the Federal Law Enforcement and Intelligence agencies? Are you interested in supporting and solving complex programmatic and technical issues? Then look no further! If you are interested in working on some of the most challenging technical and programmatic issues in support of the Nations Law Enforcement mission, we are interested in talking to you about MITREs work program and career opportunities.

Our not-for-profit, public interest charter gives us the freedom to pursue ideas that others might see as too risky. MITRE is built on the principles of objectivity and independence which enable our employees to push the technical and programmatic limits of the state of the practice.

We are currently seeking an experienced Senior Big Data Software Engineer who will perform a variety of functions with the following qualifications.

KEY FUNCTIONS:
Advise a federal government agency on best practices and industry standards as applicable to the current and future capabilities of a big data visualization application
Serve in a technical advisor role, to include driving technology direction through sound recommendations based on experience and research
Provide technical expertise on potential enhancements and development efforts, specifically related to migration to the cloud environment
Lead the effort of building a framework for the cloud environment, to include setting up a test and development environment in AWS.
Architect and prototype code to enhance application functionality within the cloud environment.
Facilitate application implementation with external Intelligence Community partners into existing cloud environments.
Build solutions for data movement and transformation needs using Java code that leverages cloud technologies
Work as a member of an Agile Scrum Team to achieve short and long-term business objectives
Use Big Data Streaming technologies to drive development efforts with tools such as Spark, Hadoop, and Accumulo

Qualifications
Required Qualifications:
TS Clearance
Able to work 100% from Chantilly
BA or BS in Computer Science or related field
Excellent communication skills
4+ (recent) years of experience across all phases of software application development - requirements analysis, development, testing, troubleshooting
Experience with Big Data Streaming technologies
4+ years of software development in Java or similar JVM language
Basic familiarity with setting up dev/test environments in AWS EC2
Able to break down and work on high level tasks with minimal guidance/supervision
Able to troubleshoot end user issues remotely and provide solutions in a timely manner
Experience working in the Agile/Scrum environment
Effective time management skills
Understanding of data flows, data architecture, ETL and processing of structured and unstructured data
Preferred Qualifications:
TS/SCI with CI Polygraph
MS or Ph.D. in CS or related field
Experience developing Java desktop applications using Swing or SWT
2+ years of experience developing scalable applications within the Hadoop ecosystem  particularly Spark, HDFS, and Accumulo
Proficiency with either SVN or Git versioning tools
Experience using JIRA
Experience with using and debugging OpenGL or JOGL (Java OpenGL) library
AWS Certification
Primary Location : United States-Virginia-Chantilly

Work Locations : Chantilly, Virginia

20151-3707

Job : SW Eng, Comp Sci & Mathematics
This requisition requires a clearance of: Top Secret/SCI
Travel : Yes, 5 % of the Time
Job Posting : Oct 17, 2017, 10:00:00 PM"
Project Engineer  Project Administrator Data Networking,The Kohl Group,"Arrangement:
Permanent position

Location:
New York, NY

Start Date:
September 2016

Required skills:

PROJECT ENGINEER
A valid Professional Engineers License is required.
A Bachelors Degree in Electrical or Communications Engineering, and at least six (6) years of related practical experience in the engineering field including at least two (2) years of managerial/supervisory experience. A satisfactory combination of education and experience may be considered for candidates who possess the required license.
PROJECT ADMINISTRATOR
A Bachelors Degree in Electrical or Communications Engineering, or Computer Science, and at least ten (10) years of related practical experience in the engineering field and at least two (2) years in a managerial/supervisory capacity. A satisfactory combination of education and experience may be considered.
Desired skills:
In-depth knowledge of communications data networking engineering, including communications standards (e.g. NEBS, IEEE, IETF, ITU-T), engineering best practices and standards of performance, and a working knowledge of Systems Engineering methods. The incumbent should be knowledgeable of various networking protocols such as IP, OSPF, PIM-SM and Ethernet 802.1 & 802.3 series of standards. The incumbent should be knowledgeable of fiber optics specifications and installation standards, including characteristics and installation constraints for conventional singlemode and multimode fiber cable systems.
Strong written, verbal and interpersonal skills and knowledge of construction scheduling and estimating.
Description of work:
Responsible for providing design and construction support of multiple communications projects that involve data networking, fiber and copper-based cabling and associated ancillary systems. Specifically, this will include Local Area Network (LAN) and Wide Area Network (WAN) design, implementation and testing utilizing a wide range of network technologies (ROADM, SONET, ATM, TCP/IP, Connection Oriented Ethernet, Ethernet, PBX/Telephone Systems, etc.). Implementation and practical construction of various communication systems, integrate with other systems and apply practical construction method for various communication systems. Develop, check and evaluate the feasibility / constructability of a design as depicted on engineering drawings and specifications.
Project Engineers work with clients, Construction Managers (CMs) and Design Managers (DMs) to ensure practices conform to standards and guidelines. Project Engineers work with other staff to incorporate feedback and revisions to standards and guidelines.
Take action to ensure performance of assigned staff. Coordinate with other engineering disciplines such as mechanical, electrical and structural or architecture.
Conduct testing methodologies, including benchmarking methodologies contained in IETF RFC 2544 and ITU-T Y 1564, cable testing methodologies for OTDR, CRL, CE and PMD.
Design and implement copper and fiber network infrastructure, communications room facilities and ensure adherence to communications standards and building codes.
Report on ongoing performance. Prepare and deliver presentations, conduct analysis and make recommendations on project alternatives.

If you are interested, please email your resume in Word format to Ann Boschetti at aboschetti@KOHLGROUP.COM with a brief note mapping your skills and experience to the specific requirements of this job description.

THE KOHL GROUP

Since 1984, The KOHL Group has provided technical consulting and resources to Fortune 50 companies in the US and internationally. For more information on The KOHL Group, please visit our website @ www.kohlgroup.com ."
Senior Principal Data Engineer,Apple,"The Siri Search team is creating groundbreaking technology for algorithmic search, machine learning, natural language processing, and artificial intelligence. The features we create are redefining how hundreds of millions of people use their computers and mobile devices to search and find what they are looking for. Siris universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup. As part of this group, you will work with one of the most exciting high performance computing environments, with petabytes of data, millions of queries per second, and have an opportunity to imagine and build products that delight our customers every day.

Key Qualifications
You are highly experienced and battle tested, a lead or core contributor on storage or compute projects in the 100PB+ range.
You are pragmatic, the architecture may be a blend of bespoke, open source and tools from other parts of Apple at similar scale.
This is a hands-on position, expect to write more code than documentation
You will take a key role in attracting talent and screening additional members of the project as it expands.
Excellent communication skills, an ability to lead technical discussions and engage with downstream teams on their rewrites.
You are excited by ambitious projects, and ready to tackle the next order of magnitude.

Description
The mission of the Data Engineering Team at Siri Search is to store and protect Siris knowledge of Apple and the world.

We gather search telemetry from a billion physical devices, we collect dozens of data sets from Apple, our global partners, and the Internet, and we bring all this data together into a single environment to enable the work of hundreds of talented engineers and data scientists, on the largest distributed compute clusters at Apple.
We are seeking a small number of exceptional individuals to design and lead elements our next generation Exascale storage and compute architecture.
Education
Master's Degree in Computer Science or Computer Engineering, or equivalent work experience"
Data Engineer - Product,Showtime,"# Data Engineer - Product

**REF#: ** 29524

**CBS BUSINESS UNIT: ** Showtime

**JOB TYPE: ** Full-Time Staff

**JOB SCHEDULE: ** Full-Time

**JOB LOCATION: ** New York, NY

**ABOUT US: **

SHOWTIME and its critically-acclaimed, award-winning original series continue to make their mark on the cultural landscape, with one of the most successful programming slates in all of television. With an impressive line-up of new and returning original series, the SHOWTIME hit dramas and comedies include HOMELAND, SHAMELESS, BILLIONS, RAY DONOVAN, THE AFFAIR, EPISODES, DICE, the upcoming drama IM DYING UP HERE, the limited series GUERRILLA and PURITY, and the highly anticipated return of TWIN PEAKS.

Original series play a key part in the SHOWTIME programming mix, along with box office hits, comedy and music specials, provocative documentaries, and hard-hitting sports programming, including the flagship franchise SHOWTIME CHAMPIONSHIP BOXING, the Emmy Award-winning documentary series ALL ACCESS, A SEASON WITH and the Emmy Award-winning veteran series INSIDE THE NFL.

Showtime Networks Inc. (SNI), a wholly-owned subsidiary of CBS Corporation, owns and operates the premium television networks SHOWTIME, THE MOVIE CHANNEL and FLIX, and also offers SHOWTIME ON DEMAND, THE MOVIE CHANNEL ON DEMAND and FLIX ON DEMAND, and the network's authentication service SHOWTIME ANYTIME. Showtime Digital Inc., a wholly-owned subsidiary of SNI, operates the stand-alone streaming service SHOWTIME. SNI also manages Smithsonian Networks, a joint venture between SNI and the Smithsonian Institution, which offers Smithsonian Channel, and offers Smithsonian Earth through SN Digital LLC. SNI markets and distributes sports and entertainment events for exhibition to subscribers on a pay-per-view basis through SHOWTIME PPV. For more information, go to [1] www.SHO.com.

References

Visible links

1. http://www.sho.com/

**DESCRIPTION: **

The Showtime Product team is looking for a curious and creative Data Engineer to help us pursue answers to our increasingly interesting and complex business questions and empower our team to incorporate data-driven features and machine learning into our products, which include our standalone service SHOWTIME and our TV Everywhere service, Showtime Anytime.

The big data platform at Showtime is relatively new, but is now being used across the company for critical functions and features like: recommendations, analyzing customer experience, understanding programming consumptions effect on subscriber lifetime, building churn/retention prediction models, and more. In this role, you will work our dedicated Product Analytics team, the Showtime Research and Data Strategy teams, the CRM team and our in-house engineering team, and you will architect and enable technologies, systems and workflows that enable our analysts and data scientists to focus more on algorithms and analyses than on the associated engineering.

Ideal candidates will be innovative, self-motivated, a quick study, and willing to develop new skills while constantly improving existing abilities.

Key Technologies:
Java, Scala, Groovy, Spark, AWS, AWS/EMR, Spring, Mongo, Git, Redis, Bamboo, JIRA etc

Responsibilities

Develop understanding of key business, product and user questions.
Collaborate with other Engineering team members to develop, test and support data-related initiatives. Work with other departments to understand their data needs.
Evolve data-driven feature prototypes into production features that scale; streamline feature engineering, so that the underlying data is efficiently extracted.
Build flexible data pipelines that we can rapidly evolve as our needs change and capabilities grow.
Develop and enhance data warehouse in AWS S3.
Employ data mining, segmentation, and other analytical techniques to capture important trends in our user base.
**QUALIFICATIONS: **

3+ years of relevant experience in a comparable data engineering role
Expert-level knowledge of SQL/Spark SQL
Experience in pursuing and applying data-backed decisions, such as recommendations, trends etc. to make the core product better
You like to dive-deep on data analysis or technical issues to come up with effective solutions and are comfortable summarizing key insights graphically when needed
You believe in writing code that is easy to understand, test and maintain
You thrive in a workplace that values autonomy, applauds ideas and a enjoys a sense of humor
#dice

**EEO STATEMENT: **

Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
Senior Data Engineer,Slalom Consulting,"Slalom is a purpose-driven consulting firm that helps companies solve business problems and build for the future, with solutions spanning business advisory, customer experience, technology, and analytics. We partner with companies to push the boundaries of whats possibletogether.

Founded in 2001 and headquartered in Seattle, WA, Slalom has organically grown to nearly 4,000 employees. We were named one of Fortunes 100 Best Companies to Work For in 2016 and are regularly recognized by our employees as a best place to work. You can find us in 25 cities across the U.S., U.K., and Canada.

Job Title:
Senior Data Engineer

Do you like working with data? Do you want to use data to influence decisions about products and services being used by over half a billion people every day? If yes, we want to talk to you. Slalom is seeking an outstanding Senior Data Engineer to join our Information Management & Analytics team.

As a Senior Data Engineer, you should have expertise in the design, creation, management, and business use of extremely large datasets. In this role, you will be working across industry sectors such as retail, marketing, healthcare and high-tech you'll get an opportunity to solve some of the most challenging business problems on the web and mobile Internet to answer key business questions and drive change.

Qualifications:
10+ years of demonstrated data science experience with impact to a business
3+ years of experience with Big Data Technologies like Hadoop or Hive
5+ years of experience scripting using Perl, Python, Ruby, or other programming languages
Advanced knowledge and expertise with data modeling skills, Advanced SQL with Oracle, MySQL, and Columnar Databases
3+ years experience in data warehouse space
3+ years experience in custom ETL design, implementation, and maintenance
Bachelors degree in CS or related discipline
Slalom Is An Equal Opportunity Employer And All Qualified Applicants Will Receive Consideration For Employment Without Regard To Race, Color, Religion, Sex, National Origin, Disability Status, Protected Veteran Status, Or Any Other Characteristic Protected By Law."
Big Data Engineer,Data Works,"Data Works is looking for senior Big Data Engineers able to lead the way in tackling the most difficult engineering challenges in Big Data systems.

This position requires U.S. Citizenship and an active TS/ SCI security clearance.

About Data Works

Data Works is an employee-focused small company that supports the Intelligence Community by providing Big Data and Cyber Security solutions. We favor a high quality workforce over aggressive growth and provide opportunities on programs that fill mission-critical needs. Our core competency is in the technical stack between data collection and analysis. Positions are available in various customer locations between McLean and Dulles in Virginia.

Position Description

Data Works is seeking a Big Data Engineer with demonstrated experience in leading large scale data warehousing projects. A successful candidate will be strong in Map Reduce, Java, and possess an understanding of data science concepts such as machine learning and trend analysis. Candidate should also be familiar with indexing products such as Lucene and Elasticsearch. Relevant certifications considered but not required.

Technical Requirements

Experience with distributed computing technologies including Hadoop, HBase, Cassandra, Elasticsearch, Apache Spark and Greenplum
Development experience with Java, C++, Scala, Groovy, Python, and/or shell scripting
Experience with data warehousing tools and technologies
Ability to work within UNIX /Linux operating systems

AWS experience a plus

Company Benefits

6 weeks PTO

Paid Overtime
Annual Bonuses
10% Employer 401k Contribution
Health/Vision/Dental/Disability/Life Insurance
Annual Training and Tuition Budgets
Technology/Fitness/Communications Reimbursement
Charity Matching Program

EOE /M/F/Vet/Disabled"
"Back-End Software Engineer, Data Infrastructure",Houzz,"About Houzz
Join Houzz in revolutionizing the home and remodeling industry and have an impact on over 40 million homeowners and 1.5 million professionals around the world. Frequently ranked as one of the top 10 most disruptive companies in the world and backed by marquee venture capitalists, Houzz will allow you to drive the future of an industry worth $1.2 trillion in the US and Europe alone.

Our entrepreneurial culture and warm family atmosphere will surround you with a fun and dynamic work environment you will only find at Houzz.

Our engineers play a direct role in the direction of our company and are able to work across multiple groups to implement fresh ideas that allow Houzz to be the industry leader. If you are interested in applying your passion to create products that will transform the lives of millions of users remodeling and decorating their homes, then welcome to Houzz.

Position Overview
We are looking for a world-class software engineer to build systems that will scale with the growth of our site, the most popular home remodeling site in the world. Youll make data easy to use for our entire team. Your tools will make it easy for our data analysts to find deep insight into user behavior. You will build scalable systems that make it easy for our team of machine learning experts to bring powerful recommendation and personalization models to production.

Position Responsibilities
Build scalable systems that process, store, and serve terabytes of data
Optimize systems from hardware to software level
Gather requirements and build new solutions to solve the pain points of analysts and developers
Desired Skills & Experience
Experience owning complete pipelines or data stores
Production experience with large scale distributed systems including triaging failures and performance tuning
Proficiency in modern compiled and scripting languages such as Java and Python
Experience with modern data technology including Hadoop, NoSQL data stores, key-value stores, MPP warehouse solutions, etc.
Self-directed, independent; amazing ability to get stuff done
Strong understanding of algorithms, web services, and data management
B.S., M.S., or Ph.D. in Computer Science or equivalent
5+ years experience ideally including technical lead roles
Together, we dont just accept differences -- we celebrate, support, and thrive on inclusivity for the betterment of our products and our community."
Data Governance Engineer,Groupon,"Responsibilities
Extend and maintain Groupon centralized data ingestion pipeline, specifically:
Streaming Transformations

Streaming and Batch Data Quality Validations

Schema design and maintenance for persistence layer, including partitioning

Analyze and translate functional requirements/change requests into technical specifications

Onboard new data sources and help upstream engineers/data PMs with configurations

Coordinate and collaborate with EDW Engineering team and business users to implement required streaming transformations functions (UDFs)

Create/execute unit tests and validate expected results

Create and maintain clear documentation on data models/schemas as well as transformation/validation rules

Troubleshoot and remediate data quality issues raised by pipeline alerts or downstream consumers.

Execute and evolve proactive data quality measures, specifically:
Work with upstream QA teams on integrating data validation tests. Evaluate new releases from data quality perspective.

Contribute to anomaly detection and alerting tools evolution in the pipeline (both real-time and batch)

Conform to adopted SDLC procedures (2-week sprints, daily standups and daily ticket triaging)

Requirements
Technical skills:
100% fluency in ANSI SQL. Hive/spark SQL. R is a plus

Scripting skills: python, shell, pig etc.

Basic OOO and/or functional programming skills (Java, Scala).

Working knowledge of big data persistence. Hadoop and Teradata are preferred.

Hands-on experience in RDMS technologies: Mysql, Oracle etc

Knowledge of data modeling and designing relational databases, specifically for OLAP (star/snowflake schemas etc)

Ability to work in a fast-paced environment on several projects concurrently

Strong analytical and troubleshooting skills

Ability to develop and organize high-quality documentation"
Senior Data Engineer,,"Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask How did I ever live without Coupang? We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the 50 Smartest Companies in the World by MIT Technology Review and 30 Global Game Changers by Forbes.

Coupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

About the role:
Coupang is one of the largest and fastest growing e-commerce platforms on the planet. Our mission is to create a world in which Customers ask How did I ever live without Coupang?

We are looking for passionate builders to help us get there. Powered by world-class technology and operations, we have set out to transform the end-to-end Customer experience -- from revolutionizing last-mile delivery to rethinking how Customers search and discover on a truly mobile-first platform. We have been named one of the 50 Smartest Companies in the World by MIT Technology Review and 30 Global Game Changers by Forbes.

Coupang is a global company with offices in Beijing, Los Angeles, Seattle, Seoul, Shanghai, and Silicon Valley.

Job Description:
Do you love working with petabytes of data using big data technologies? Do you possess deep expertise in designing and building the Big Data systems of tomorrow? Do you want to be part of an exciting group chartered to implement the next generation Data platform for Coupang? If you answered yes to the above, then we want to talk to you.

Essential Job Functions

As a Data Engineer, you must be an expert with Big Data technologies like Hadoop, Hive, Hbase, Spark, and various AWS technologies.

You will architect and drive the build out of Coupangs next generation data platform

As a hands-on engineer you will influence all architecture decisions

You will build durable code, with the ability to scale with very large data volumes.

Everything you build will need to scale and perform

As a senior member on the team you will also mentor junior engineers on the team.

What were looking for:
10+ years of Data Engineering experience

5+ years of experience in Hadoop MapReduce, Hive, Pig, Spark, and Yarn

2+ years of experience working with either AWS or Azure technologies  EMR, HDInsight

Experience in architecting, building and maintaining large-scale data infrastructures

Code/Build/Deployment: GIT, GITLab, etc.

A strong team player Ability to quickly triage and troubleshoot complex problems

Bonus points for:
BS degree in Computer Science

Excellent communication skills, both written and verbal

Strong ability to interact, communicate, present and influence within multiple levels of the organization

Perks:
Autonomy to make decisions in a rapidly growing company

Free medical, dental, and vision insurance

15 days PTO + 15 national holidays off

401K matching

Pre-IPO stock options

Mobile & fitness reimbursement

Catered lunches twice a week

Flexible working hours

It is the policy of Coupang Global LLC to afford equal opportunity for employment to all individuals regardless of race, color, age, national origin, physical or mental disability, history of disability, ancestry, citizenship status, political affiliation, religion, gender, transgender, gender identity, marital status, status as a parent, sexual orientation, veteran status, genetic information or other factors prohibited by law, and to prohibit harassment or retaliation based on any of these factors."
"Engineer, Data Platform",O'Reilly Media,"Data Platform Engineer
About Your team
Data drives much of what we at OReilly Media do, making the Data Science Team central to how the company operates and continues to grow its influence. We build, maintain and evolve the Analytics Platforms and solutions (work that includes data warehousing & data cleansing, data science & machine learning processes, and reporting and business analysis services) to enable key business processes and decision making. We use Agile Development and have a bias toward open source solutions. We have a very open work culture preferring to work with experienced, self motivated data science professionals, and encourage collaboration among the team. All team members are encouraged to learn new skills and technologies as they relate to our mission. We believe work should be fun and friendly. And we hold dear our core belief in maintaining a healthy work/life balance.
About the Job
The Data Platform Engineer will help steer the development of our analytics platform, including maintaining, expanding, and scaling our ETL pipeline. We consume data from a wide array of sources, including other microservices and applications on our platform and third-party APIs, and are looking for candidates who are comfortable with building software that works with a diverse range of data. The Data Platform Engineer will also be responsible for building tools that help feed analytics data to people and platforms within OReilly to aid decision making and create new product features. These tools include RESTful web services, custom analytics dashboards, and data visualization.
Our ETL platform is primarily developed in Python, and we make heavy use of PostgreSQL for storing and querying data. We support a Mondrian/Pentaho system for reporting, along with several RESTful web applications developed in Django. We also use Haskell, Redshift, Hadoop and Spark for some higher volume data ETL and analysis. The Data Science teamand software engineering at OReilly in generalis heavily invested in containerization our our of applications, and we use Docker, Jenkins, and Kubernetes to build, deploy, and manage a large range of services. The Data Platform Engineer will:
Write stable, scalable, idiomatic Python code for ETL and reporting tools
Help build the roadmap for our BI platform, including machine learning, recommender systems, and Natural Language Processing capabilities
Work with business sponsors/partners to understand the business need and help translate into successful technical solutions.
Build systems to retrieve and aggregate data from distributed message queues as part of a microservices architecture.
Build data normalizing processes to wrangle dirty and/or inconsistent data (e.e. normalizing company name)
Deploying platforms to analyze medium-to-large-scale data sets (e.g. 1+ billion records)
Improving instrumentation and monitoring of existing ETL processes
Build deep knowledge of the data and data sources underlying the business to enable meaningful analytics and insights.
About You
Were looking for a candidate with strong software engineering skills. The Data Science Team has a focus on best practices, and we are looking for candidates who have:
experience with Agile software development and the ability to work comfortably with collaborators who are distributed across several cities and timezones
a strong foundation with data warehousing and ETL principals
a strong commitment to testing and documentation of all code they write
a desire to work in a team environment where code reviews are the norm
an interest in continuous integration and deployment
A high level of comfort with devops work.
In addition to Python, we write a great deal of SQL and a successful candidate should have experience writing and profiling queries, and an understanding of optimization techniques for their RDMBS of choice (PostgreSQL preferred). Experience with at least one MPP/Big Data platform (Hadoop, Spark, or Redshift) is desirable. In addition, you should be familiar with OLAP and reporting tools like Pentaho/Mondrian, Oracle Business Intelligence ."
Data Analytics Engineer (2018 Internship),Motorola Solutions,"MSIs Corporate Strategy team works on highly visible projects that have a significant impact on the direction and performance of the business. The group identifies, prioritizes, and develops long term strategies to improve MSIs business performance. The strategy team manages the strategic planning process and conducts in-depth analysis of market trends, competition and internal business metrics.
The Business Strategy Senior Consultant will collaborate closely with other members of the Corporate Strategy team and executives from the products and services business units to identify, analyze, and prioritize opportunities to drive shareholder value.

Specific job duties include:
Conduct research and analysis to support the strategic planning process  i.e., analyze industry trends, the competitive landscape, organic/inorganic growth opportunities, etc.
Create statistical models to identify key trends/patterns in external and internal data, forecast business performance, simulate the impact of potential strategic options/decisions
Conduct research and benchmark industry best practices
Support due diligence efforts to evaluate potential partners or acquisition candidates
Create presentations used by senior leadership for internal and external audiences Knowledge and Skill Required:
Excellent strategic thinking, analysis, and problem solving skills.
Extensive knowledge of and ability to manage statistical analysis and financial modeling.
Ability to identify and draw on leading-edge analytical tools and techniques to develop creative approaches and new insights to improve MSIs business
Team player who works well cross-functionally with others at all levels of the organization
Experience in a fast-paced team environment and comfortable in ambiguous and rapidly evolving situations
Able to structure complex problems in simple, easy-to-understand ways
Very strong verbal and written communication skills, including negotiation, presentation, and influence.
Excellent project management skills, including project structuring and managing multiple work streams independently.
Strong relationship management skills, including partnering and consulting.
Highly proficient in Excel and PowerPoint.

MBA or Masters Degree in Financial Mathematics or other relevant field
2+ years of experience in a top-tier management consulting firm, corporate strategy, investment banking or any strategic function with exposure to consulting, analytics and modeling"
Senior Data Engineer,"Placester, Inc.","Join our growing engineering team. Help us build large-scale products using cutting-edge cloud technologies that well let you push to their limits. Placester is a SaaS solution helping real estate agents grow their businesses. Youll be acquiring and managing Real Estate data and making it available across our suite of applications to provide meaningful and timely insights to our customers and their clients. We want real estate agents using the Placester platform to be empowered by cutting-edge technology that improves the relationship and process of buying and selling a home.

Why join our team? Youll be able to make a huge impact on the product while working on a flat team with some incredibly smart engineers who push each other daily to grow professionally and personally. Weve got a great platform and market opportunity with the freedom to build impactful technology. If you would like to leave your fingerprints on a high-growth company, reach out  wed love to connect.

Desired Data Engineer Experience:
Designing and building distributed cloud-based Data-Driven applications

API/Web services development (Java/Scala, Node.js, Rails)

Stream processing and Analytics platforms such as Kinesis or Kafka

Search technology and integration with Solr and/or ElasticSearch

SQL and/or NoSQL database experience (MySQL, MongoDB, Cassandra, DynamoDB)

Containerization and serverless infrastructures (Docker, Lambda, ECS, Kubernetes, DC/OS)

Strong belief in (TDD) unit testing, performance, load, or end-to-end integration tests

Exercise SOLID object-oriented design patterns

Collaborate effectively in a distributed Agile team (Jira, Confluence, Github)

Experience with CI/CD tools and platforms (Terraform, Codeship, CloudFormation, Fabric)

Qualities:
Takes initiative and is willing to go the extra mile to deliver the best results for the team

Proactively looks for ways to contribute to the project and the team and ways to improve efficacy

Identifies opportunities to use better-suited technologies, yet cognizant of the trade-off between spending extra effort to incorporate a new unproven approach and the delivery timeline commitment

Detail-oriented to ensure the highest quality of delivery

Naturally curious and able to ensure that all edge cases are covered

Passionate about your craft and willing to share with others

Focused on helping customers succeed

A great teammate ...we rely heavily on each other to hit our goals

Flexible and able to embrace change - were a growing start-up  things go fast

People with a bias to action who can acknowledge when things dont work and learn from it

People who value open communication and like being part of the solution to an issue

Benefits:
Competitive salary and equity

Health/dental/vision

Company-paid short term, long-term, and life insurance

Flexible PTO

Opportunity to regularly work with great people on cool business problems"
Senior Data Engineer,Redfin,"Redfin is combining technology and customer service to reinvent the end to end experience for buying and selling a home in the consumer's favor. The opportunity is huge, with $75 billion spent every year on real estate commissions and the industry is ripe for change. So far, we've helped over 20,000 people buy and sell homes, saving them over $100M in fees, and doing it all with a 97% customer satisfaction score.

As a Senior Data Engineer for the Data Engineering Team, your job is to integrate, sanitize, and productize our massive store of market and user data to turn it into a competitive weapon. You will have ownership of Redfin's Data Warehouse platform, overall architecture, data integration and operational excellence. You will also be working closely with marketing team to provide key business KPIs and enable marketing automation.

Job Responsibilities
Design data warehouse solutions using dimensional methodologies to support ETL processes and data analytics applications
Develop, implement and tune ETL processes
Write and tune SQL including database queries, ddl and dml, stored procedures, triggers, user defined functions, analytic functions, etc.
Create code that meets design specifications, follows standards, and is easy to maintain
Own features that you develop end to end. Work with end users on requirements gathering, develop and test your code, implement new processes in production, then maintain and support them over time
Drive our data platform and help evolve our technology stack and development best practices
Develop and unit test assigned features to meet product requirements
Work with Analytics and Digital Marketing teams to provide them the data they need to make efficient decisions
Work with Quality Assurance team to ensure that the processes are fully tested
Support and maintain dev/test/prod environments to meet business delivery specifications and needs
Assist with adhoc report generation and data analysis for customers
Be part of monthly on call rotation

Job Requirements
Expert level SQL skills
7-10 years experience in database technologies (i.e., Postgres, MySQL, SQL Server, Oracle, RedShift etc.)
Minimum 5 years of experience in Data Warehousing
Working knowledge of dimensional modeling techniques
Working knowledge of data quality approaches and techniques
Experience with Redshift is highly desired
Experience with AWS tools (S3/Redshift/DynamoDB/IAM) is highly desired
Experience with Python
Experience working with a standard ETL tool (i.e., Informatica, SSIS, Talend, Pentaho, etc.)
Architectural insight on where to store data and modeling experience to recommend how it should be structured to make it accessible, performant, and resilient to change
An entrepreneurial spirit, a drive to ship quickly, and familiarity with agile software development practices
The ability to deal with ambiguity, communicate well with partner teams - both technical and non-technical, and a strong empathy for the customer experience
Experience working with Linux is a plus
API development experience is a plus
Working with the Agile/Scrum development process is a plus

Redfin is an equal opportunity employer."
Data Software Engineer (Data Engineering),Rally Health,"Data Software Engineer- 3 years hand on experience already needed

Be part of a fast growing company whose primary focus is to improve and advance health and health care using modern technologies and end-user engagement. Rally Health is a privately owned company of 500 employees with a major financial backing from UnitedHealth Group. We have partnered with several insurance companies including UHG and other large corporations for user acquisition, clinical data feeds, and monetization.

This position is with the horizontal (non-product specific) engineering organization whose mission is to provide data engineering, authentication, identity and session management, authorization, product eligibility, and related services to the rest of the company. We have openings across the entire org and there are opportunities to move across individual teams and work on different services. Our technologies include:

Scala, Play, Akka
MongoDB, Postgres, Neo4j
AWS, Docker, Mesos
Data: Kafka, Hadoop (CDH), Avro, Spark and Spark Streaming, Airflow
Authentication: PingFederate
Here is what you need:
(3 years hands on work experience with Java or Scala experience to be considered)

Command of Scala (or Java and a desire to learn Scala)
Experience with distributed systems and application design in a SOA environment
Ability to track down root causes and fix them in systematic ways
Having strong computer science fundamentals
Being analytical with strong problem solving skills and attention to detail
Being a self-starter and results-oriented with a strong sense of ownership
And for bonus points:
Knowledge of high-scale performance and optimization tools and techniques
Technical understanding and experience using Identity Management software at an enterprise level
Familiarity with NoSQL and RDBMS
Experience with AWS (configuring, deploying, managing, monitoring) services and distributed applications
Experience with configuration management tools (Chef, Puppet, Ansible, Salt), monitoring (Sensu, Nagios) and metrics (Graphite, Librato, statsd)
Experience with Hadoop computational and storage layers. Experience with Spark is a plus
Rally Health is committed to ensuring that its workforce reflects Americas diverse population. Rally Health knows that such diversity will enrich us with the talent, energy, perspective and inspiration it needs to achieve its mission. Rally Health believes in a policy of equal employment and opportunity for all people based on merit and commitment to the principles of diversity. It is our policy to recruit, hire, train, and promote individuals in all job titles, and administer all programs, without regard to race, color, religion, national origin or ancestry, citizenship, sex, age, marital status, pregnancy, child birth or related medical conditions, personal appearance, sexual orientation, gender identity or expression, family responsibilities, genetic information, disability, matriculation, political affiliation, veteran status, union affiliation, or any other category protected by applicable federal, state or local laws.

Individuals with disabilities and veterans are encouraged to apply. Applicants who require an accommodation related to the application and/or review process should notify Talent Acquisition (recruiting@rallyhealth.com ( recruiting@rallyhealth.com )).

Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
"Data Engineer, OrderUp",Groupon,"Responsibilities
Analyze & translate functional specifications & change requests into technical specifications.

Design, develop, implement ETL framework for Enterprise DW system.

Executing unit tests and validating expected results; iterating until test conditions have passed. Translate business needs into end user applications.

Ensure accuracy & integrity of data & applications through analysis, coding, writing clear documentation & problem resolution.

Troubleshoot and remediate issues impacting processes in ETL framework.

Modifying existing code to provide defect fixes for existing ETLs.

Coordinate and Collaborate with ETL team and business users to implement all ETL procedures for all new projects and maintain effective awareness of all production activities according to required standards and provide support to all existing applications.

Provide support to all ETL schedule and maintain compliance to same and develop and maintain various standards to perform ETL codes and maintain an effective project life cycle on all ETL processes.

Keep abreast of the tools, techniques and components being used in the industry through research and applies this knowledge to the system(s) being developed.

Requirements
Bachelors degree in Computer Science, Electronics Engineering, Computer Engineering, Computer Information Systems, Management Information Systems or related field or foreign academic equivalent.

At least 3 years hands-on experience with all aspects of designing, developing, testing and implementing ETL solutions

Knowledge of Big data systems - Hadoop, pig, hive, spark etc. required.

Packaging and release management.

Working knowledge with MPP systems

Knowledge of data modeling and designing databases

Ability to work in a fast paced environment on several projects

Solid understanding of SDLC, software best practices and Agile development methodologies

Strong analytical and diagnostic skills

Good knowledge of metadata, and using/managing metadata

Ability to develop and organize high-quality documentation

Take responsibility for performance tuning

Experience with data warehousing and star-schema (dimensional) data models a plus.

Working knowledge on Linux/Unix Operating systems

At least 3 years of hands on experience in Database technologies like Oracle, Mysql , Teradata or likes.

Strong scripting skills - python ( a big plus ), Perl , shell etc.

Excellent verbal and written communication skills"
Sr. Data Engineer,Grubhub,"About The Opportunity Welcome!

Hungry for a new venture?

Grubhub is the nation's leading online and mobile food ordering company, dedicated to connecting hungry diners with their favorite local restaurants. The people who work at Grubhub are our company's greatest asset; each person at Grubhub plays an integral part in building tools and technology that help restaurants succeed, and in cooking up fresh new ways to delight our diners.

The companys online and mobile ordering platforms allow diners to order directly from more than 55,000 takeout restaurants in over 1,100 U.S. cities and London. The Grubhub portfolio of brands includes Grubhub, Seamless, AllMenus, and MenuPages.

We want you to enjoy where you work, who you work with and what you work on. At Grubhub, you can order your cake and eat it too!

The Decision Data team at Grubhub is a geographically dispersed team tasked with ensuring data required for accurate business reporting is available and flowing throughout all our systems to enable faster, more consistent, data driven decisions.

As a Sr. Data Engineer youll play a key role in shaping a data driven organization. Youll need to have a keen understanding of business processes as well as how data flows in our data and technology stacks. You will be talking to stakeholders to anticipate questions and drill down into the data points that must be captured in our data warehouse for quick and accurate reporting. Clear communication is a must in order to ensure data is flowing.

Some Challenges Youll Tackle
Thread business requirements through our state-of-the-art data and tech stacks. Were talking infinitely scalable microservices backed by a robust big-data engine
As an early member of the Decision Data team youll have lots of influence on our patterns and practices as the team grows and evolves
Take a problem, break it down, find commonalities that will enable wins cross business units
Collaborate with the right people to define and refine scope and priorities
Doing deep dives on business verticals where you become one of the foremost experts on that vertical in the company
Analyze data to measure impacts of data schemas and use it to iterate on improvements
Translate from technical to business, and vice versa. You need to be able to speak with the least technically-minded client (internal or external) and make technology make sense to them. Then turn around and do it the other way

You Should Have
5-7 years of experience with business/data analysis, project management, data warehousing and/or software/data engineering
A bachelor's degree, preferably in a computer-related discipline
SQL Skills; youll need to be able to weave data into shapes that are easy to dig into and find answers
Experience in data warehousing, big data tools (Spark/Hive) or data governance a plus
Excellent communication skills, including the ability to crystallize and broadly socialize insights
Problem analysis and problem-solving skills
Proficiency in working with and solving for our business stakeholders
Rigorous attention to detail and accuracy
Adaptability and collaborative skills
Enthusiasm for the job. Are you excited about data? Do you love your users? Good, the same goes for us

And Of Course, Perks!
Unlimited paid vacation days. Choose how your time is spent.
Never go hungry! We provide weekly GrubHub/Seamless credit.
Regular in-office social events, including happy hours, wine tastings, karaoke, bingo with prizes and more.
Company-Wide Initiatives encouraging innovation, continuous learning and cross-department connections.

Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. The EEO is the Law poster is available here: DOL Poster. If you are applying for a job in the U.S. and need a reasonable accommodation for any part of the employment process, please send an e-mail to TalentAcquisition@grubhub.com and let us know the nature of your request and contact information. Please note that only those inquiries concerning a request for reasonable accommodation will be responded to from this e-mail address."
Senior Data Engineer,The Hartford,"The Hartford Financial Services  Sr. Data Engineer

Ready to grow your career leveraging the latest DATA technologies?

Join a fast-paced and talented Agile Scrum team to unlock Data Capabilities for The Hartfords Commercial business. You will have an opportunity to participate in the entire software development lifecycle process in support of continuous DATA delivery, while growing your knowledge with emerging technologies. We use the latest DATA technologies, software engineering practices, Agile delivery framework, and are passionate about technology and building well architected and innovative solutions that drive optimal business value generation.

This cutting edge and forward focused team presents the opportunity for collaboration, self-organization within the Scrum Team and visibility as we focus on continuous Business data delivery.

Whats in it for you?
Experience deeper understanding of Data analytics, Emerging technologies and Development practices
 Collaboration with a high-performing, forward-focused team, Product Owner(s) and Business stakeholders engagement
Opportunity to expand your communication, analytical, interpersonal, and organization capabilities
Experience working in a fast paced environment  driving business outcomes in Agile ways of working
Enable and influence the timely and successful delivery of business data capabilities and/or technology objectives
Enhance your e ntrepreneurial mindset  network opportunity and influencing outcomes
Further develop your engineering capabilities using tools such as Informatica, B2B, PL/SQL, Hadoop, etc. to build data assets that enable business value generation
Appreciation and opportunity to learn and support rapid software construction and deployment using DevOps and Cloud based future technologies
Supporting environment that fosters can-do attitude and opportunity for growth and advancement based on consistent demonstrative performance
Optimize business value by leveraging your DATA experience and depth
Be part of a Scrum Team  driving work independently or collaboratively towards achieving business outcomes
Act as a technical leader for colleagues with less experience
Qualifications
Qualifications

What is The Hartford looking for?
Strong technical leadership in supervising, planning and delivering highly complex projects, consisting of a large number of deliverables, and complex dependencies
Clear and concise communication, interpersonal, organizational and team building skills, business judgment, and proven expertise in directing the efforts of a skilled technical staff
Bachelor degree with at least 5-7 years of applicable work experience with respect to Data Analysis, manipulation and technical development
Desired educational experience include, but are not limited to: Computer Science, Engineering, IT, Management Information Systems, Data Analytics, Applied Mathematics, and Business
Desire 1-3 years of experience in the insurance or investment industry
Solid project management skills, ability to multitask and manage projects in a cross-functional environment
Solid delivery experience  full lifecycle from conception to successful implementation
Comprehensive understanding of end-to-end Data Warehousing architecture (i.e. Kimball/Dimensional modeling techniques/HADOOP architecture and best practices), knowledge of ETL process across various tools, and understanding of the data usage within Business Intelligence reporting tools
Technical leadership with respect to design, implementation and overseeing complex technical processes
Practice with Big Data technologies and concepts on a Hadoop platform (e.g. Scoop, Hive, Pig, NoSQL, etc) and willing/adapting to future technologies
Understanding of current and emerging IT products, services, processes and methodologies
Familiarity with emerging data centric technologies such as data virtualization and utilization of data wrangling technologies

Behaviors at the Hartford
Deliver Outcomes  Demonstrate a bias for speed and execution that serves our shareholders and customers.
Operate as a Team Player  Work together to drive solutions for the good of The Hartford.
Build Strong Partnerships  Demonstrate integrity and build trust with others.
Strive for Excellence  Motivate yourself and others to achieve high standards and continuously improve.

Equal Opportunity Employer/Females/Minorities/Veterans/Disability/Sexual Orientation/Gender Identity or Expression

** NO AGENCIES PLEASE **

Job Function : Data Engineering
Primary Location : United States-Connecticut-Hartford

Schedule : Full-time
Job Level : Individual Contributor
Education Level : Bachelor's Degree (16 years)
Job Type : Standard
Shift : Day Job
Employee Status : Regular
Overtime Status : Exempt
Travel : Yes, 10 % of the Time
Job Posting : Jan 3, 2018, 1:24:14 PM
Remote Worker Option : No"
Senior Data Engineer,Hartford Steam Boiler,"This position will be responsible for data management and analysis within the data & analytics governance office of HSB. This individual will create high quality modeling datasets and manage the technical environment for analytics model development. The successful candidate will work with IT & data scientists to improve the data management capabilities of the analytics model development process, and work to create modeling datasets by combining internal & external data assets. The position is considered a development position meant to develop the necessary skillset over a few years time to ultimately become a data modeler/scientist.

Qualifications

Required:
Bachelors degree in Computer Science, Information Management, Business, Math, Statistics, or equivalent work experience.
3-5+ years of experience as a data engineer within a modeling environment.
3-5+ years of data & analytics experience within P&C industries preferred.
SQL, SAS, SAP DQ, Business Object, Web Development, SharePoint Development,
Strong analytical and reasoning capabilities.
Detail oriented with a commitment to documentation of work performed, assumptions and results.
Well developed communication and presentation skills.
Aptitude for performing multiple tasks and dealing with changing deadline requirements.
Well organized with the ability to set and meet deadlines.

Desired:
Graduate degree.
3+ Years of SAS. Python & R programming experience within an analytics environment.
3+ Years Experience with P&C Industries.
Strong data management experience.
Strong experience with SAS.
Some experience with building analytics model.
Desire to learn new software, data sources and skills to improve data management and analytics model deployment.

Additional Information

We are an equal opportunity employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability or protected veteran status.
The work environment characteristics, and any physical and mental requirements described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
This job description is not designed to cover or contain a comprehensive listing of activities, duties or responsibilities that are required of the employee. Other duties, responsibilities and activities may change or be assigned at any time with or without notice."
Data Engineer II,OSF Healthcare,"Under general supervision, maintains database architecture, data models and metadata that support the Enterprise Data Warehouse (EDW). Designs, develops, tests and maintains ETL (Extract, Transform & Load) processes necessary to extract data from source systems and transform data according to business needs. Evaluates data reusability and reviews data usage to structure data for better management and quicker access. Supports project initiatives for process improvement and performance tracking.

This position will provide consultation and support in the development, analysis, design and management of a variety of complex data sources.
Qualifications
REQUIRED QUALIFICATIONS:

Bachelors degree in computer science/engineering /math or equivalent combination of education and job experience.

3-5 years experience in Information Technology.

Excellent written and verbal communication skills.

3+ years progressively complex experience in Informatica, SQL, and programming.

3-5 years relational database experience required, Teradata preferred.

3 years experience in data warehousing and/or business intelligence.

1 year experience in Epic Systems.

Experience creating datamart and multi-dimensional data structures.

Strong analytical and complex problem-solving skills.

PREFERRED QUALIFICATIONS:

Experience with agile and rapid development processes is a plus.

Strong understanding of statistics and/or analytical methodologies preferred.

Experience with other technologies common to OSF Healthcare System environment preferred.

EOE/Minorities/Females/Vet/Disabled

Applicants will be afforded equal opportunity without discrimination because of race, color, religion, sex, marital status, national origin or ancestry, age physical or mental handicap unrelated to ability or an unfavorable discharge from military service."
Data Integrations Engineer,Dropbox,"Dropbox is looking to fill a lead position in its data integration program. We need a data integration engineer to synchronize data between operational cloud systems such as Salesforce and Workday, as well as provide transformed data extracts for analytical reporting. This position requires a combination of technical and business analysis. From requirements gathering to development through operations, the position is broad yet requires an individual with hands-on experience in data-centric technologies outlined below.

Responsibilities Use and manage Dropboxs Dell Boomi integration platform as a service (iPaas). Integrate, map and translate data among SaaS and on-premise systems for both operational and analytical purposes Design, build and test integration processes based on business requirements. Transition personal knowledge of the various technologies to other team members so that day-to-day support can be expanded and improved. Build scalable data architectures that are robust and highly available.

Work with the enterprise architecture team to ensure data architectures are aligned to enterprise-scope initiatives, designs, and principles. Work with internal and external partners to resolve issues. Requirements Deep expertise with designing, implementing, and administering large volume Dell Boomi Integrations, Atoms and Molecules. Experience verifying and ensuring data quality for migrations/ETL.

Demonstrable SaaS integration experience with enterprise business applications such as Workday, Salesforce, Marketo, or Oracle Fusion. Familiarity with various data formats such as JSON, XML (including XPath and XSLT), and flat files. Deep understanding of web services, specifically around consuming data from RESTful and SOAP-based HTTPS endpoints. Coding skills in any of the following languages: JavaScript, Python, Ruby or Groovy.

Familiarity with SQL and relational databases such as MySQL or Amazon Redshift. Familiarity with Hadoop and its constituent components such as Hive, HDFS, etc. Understanding of Agile methodologies and processes. Team player and motivated to solve complex issues.

Benefits and Perks 100% company paid individual medical, dental, & vision insurance coverage 401k + company match Market competitive total compensation package Free Dropbox space for your friends and family Wellness Reimbursement Generous vacation policy 10 company paid holidays Volunteer time off Company sponsored tech talks (technology and other relevant professional topics)"
Senior Data Engineer,Pray,"Pray.com is a social impact company: were driven by a mission to help solve societys challenges by creating tools that make peoples lives better. We walk the talk, live the dream and combine our talents to build something bigger than ourselves. Our team is comprised of people from all kinds of backgrounds  some of us pray, some of us meditate, and some of us just want to see more empathy in the world. We all feel fortunate to be here.

As a product, Pray is a private social network and donations platform for non-profits and faith-based organizations. We make it easy for people to connect with their communities, get involved on a face-to-face level, and provide support in times of need. We are dedicated to creating a world where everyone leaves a legacy of helping others.

Want to work with teammates driven to make the world a better place?

At Pray.com, you get both a career and a cause.

Is there anything more rewarding than collecting a paycheck and making a positive impact on the world every day?

We believe that you can do well financially while doing good for your community. Our business was built on this principle and our investors have generously given us faith, and capital. In return, were scaling the company for maximum impact. With competitive salaries and a generous equity package, now is an amazing time to join our team. If making an impact while being part of a remarkable team sounds good to you, wed love to hear from you.

Were looking for a Big Data Engineer to join the Pray app. Were looking for someone who is passionate about applying quantitative information to solve tough problems and partnering with various teams across the company to help them harness the full potential of data. Youll help build out the data pipelines and data analysis tools that will enable us to take advantage of our amazing data set. Being able to gain valuable insights from data is a key part of this mission!

What you'll do:
Build data expertise and own data quality for the pipelines you build
Architect, build, and launch new data models that provide intuitive analytics to the team
Design, build, and launch extremely efficient and reliable data pipelines to move data (both large and small amounts) to our ridiculously large Data Warehouse
Design and develop new systems and tools to enable team members to consume and understand data faster
Use your expert coding skills across a number of languages from PHP, Python, and JavaScript
Work across multiple teams in high visibility roles and own solutions end-to-end
What we're looking for:
M.S. degree in Computer Science or related field
Proficiency in Python,Java,SQL (BigQuery, Oracle, Hive, etc)
2+ years of experience with dimensional data modeling and schema design in Data Warehouses
2+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation, and maintenance. Experience working with either a Map Reduce or a MPP system on any size/scale to write well-abstracted, reusable code components
Excellent communication skills, including the ability to identify and communicate data-driven insights
Experience building web applications and knowledge of web technologies (HTML/CSS/JS)
Our Culture
Every person at Pray.com is devoted to our vision of creating a world where everyone leaves a legacy of helping others.

Were looking for talented engineers who understand and embrace our core values, D.R.E.A.M., which include: Devotion, Research, Empathy, Application, and Mindset. At Pray.com , respect for colleagues coming from all faith (and non-faith) backgrounds is a must. We support First Amendment rights and we have the unbelievable honor and privilege of working with incredible people.

At Pray.com we take pride in being a highly-productive team. We collaborate using SCRUM Teams, we provide candid constructive feedback to one another, and we leverage agile business principles in an open office space that is located 2 blocks from the beach.

More About Us
Founded in 2016 by serial entrepreneur Steve Gatena and his team of rockstar co-founders, Pray.com was incubated by Mike Jones (Current CEO of Science and former CEO of MySpace) and currently operates out of the Science space in Downtown Santa Monica, which has also seen the likes of Dollar Shave Club, DogVacay, and FameBit pass through its doors."
Big Data Engineer,Realogy Corporate,"Requisition 45268

About ZapLabs LLC
ZapLabs ( www.zaplabs.com ) is the innovation and technology hub for global real estate leader Realogy Holdings Corp. (NYSE: RLGY), and the driving force behind inventive technology for brokers and agents affiliated with Realogys world renowned real estate brands. ZapLabs delivers its highly effective consumer, sales associate and broker solutions in support of Realogys leading franchise brands and is positioned to expand the scope of its technology development efforts to encompass Realogys other business segments, including residential brokerage, relocation and settlement services. ZapLabs is headquartered in Emeryville, California.

Job Title: Big Data Engineer

At ZapLabs, we work to build and improve a platform that helps real estate professionals work effectively, and helps delight home buyers and sellers with an excellent experience. We do that by combining great technology with great people  and were looking for a Big Data Engineer to join our team.

What were looking for :
Youre a talented, creative, and motivated engineer who loves developing powerful, stable, and intuitive apps  and youre excited to lead a team of individuals with that same passion. Youve accumulated years of experience, and youre excited about taking your mastery of SQL, Java, and Scala to a new level. You enjoy challenging projects involving large data pipelines and are cool under pressure. Youre no stranger to fast-paced environments and agile development methodologies  in fact, you embrace them. With your strong analytical skills, your unwavering commitment to quality, your excellent technical skills, and your collaborative work ethic, youll do great things here at ZapLabs.

What youll do :
As a Big Data Engineer, youll be responsible for designing and building high performance, scalable solutions for large volume data ingestion, clean-up, and processing that meet the needs of millions of agents, brokers, home buyers, and sellers. Youll design, develop, and test robust, scalable microservices, Spark Streaming Applications, and RESTful web services. Youll work with a variety of teams and individuals, including product engineers, to understand the MLS and other real estate related data pipeline needs and come up with innovative solutions. Youll be a self-organized member of a team of talented engineers and collaborate with product managers and designers to help implement and execute new products and features.

Skills, accomplishments, & interests you should have :
BS in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience
4+ years core Java experience: building business logic layers and back-end systems for high-volume data pipeline backend applications
Current experience using Scala development
Current experience developing microservices
Current experience using Java development, SQL Database systems, and Apache products (Tomcat, Spark, Hadoop, Cassandra)
Current experience developing RESTful web services
Current experience high-speed messaging frameworks and streaming (Kafka, Akka, reactive)
Current experience developing and deploying applications to public cloud (AWS)
Experience with DevOps tools (GitHub, Jira) and methodologies (Agile, Scrum, Kanban, Test Driven Development)
Ability to work quickly with an eye towards writing clean code that is efficient and reusable
Ability to build prototypes for new features that will delight our users and are consistent with business goals
Ability to iterate quickly in an agile development process
Ability to learn new technologies and evaluate multiple technologies to solve a problem
Ability to refactor early and often to continuously improve code quality
Excellent analytical/troubleshooting skills
Excellent written and verbal communication skills in English
Strong work ethic and entrepreneurial spirit

Nice to haves:
Experience working on SMACK stack

Were always looking for folks to join our culture of passionate hobbyists! We tend to nerd out on everything from extensive LOTR and Star Wars knowledge to competitive baking to our company soccer team, the Lab Rats.

EOE AA M/F/Vet/Disability
#LI-LY1"
Senior Data Engineer,Dropbox,"The Data Engineer is responsible for designing and developing robust, scalable solutions for collecting, analyzing large data sets, creating and maintaining data pipelines, data structures and reports to be used by the revenue organization at Dropbox. Responsibilities Understand business processes, applications and how data is gathered; and tie application telemetry to transactional data model. Design, build and manage data marts to satisfy our growing data needs. Develop and manage data pipelines at enterprise scale Build data expertise and own data quality for various data flows Launch and support new data models that provide intuitive analytics to internal customers Design and develop new framework and automation tools to enable teams to consume and understand data faster Use your expert coding skills across a number of languages like SQL, Python and Java to support analysts and data scientists Interface with internal data consumers to understand data needs Collaborate with multiple teams in high visibility roles and own the solution end-to-end Requirements 5+ years of Python or Ruby development experience is necessary; 8+ years of SQL (Oracle, AWS Redshift, Hive, etc) experience is required, No-SQL experience is a plus; 8+ years of experience in custom or structured (ie.

Informatica/Talend/Pentaho) ETL design, implementation and maintenance; 8+ years of experience with schema design and dimensional data modeling. Ability to analyze data to identify deliverables, gaps and inconsistencies. Ability in managing and communicating data warehouse plans to internal clients. Experience designing, building and maintaining data processing systems Experience working with either a Map Reduce or a MPP system on any size/scale Experience working with visualization tools like Tableau or MicroStrategy Communication skills including the ability to identify and communicate data driven insights BS or MS degree in Computer Science or a related technical field"
Data Warehouse Engineer,Fullscreen,"We are looking for a Data Warehouse Engineer responsible for building a scalable and modern data warehouse and operational database environment, data modeling, and ETL data management tools & processes for Fullscreen's Video Labs. The ideal candidate would have a passion for and knowledge of social video platforms, digital entertainment and have a proven track record supporting and fulfilling the strategic objectives of a highly cross-functional organization.

*** This is a contract position (12+ months) with an option to go Full-time (hire) ***

RESPONSIBILITIES
Responsible for designing, implementing and maintaining Data Warehouse solutions which will handle the growing business needs
Interface with other technology teams to extract, transform, and load (ETL) data from a wide variety of in-house and 3rd party data sources
Design, implement, and support infrastructure providing secured access to large datasets
Utilize blended data from multiple data sources to provide comprehensive reporting solutions
Perform ad-hoc analyses and respond to data/analytical requests
Glean insights from a large data warehouse using SQL
QUALIFICATIONS
5+ years of Business Intelligence/Data Wharehouse experience
Deep proficiency with SQL & Python
Knowledge of working with an enterprise data warehouse and large sets of data
Extensive experience with relational databases both, normalized and denormalized star schemas.
Strong knowledge of data warehousing concepts
Experience in ETL and using databases in a business environment with large-scale, complex data sets
Proven ability for looking at solutions unconventionally and explore opportunities and devise innovative solutions
Excellent communication skills (verbal and written) and interpersonal skills to convey key insights from complex analysis in summarized business terms and an ability to effectively communicate with technical teams
BONUS POINTS
NoSQL Databases & Technology (e.g. Hadoop)
Amazon Redshift and Amazon Web Services: Glue, Lambda, S3, and Athena
Django web development
Experience working with Spark (pyspark)
Experience with Tableau or a similar BI visualization tool developing reports & dashboards using Tableau or Amazon Quicksight
Experience developing jobs in Luigi
Digital Media industry experience"
Data Engineer,Dailymotion,"Job Description

Build, scale and maintain data pipelines to process billions of daily events
Develop Web services for data collection from internal and external sources
Work closely with data scientists to construct creative solutions for their analysis tasks Qualifications

5+ years of experience developing and debugging distributed systems in Go, Scala or Java
2+ years of experience with Kafka, Spark or other streaming technologies
Familiar with NoSQL technologies such as Aerospike or Cassandra
Knowledge of ad serving platforms and online advertising systems is a huge plus
Comfortable working with Linux and the AWS stack
Masters Degree in Engineering or equivalent work experience

Additional Information

We offer great health / dental / vision insurance
401k (company Safe Harbor contribution) summer hours, paid vacation, sick time, paid holidays, company sponsored Training Opportunities (upon eligibility) and much more."
Senior Data Engineer - Minecraft,Microsoft,"Minecraft is one of the most popular video games of all time, with more than 122 million copies sold. Minecraft inspires players to create together and has created one of the most active and passionate player communities in the world.

As a Senior Data Engineer, you will be responsible for leading the engineering function for the data solutions both the Minecraft Data Science team and the studio depend on. Working closely with the embedded Data Science function you will be play a leading role defining both our data development roadmap and aligning with our data architecture strategy. This is a central role and essential to ensuring we have sufficient technologies in place to enable data informed decisions.

An appropriate candidate will have deep technical knowledge of BI systems design, big data architecture and technology landscape, service architecture, ETL/ELT orchestration, business intelligence tools such as Power BI as well as large scale event telemetry ingestion and aggregation systems. The candidate should have excellent organizational and communication skills and feel comfortable in a fast-paced environment.

So, if you are you an expert in designing and building business intelligence solutions, big data and are fascinated by how analytics driven organizations can transform data into actionable insights, we have a dream job for you!

Key Responsibilities:
Design, implement data solutions
Define and execute the data engineering roadmap
Provide technical leadership to the Minecraft Data Science Team
Lead the data engineering function
Operational Responsibilities:
Contribute to the continual improvement of our data architecture
Conduct code reviews ensuring development follows best practice
Establish and maintain regular updates detailing, current issues, cost, and development progress
Direct research on potential technology solutions and implementations in support of new initiatives and opportunities.
Effectively manage vendor budgets
Minimum Qualifications:
Bachelors degree or 5 years software engineering experience
Preferred Qualifications:
We are looking for a skilled and experienced Data Engineer with an extensive background in developing large scale data solutions
Proven background and strong hands on experience with one or more of the following Big Data technology stacks: Hadoop in the cloud, MPP SQL (Hive preferred), ETL orchestration systems
The engineer will be highly experienced with relational databases (SQL Server or similar)
The engineer will be highly experienced with MPP SQL such as Hive (or similar SQL on Hadoop implementations)
In-cloud development and Linux experience a plus
Strong software engineering background and understanding of the software development lifecycle
Ability to effectively prioritize and execute tasks in a fast-paced environment
Ability to express complex technical concepts effectively, both verbally and in writing
Highly selfmotivated, selfdirected, and attentive to detail
Strategic thinker, combined with strong ability to execute
Solid verbal and written communication
Experience implementing service-oriented architecture a plus
Our processes include requirements analysis, continuous development cycle, integration from multi partner orgs and teams, enhancement, maintenance, testing, customer scenario evaluations and root cause analysis/resolution.

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to age, ancestry, color, family or medical care leave, gender identity or expression, genetic information, marital status, medical condition, national origin, physical or mental disability, political affiliation, protected veteran status, race, religion, sex (including pregnancy), sexual orientation, or any other characteristic protected by applicable laws, regulations and ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to askstaff@microsoft.com."
Database Engineer,Cardinal Health,"Position Summary

The Database Engineer is part of the engineering team. This position is primarily responsible for the planning, execution, and maintenance of naviHealth databases, both relational and NoSQL, while supporting development engineers to develop application query leveraging best practices and leading-edge database technologies. The person should be able to write SQL query to support business defined needs using Oracle based technology/tools, like SQL/PLSQL routines, in an agile development environment.

This is a hands-on development role at early to mid-level and the ideal candidate should have a history of building fantastic enterprise-grade products quickly and with quality. We have a culture where people do well based on their merits and their ability to contribute to the success of the larger team. The right candidate will have a key role in our challenging and fun mission that values a work-hard/play-hard model.

Principal Duties and Responsibilities

Create Oracle PL/SQL packages, complex stored procedures, partitioned tables, ad-hoc queries to support development and business needs.
Assist senior database engineers with the day-to-day activities, including databases maintenance such as installation, backup, restore and security control.
Collaborate with stakeholders to plan and deploy new database releases.
Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries.
Troubleshoot production support issues in the application environments.
Work as a part of a development team to implement product enhancements.

Minimum Qualifications

Technical Skills:
2+ years of experience with development in databases using traditional SQL databases like Oracle.
1+ years of experience with development in databases using NoSQL databases like MongoDB.
1+ years of experience in DBA, data modelling, and ER schema, star schema.
Fluency with ANSI-SQL and Oracle PL/SQL.
Familiarity with database design and architecture for relational/NoSQL database is a plus.
Understanding of data analysis, quality assurance and data quality principals as applied to database is a plus.

Soft Skills:
Critical thinking abilities to take complex, ambiguous, abstract requirements and break them into smaller components, patterns, views and features.
Strong ability to communicate technical and data concepts to non-technical audiences, and business concepts to technical audiences.
Strong analytical, organizational, verbal communication and business writing skills.

About naviHealth

naviHealth partners with health plans, health systems and post-acute providers to manage the entire continuum of post-acute care. We utilize evidence-based protocols to optimize care and bundled payment methodologies to align all stakeholders. The result: optimized care and outcomes, reduced inpatient days, reduced hospital readmissions, and increased patient satisfaction.

naviHealth Values
We care about the people we serve.
We care about each other.
We care about our communities.
We embrace innovation.
We like simple.

naviHealth  is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining and maximizing the performance of a diverse and inclusive workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."
Data Engineer- Internet Monitoring,Amazon.com,"While we tend to think of it as single entity, the Internet is actually comprised of tens of thousands of independently-administered networks, with routing protocols facilitating the constant flow of traffic within and between them. As if that werent complicated enough, the flow of traffic across the Internet is also subject to various political, regulatory, and business requirements, not to mention the traffic engineering done by the large operators and players. The Internet is amazingly complex and even when things are humming along smoothly, we make it our mission to study its behavior and how it breaks. More specifically, we study how Amazons network connects to the public Internet in order to detect when those connections are disrupted and impact our customers. In light of this, we are looking for a Data Engineer to join our R&D effort and help drive insight into this fruitful and wide-open problem space.

The ideal candidate will have a demonstrated affinity for engineering best practices, data management fundamentals, data storage principles, and be current on recent advances in distributed systems as it pertains to data storage and computing. Building and operating scalable infrastructure for data analysis, you strive to answer the big questions with data

Basic Qualifications
Responsible for designing, building, and maintaining an efficient, extensible, and scalable data infrastructure(s) for processing high-volume and high-velocity data
Proven track record of processing unstructured and/or complex semi-structured data streams and building self-service business intelligence infrastructure
Proven track record in building, operating and optimizing distributed, large-scale data storage and analysis solutions (e.g. RedShift, Dynamo, Cassandra, Spark, EMR, Hadoop)
Experience programming in a general-purpose language (e.g. Java, Scala, Python, Go, Ruby, C/C++, JavaScript, etc.)
Knowledge of relational data modeling concepts and basic SQL and data analysis skills
Strong problem-solving and troubleshooting abilities
Knowledge of data storage best practices and use cases

Preferred Qualifications
Familiarity with streaming data analysis solutions (e.g. Kafka, Kinesis)
Experience in ETL pipelining and data warehousing
Web development and Data Visualization experience a plus
Familiarity with networking protocols (TCP/IP, ICMP, BGP)"
Senior Data Engineer,Retention Science,"We are growing rapidly and looking for great engineers to join our team. Our client list is growing, and we're hitting larger and larger amounts of data. We are looking for back-end engineers to develop new features and scale out our platform to crunch billions of data points. Currently, our platform powers thousands of campaigns and processes billions of behavioral signals each day. Weve sent over a billion individually personalized campaign interactions since our inception!

We are looking for an experienced data pipeline engineer who is passionate about writing clean, well-tested code. You should want to make a huge impact in a fast-paced and cutting-edge start-up environment. You are a spark/scala expert, and youll be building robust real-time data pipelines that process billions of events per day.

In addition to having meaningful responsibilities and gaining experience in product development, you will also receive comprehensive exposure to all aspects of our business. The code and ideas that you contribute will have a tangible impact on the business as a whole. Your code will touch millions of end-users. You will have full responsibility for your projects and have a real sense of product ownership. You will also have the opportunity to learn tremendously from our awesome team of humble yet super engineers. You'll work with and learn the latest technologies and apply them across our distributed systems.

About You:
5+ years experience working on production data products in Scala or similar
You are a Hadoop expert, and also have 1+ years experience with Scala / Spark
Proficient in at least one statically typed language
Like functional programming
Experience designing and implementing large, scalable services
Passionate about enforcing software engineering principles, production code quality, and regular use of design patterns
Experience interfacing with APIs - SOAP, REST, etc.
Experience creating robust RESTful APIs
Comfortable using Git, Bitbucket/Github
Strong belief that tests and code go hand-in-hand
Deep understanding of SQL, query optimizations, joins etc.
Excellent CS foundation: data structures, time complexities, algorithms, etc.
Startup work experience a major plus!
Retention Science empowers marketers through Artificial Intelligence. Our technology combines deep machine learning and true automation, turning data-driven insights into intelligent actions that further customer retention. We help build the lasting relationships that build your business.

Named ""Innovation Agent"" by Fast Company, ""Top 10 Big Data Company of the Year"" by CRN, and ""Top 10 Software Company in Southern California"" by SocalTech."
Data Engineer - Data & Insights,Abercrombie & Fitch,"Data Engineer - Data & Insights
Position Title: Data Engineer - Data & Insights Location: New Albany, Ohio WHQ

At Abercrombie & Fitch, quality is in our roots and were on a mission to honor this rich heritage. With an unwavering focus on our customer, we strive every day to deliver a unique and welcoming experience, whether in our stores or online. Our three global brands, Abercrombie & Fitch, abercrombie kids and Hollister Co., are the embodiment of our passion for incredible product. At the heart of it all is our amazing 500-acre campus, nestled just outside of Columbus, Ohio. With an open work space, inspiring views, and even a place to gather as a team around the fire pit, our Home Office fosters a diverse and inclusive culture that consistently seeks the input of our associates and focuses on the future. We are looking for leaders, visionaries, and creatives who are willing to roll up their sleeves and write the next chapter in our brands legacy.

Information Technology at Abercrombie & Fitch is fundamental to designing, sourcing, developing, and delivering fashion forward merchandise to our customers. We are committed to implementing new strategic and systematic approaches to generate dynamic technology solutions for our growing business. The Data Engineer role is responsible for the management and delivery of data and reporting across the enterprise as part of the IT team. Were seeking a candidate that is constantly challenging the status quo and seeking out innovation in an ever-evolving retail environment.

What will you be doing?
Design, develop and implement complex data and reporting solutions for Abercrombie & Fitch
Collaborate with other Abercrombie & Fitch IT leads and developers to identify opportunities for improvement and innovation
Discover, evaluate, and implement new technologies and techniques to maximize development efficiency
Develop team members by providing coaching, code review, and process guidance
Support critical production systems to ensure stability and uptime
Assist business partners in applying data to evaluate and tackle business problems
Create and uphold technology standards, processes, and audit methodologies
Continuously improve IT platforms based on retail and data management technology trends in support of business strategies
What will you need to bring?
5+ years of hands-on working experience delivering data solutions using an Enterprise BI tool (i.e. Cognos, OBIEE, MicroStrategy, Tableau, etc)
Significant experience with a variety of database and/or data management technologies including Hadoop (i.e. HBASE, Hive, MapReduce, Sqoop, Spark), Netezza, etc
Experience in both build & support functions of BI solutions including advanced SQL views, development and management of reporting models, complex report development and business user support.
Experience leading small to medium BI and/or data focused initiatives including direct interaction with business partners
A 4 year degree in an IT related field
Retail experience is a plus
*LI-JB1

Primary Location OH-Columbus

Schedule Full-time"
Senior Data Engineer,Comcast,"Comcast brings together the best in media and technology. We drive innovation to create the world's best entertainment and online experiences. As a Fortune 50 leader, we set the pace in a variety of innovative and fascinating businesses and create career opportunities across a wide range of locations and disciplines. We are at the forefront of change and move at an amazing pace, thanks to our remarkable people, who bring cutting-edge products and services to life for millions of customers every day. If you share in our passion for teamwork, our vision to revolutionize industries and our goal to lead the future in media and technology, we want you to fast-forward your career at Comcast.
Data engineering and data science skills combined with the demands of a high volume, highly-visible analytics platform make this an exciting challenge for the right candidate.Are you passionate about digital media, entertainment, and software services? Do you like big challenges and working within a highly-motivated team environment?As a Data engineer in Data Platform Engineering and Research (DPER) team, you will research develop, support and deploy solutions real-time distributing computing architectures. You will also employ your skills to deliver insights into customer and network behavior on a rapidly-growing video-over-IP platform. The DPER data team is a fast-moving team of world-class experts who are innovating in end-to-end video delivery. We are a team that thrives on big challenges, results, quality, and agility.Who does the data engineer work with?Data engineering is a diverse collection of professionals who work with a variety of teams ranging from other software engineering teams whose software integrates with analytics services, service delivery engineers who provide support for our product, testers, operational stakeholders with all manner of information needs, and executives who rely on data for data based decision making.What are some interesting problems you'll be working on?Develop systems capable of processing millions of events per second and multi-billions of events per day, providing both a real time and historical view into the operation of our wide-array of systems. Design collection and enrichment system components for quality, timeliness, scale and reliability. Work on high performance real time data stores and a massive historical data store using best-of-breed and industry leading technology. Design, develop, and apply advanced statistical methods and Machine Intelligence algorithms.Where can you make an impact?Comcast DPER is building the core components needed to drive the next generation of data platforms and data processing capability. Building data products, identifying trouble spots, and optimizing the overall user experience is a challenge that can only be met with a robust data architecture capable of providing insights that would otherwise be drowned in an ocean of data.Success in this role is best enabled by a broad mix of skills and interests ranging from traditional distributed systems software engineering prowess to the multidisciplinary field of data science.Responsibilities:
Lead development for new products
Analyze massive amounts of data both in real-time and batch processing
Prototype ideas for new tools, products and services
Employ rigorous continuous delivery practices managed under an agile software development approach
Ensure a quality transition to production and solid production operation of the software
Raise the bar for the Engineering team by advocating leading edge practices such as CI/CD, containerization and TDD
Enhance our DevOps practices to deploy and operate our systems
Automate and streamline our operations and processes
Build and maintain tools for deployment, monitoring and operations
Troubleshoot and resolve issues in our development, test and production environments
Here are some of the specific technologies we use:
Spark (AWS EMR)
Spark Streaming and Batch
Avro
Kafka
MemSQL, Cassandra, HBase, MongoDB
Caching Frameworks(ElasticCache)
Java, Scala, Go
Git, Maven, Jenkins
Rancher, Puppet, Docker, Kubernetes
Linux
Hadoop (HDFS, YARN)
Skills & Requirements:
5+ years programming experience
Bachelors or Masters in Computer Science, Statistics or related discipline
Experience in software development of large-scale distributed systems including proven track record of delivering backend systems that participate in a complex ecosystem.
Experience in data related technologies and open source frameworks preferred
Proficient in Unix/Linux environments
Test-driven development/test automation, continuous integration, and deployment automation
Enjoy working with data analysis, data quality and reporting
Excellent communicator, able to analyze and clearly articulate complex issues and technologies understandably and engagingly
Great design and problem solving skills
Adaptable, proactive and willing to take ownership
Keen attention to detail and high level of commitment
Thrives in a fast-paced agile environment. Requirements change quickly and our team needs to constantly adapt to moving targets
About Comcast DPER (Data Platform Engineering and Research): Data Platforms Engineering and Research(DPER) is a result driven, data platform research and engineering team responsible for the delivery of multi-tenant data infrastructure and platforms necessary to support our data-driven culture and organization. DPER has an overarching objective to gather, organize, and make sense of Comcast data with intention to reveal business and operational insight, discover actionable intelligence, enable experimentation, empower users, and delight our stakeholders. Members of the DPER team define and leverage industry best practices, work on extremely large scale data problems, design and develop resilient and highly robust distributed data organizing and processing systems and pipelines as well as research, engineer, and apply data science and machine intelligence disciplines.
Comcast is an EOE/Veterans/Disabled/LGBT employer"
Senior Data Engineer,Reflektive,"ABOUT REFLEKTIVE'S ENGINEERING TEAM

Reflektive is seeking a Data Engineer to deliver our talent development platform to the world's best places to work. Reflektive is a rapidly scaling company making this the best environment to take on ownership as well as learning how to grow a company.

You'll join a lean, prolific team where everyone, including you, is active in the product defining and development process (where deploying new features every 2 weeks is common). You'll know the customers we're talking to, and the needs of each one. As a result, you know where your initiative and drive can best make a difference (and be recognized!)

Our engineering team consists of developers from a wide array of backgrounds. Our data team primarily focuses on Java, SQL and Python. Our team is a tight knit, friendly group of engineers that are dedicated to learning from and teaching to each other. Team members regularly contribute to and optimize our engineering practices and processes. Our team wants to make software engineering fun, easy, and fulfilling, so we've come up with a set of values that we apply to our software every day: Simple, Flexible, Consistent, Predictable, Efficient, and Pragmatic.

Responsibilities
Working on Reflektive's Data & Infrastructure Projects
Experience or demonstrated interest in Big Data Technologies
Enhance and further develop Big Data processing pipelines for data sources containing structured and unstructured data
Monitor and optimize key infrastructure components such as Databases, EC2 Clusters, and other aspects of the stack
Help promote best practices for Big Data development at Reflektive
Act as a bridge between the infrastructure and application engineering teams
Provide infrastructure support with a focus on cloud based computing
Build and support visualization and exploration capabilities around our Data Sets
Work with the Data Extraction and Data Science engineers on normalization and analytical processes
Work in an Agile manner with business users and data scientists to understand and discover the potential business value of new and existing Data Sets and help productize those discoveries
Help design and implement disaster recovery efforts
Analyzes requirements and architecture specifications to create detailed design
Research areas of interest to the team and help facilitate solutions
Desired Skills and Experience
5+ Years of professional experience as a Data Engineer or a Backend / Full Stack Software Engineer looking to move into Data Engineering
Skills required: Java, SQL
Great if candidate has experience in any of the following Python, Ruby on Rails, PostgresSQL, Redshift, Elasticsearch, ETL, Spark, Storm
Startup experience
Flexible team player
Willingness to roll up your sleeves and get stuff done
Willingness to learn and do whatever it takes to meet deadlines in a quick, ever-changing environment
In 30 days, you will have deployed fixes to production and have an understanding of how Data is consumed/transformed/stored and the Key Infrastructure Projects
In 60 days, you will have taken on a key greenfield production project, working independently or with team mates to deliver the project
In 90 days, be a key individual in the team and wider engineering department, helping to deliver important data driven solutions"
Senior Data Engineer,"Lumos Labs, Inc.","Lumosity is a brain training program with over 85 million users and 4.4 billion games played. In addition to producing more games for Lumosity, were also creating new products that combine science, engineering and game design to leverage social interactions, creativity, and more.

The Data Engineering team powers Lumosity by building systems and infrastructure to effectively collect, distribute, and analyze data at scale, as well as creating next-generation tools to make sense of it all. Were looking for an experienced engineer who has a passion for data, clean code, and operational efficiency.

Learn more about what we're working on here !

YOU WILL
Drive the design and implementation of systems at the core of our data infrastructure on a small, but effective team of data engineers
Work with our data scientists to build high-throughput systems that allow us to continually invest in a rich, engaging experience
Help envision, implement, and evangelize new features, pipelines, and platforms that not only deliver data, but insight
Collaborate with product, engineering, and analysis teams across the organization, as our systems are loosely-coupled, but highly utilized
YOU HAVE
A consistent track record of building highly-available back-end systems that scale
A deep understanding of patterns of data architecture, back-end system design
Proficiency in Scala, Java, or other modern systems languages
Strong analytical and debugging skills
A dedication to engineering excellence
NICE TO HAVE
Exposure to the principles of functional programming
Some experience with front-end technologies and/or statistical analysis tools
Experience with A/B testing or machine learning infrastructure
Experience with Kafka, Spark, Flink, Storm, or similar
WE OFFER
A collaborative culture where promotion from within is encouraged
Competitive health benefits (medical, dental, vision)
Flexible PTO  take the time off when you need it
Equinox gym reimbursement (or $ towards the gym or classes of your choice)
Catered lunch 4 times a week, fully stocked kitchen & snacks, and dinner if you work late
Weekly happy hours and bi-monthly movie nights
Game room with Smash tournaments, tons of board games, ping pong, chess, and Legos
Team off-sites (past excursions include archery, rafting, indoor skydiving)
Company-wide Hackathons"
Senior Data Engineer,CircleUp,"CircleUp harnesses the power of machine learning and predictive analytics to discover the fastest-growing companies in the consumer & retail sector. We want to bring the data-driven revolution that has occurred in the public markets to the private markets , starting with consumer & retail.

Our mission is to help entrepreneurs thrive by giving them the resources and capital they need. CircleUp has been named one of the Top 5 Most Disruptive Companies in Finance by CNBC, one of the 50 Best Fintech Innovators by KPMG, and one of America's Most Promising Companies by Forbes. We are backed by top-tier investors including Google Ventures, Union Square Ventures (backers of Etsy/Kickstarter), and the ex CEOs/Presidents of Goldman Sachs, Morgan Stanley, Thomson Reuters, the Stanford Endowment and Capital One.

We are looking for a senior data engineer who can help us scale a complex machine learning data pipeline. We currently lean heavily on technologies like Python, Postgres, and Apache Spark, but this role will have substantial influence over our technical architecture moving forward, so anything is on the table.

Our mission helps entrepreneurs outside the world of tech (there's some evidence it helps female entrepreneurs the most), and represents a cutting edge application of Big Data and Machine Learning.

We are a distributed team so remote candidates are welcome. You can work from home and we'll fly you out to visit sunny San Francisco once a quarter! (U.S. based only please)"
Python Data Engineer,VideoAmp,"What we are looking for:
We're looking for a software engineer with experience working on a large volume low-latency data platform who is eager to tackle new problems in the programmatic media-buying space.

The Data-Platform team uses Kafka, Spark, Druid, Cassandra, PostGres, Finagle, Airflow, HDFS, and other technologies to support an ETL pipeline, data warehouse, and DMP. We process and report on over 20B auctions per day, billions of user attributes per day, and a variety of other input streams.

Responsibilities for this role include:
Adding new reporting features (i.e. low latency data-marts, HTTP services, and other custom reporting solutions)
Building new, and scaling out existing Spark apps.
Building out new ingest pipelines (Kafka, S3, third-party web services, etc)
Coordinating data models with other engineering teams
Working with the Dev Ops team to increase our monitoring and alerting coverage as needed, and scale out our NoSQL appliances
Tuning Spark apps to get the most out of the quickly evolving platform
Qualifications
A solid and demonstrable understanding of Python
Experience building and managing ETL pipelines
SQL fluency and an understanding of relational data models.
Comfortable in the Linux environment.
A master of all trades mentality and an ability to embrace new challenges regularly.
Eager and energized to work with some of the latest open source technologies.
Prior experience working with Spark, Hadoop, or other big data processing platform in high-volume environments
Authorized to work in the US
Able to work on-site in Santa Monica
VideoAmp perks:
Small, friendly, and highly collaborative team, with no brilliant jerks - but lots of highly intelligent and motivated coworkers
Unlimited PTO
Unlimited use of personal trainer and weekly in-office yoga
Catered meals and fully stocked kitchen (premium + cold brew coffee, fresh produce, hot breakfast, variety of beverage options)
Corporate support for hackathons, lunch and learns, and attending conferences
In office gym
Optional company outings, i.e. Bubble Soccer, White Water Rafting, Star Wars movie viewings, Blizzcon trip, Camping, Beach Volleyball, etc..."
Expert Data Engineer - Nike Technology,NIKE INC,"As our Expert Data
Analytics Engineer, you will be driving architecture and design patterns to
provide consumer insights that drive Nike to new heights. You will
be leading innovation in driving the Data & Analytics Platform and
frameworks in the cloud. To meet the demands of our business, we are
rapidly embracing new software development and deployment technology and tools
and delivering solutions using agile methodology. If that sounds exciting, this
role is for you. Your Role on the Team Here are some highlights of the
responsibilities for this critical role: Evangelize and cultivate adoption of Analytics
Platform, open source software and Dev Ops principles within the organization Meet increasingly high demands of fast and
flexible, self-service analytics on high volume and high velocity data Lead next generation big data analytics platform
in the cloud, with emphasis on automation and scale Work and Lead data engineers and system engineers
to build innovative data products and insights Be a key contributor to overall architecture,
framework and design of BI and Analytics environment Help the engineering group to shift the paradigm
from traditional, relational BI to Big Data and NoSQL ecosystem in the Cloud We're looking for someone special, someone who had
these experiences and clearly demonstrated these skills: MS/BS in Computer Science or related field 7+ years of experience in large-scale software
development with emphasis on data analytics and high volume data processing 5+ years of experience architecting and building
scalable data architecture 3+ years of Hands-on experience with AWS, Hadoop
and tools such as Hive, Sqoop, Impala and Spark Experience with Snowflake strongly preferred Strong experience with SQL and Relational
databases and scripting languages such as Shell and Python Ability to architect, design and implement
solutions with AWS Virtual Private Cloud, Elastic Compute Cloud, AWS Cloud
Formation, Auto Scaling, AWS S3, Route 53 and other AWS products. AWS managed services such as EMR, Kinesis,
Lambda, etc. Lead other admins and platform engineers through
design and implementation decisions to achieve balance between strategic design
and tactical needs. Research and deploy new tools and frameworks to
build a sustainable big data platform. 6-months+ experience with Packer, Puppet, Salt,
Chef or AWS OpsWorks. A variety of programming languages, including
Java, Scala, Node.js/JavaScript, Python, etc. Spark, Kafka, Fluentd, Docker, Jenkins, the ELK
stack (Elasticsearch/Logstash/Kibana), and more!"
Machine Learning Engineer,Ntrepid Corporation,"Ntrepid is looking for a creative and versatile research software developer to join our Data Sciences Research Group.

As part of our internal research and development team you will help us develop and implement data analysis strategies and tools to improve our products and support our customers. You will contribute to research and development efforts that integrate and advance the state of the art in social media content and network analysis, as well as multilingual text analysis.

Working to advance tools and methods in these areas will offer ample opportunities to develop your multiple talents. You will participate in: research and algorithm design, the development of data visualizations and user experience, and the deployment and scaling of innovative data collection and analytic tools developed by our group used both internally and currently supporting existing external customers.

Responsibilities
Evaluate current research and new technologies in light of our products and customer needs, and use your insights to deliver new capabilities
Research and implement Machine Learning, Network Analysis, and Natural Language Processing algorithms to support social media data analysis
Collect and process a variety of data, structured and unstructured.
Develop quantitative models and data analysis strategies.
Design, implement, evaluate, and refine product prototypes.
Develop and deploy web-based services and applications.
Our ideal candidate will also be passionate about DevTools implementation and innovation in all aspects of software development and delivery that get in the way of developer happiness.

Qualifications
U.S. Citizenship Required
Advanced degree or relevant experience in a quantitative field
Proven capacity to model and solve problems using quantitative approaches
Experience and clear understanding of machine learning and statistical modeling
Experience managing and working with structured and unstructured data
Ability to rapidly develop prototypes as well as write production quality code
Experience with static (Java, C, C++), dynamic (Ruby, Python), and mathematical and statistical (R, MATLAB, Julia) languages; 5+ years combined experience, with 2+ years in any particular language
Experience managing and modeling data using relational (Postgres, MySQL), NoSQL (Redis, Neo4j, MongoDB) and search (Elasticsearch, Solr) technologies.
Familiarity with modern web frameworks, application architectures, languages, and libraries (Rails, Django, MVC, JavaScript, Bower, Node, D3, Plotly)"
Cloud Data Engineer AWS Greenfield,Princeton Consulting Group LLC,"This is an exciting greenfield opportunity to help build a new Data Analytics platform and Cloud infrastructure. FTE paying 175k-300k. Client cannot sponsor H1 Visa at this time. Local Candidates only please.

Design and implementation of a large scale data analytics platform in a cloud based environment for ongoing production analytics is required. The candidate should have experience with both cloud-native data pipeline and transformation tools, such as AWS Kinesis, Redshift, Lambda, and EMR, as well as with open source tools such as NiFi, Kafka, Flume, Hadoop, Spark, and Hive.

Principal Responsibilities/Qualifications

Extensive background in designing large scale data analytics platforms
Experience in designing data pipelines in a cloud environment, preferably AWS
Build solutions to process structured and unstructured data from multiple sources
Experience with OLTP and OLAP systems
Experience performing ETL at scale with open source and cloud native tools
Strong logical and physical schema design and implementation
Experience with Lucene based search engines, with a preference for Elasticsearch
Work closely with the data science team
Design and implement highly available, scalable and encrypted storage solutions
Strong coding skills in Python or Java using analytic packages
Management and operations of all environments
Qualifications/Skills Required

Extensive experience managing data engineering for production environments
Experience designing, building, and automating, AWS or other cloud environments
Data persistence on AWS platforms such as S3, RDS, EMR, DynamoDB, and Redshift
AWS pipeline and transformation tools such as Kinesis, Lambda, and EMR
AWS APIs and automation including Boto3
Cloud Formation or other cloud environment build automation tools
Building elastically scalable environments that leverage both horizontal and vertical scaling
Developing cost saving strategies using both reserved and spot instances
Experience developing collaboratively, including infrastructure as code, in Python with Git"
Python Engineer - API - Data Cloud,Oracle,"Python Engineer - API - Data Cloud-17001BME
Preferred Qualifications

About Moat
We help brands and media companies push the envelope on privacy-friendly models of advertising, help them make sound product decisions that involve running fewer ads, help make their ads load faster, fight botnets, and strive to de-fund fake news. We help the world's top publishers and marketers make smarter decisions through real-time data. Our analytics and intelligence software is used by The New York Times, Cond Nast, Kellogg's, P&G, Spotify, and hundreds of other major platforms and advertisers, and we have unique measurement integrations into Facebook, Instagram, Twitter, Snapchat, and YouTube.

About the Opportunity
Architect and build low-latency, fault-tolerant applications to improve site availability and uptime.
Work with large data sets from some of the largest and well-known publishers and advertisers in the world.
Use test-driven development in order to help us write robust solutions to complex problems.
Harden existing application code and ensure new code meets strict security standards.
Ability to construct and analyze relational database queries to extract insightful data from large datasets for use in the API.
Improve API performance by utilizing caching, such as Redis or a similar key/val store.

About You
Solid understanding of good object-oriented design, both in theory and practice.
Experience with programming in Python.
Experience using a debugger to walk through a running Python application is a big plus.
Familiarity with MVC frameworks such as Flask, Pyramid or Django.

Detailed Description and Job Requirements

Design, develop, troubleshoot and debug software programs for databases, applications, tools, networks etc.

As a member of the software engineering division, you will take an active role in the definition and evolution of standard practices and procedures. You will be responsible for defining and developing software for tasks associated with the developing, designing and debugging of software applications or operating systems.

Work is non-routine and very complex, involving the application of advanced technical/business skills in area of specialization. Leading contributor individually and as a team member, providing direction and mentoring to others. BS or MS degree or equivalent experience relevant to functional area. 7 years of software engineering or related experience.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
Data Engineer - Breast Imaging Research,Massachusetts General Hospital(MGH),"We are seeking a top-notch data engineer with a passion for creating data driven solutions that are best-in-class. The role of this position will be to help create tools, levering the latest in advances in data science, machine learning and deep learning, to impact healthcare across a spectrum of focus areas including medical imaging, diagnostics, clinical informatics and population health.

The ideal candidate will be a data engineer who is driven by building pipelines that feed data scientists with data and can work both both independently as well as part of a competitive and collaborative team. We are seeking a candidate with the demonstrable ability to find solutions where others couldnt, who has the drive, energy and determination to pull the team forward and persevere until the impossible is possible. Were looking for self-starters with a strong sense of urgency, comfort moving forward independently and sourcing input where required, and a tolerance for operating in a fast-paced early stage environment with a high degree of ambiguity.

PRINCIPAL DUTIES AND RESPONSIBILITIES
Write clean, maintainable performance code ensuring data is flowing smoothly between source and destination

Comfortable transforming, normalizing and merging multiple sources of data in both batch and streaming environments.

Build pipelines that feed data scientists with data: Develop andmanage extraction tools, wrap the data, send it forward in the data pipeline. Correct, transform and enrich the data. Quickly and efficiently load bulk data

Work tightly with the broader date science and software team to identify the path to a successful product

Take responsibility for strengthening the team by facilitating the adoption of processes that will allow us to work faster and hire exceptional team-mates

Use the Partners HealthCare values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration

SKILLS/ABILITIES/COMPETENCIES
Strong sense of urgency and proactiveness

Ability to function effectively and independently in a fast-paced environment, organize and prioritize work independently, and meet tight deadlines

Self-motivated, with an entrepreneurial mindset and ability to learn quickly

Strong analytical, planning, organization and time management skills with a strong attention to detail

Excellent interpersonal skills to effectively communicate with technical teams, cross-functional teams, and staff at all levels of the organization including both technical and non-technical personnel

Ability to successfully negotiate and collaborate with others of different skill sets, backgrounds and levels within and external to the organization

Ability to effectively conduct meetings and lead and facilitate large working sessions with all levels of staff and across various stakeholder groups

Demonstrates strong evidence of algorithmic and structured thinking, with an intuition for logic, pattern matching, what-if analysis, problem decomposition and synthesis.

Demonstrated ability to organize and incorporate complex systems requirements into product features and prioritize features effectively

Qualifications
Bachelors degree or equivalent combination of education and demonstrated front-end experience required. Computer science, engineering, or equivalent undergraduate and graduate degrees are preferred, though not strictly required necessary

A minimum experience of 3+ years in ETL Data Engineering

Clear, demonstrable evidence of exceptional productivity and performance in competitive environments

Expert knowledge of Python. Expert knowledge of database software (SQL + variants, MongoDB) and distributed computing (i.e. Hadoop, Spark) are strongly preferred.

Familiarity with Node.js, Flask, Express, micro services with containers, and .NET are strong pluses

Familiarity with common tools for machine learning (i.e. Tensorflow, Theano, Torch) and general scientific computing are pluses

Knowledge of software team management philosophies (e.g. Agile, Scrum) and various product management/software development tools (e.g., JIRA, Trello, etc.) are pluses

Experience with software development for healthcare products as well as familiarity with common clinical scenarios, regulatory and quality standards, payer and provider considerations, are beneficial but not necessary

EEO Statement Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.

Primary Location : MA-Boston-175 Cambridge - MGH

Work Locations : 175 Cambridge - MGH
175 Cambridge Street
Boston 02114

Job : IT/Health IT/Informatics-Engineer
Organization : Massachusetts General Hospital(MGH)
Schedule : Full-time
Standard Hours : 40
Shift : Day Job

Employee Status : Regular
Recruiting Department : MGH Radiology Research
Job Posting : Jan 2, 2018"
"Data Engineer, MTS2",Fanatics Inc.,"Overview As the global leader in licensed sports merchandise, Fanatics is changing the way fans purchase their favorite team merchandise by partnering with top leagues, clubs and soccer brands worldwide to offer the largest collection of timeless and timely gear from every pro and college team online, on your phone, in stadiums or on-site at the worlds biggest sporting events. A top 50 Internet Retailer Company, Fanatics comprises the broadest online assortment by offering hundreds of thousands of officially licensed items via its Fanatics ( www.fanatics.com ), FansEdge ( www.fansedge.com ), Kitbag ( www.kitbag.com ), and Majestic ( www.majesticathletic.com ) brands, as well as the largest selection of sports collectibles and memorabilia through Fanatics Authentic ( www.fanaticsauthentic.com ). A multi-channel company, Fanatics operates more than 300 online and offline stores, including the e-commerce business for all major professional sports leagues (NFL, MLB, NBA, NHL, NASCAR, MLS, PGA), major media brands (NBC Sports, CBS Sports, FOX Sports) and more than 200 collegiate and professional team properties, which include several of biggest global soccer clubs (Manchester United, Real Madrid, Chelsea, Manchester City). The company's in-venue and event retail portfolio includes the NBA, NHL, NASCAR, Wimbledon, Kentucky Derby, The Ryder Cup, Manchester City, Texas Longhorns, Pittsburgh Pirates and New Jersey Devils, allowing fans to experience a seamless shopping experience across online, mobile and physical store locations.

At Fanatics, we are passionate about leveraging data to drive growth and operational efficiencies. We firmly believe in putting data at the forefront of delivering an engaging experience for our sports fans!

Fanatics is looking for highly motivated individuals to join our Data Science and Engineering team. You will collaborate closely with data scientists and cross functional business units to develop data pipelines, infrastructure and applications in the following areas:
Behavioural Tracking and Analytics : core infrastructure for monitoring and deriving insights from site traffic.

Product Recommendation and Personalization : providing personalized product recommendations and user experience for sports fans in e-commerce sites as well as on emails.

Inventory optimization : forecasting demand and optimizing the quantity and location of inventory.

Product Finding : deploying key components of online-search, including ranking models, spell correction, query expansion and query understanding.

Job Responsibilities
Develop robust and scalable data infrastructure components for monitoring and tracking, messaging, real-time streaming and batch data processing use cases.

Integrate and productionize machine learning and statistical models into real-time services, as well as BI & Visualization layers.

Work closely with data scientists to assist on feature engineering, model training frameworks, and model deployments at scale.

Provide engineering leadership in the the design and architecture of data infrastructure and applications.

Job Requirements
2+ years of experience with big data processing in Spark, Hadoop, Hive and/or Redshift.

Expertise with Java or Scala.

Experience designing and developing data models, integrating data from multiple sources, building ETL pipelines, and supporting all aspects of the software development lifecycle. At this time, this role is not open to recent grads.

Strong SQL programming knowledge and experience.

Experience developing web services, API integrations, and data exchanges with third parties is preferred.

Ability to communicate quantitative analysis and analytical findings in a clear, precise, and actionable manner.

Energetic, enthusiastic, detail-oriented, and passionate about producing high-quality analytics deliverables.

Knowledge of basic statistical analysis and machine learning a plus.

MS/BS in Computer Science, MIS, Mathematics, Physics or other quantitative field or relevant work experience."
Senior Big Data Engineer,Apple,"As a member of the Siri Data Engineering Operations team within the Siri organization, you will be faced with highly complex issues in a large scale, distributed system environment. In order to ensure a reliable and rewarding Siri experience, you will be empowered to develop and design new solutions heavily focused on system automation. We look for talented engineers in both the Operations and Development space to bring these unique solutions to production at a rapid pace. From an Operations perspective, this makes us the most successful personal assistant in the industry.
Key Qualifications
Knowledge of the Linux operation system (OS, networking, process level)
Understanding of Big Data technologies (Hadoop, Hbase, Spark, Kafka, Flume, Hive, etc)
Understanding of one or more object-oriented programming languages (Java, C++)
Fluent in at least one scripting language (Shell, Python, Ruby, etc.)
Strong verbal and written communication skills
Passionate about being a part of a tight-knit Operations team
Description
The Siri Data Engineering Operations team manages the platform for both ingestion and analytics of worldwide Siri events. To accomplish this, we build automation tools and services to prevent failures and page out individuals when there really is a problem, not just noise.

Our engineers not only work closely with Operations, but also with the development and analytics engineers within Siri, as well as outside organizations. We build data pipelines for maximum efficiency, scalability and reliability to allow domain specific engineers to focus on their specialties. A successful candidate will have experience in being a Systems Administrator that has moved on to development and automation in their career.

Operate Apples largest infrastructure supporting millions of Siri customers at double digit PB scale
Troubleshoot complex issues across the entire stack
Design and develop automation frameworks to handle both development and production events at scale
Advise other teams (within and outside of Siri) on technical direction
Make changes to our environment with the purpose of pushing Siri to the next level
Education
B.S. degree in Computer Science or 3+ years of building data pipelines experience or equivalent.
Additional Requirements
(Nice to have)

Java JVM performance tuning/optimizations
Mesos compute platforms and job schedulers"
Big Data Engineer,Prudential,"http://video.digi-me.com/digime/jobs/IT/Prudential/Software Developer/ZP0020
Build distributed, scalable, and reliable data pipelines that ingest and process data at scale and in real-time.
Collaborate with other teams to design and develop and deploy data tools that support both operations and product use cases.
Own product features from the development phase through to production deployment.
Troubleshoot data issues, deep dive into root cause analysis, recommend, test and implement solutions
Develop and document technical requirements and solutions
Participate in design and code reviews
Troubleshoot issues making recommendations and delivering on those recommendations
BS in Computer Science or related area
Around 8 years software development experience
Minimum 5 years of active SQL development, Performance tuning, Data Modeling and design preferably in Oracle.
Minimum 1-2 Year Experience on Cloud computing, AWS preferable.
Strong automation skills using Python and Ansible is highly preferred
Good analytical and programming experience with Java, Oracle, SQL
Flair for data, schema, data model, how to bring efficiency in big data related life cycle
Proficiency with agile or lean development practices
Strong object-oriented design and analysis skills
Excellent written and verbal communication skills

Qualifications
Top skill sets / technologies in the ideal candidate:

Required:
Database  Oracle 11g / 12c, complex SQL queries, performance tuning concepts, Backups, Recovery, DR, BCP
Programming language -- Java, Python, SQL
AWS -- RDS, EC2, RedShift, S3 and others. AWS cloud Data migration, AWS Security
NoSQL -- HBase, MongoDB, Cassandra, Riak
ETL Tools -- Data Stage, Informatica
Code/Build/Deployment -- Ansible, Git, SVN, Maven, sbt, Jenkins, Bamboo, Terraform

Desired:
AWS Associate / Solution Architect certified
Batch processing -- Hadoop MapReduce, Cascading/Scaling, Apache Spark, AWS EMR
Stream processing -- Spark streaming, Apache Storm, Flink
Excellent communication and decision making skills are essential.
Strong analytical, problem solving and decision-making skills.
Zeal to lean new technologies, frameworks and appetite for growth
Identify project risks and recommend mitigation efforts.
Identify project issues, communicate them and assist in their resolution.
Assist in continuous improvement efforts in enhancing project team methodology and performance.
Cooperative team focused attitude

Prudential is a multinational financial services leader with operations in the United States, Asia, Europe, and Latin America. Leveraging its heritage of life insurance and asset management expertise, Prudential is focused on helping individual and institutional customers grow and protect their wealth. The company's well-known Rock symbol is an icon of strength, stability, expertise and innovation that has stood the test of time. Prudential's businesses offer a variety of products and services, including life insurance, annuities, retirement-related services, mutual funds, asset management, and real estate services.

We recognize that our strength and success are directly linked to the quality and skills of our diverse associates. We are proud to be a place where talented people who want to make a difference can grow as professionals, leaders, and as individuals. Visit www.prudential.com to learn more about our values, our history and our brand.

Prudential is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, genetics, disability, age, veteran status, or any other characteristic protected by law.

Note that this posting is intended for individual applicants. Search firms or agencies should email Staffing at staffingagencies@prudential.com for more information about doing business with Prudential."
Data Warehouse Engineer,Guardian Industries,"Overview Guardian Industries, a global company headquartered in Auburn Hills, Michigan, employs around 17,000 people and operates facilities throughout North America, Europe, South America, Africa, the Middle East and Asia. Guardians diverse group of companies are involved in the manufacture of high-performance float, coated and fabricated glass products; the manufacture of high-quality chrome-plated and painted plastic components for the automotive and commercial truck industries; and the distribution of specialty building products. Guardians vision is to create value for customers and society through constant innovation using fewer resources. Guardian is a wholly owned subsidiary of Koch Industries, Inc. Visit guardian.com.

Responsibilities Develops and supports the organizations data warehouse applications and related systems; consults and collaborates with stakeholders, functional and technical staff, and other data consumers to assess, identify, determine, and document user needs and requirements, design solutions, and model data. Designs, implements, tests, troubleshoots, documents, and supports data models, programs, scripts, and ETL (Extract, Transform, and Load) processes that produce appropriate and useful business intelligence capabilities. Develops and modifies measures and dimensions as business requirements demand. Determines and evaluates alternative approaches to meet user needs, and recommends optimal solutions.

Primary Responsibilities:
Work with DBA, Data Architect and other development resources to develop and support ODS (Data Lake) and Data Warehouse solutions including normalized and dimensional data structures, ETL solutions, integration solutions, maintenance, etc.
Participate in business requirements gathering and assist in translating business requirements into technical requirements for Developers.
Manage technical execution of development tasks to ensure that solutions add real business value, system architecture is aligned with business goals, and the design is consistent with the enterprise architecture strategy
Manage support tasks and ensure root causes are understood and resolved.
Guide trouble shooting and conduct code and QA review.
Embrace key technology trends and help the development teams leverage those technologies to provide business value.
Participate in enterprise data management strategies and drive the execution of these strategies with business product managers and other stakeholders.
Work with business stakeholders from a variety of departments and at all levels of the organization.
Provide training to key business users on how to use data in a tabular or multidimensional cube.
Minimum Qualifications:
Expertise in Microsoft technologies including SQL server, SSAS and SSIS
Strong knowledge of tabular and multidimensional cubes; normalized and dimensional modeling
Experience in enterprise data management solutions
Demonstrated sense of logic, creativity, decision making and problem resolution
Excellent communication skills
Valued Experience:
Understanding of AWS and the strategies of hosting a cloud based solution.
Understanding of PowerBI Premium
Understanding of Microsoft TFS
Understanding of Virtualization Layer like Denodo or IBI iWay
Experience in an Agile development environment
The environment you would be working in:

Tooling
Visual Studio 2017
MS SQL Server 2016
VSTS\TFS
Git
Hardware
High-end hardware (current generation i7, 32GB RAM, 500GB SSD)
Multi-1080p or single 4k monitor configuration
Office Life
Mid-height cubes in bullpen configuration
Standing desks available
On-site cafeteria that is good and budget-friendly
Beautiful building and grounds with quiet areas to escape to, when needed
People
You will be paired with a mix of mid-to-senior developers that support the existing applications
Most developers are full-stack and well-versed in current technology platforms, but not necessarily in our desired platforms
Some developers, and architects, will have academic knowledge of desired platform but limited practical experience


Qualifications
A culture that places top priority on integrity and compliance
Encouragement to challenge the status quo and share knowledge
Responsibilities and rewards based on contributions
Competitive compensation and a wide variety of benefits including health/dental/life insurance, prescription coverage, STD and optional LTD, paid vacation and holidays, and a 401 (k) with a $1 for $1 match up to 7%.

For more information about Guardian Industries LLC., visit us at www.guardian.com.

Guardian Industries LLC. is an Equal Opportunity Employer.

#LI-BS1"
AWS Big Data Analytics Engineer,DXC Technology,"Job Requirements: - Big Data Engineer with AWS Cloud experience on Analytics Services, Spark, Python, and Scala code development. Should be able to do end to end design and development for large enterprise scale, resiliency and maintainability from data ingestion to transformation that meets the technical, security and business requirements. Ability to contribute and learn evolving patterns in the Big Data space and provide input to the technology roadmap."
"UI Engineer, Data Visualization",Workday,"Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description

In this role, you will be responsible for developing high performance, responsive user interfaces for data visualization. The clean, intuitive, and beautiful user interface you will build plays a critical role in the user experience -- allowing users to explore and visually manipulate large data sets to gain new insights and also to share and tell stories with their data.
You will develop core UI components of a fast, scalable, complex web application using HTML5, CSS, JavaScript, ReactJS/Redux, etc. You will utilize your knowledge of data structures and algorithms to develop features for the tabular and graphics libraries that we have built in-house using JavaScript. The sleek, beautiful results of your work make you an integral part of our product/engineering group as you define, design, and develop new features with a top-caliber team.
About You
You seek to be a part of a collaborative team that is focused on building a product that will delight customers.
You have a proven track record of complex interactive applications for the web.
You look for ways to push JavaScript and the web browser beyond what others have already done.
You are a skilled programmer with passion for the way the pixels look on the page.
You are experienced with JavaScript, CSS and semantic HTML and have strong debugging abilities across all major browsers.
You have worked in large JavaScript codebases and have kept the code modular and loosely coupled.
*LI-MB"
Machine Learning Software Engineer Intern,Raytheon,"***Great opportunity for current college students entering their Junior or Senior year by Fall 2018 or college graduate students (entering or currently enrolled) in a graduate program by Fall 2018. If you are not a current college or graduate student, please visit our other opportunities atwww.raytheon.com/careers and apply!***

Job Description:

Are you inspired by a job that challenges you to be proactive, make design decisions, and generate new ideas that will push products above and beyond?
Raytheon Applied Signal Technology has a need for a Software Engineering Intern to help design, build, test, and maintain machine learning software for the analysis and interpretation of high-speed communications data for the U.S. intelligence community.
The successful candidate will work with system engineers and software developers in our Sunnyvale, CA, office to apply cutting edge technology to engineer products for the protection of our country. The successful candidate has strong written and verbal communication skills and the ability to work independently yet collaboratively in a team environment.

Required Skills:
Experience with C, C++ , Java, or Python
Knowledge of data structures, object-oriented principles, and algorithms
Experience with machine learning concepts, such as linear algebra, error analysis, supervised/unsupervised learning, natural language processing
Experience with numerical analysis tools and libraries (e.g., MATLAB, NumPy, pandas)
Must be a college Junior, Senior or Graduate student by Fall 2018
U.S. Citizenship is required as this position will need a U.S. Security Clearance within one year of start date.

Desired Skills:
Coursework in one or more of the following: Databases, Linear Algebra, Statistics, Probability, Machine Learning
Experience with Qt or other application frameworks
Experience with web development using Javascript (Node.js, D3.js), HTML5, and CSS
Familiarity with Unix/Linux
Knowledge of network and communication protocols
Minimum 3.0 cumulative GPA (Please upload an unofficial transcript when applying)

Required Education: Currently pursuing a BS / BA degree from an accredited four-year university and entering as a college Junior or Senior year OR pursuing a MS / MA / PhD by Fall 2018. Must be working towards a degree in Computer Science, Software Engineering, Computer Engineering, or equivalent technical discipline. Must be appropriate grade level listed above.

This position requires a U.S. person or the ability to obtain an Export Authorization from the appropriate government agency for non-U.S. persons.
105036
Raytheon is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, creed, sex, sexual orientation, gender identity, national origin, disability, or protected Veteran status."
"Software Engineer - Data Processing - FlightStats - Portland, OR",Reed Business Information,"SOFTWARE ENGINEER- FlightStats- Portland

FlightStats is looking for a highly skilled Software Engineer to join our Flight Data Processing Engineering team and help build the core back-end services that make our business work. This team tackles a wide variety of exciting projects, including intelligent processing, transport and delivery of our data products to internal and external customers.
This team is where flight data from a variety of sources comes together into a product. The right candidate will display a positive team-oriented attitude, possess intrinsic motivation for quality engineering, and come to us with a solid foundation of relevant experience, ready to jump in and contribute to our success.

Responsibilities:
Work closely with the product manager and other team members to analyze and conceptualize the design of products and requirements.
Convert product requirements into deliverables, set priorities, and establish timelines.
Design, write, test and deploy production ready code.
Install, troubleshoot and support a variety of software products and services that products are built upon and developed against.
Qualifications:
Bachelors degree in computer science, math, or statistics, or equivalent experience.
A minimum of 4 years software development experience.
Experience with developing highly concurrent, scalable applications.
Strong Java skills and experience.
Familiarity with SQL, PostgreSQL experience is a bonus.
Experience working with REST-like, lightweight web services.
Working knowledge of Linux, comfortable with the command line.
Ability to troubleshoot performance, capacity, and scalability issues.
Familiarity with lightweight agile software development processes.
Strong data modeling experience a plus.
Please note: This is a regular, full-time position which requires working out of our office located in downtown Portland, Oregon.

About The Company:
FlightStats provides data services and end-to-end data solutions to customers serving the global travel industry. The company has established a leadership position as a provider of real-time global flight information, serving airlines and airports, travel agencies, developers, consumers, and more. The company is leveraging the platform and domain knowledge it has developed to expand into new data sets and new products that deliver value to the companys core markets. We work in an interesting field, we have demonstrated success in what we do, and we are well positioned for future growth.

Our Environment:
We hire the best talent in the travel and technology industries. To support our talented team, we offer an extraordinary work environment that places trust and respect at the forefront of our company values. These values enable our employees to do their best work by creating an open, supportive environment that promotes creativity. Employees are also provided outstanding benefits, including flexible working hours and a ""take it when you need it"" time off program that encourages a healthy life/work balance. If that weren't enough, we work in a beautifully remodeled downtown Portland office, which provides lots of natural light and includes a well-stocked refrigerator and snack bar! We think it's a great place to do great work, and if you like the sound of this as well, we encourage you to consider this opening.

Reed Business Information
FlightStats is part of Reed Business Information. Reed Business Information (RBI) provides information, analytics and data to business professionals worldwide. Our strong global products and services hold market-leading positions across a wide range of industry sectors including banking, petrochemicals and aviation where we help customers make key strategic decisions every day. RBI is part of RELX Group plc, a world-leading provider of information solutions for professional customers across industries.
http://www.reedbusiness.com

RBI is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact jobs@rbi.co.uk . EEO is the Law . EEO is the Law Supplement

RBI General Benefits Package
- 401k match of 5%
- Paid Charity Days  2 days per year
- Benefits start the upon hire date"
Data Engineer - AWS Artificial Intelligence,"Amazon Fulfillment Services, Inc.","Come and be part of the Amazon AWS Artificial Intelligence team! We are a cloud AWS service that helps customers run machine learning algorithms on various Big Data systems in a scalable and cost-effective manner. With Machine Learning, businesses now ask our machines to do more than repetitive, strictly-defined tasks. We are taking it one step further and have begun to ask them to not only learn on their own but to also interpret data and report to the customer before they even knew they needed it. It's a step in history for you to be a part of. You will be building a platform that incorporates best practices and runs advanced algorithms at production scale and reliability. In this role, you will be involved designing, architecting, and implementing a data platform for collecting and processing large amounts of data used in the expansion of AWS Artificial Intelligence services.

As a Data Engineer, you should have experience in the architecture of DW solutions for the Enterprise using multiple platforms. You should excel in the design, creation, management, and of extremely large datasets. You should have excellent technical and communication skills to be able to work with data scientists and engineers to determine how best to design the data warehouse for collecting and exposing data that will be used to train machine learning models. You should be proficient with python and data processing scripting.
Basic Qualifications
A desire to work in a collaborative, intellectually curious environment.
Degree in Computer Science, Engineering, Mathematics, or a related field
Data Warehousing Experience with Oracle, Redshift, PostgreSQL, etc.
Query performance tuning skills
Coding proficiency in at least one modern programming language (Python, Ruby, Java, etc.)
2+ years experience in python and common python libraries used for data processing
Preferred Qualifications
Industry experience as a Data Engineer or related specialty (e.g., Software Engineer, Business Intelligence Engineer, Data Scientist) with a track record of manipulating, processing, and extracting value from large datasets.
Experience building/operating highly available, distributed systems of data extraction, ingestion, and processing of large data sets
Experience building data products incrementally and integrating and managing datasets from multiple sources
Experience with large-scale data warehousing and analytics projects, including using AWS technologies  Redshift, S3, EC2, etc.
Linux/UNIX.
Experience with AWS"
Entry Level Software Engineer,Motorola Solutions,"Motorola Solutions, Inc. (MSI) aspires to help our First Reponder, Public Safety, Government, and Industrial customers be their best in the moments that matter. https://www.youtube.com/watch?v=DONetk72GI4

MSIs Chief Technology Offices Data Analytics team matures breakthrough data science and data engineering technology solutions and drives proof of concept realizations that result in customer excitement by turning data into information, information into insights, and insights into action.

As a member of the Data Analytics team, you will work with industry leaders to implement end-to-end use cases for first responders, bringing to life analytics capabilities that help save lives. The Data Analytics team is responsible for the successful investigation, development, execution and proliferation of data science, machine learning, predictive models and other related analytics and software technologies to be used inside and outside of MSI. The Data Analytics team is looking for interns who are passionate and proactive about building insightful data-driven solutions.
This summer intern will work with data scientists and data engineers in the investigation and maturation of data-driven solutions utilizing Python, R, and other data science tools. Focus areas are Structured and Unstructured data relating to police officer, firefighter, dispatcher, and other roles, as well as associated tasks and duties. Data sources are wide-ranging, including Body worn cameras, Computer Aided Dispatch (CAD) crime incident records, Smart City (IoT) data, Audio from firefighters radios, Handset location data, and other agency-specific and open data inputs.

Projects include research of machine learning, artificial intelligence, and other structured and unstructured data analytics technologies, detailed analysis of new and existing solutions, and implementation of prototype designs for customer feedback and eventual product development.

Projects may include aligning next generation user experience visions to available and/or emerging technologies.

Projects will include overall design, implementation, system integration, and test/evaluation efforts towards technology feasibility.

Role requires a strong ability and enthusiasm to learn new technologies in a short time.
A Masters Degree is required, and/or progress toward a PhD; major in Computer Science, Computer Engineering or Mathematics highly preferred.

Strong oral and written communication skills are absolutely required.

Data Science and Data Engineering skills and experience are required.

Strong programming skills  experience with at least one of Python, R, Java, C++, JavaScript - are required.

Strong Mathematics and Statistical background.

Excellent debugging skills to solve challenging problems.

Ability to work collaboratively.

Knowledge of the following highly desired: Machine Learning and Artificial Intelligence. Knowledge of statistics and experience using statistical packages for analyzing datasets (Excel, SPSS, SAS, etc) is also highly desired.

Working knowledge of the following is also a plus: Deep Learning, Neural Networks, Image, Video, and/or Audio processing, Radio Network Architectures, User Interface design and implementation, and Data Visualization techniques and tools (Kibana, etc).

The candidate must be highly self-motivated with a desire to explore new approaches and technologies in a fast-paced group."
Senior Data Engineer,Atlantic Media,"The Atlantic has, for more than 160 years, advanced ideas that matter and sparked global conversation on the most important issues of our time. We aim to bring clarity and original thinking to questions of consequence, on topics ranging from politics, the economy, and global affairs to technology, science, and culture. As the third-longest-running magazine in America, we find ourselves at a remarkable moment: one of both continuation and transformation, of upholding our legacy while continuously reinventing ourselves for the future.

The Atlantic is seeking an experienced engineer to help design and build the infrastructure that powers our analytics products. The Senior Data Engineer falls under the Data and Growth function and is responsible for creating the data infrastructure to support the extraction of insights to optimize business strategy and deliver on growth targets for the business.

What youll do :
Help establish a vision, agenda, and continuously refined set of priorities for our data infrastructure. The Sr. Data Engineer will work closely with partners in Consumer Revenue, Advertising, Editorial, and Product to create data pipelines that enable analysts to understand performance, identify opportunities for improvement, and develop plans of action to maximize revenue and audience growth. This is a significant opportunity to be part of a growing team and help shape the way we develop, deploy, and operate production quality systems. In this role, your work will make a significant impact on The Atlantics data stakeholders and analysts.

Identify the critical data-related challenges our business faces, and execute technical approaches to solve them. Great candidates should have the software engineering chops not only to build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data.

Primary Responsibilities :
Help drive the optimization, testing, and tooling to improve data quality

Communicate and collaborate with both technical and non-technical members of the team

Work with data scientists, engineers, and product managers to understand data needs

Design, build, and launch new data stores and processing pipelines

Design, build, and launch new data extraction, transformation, storage, and loading processes and tools

Requirements :
The conviction that high-quality journalism is fundamentally importantand the desire to help it thrive

Experience managing projects and teams, mentoring employees, presenting to audiences, and communicating effectively and empathetically with clients and other stakeholders

Experience creating and maintaining relational databases, along with strong SQL skills

Solid knowledge of more than one programming language, with a demonstrated ability to learn new ones

Strong familiarity with Linux and comfort on the command line

Demonstrated success in ingesting, analyzing, and presenting large data-sets.

Experience setting up monitoring and alerting for data pipelines and ETL processes

Strong preference will be given to candidates who have experience :

Maintaining an Amazon Web Services stack

Solid experience with Python

Experience with Googles BigQuery

Experience with NoSQL and Map Reduce.

Please note this position is based out of our New York City office and we are not considering remote candidates at this time.

Atlantic Media Values:
Force of Ideas: At the center of Atlantic Media work are the ideas within our writing. We believe that ideas  to the good and not  have consequence. Our highest work is bringing rigor, insight, intellectual integrity, to that ultimate purpose of separating the bad from the good, and giving voice to the latter.

Spirit of Generosity: Atlantic Media seeks in its ranks those with natural disposition toward service and selfless conduct. Atlantic Media writing should be cut from the same cloth  critical on the merits, but informed by charity and forbearance in measuring motive and personal character.

Atlantic Media is an EOE of Minorities/Women/Vets/Disability."
Database Engineer,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities The individual selected for this position will assist in designing, developing, building, analyzing, evaluating and installing and reverse engineering database management systems, including database modeling and design, relational database architecture, metadata and repository creation and configuration management. Uses data mapping, data mining and data transformational analysis tools to design and develop databases, determine data storage and optimum storage requirements; prepare system requirements, source analysis and process analyses and design throughout the database implementation; evaluate size and select technology components, such as software, hardware, and networking capabilities, for database management systems and application databases. Writes and tunes SQL queries for performance and scalability. Implements comprehensive backup and database replication solutions.

Qualifications
QUALIFICATIONS: The following knowledge, skills, and abilities are required for successful performance of the activities associated with this position:
 Bachelors degree from an accredited college in a related discipline, or 4 years equivalent experience/combined education, with minimum of 15 years or more of professional experience.
 Demonstrated ability to determine work requirements and perform work without supervision
 Demonstrated capability to lead a team ranging from entry through senior level SE and/or SME to accomplish a technical function being supported
 Excellent written and oral communication skills
 Proficient in use of Microsoft Office Tool Suite
 Excellent interpersonal and organizational skills
 Demonstrated ability to multi-task and work independently; self-starter
 Active TS/SCI clearance with poly required

DESIRED QUALIFICATION:
 Masters degree in a field relevant to a technical function being supported
 Minimum 4 years experience working in the customers domain"
BIG DATA ENGINEER/TRAINEE,Orzota,"This position will require re-location to client locations after appropriate training. As a data engineer, you should be familiar with and have hands-on experience with all aspects of big data engineering from data ingestion of various types of sources and common data cleansing and transformation techniques. Alternatively, you must have 3-4 years of strong Java/J2EE, database and data warehouse (ETL, data modeling, analytics) experience. Ability to develop, debug and deploy applications using common IDEs in a Linux environment is required. Required experience :
Big Data engineer: 1+ years
Java/J2EE developer: 3+ years
Required education:
Minimum of Bachelors Degree in Engineering, Masters preferred

What you will need:
Strong experience with object-oriented design, coding and testing patterns
Experience building big data solution using Hadoop technologies
2+ years Java development experience
Strong knowledge of Linux and scripting
Solid understanding of SQL and experience Hands on experience using Map/Reduce, Hive, and/or Pig
Experience using one or more of Kafka, Storm, Flume
Understanding of HDFS, Map/Reduce, HBase
Experience with stream processing technologies such as Storm/Spark is a plus
Experience with other technology such as amazon ec2 is a plus
Ability to quickly triage and troubleshoot complex problems. A strong team player"
Database Engineer,Ytel,"We're a software company with a passion to help businesses better communicate with leads, prospects and customers. We geek out over the challenges of telecom and deliver cutting edge, cloud-based communications solutions. As a full-service carrier, with a company owned network, Ytel truly offers a one-of-a-kind solution. Over the years weve been able to provide cost effective, secure ways to power modern communications through voice, text, email and direct mail.

We need someone with a passion for technology, who gets excited about the idea of leaving their fingerprints on change.

What You'll Be Doing:
Administrating and maintaining highly available and scalable databases
Performing database tuning, optimization and recovery
Leading installations, upgrades, and data migrations
Monitoring the performance of production databases, identifying areas where performance needs to be improved and finding a solution
Consulting with our development team as the database SME
Developing queries and building databases from scratch

The Ideal Candidate Will Have:
An expert understanding of databases (Cassandra, Couchbase, Redis, Postgres, MySQL)
The ability to design, develop, and administrate large databases
Previous experience in a highly available, scalable environment
The ability to work on a fast paced team and with offshore teams
Experience with database replication and scaling issues is a big plus
* Successful applicants will be asked to show proof that they can legally work in the US *"
Software Engineer,Motorola Solutions,"ANZ Channel Team
Motorola Australia Pty Limited

Channel Sales Manager  Job Description

Title: Channel Sales Manager

Business Group : ANZ Channel

Location: New South Wales or Queensland

Reports To: General Manager Commercial Channels - ANZ

Objectives:
To meet & exceed sales and gross margin goals for the full range of products systems & solutions in its portfolio through the management & development of an effective Dealer distribution channel.
To provide leadership and management of the indirect channel within the defined area. Developing, communication and implementing strategies to ensure that growth, development, customer service and business goals are met.
To grow the overall business in the defined region, working closely with the Pre and Post sales teams, Marketing, Finance and Peers in the channel team

Responsibilities:
Effectively manage the relationship with the channel. Communicate Motorolas strategies, plans, goals, direction & policy. Plan, develop, coordinate & present appropriate training programmes, specifically to enable channel partners to address higher end solution opportunities.
Meet regularly with all channel partners to review plans & actions, agree on goals & objectives & discuss strategies for winning major opportunities. Review channel partner performance & agree on actions to correct anomalies. Liaise with appropriate personnel to ensure that channel partner & customer issues are resolved in a timely manner, escalating where necessary.
Provide day-to-day local support to named/shared accounts.
Work with Dealers and attend customer meetings where appropriate.
Liaise with the Systems Solutions Sales & Engineering teams on complex system solutions.
Maintain & facilitate the local relationship between the customer, the channel partner & G&PS. Maintain a watching brief to keep abreast of new potential direct opportunities and provide feedback to the System Solutions Sales Team.
To provide backup support to the channel sales team when members are on leave.
Manage Technical support from Motorola to ensure effective sales & implementation of complex solutions.
To provide accurate & timely reports of channel partners & managers activity, forecasts, plans & actual performance. Develop & maintain accurate & effective account plans & strategies for all channel partners in the Area.
To make recommendations on such issues as product selection & pricing with appropriate justification.
To continually work towards Motorolas fundamental objectives of Total Customer Satisfaction & Six Sigma Quality in all areas under the managers control or influence.
Implement and continually drive Motorola Salesway methodologies.
To comply with Motorolas requirements with regards to internal controls & code of conduct. To properly care for & maintain company assets under the managers control including computers & FEA equipment.
Agreed sales targets are consistently achieved.
Keep abreast of & provide feedback on market & competitive developments & recommend actions.
Ensure that agreed training plans are achieved.
Work proactively with internal sales and customer support.

GENERAL EDUCATION:
Essential: Tertiary qualification in Economics, Commerce, Marketing or Engineering.

OCCUPATIONAL TRAINING:
Essential: Advanced sales & business management training.

Desirable: P.C word processing, spreadsheeting & powerpoint

EXPERIENCE:
Essential: Minimum 8 years sales or channel management
Desirable: Software and Services sales experience

CAREER PROPECTS:
More senior sales & marketing roles in Australasia & overseas.

PERSONAL ATTRIBUTES:
Maturity, enthusiasm, self-motivation, goal driven, initiative, excellent verbal & written communication skills, strong interpersonal skills, creativity.

Key performance Measurements:
Channel sales through distribution channels.
Development of identified vertical markets.
Improvement in customer satisfaction measurements.
Channel skills development through training.
As per JD"
"Data Engineer, Data Sciences",Change Healthcare,"The new Data Science group was formed to dramatically increase leverage of Emdeons data assets to create material new revenue opportunities, both within specific Business areas and also across multiple lines of business. With data and transactions from more than 1,200 payers and 340,000 providers, Emdeon is uniquely positioned to impact US Healthcare.

Do you like working with data? Do you want to use data to influence product decisions for products being used by over a third of US healthcare? If yes, we want to talk to you. Our data engineering team works very closely with Product Managers, Data Scientists and Architects to figure out ways to acquire and maintain data to support new and existing data products. In this role, you will see a direct link between your work and company growth. In this role, you will work with some of the brightest minds in the industry, and you'll get an opportunity to solve some of the most challenging business problems in healthcare at a scale that few companies can match.

This role has the opportunity to be based in either San Mateo, CA or Nashville, TN.

Responsibilities
 Interface with data scientists, engineers and product managers to understand data needs
 Build data expertise and own data quality for allocated areas of ownership
 Design, build and launch new data models
 Design, build and launch new data extraction, transformation and loading processes
 Building key data sets to empower operational and exploratory analysis
 Define and manage SLA for all data sets in allocated areas of ownership
 Work with data infrastructure to triage infra issues and drive to resolution

Minimum Qualifications
 BS/BA in Computer Science or Mathematics
 3+ years experience in the data warehouse space
 3+ years experience in custom ETL design, implementation and maintenance
 3+ years experience working with either a Map Reduce or an MPP system
 3+ years experience with programming languages, Python preferred
 Hands on and deep experience with schema design and dimensional data modeling
 Ability to write efficient SQL statements
 Ability to analyze data to identify deliverables, gaps and inconsistencies
 Excellent communication skills including the ability to identify and communicate data driven insights
 Ability and interest in managing and communicating data solving problems through data

Preferred Qualifications
 Masters degree in Computer Science or related technical field.
 Experience with MapReduce, Hadoop or NoSQL technologies
 Strong understanding of technologies and their application including database warehousing
 Extensive use of visualization tools (e.g. Tableau)
 A proven record of managing project plans and collaborating across functions

#LI-PD1"
Data Analytics Engineer,TrendMiner,"TrendMiner develops discovery, diagnostic & predictive analytics solutions for industry. Our cross-functional agile development team is developing a brand new data mining product for process industry. Our company is growing fast, so were always on the lookout for ambitious and
enthusiastic new talent. We currently have a job opening for a data analytics engineer.

Your challenge Supported by the Customer Success Manager, Research & Development and the Customer Success team you ensure a professional and smooth integration of the added value of analytics into successful operations at our customers using our own TrendMiner:
You support our customers in the use of TrendMiner as their prime analytics toolbox
Team up with process technologists and improvement teams to
discover potential areas of improvement
do data analysis to find the root-cause of identified issues
suggest and implement analytics solutions
support and train our customers in the use of TrendMiner
Support the sales process by giving demonstrations of TrendMiner
Collaborate with R&D and Product Development in defining requirements for new features for TrendMiner
Collaborate with Customer Success Manager and Operations Manager to find new business leads and opportunities
You transfer knowledge to the team with respect to improvement results and the way the improvement is achieved
Your qualities
Minimal level of education MSc Chemical Engineering, Biotechnology or equivalent
Experience within the use of analytics, preferably in process industry
Knowledge of analytical tools (eg. Matlab, R, Python)
Excellent communication skills, customer focus and result oriented
Willingness to travel to collaborate with customers abroad
Motivational and inspirational personality
Pro-active leader with hands-on attitude"
Data Strategy Engineer,Manulife Financial,"Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.

The Data Strategy Engineer in the Global Data Office will report to the AVP Global Data Strategy & Innovation and will support global data strategy, innovation, knowledge & collaboration. This is an influencer role with the charter of building the roadmap for data governance at the enterprise level.
Responsibilities for this role include:
Assisting to understand, document and standardize existing data strategies
Develop Enterprise Information Management Roadmaps for the strategies
Demonstrate existing capability
Visualize potential future capability
Qualify experiments for data technology and data set innovations
Track experiments around the globe
Facilitate experiments within the Global Data Office
Support global data knowledge management
Facilitate global data collaborations
Requirements for this role are as follows:
A minimum of three plus years in big data management, along with five years of enterprise information management. Overall we need ten plus years in data management at the enterprise level.
Experience in enterprise information management roadmaps
Experience in knowledge management
Experience in driving enterprise collaborations for outcomes with data
Experience with program/project management in technology as well as in business.
Nice to have some experience with data science, business intelligence and advanced analytics.
This is a full time permanent role that can be worked out of our Boston MA or Toronto ON offices.

About John Hancock Financial and Manulife

John Hancock is a division of Manulife, a leading Canada-based financial services group with principal operations in Asia, Canada and the United States. Operating as Manulife in Canada and Asia, and primarily as John Hancock in the United States, our group of companies offers clients a diverse range of financial protection products and wealth management services through its extensive network of employees, agents and distribution partners. Assets under management and administration by Manulife and its subsidiaries were $1 trillion (US $754 billion) as at March 31, 2017. Manulife Financial Corporation trades as 'MFC' on the TSX, NYSE and PSE, and under '945' on the SEHK. Manulife can be found on the Internet at manulife.com.

The John Hancock unit, through its insurance companies, comprises one of the largest life insurers in the United States. John Hancock offers and administers a broad range of financial products, including life insurance, annuities, investments, 401(k) plans, long-term care insurance, college savings, and other forms of business insurance. Additional information about John Hancock may be found at johnhancock.com.

JOHN HANCOCK IS AN EQUAL OPPORTUNITY EMPLOYER - AA/F/M/D/V
*LI-JH"
Data Strategy Engineer,John Hancock,"Data Strategy Engineer-1710642
Description
Are you looking for unlimited opportunities to develop and succeed? With work that challenges and makes a difference and a flexible and supportive environment, we can help our customers achieve their dreams and aspirations.

The Data Strategy Engineer in the Global Data Office will report to the AVP Global Data Strategy & Innovation and will support global data strategy, innovation, knowledge & collaboration. This is an influencer role with the charter of building the roadmap for data governance at the enterprise level.

Responsibilities for this role include:
Assisting to understand, document and standardize existing data strategies
Develop Enterprise Information Management Roadmaps for the strategies
Demonstrate existing capability
Visualize potential future capability
Qualify experiments for data technology and data set innovations
Track experiments around the globe
Facilitate experiments within the Global Data Office
Support global data knowledge management
Facilitate global data collaborations
Requirements for this role are as follows:
A minimum of three plus years in big data management, along with five years of enterprise information management. Overall we need ten plus years in data management at the enterprise level.
Experience in enterprise information management roadmaps
Experience in knowledge management
Experience in driving enterprise collaborations for outcomes with data
Experience with program/project management in technology as well as in business.
Nice to have some experience with data science, business intelligence and advanced analytics.
This is a full time permanent role that can be worked out of our Boston MA or Toronto ON offices.

About John Hancock Financial and Manulife

John Hancock is a division of Manulife, a leading Canada-based financial services group with principal operations in Asia, Canada and the United States. Operating as Manulife in Canada and Asia, and primarily as John Hancock in the United States, our group of companies offers clients a diverse range of financial protection products and wealth management services through its extensive network of employees, agents and distribution partners. Assets under management and administration by Manulife and its subsidiaries were $1 trillion (US $754 billion) as at March 31, 2017. Manulife Financial Corporation trades as 'MFC' on the TSX, NYSE and PSE, and under '945' on the SEHK. Manulife can be found on the Internet at manulife.com.

The John Hancock unit, through its insurance companies, comprises one of the largest life insurers in the United States. John Hancock offers and administers a broad range of financial products, including life insurance, annuities, investments, 401(k) plans, long-term care insurance, college savings, and other forms of business insurance. Additional information about John Hancock may be found at johnhancock.com.

JOHN HANCOCK IS AN EQUAL OPPORTUNITY EMPLOYER - AA/F/M/D/V
*LI-JH

Primary Location : US-MA-Boston
Other Locations : US-MA-Boston

Business Unit : GlobalData
Job Function : Business Operations
Job Type : Standard
Schedule : Full-time

Job Posting : Nov 8, 2017, 9:01:40 PM
Unposting Date : Ongoing"
Deployment Solutions Engineer III,Sprint,"Job Summary

Provides deployment solutions engineering services required to plan, develop and implement moderate to highly complex network deployment plans for all aspects of technical projects including deployment/ integration, migration strategies, SOW development, timing, functionality, quality, cost and all project management services to cover these activities. Develops and implements the strategic sourcing and vendor relationship strategies. Identifies, selects, and monitors vendors and suppliers. Negotiates service level pricing. Manages all deployment related plans, budgets, invoices, and project timelines. Utilizes expertise and leadership skills to direct team members and to resolve issues to ensure project goals and requirements are met. Typical tasks include: Creating project plans and deployment strategies for highly complex, established process and new build projects. Defining service scopes and SOW development for highly complex, established process and new build projects. Negotiate service pricing and track cost savings.
This position will be responsible for the successful deployment of new technology and capacity/performance programs into Sprints Core Network as measured by the delivery of scope, speed, quality, and cost.
Basic Qualifications

Bachelor's degree and four years related work experience or eight years related work experience post high school
Three years experience managing technical projects
Three years experience with reporting and budgets
Preferred Qualifications

Four years of experience and current knowledge of Sprint's wireless and wireline telecommunication's network
Formal Project Management training
At Sprint, we're more than just talk. We've changed the way people communicate, how they work and how they stay connected on the go. Sprint is widely recognized for developing, engineering and deploying innovative technologies, including the first wireless 4G service from a national carrier in the United States; offering industry-leading mobile data services, leading prepaid brands including Virgin Mobile USA, Boost Mobile, and Assurance Wireless; instant national and international push-to-talk capabilities; and a global Tier 1 Internet backbone. Behind every decision we make, there is a passion and drive to develop technologies to enhance and simplify our customers' lives.
Are you ready for the big time?
Sprint is a background screening, drug screening, and E-Verify participating employer and considers qualified candidates with criminal histories consistent with applicable law. EOE .
Minorities/Females/Protected Veterans/Disabled.

Metro Area

US-KS-Kansas City

Sprint is an Equal Opportunity Employer and has been recognized for its commitment to diversity and creating an inclusive workplace where all employees backgrounds, talents and contributions are valued. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, nation origin, disability or protected veteran status. Sprint is a background screening, drug-free workplace.

Equal Opportunity Employer/Disability/Protected Veterans

If you are a qualified individual with a disability or a disabled protected veteran and need an accommodation or accessibility assistance to apply for one of our positions, you may submit a request by sending an email to careers@sprint.com or by faxing your request to 913-523-9980 .

Applicants have rights under Federal Employment Laws Family and Medical Leave Act (FMLA) Poster ; Equal Employment Opportunity (EEO) Poster ; Supplemental Equal Employment Opportunity (EEO) is the Law Poster ; Employee Polygraph Protection Act (EPPA) Poster ; Pay Transparency Notice Poster ; and the Philadelphias Fair Change Hiring Law Poster ."
R&D Data Engineer,Earnest Research,"EARNEST RESEARCH

Earnest Research is a VC-backed data innovation startup driven to change the way professionals understand consumer and business behavior. Working with world-class data partners, we transform raw data into a source for business and investment professionals to ask better questions so they can make better decisions. We believe, in the right hands, data has the power to change the way we work.

R&D DATA ENGINEER

At Earnest Research, we are passionate about finding signals in the noise of large datasets. As a member of the R&D team, you will be at the vanguard of expanding the insights we offer our clients: building products from new and existing datasets. You will work directly with our R&D data scientists and Director of Partnership Development to rapidly build prototype software solutions that scale to large amounts of messy data and enable the R&D team to extract comprehensible and actionable insights that our clients use to make investment decisions, research markets and drive their competitive strategy. The ideal candidate is an intellectually curious team player who likes to rapidly build quick and dirty solutions to rapidly evolving data problems employing a combination of computer science and statistical skills along with an ability to rapidly understand new data sets.

RESPONSIBILITIES

Evaluation of new datasets
Working with the R&D data scientists to rapidly assess the predictive power and commercial potential of new datasets by:
Quickly familiarizing yourself with, and implementing scalable processing and analysis solutions on new datasets
Building quick prototypes on new kinds of data utilizing open source libraries and employing creative problem solving.
Proactively identifying issues you encounter (e.g. duplicated data, bad data values)
Data Analysis and product development
Rapidly implementing transformation and wrangling of unstructured and semi-structured data from a variety of sources
Work with other teams within the organization to develop prototypes of new products and new product features
QUALIFICATIONS

Required Skills:
2+ years engineering experience in a data oriented role
Passion for data driven research and all the discovery, messiness and ambiguity that such research entails
Experience implementing algorithms, such as recommender systems, on large datasets
Experience handling and processing large data sets (10s of TBs or more) in a business environment
Exposure to and interest in data science and statistics
Strong communication, coordination, and organizational skills
Ability to set clear expectations, prioritize activities and follow through to completion on multiple projects with varying deadlines
Ability to handle confidential/sensitive information and exercise exceptional judgment
Proficiency with:
Distributed computing frameworks (e.g. Hadoop, Spark) for large datasets
One of Java, Scala, or a similar programming language
Algorithms, data structures, and performance optimization techniques
SQL
A Unix-like computing environment, and using command line tools including knowledge of shell/Python scripting for automating common tasks
Additional Preferred Skills:
Experience with AWS tools, especially EMR, Redshift database and Data Pipeline features.
Some knowledge of natural language processing and machine learning"
Business Operations Engineer,Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. This is your chance to work on the team that helps Facebook Operations by building and maintaining better support systems through enhanced tooling & infrastructure. As a member of Business Operations Engineering team, you will be the technical arm of our operational teams that support a number of Facebook products and business needs. You will be responsible for ensuring our Workplace Operations, Media Operations, Risk & Payments and Intellectual Property & Privacy teams have optimized tooling solutions ahead of product launches as well as handling any bug fixes or feature requests that come from internal stakeholders. You will assist our product engineering teams by providing creative solutions to challenging technical operational issues, while also ensuring all issues raised are dealt with in a timely manner. Responsibilities
To assess & build a common set of tools that are needed across a number of operational teams.
Become an expert in our products, our internal operational tools and the needs of internal stakeholders by providing effective technical consultation.
Solve challenging technical, operational and escalation issues through our tooling, delivering the highest level of stakeholder satisfaction.
Help scale tooling solutions that are flexible and easily customizable when new products are shipped.
Engage with the operations and product engineering teams to build, drive and improve our tools and processes.
Provide product insights / data reporting from data captured work with cross-functional partners to take appropriate action.
Deliver data-driven, impactful results.
Build understanding about the wide variety of operational tools across the company at a deep engineering level.
Minimum Qualifications
Bachelors degree in Computer Science or related field (or equivalent degree and experience).
At least 1 year of experience in software development.
Programming experience in one or more of the following languages: Java, PHP, JavaScript, Ruby, Python.
Problem-solving approach and analytical experience.
Experience working in an operational or support role."
Data Analytics/Statistal Engineer (PT),Mitre Corporation,"Data Analytics/Statistal Engineer (PT) - (00047613)
Description
Why choose between doing meaningful work and having a fulfilling life? At MITRE, you can have both. That's because MITRE people are committed to tackling our nation's toughest challengesand we're committed to the long-term well-being of our employees. MITRE is different from most technology companies. We are a not-for-profit corporation chartered to work for the public interest, with no commercial conflicts to influence what we do. The R&D centers we operate for the government create lasting impact in fields as diverse as cybersecurity, healthcare, aviation, defense, and enterprise transformation. We're making a difference every dayworking for a safer, healthier, and more secure nation and world. Our workplace reflects our values. We offer competitive benefits, exceptional professional development opportunities, and a culture of innovation that embraces diversity, inclusion, flexibility, collaboration, and career growth. If this sounds like the choice you want to make, then choose MITREand make a difference with us.

We are currently in search of a part-time Data Analytics/Statistical Engineer.

This part-time position (20 hours per week) will provide support to our Office of the Secretary of Defense, providing analysis, modeling, and design of experiments assessments to DoD Major System Acquisitions. Additional opportunities will be available to perform data analytics to support a variety of DoD customers. Q
ualifications
Bachelor in mathematics, systems engineer or other related discipline
Four years of experienced data analysis, and statistical analysis required.
Experience with development of STATA algorithm required.
Modeling experience is highly desired
Experience with Department of Defense desired.
Minimum qualifications:
Bachelors in mathematics, systems engineering or related discipline.
Primary Location : United States-Virginia-McLean Job : SW Eng, Comp Sci & Mathematics This requisition requires a clearance of: None Travel : Yes, 10 % of the Ti

me Job Posting : Sep 27, 2017, 2:50:56 PM"
Application Engineer - Metrology / Lithography / Data Analyst,ASML,"Introduction
ASML/HMI brings together the most creative minds in science and technology to develop lithography & metrology machines, as well as computational lithography & control software that are key to producing faster, cheaper, more energy-efficient microchips. We design, develop, integrate, market and service these advanced machines, which enable our customers - the worlds leading chipmakers  to reduce the size and increase the functionality of their microchips, which in turn leads to smaller, more powerful consumer electronics. Our headquarters are in Veldhoven, the Netherlands, and we have 18 office locations around the United States including main offices in Wilton, CT, Chandler, AZ and San Jose, CA.

We are proud of bringing together the brightest minds all over the world to tackle the most challenging problems in making faster and smaller chips. As part of our team, you will have the opportunity to go beyond yourself in developing more advanced techniques and push the technology boundaries.
ASML San Jose, CA is an industry leader in computational lithography for integrated circuits. Our Tachyon platform enables capabilities that address chip design, photomask making and wafer printing for semiconductor manufacturing and the LithoTuner platform enables lithography optimization for ASMLs leading photo lithography tools, known as scanners.

Job Mission ASML/HMI is hiring an Applications Engineer. In this role you will support a leading customer with their current and future challenges in defect inspection and control. You will identify strategies to stretch the equipment, software and associated processes beyond their specification to deliver the customer required product performance. You will cooperate with and drive the applicable ASML and HMI organization in order to define and implement solutions that meet the customers need. As an Applications Engineer you will also be responsible for communication with Customer engineers as well as ASML/HMI engineers in the respective headquarter offices.

Job Description
Insight into leading-edge chipmakers needs in terms of software, data analysis tools and control strategies
Knowledge of optical physics, imaging and semiconductor process technology
Interact and develop a solid working relationship with the customer to help understand and develop valued solutions to their fabrication issues
Work with account team and marketing organization at the field site, in San Jose and The Netherlands to define the scope of the package that meets customers requirements
Demonstrate technical leadership through problem solving, training and presentations
Own a broad technical knowledge of lithography, metrology and defect inspection
Give strategic guidance to junior engineers
Own the day to day project management activities and drive a technical team towards integration of solutions with goal to meet customer requirements
Willingness to travel up to 30% is required

Education M.Sc. and above in Electrical Engineering, Physics, and/ or Chemical Engineering, IC manufacturing, semiconductor process technology, and related fields or equivalent experience.

Experience
5 or more years of experience in engineering in semiconductor environment with a strong emphasis on lithography, defect metrology, and/or semiconductor process
Technical background in at least 2 relevant competences
Hands on experience with lithography, metrology, simulation, and analysis tools required
Script level programming skills; Matlab, Python or R
Data analysis and visualization (JMP/Spotfire)

Personal skills
Must able to work independently and also effectively as a member of a team
Strong interpersonal skills, able to quickly build credibility with customers and internally, goal-oriented, drives to results

Context of the position PHYSICAL DEMANDS AND WORK ENVIRONMENT
This position primarily works in an office environment. It requires frequent sitting, standing and walking. Daily use of a computer is required. May stand for extended periods when facilitating meetings.
The physical demands of the position described herein are essential functions of the job and employees must be able to successfully perform these tasks for extended periods. Reasonable accommodations may be made for those individuals with real or perceived disabilities to perform the essential functions of the job described.

Other Information EEO/AA (W/M/Vets/Disability) Employer
27 days ago - Save Job
- Original Job
View or apply to job
Other Jobs You May Like
Imaging/Dry Film Process Engineer

Sanmina Corporation
-
San Jose, CA

13 days ago


Process Development Engineer

Flex
-
Milpitas, CA

3 days ago


Business Systems Analyst/BSA

Zenaide Technologies
-
San Jose, CA

17 days ago

Easily apply

Process Systems Engineer

Boecore
-
El Segundo, CA

30 days ago

Easily apply

Master Data Business Systems Analyst

Infoblox
-
Santa Clara, CA

19 days ago

Application Engineer jobs in San Jose, CA
Jobs at ASML in San Jose, CA
Application Engineer salaries in San Jose, CA"
"Software Engineer - Scala, AWS, Big Data",HERE,"Description HERE, the Open Location Platform company, enables people, enterprises and cities to harness the power of location. By making sense of the world through the lens of location we empower our customers to achieve better outcomes  from helping a city manage its infrastructure or an enterprise optimize its assets to delivering drivers to their destination safely. To learn more about HERE, including our new generation of cloud-based location platform services, visit http://360.here.com .

Responsibilities

You will have an opportunity to work on various Java and Scala projects focused on integrating and processing large datasets for creating products. You'll also be able to participate in most of the stages of software development including gathering requirements, software system design, coding, testing, release, and support. Our team follows a documented software development process and is accountable for conformance to software quality standards including the creation and execution of unit test plans. We also work together collaboratively as well as with other engineering teams to ensure cross functional goals are met.

What Youll Get:
Challenging problems to solve
Opportunities to learn cool new things
Work that makes a difference in the world
Freedom to decide how to perform your work
Variety in the types of projects
Feedback so you will know how well you are doing
Collaborative, Supportive Colleagues
Design and development of software components.
Unit and Integration testing of Software components.
Investigation and root cause analysis of software and system defects.
Ability to quickly learn new technologies and programming languages.
Develop and maintain tools and processes that support development, automation testing and deployment of software components.
Develop and enhance configuration management tools for software components.
Create and execute approved test plans and analyzes and report results when necessary.
Decompose requirements for testability and provide feedback to the requirement gathering process.
Interact with customers/users to support software acceptance tests.

Qualifications Ideal:
Graduate degree or undergraduate degree in Computer Science or Computer Engineering with at least 2 to 3 years of professional experience is needed.
Candidate should have strong OO concepts.
Demonstrated expertise building and enhancing Java / Java EE based applications. The candidate should be proficient in Java and Java EE technologies.
Candidate should have experience in working with relational databases.
Experience in SQL and some knowledge NoSQL technologies is needed
Strong oral and written communication skills in English are needed
Exposure to version control tools (SVN or Git) is needed.

Nice to have:
Experience in functional programming language like Scala is a strong plus.
Strong knowledge of Linux/Unix scripting is needed and Python/Perl scripting is a strong plus.
Exposure to Enterprise Integration Pattern implementations like Apache Camel or Spring Integrations is a plus. Exposure to Agile software development process is a plus.
Exposure with Amazon Web Services and associated technologies is a strong plus.
Exposure with Big Data technologies like Spark, Hadoop M/R, Hive, Pig is a strong plus.
Experience in JAXWS, JAXB & JavaScript is a plus

Equal Opportunity Employer: Race/Color/Sex/Sexual Orientation/Gender Identity/Religion/National Origin/Disability/Vets."
Sr. Data Engineer,Becton Dickinson & Company,"Job Description Summary Digital Health is a new business unit within Becton Dickinson charged with building innovative new product lines. The goal of Digital Health is to work alongside highly successful business units and bring software into their portfolios. Digital Health is bringing agile software practices into a leading global medical technology company - pushing the boundaries of the industry, the business and the products. Our Digital Health innovation center will be launched in the Boston/Cambridge area to rapidly prototype and then commercialize new concepts in digital health.

Over time, Digital Health will spawn multiple products and services which enable consumer engagement, better health outcomes, and stickiness for BD products. Job Description Why Is This Role Critical? As a Sr. Software Engineer you will be responsible for building and delivering innovative new products to market.

What you build will be used by millions of users worldwide. Your focus on lean software practices and delivering quality products will be critical in the success of Digital Health and our mission of transformation. You will be making a real difference in making healthcare more efficient, safer and more effective. Key Roles and Responsibilities Innovation, prototyping & development: Be a key member of a software development team working to rapidly develop and iterate on new product concepts for mobile apps and smart devices.

Adopt a lean start-up approach to building and testing a minimum viable product. Implement an early feedback loop to enable continuous learning and iteration for ventures, products and services. Leverage the relevant capabilities of BD to accelerate delivery Contribute and lead technical architecture and design discussions Assist in defining data modeling best practices Build out and maintain HIPAA certified data models and data stores Build out and maintain deidentification processes and tools Build out and define data and analytics technical roadmap Your keys to success Bachelor degree in Computer Science or related field 5+ years of experience building enterprise-quality software. Ability to work in a startup-like, fast-paced environment Experience with agile, lean practices and methodologies Ability to work both independently and in a team-oriented collaborative environment Ability to model and tune databases Excellent written and oral communication skills Results-oriented: Track record of delivering enterprise-grade software Primary Work Location USA MA - Andover Additional Locations Work Shift"
Predictive Analytics and Data Science,Citi,"Primary Location: United States,Florida,Tampa
Education: Bachelor's Degree
Job Function: Legal
Schedule: Full-time
Shift: Day Job
Employee Status: Regular
Travel Time: Yes, 10 % of the Time
Job ID: 17082314
Description
About Citi Citi, the leading global bank, has approximately 200 million customer accounts and does business in more than 160 countries and jurisdictions. Citi provides consumers, corporations, governments and institutions with a broad range of financial products and services, including consumer banking and credit, corporate and investment banking, securities brokerage, transaction services, and wealth management. Our core activities are safeguarding assets, lending money, making payments and accessing the capital markets on behalf of our clients. Citis Mission and Value Proposition explains what we do and Citi Leadership Standards explain how we do it. Our mission is to serve as a trusted partner to our clients by responsibly providing financial services that enable growth and economic progress. We strive to earn and maintain our clients and the publics trust by constantly adhering to the highest ethical standards and making a positive impact on the communities we serve. Our Leadership Standards is a common set of skills and expected behaviors that illustrate how our employees should work every day to be successful and strengthens our ability to execute against our strategic priorities. Diversity is a key business imperative and a source of strength at Citi. We serve clients from every walk of life, every background and every origin. Our goal is to have our workforce reflect this same diversity at all levels. Citi has made it a priority to foster a culture where the best people want to work, where individuals are promoted based on merit, where we value and demand respect for others and where opportunities to develop are widely available to all. The ideal candidate is a data alchemist with a restless passion for global insights, metrics, and teamwork. S/he has experience in legal, data science, and quantitative research. S/he also has a track record of outstanding professional performance or academic achievement, along with excellent analytical, verbal communication, project implementation, and time management skills. Candidates must be able to proactively work with multiple stakeholders, re-imagine traditional uses of data, re-engineer data extraction processes to achieve valuable insights for decision-making and process improvement, execute projects with speed and accuracy, and present results to both expert and non-expert audiences. S/he is a critical and valuable team member. Specific responsibilities may include:
Design data visualization methodologies and simplify its existing processes
Assess and enhance the underlying data storage and transformation layers for scalability, completeness and maturity
Break down complex business problems into functional / technical requirements for reporting and analytical deliverables
Efficiently research and collect data on technical requirements from users and technical staff
Utilize Citis technologies and architecture to transform raw data into meaningful and useful information to support regulatory, compliance, and business-performance management efforts
Execute on opportunities to automate and / or use artificial intelligence and machine learning in existing or new processes
Develop business insights & discover commercialization opportunities
Collaborate with stakeholders and external-facing teams to develop solutions that address research and business challenges
Harness big data technologies to efficiently ingest, store, and analyze data
Share insights with the Legal community and enterprise-wide
Use existing and alternative data sources to gain insights in competitive intelligence Duties and Responsibilities: PEOPLE LEADER
Coordinate with technology and business partners to define data requirements to support analysis needs; test, design and deploy new reporting tools as needed; communicate activity status to project manager and help keep stakeholders informed of project status and all related issues.
Demonstrate leadership to his / her business partners and acts as a mentor to less-tenured staff; utilize effective communication skills to influence others.
Contribute to a positive work environment by demonstrating cultural expectations and influencing others to reward performance and value can do"" people, accountability, diversity and inclusion, flexibility, continuous improvement, collaboration, and creativity.
Follow a methodical approach to problem-solve and evaluate complex data while understanding the needs and goals of customer base.
Explain analysis findings to business users using presentations in a simple but effective format.
Take ownership in projects and suggest revising, repairing, or expanding existing programs to increase value of business intelligence.
Proactively work to identify, communicate, and resolve project issues and risks that interfere with project execution or success across teams and stakeholder groups.
Work independently and self-lead is required. We seek candidates that are equally comfortable speaking with business partners and with highly-technical developers. DATA MODELING AND VISUALIZATION CAPABILITIES
Develop metrics, scorecards, dashboard, and key-performance indicators (KPIs) by designing and implementing innovative reporting tools and methodologies.
Drive the implementation of visualization tools by monitoring industry trends and best practices, leading data testing and providing subject matter expertise to train team on appropriate new tools for specialized data projects.
Deliver quality reports and dashboards by analyzing and testing data and escalating complex issues to troubleshoot errors and to ensure valid information; assure the integrity of legal data, including data extraction, storage, manipulation, processing, and analysis.
Validate and if necessary, enhance the architecture of the data warehouse, ensure the models are robust and the transformation layer is scalable to meet the visualization requirements BUSINESS INSIGHTS & COMMERCIALIZATION
Develop the vision and roadmap for the data and analytics techniques and models that ensures reliable delivery of high quality data insights
Provide effective customer service by analyzing trends and communicating with internal key stakeholders to clarify their goals and ensure alignment with business objectives.
Provides strategic thought leadership and reporting discipline to drive a consistent data and statistical model for advancing our data capabilities
Qualifications

STEM Masters or PhD preferred; Bachelors Degree in an analytical filed such as Engineering, Mathematics, Statistics, Accounting, Finance, or Information Technology. Advanced degree required.
3+ years of experience in quantitative analysis OR relevant academic research experience building analytic products OR 5+ years IT business or systems analysis experience documenting business intelligence or reporting solutions with deep experience in predictive and statistical data analytics.
Experience communicating complex scientific concepts to non-technical stakeholders, either in a business, research, or academic environment
Experience with legal data science preferred but not required Skills
Demonstrated knowledge of data technologies, including large relational databases, data modeling and management, and hands-on experience implementing reporting and BI solutions with one or more enterprise reporting tool such as MicroStrategy or equivalent (MicroStrategy preferred)
Advanced proficiency in Excel, Visual Basic, and data structure, design, query language (e.g., SQL, SAS, Java)
Experience with functional or high-level scripting programming language commonly used in big data (e.g., Python, Scala) and relevant computational libraries (e.g., numpy, scipy, pandas, scikit-learn, matplotib, mapreduce, Spark)
Ability to design complex analyses that solve real-world problems
Comprehensive understanding of fundamentals of mathematics, large data sets, distributed systems, statistical concepts and how it can be applied to solving real-world problems
Experience with structured, semi-structured, and unstructured data formats (e.g., CSV, Excel, json, xml, html, images)"
Data Visualization & UI/UX Engineer,In-Q-Tel,"IQT Labs is standing up an initiative focused on the intersection of data visualization and user experience design. The initiative will explore and evaluate emerging technologies in these areas, conduct fundamental research on visualization and cognition, conceptualize, design and implement prototypes and proof-of-concept systems, and partner with leading organizations in these fields.

We are looking for someone with excellent analytical and interpersonal skills, a commitment to national security, and a passion for technology and research to join us and help shape the mission of this new initiative.

Responsibilities
Be part of a collaborative, interdisciplinary team that seeks to identify user needs and design and prototype solutions in a fast paced environment. Contribute to multiple research and development activities and lead some efforts.
Formulate research problems relevant to national security and conduct high-quality research.
Develop tools and prototypes of the proposed solutions and conduct experiments to evaluate their effectiveness.
Write well-structured, re-usable, and well-documented code.
Generate creative solutions and publish research results in top venues (papers).
Collaborate with experts at world-class universities, startup companies, national labs, and other relevant organizations.
Interface with intelligence community seniors about emerging technology, data visualization and UX best practices.

Qualifications Minimum Qualifications:
Strong working knowledge of UI/UX practices including user research, requirements definition, design, prototyping, implementation and testing/evaluation.
Understanding of current tools and trends in data visualization.
Proven experience in the creation of interactive web-based tools for data visualization (demonstrated through a portfolio of projects).
Strong front-end development skills and proficiency with HTML, CSS, Javascript, D3 & REACT.
Proficiency in programming in Java, C++, Python or R.
A working level understanding of data science principles including data wrangling, cleaning, collation and querying through various methods such as JS, Shell scripts or SQL.
Understanding of software development life-cycle and experience developing an idea from conceptualization through implementation.
Graduate degree or advanced training in a field related to data visualization or UX, such as Computer Science, Statistics, or Human Factors; strong analytical skills and problem-solving abilities.
Excellent written and oral communications skills, organization skills, and attention to detail.
Must be able to obtain/maintain a U.S. Government Security Clearance.

Preferred Qualifications
Working knowledge of applied statistics.
Familiarity with machine learning principles including model building, training, tuning and validation.
Demonstrated technical leadership in a field by presenting at national conferences and publishing in peer-reviewed journals.
Demonstrated leadership and project management experience.
Candidates must be able to solve problems with team members located at other corporate sites as well as independently. The requirements listed are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions."
Data Engineer Internship-Research & Development,Procter & Gamble,"Are you a relentless problem-solver? As a Data Engineer, you will help deliver faster decision making within R&D to enable faster launch timing (go-to-market) on initiatives and enable cost-savings efforts. Data wrangling, mapping, scrubbing/merging and formatting in a data-model that
'll power easy modeling and analytics will be the foundation of your role.

All of our roles provide competitive wages as well as the opportunity to engage in lifelong learning while developing innovative solutions for everyday problems. At this job, you will balance multiple projects at different stages of development simultaneously. In addition, you will need to maintain high level of curiosity to learn daily on many fronts. You should be self-motivated and able to drive technical insights into actions that improve business results.

Qualifications
If youre a good fit, youll have:
Working towards a BS/MS in Computer Science or related engineering field
Hands on coding experience in one or more languages (C, C++, Java, R, Python, Javascript)
Strong technical and observational skills to develop and validate models with broad datasets
Technical mastery, technical curiosity, and a desire to do modeling work to enable faster innovation
Experience with managing and driving actionable insights from large data sets.

If youre a really good fit, youll have:
Previous experience in Operations Research
Experience with semi-structured and unstructured data sets including harmonizing multiple (schema and schema-less) datasets
Critical thinking, active listening and strong written and verbal communication skills
The ability to learn on the job in a fast-paced environment

Just so you know:
P&G internships (Co-ops) are paid assignments typically lasting 12 weeks while you work side by side with full time research technicians to gain experience in the field. Successful interns (co-ops) could receive full time offers after graduation.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor
No immigration sponsorship is available for this position
Procter & Gamble participates in e-verify as required by law
Qualified individuals will not be disadvantaged based on being unemployed

Formal Title: Associate Scientist Intern

Qualifies for Overtime: No

Job Research & Development
Primary Location United States
Other Locations US-Ohio-Cincinnati
Schedule Full-time"
"Software Development Engineer, Data, Placed",Snap Inc.,"Placed ( https://www.placed.com/how-it-works ) is the market leader in location-driven insights and ad intelligence. We're a fast-growing organization headquartered in downtown Seattle, focusing on connecting the physical and digital worlds to give brands, agencies, publishers, and ad networks the ability to target location at scale, measure the offline impact of ads, and deliver actionable insights into consumer behavior.

Were looking for a Software Development Engineer to join Placed, a subsidiary of Snap Inc! As a member of our Data Engineering team, you will own critical pieces of the machine learning and analytics platforms, which power all product offerings within the organization. Working from our Seattle, WA headquarters, youll collaborate with Product, Engineering, and Data Science teams to create tools and processes to bring research and machine learning models to production.

What youll do:
Architect and implement scalable data processing and analytics infrastructure
Work with Data Science team to bring machine learning models into production
Build Hadoop MapReduce and Spark processing pipelines using Java, Python, and Ruby
Build REST APIs for data access by systems across our infrastructure
Focus on performance, throughput, and latency, and drive these throughout our architecture
Write test automation, conduct code reviews, and take end-to-end ownership of deployments to production
What were looking for:
BS/BA in a technical field such as computer science or equivalent experience
3+ years of software development experience
Proficiency in Python, Java, C#, and/or Ruby
Excellent communication skills, including the ability to identify and communicate data-driven insights
Experience with Hadoop MapReduce,Apache Spark, analytics systems (e.g. OLAP, BI tools), and semi-structured data (e.g. NoSQL, MongoDB, etc.) is preferred
Experience with AWS is preferred
At Placed, a subsidiary of Snap, Inc., we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Placed is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. If you have a disability or special need that requires accommodation, please dont be shy and contact us at accommodations-ext@snap.com ( accommodations-ext@snap.com )."
Database Engineer,ISYS Technologies,"Minimum Clearance Required TS/SCI

Responsibilities Functional Responsibilities:

Serve as a Senior Database Engineer / Administrator performing data architecture, data modeling, database administration, and COTS integration support. Position may require the ability to provide on-call, after-hours/weekend support, when necessary depending on program needs.

Qualifications Experience Requirements:
Minimum of 8 years of database development experience
Minimum of 8 years of experience as an ORACLE database administrator and/or Oracle Database engineer.
Experience with DDL scripts to create, maintain, and upgrade database schemas
Experience with SQL and PL/SQL to perform complex queries, data manipulations and data migrations
Experience writing and maintaining shell scripts; additional scripting experience such as Perl, Python, Ruby a plus
Experience with database optimization, performance tuning, backup and restore, table space allocation/management, managing the overall health and status of the database, managing the database schema, creating triggers and stored procedures, monitoring the Oracle alert logs and resolving database-related issues.
Familiar with current database related vendor products and current research directions.
Practiced in integration of COTS products in implementing successful customer-driven solutions.

Desired Experience:
Experience with other database products such as Postgres, MongoDB, Cassandra, etc. is highly desired
Experience with Oracle products such as Real Application Clusters, Data Guard, Application Server, Oracle Text, Label Security, Replication and Streams
Experience developing and maintaining data models
Experience with model-driven development and data modeling using tools such as ERwin, SQL Developer Data Modeler, Oracle Designer, Sybase PowerDesigner
Working knowledge of J2EE architecture and an understanding of object-oriented modeling languages.
Working with Java developers on database data access layers using JDBC, Hibernate, JPA, etc.
Experience in system accreditation using proxy users, virtual private database technology and label security.
Familiar with technologies related to high volume data handling and ingestion and high data rate extraction with integrity checking and indexing.

Clearance:
TS/SCI

Essential Requirements - Physical Demands: The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job with or without reasonable accommodation.

While performing the duties of this job, the employee will regularly sit, walk, stand and climb stairs and steps. May require walking long distance from parking to work station. Occasionally, movement that requires twisting at the neck and/or trunk more than the average person, squatting/ stooping/kneeling, reaching above the head, and forward motion will be required. The employee will continuously be required to repeat the same hand, arm, or finger motion many times. Manual and finger dexterity are essential to this position. Specific vision abilities required by this job include close, distance, depth perception and telling differences among colors. The employee must be able to communicate through speech with clients and public. Hearing requirements include conversation in both quiet and noisy environments. Lifting may require floor to waist, waist to shoulder, or shoulder to overhead movement of up to 20 pounds. This position demands tolerance for various levels of mental stress.

ISYS Technologies is an Engineering and Information Technology Company focused on providing Services to the Federal and State Government. ISYS offers a competitive compensation program and comprehensive benefits package to our employees including Health/Dental/Vision/PTO and more. ISYS Technologies is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, disability, or status as a protected veteran.

US Citizenship is required."
BIG DATA ENGINEER,Moffitt Cancer Center,"Moffitt Cancer Center is internationally recognized for our focus on personalized cancer care and translational research. The mission of Moffitt is clear, focused, and fully stated in nine words, ""To contribute to the prevention and cure of cancer."" With a tradition of excellence that began with the first patient admission in 1986, dedicated Moffitt physicians, scientists, and staff members have remained committed to excellence in an atmosphere characterized by kindness, caring, and hope.

5 years experience

Big Data Engineer

Moffitt Cancer Center is internationally recognized for our focus on personalized cancer care and translational research. The mission of Moffitt is clear, focused, and fully stated in nine words, ""To contribute to the prevention and cure of cancer."" With a tradition of excellence that began with the first patient admission in 1986, dedicated Moffitt physicians, scientists, and staff members have remained committed to excellence in an atmosphere characterized by kindness, caring, and hope.

Position Highlights:

 Data Engineer implements complex big data projects with a focus on collecting, parsing, managing, analyzing, and visualizing large sets of data to turn information into business insights using multiple platforms in Hadoop ecosystem.

 Data Engineer must have the experience and fundamental knowledge of the Hadoop ecosystem with a strong background in UNIX, RDBMS, SQL and Java.

 Data Engineer builds large-scale data processing systems, is an expert in data warehousing solutions, and should be able to work with unstructured data mining and the latest (NoSQL) database technologies.

The Ideal Candidate:

 Be able to work with under direction and handle several competing priorities simultaneously.

 Have excellent communication (both verbal and written), organizational and time-management skills.

 Has a Master's Degree in Computer Science, Information Systems/Technology, Software Engineering, or Analytics.

Responsibilities:

 Design, construct, install, test and maintain highly scalable data management systems.

 Integrate new data management technologies and software engineering tools into existing structures.

 Collaborate with end users and ecosystem partners to deploy Big Data/Analytics solutions in early adopter and production environments.

 Create custom software components (e.g., specialized UDFs) and analytics applications.

 Extract, transform and load data from various sources with minimal oversight.

Credentials and Qualifications:

 Bachelor's Degree in Business Administration or Computer, Natural or Social Science

 5+ years of technical development experience including 2+ years in a combination of relevant Enterprise data warehouse/Big Data/Analytics area

 Experience with object-oriented design, coding and testing patterns as well as experience in engineering (commercial or open source) software platforms and large-scale data infrastructures.

 Understand and be able to explain the appropriateness, merits and tradeoff of different distributed systems for data capture, processing and analysis.

 Extensive knowledge in different programming or scripting languages like Java, Linux, C++, PHP, Ruby, Python and/or R.

 Expert knowledge in different (NoSQL or RDBMS) databases such as Oracle, MongoDB, or Cassandra.

 Experience in working with ETL tools such as Informatica, Talend and/or Pentaho."
"Software Engineer, Data Infrastructure",Unity Technologies,"The world leading game engine is looking for Software Engineers to develop the world's greatest and largest computer gaming cloud infrastructure in the world. You will be part of a world class team tasked with securely and reliably delivering packaged and ""ready-to-scale"" services to our internal and external customers. With a dedication to your customers, you will use your outstanding development skills to deliver compelling solutions in an incremental, lean and self-driven environment.

Within your scrum team, you will design and review software solutions for critical parts of our big data ecosystem. In a hybrid development methodology, you will own all aspects of software development from code review to quality and test automation. Through proper planning and delivery, you will collaboratively work within your scrum team to deliver the most secure, reliable cloud experience to our customers.

Requirements:
BA or MS in computer science or related field, or equivalent experience
Must have experience with Kafka, Hadoop, Druid, Spark, or other big data technologies
At least 5 years of software engineering experience
Deep understanding of infrastructure systems and subsystems and be able to explain the pros and cons, strengths and weaknesses
Experience developing resilient, distributed services, across many geographies following stringent performance requirements
Appreciate and value incremental delivery
Balance development work with running existing services
Desired Skills:
Experience with scale-out infrastructure
Experience with troubleshooting and configuring the JVM
Experience with functional languages
Who We Are

We are product-driven, curious, and creative. We aggressively reinvest in development to keep Unity moving and evolving - from expanding usability and power, to platform reach. With over a billion installs per month, Unity touches gamers and non-gamers alike. As well as video games, our software is used in industries as diverse as medicine, visualization and shopping - democratizing game development while solving hard problems and enabling success for a huge range of developers worldwide.

Today, Unity has expanded to over 15 international offices and has employees in 23 countries. Every person at Unity Technologies has an inherent passion and driven spirit and it isnt enough to be able to do the job- were looking for people that color outside the lines, are looking to be freed from the process tangle, and can dream big!

Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Unity does not accept unsolicited headhunter and agency resumes. Unity will not pay fees to any third-party agency or company that does not have a signed agreement with Unity.

Unity is an equal opportunity employer committed to providing employment opportunities without regard to race, color, sex (including gender identity, sexual orientation, and pregnancy), age, national origin, religion, veteran status, disability, genetic information, or any other protected status in accordance with applicable law.

#LI-SP1"
"Software Engineer, Data Infrastructure","Studio Search, LLC","The world leading game engine is looking for Software Engineers to develop the world's greatest and largest computer gaming cloud infrastructure in the world. You will be part of a world class team tasked with securely and reliably delivering packaged and ""ready-to-scale"" services to our internal and external customers. With a dedication to your customers, you will use your outstanding development skills to deliver compelling solutions in an incremental, lean and self-driven environment.

Within your scrum team, you will design and review software solutions for critical parts of our big data ecosystem. In a hybrid development methodology, you will own all aspects of software development from code review to quality and test automation. Through proper planning and delivery, you will collaboratively work within your scrum team to deliver the most secure, reliable cloud experience to our customers.

Requirements:
BA or MS in computer science or related field, or equivalent experience
Must have experience with Kafka, Hadoop, Druid, Spark, or other big data technologies
At least 5 years of software engineering experience
Deep understanding of infrastructure systems and subsystems and be able to explain the pros and cons, strengths and weaknesses
Experience developing resilient, distributed services, across many geographies following stringent performance requirements
Appreciate and value incremental delivery
Balance development work with running existing services

Desired Skills:
Experience with scale-out infrastructure
Experience with troubleshooting and configuring the JVM
Experience with functional languages
Who We Are

We are product-driven, curious, and creative. We aggressively reinvest in development to keep Unity moving and evolving - from expanding usability and power, to platform reach. With over a billion installs per month, Unity touches gamers and non-gamers alike. As well as video games, our software is used in industries as diverse as medicine, visualization and shopping - democratizing game development while solving hard problems and enabling success for a huge range of developers worldwide.

Today, Unity has expanded to over 15 international offices and has employees in 23 countries. Every person at Unity Technologies has an inherent passion and driven spirit and it isnt enough to be able to do the job- were looking for people that color outside the lines, are looking to be freed from the process tangle, and can dream big!

Headhunters and recruitment agencies may not submit resumes/CVs through this Web site or directly to managers. Unity does not accept unsolicited headhunter and agency resumes. Unity will not pay fees to any third-party agency or company that does not have a signed agreement with Unity."
Software Engineer III - Big Data/Machine Learning,Walmart eCommerce,"Position Description
The Retail Core Technology team of engineers, data scientists and product leaders are responsible for the analytics, tools and platforms to support our merchandizing strategies including competitive intelligence, algorithmic pricing, product catalog, marketplace and product search.

We are looking for a Big Data Software Engineer for the Product Catalog team. Are you a self-motivated, passionate engineer with a ""can do"" attitude, who dreams of building great things? If so, we are looking for you. If you are hired, you will be doing one or more of the following:

Participate in the discovery phase of small to medium-sized projects to come up with high level design and creative solutions.
Work on small to large-sized complex projects and build new features and/or capabilities.
Implement highly scalable modules/components for data ingestion, precomputation/analytics pipelines.
Support the implementation of solutions for business use cases.
Follow the engineering best practices and deliver opensource like quality code.
Comply with company policies and procedures and support company mission, values, and standards of ethics and integrity
Use your debugging and troubleshooting skills to determine the root causes of issues and resolve them.
Minimum Qualifications
Experience with Java, SOA, Apache Camel, REST, Web-Services, JEE technologies, Spring, Hibernate, Apache Cassandra, Oracle desirable
Strong aptitude for writing efficient code
Attitude to thrive in a fun, fast-paced start-up like environment
Bachelor's Degree in Computer Science or related field with 0-3 years of experience.
Additional Preferred Qualifications
Masters degree or higher in Computer Science or related field
Experience with any other NoSQL solutions, Big Data technologies like Hadoop, Hive, Kafka would be a plus
Scripting skills in at least one of the following: Perl, Python, Shell, Bash, or Ruby
Experience with Ecommerce applications and/or building highly scalable systems.
Company Summary
Walmart Global eCommerce is comprised of Walmart.com, VUDU, SamsClub.com, and our technical powerhouse @WalmartLabs. Here, innovators incubate next gen e-commerce solutions in real-time. We integrate online, physical, and mobile shopping experiences for billions of customers around the globe. How do we do it? We continuously build and invest in new technology including open source tools and big data innovations. Data scientists, front and back-end engineers, product managers, and web and UX/UI teams collaborate alongside e-commerce experts to envision, prototype, and bring revolutionary ideas to life in a dynamic, flexible and fun work culture.

Position Summary
Software Engineer for Retail Core Technology team!"
"Research Scientist, Machine Learning and Intelligence",Google,"Research in machine intelligence has already impacted user-facing services across Google including Search, Maps and Google Now. Google Research & Machine Intelligence teams are actively pursuing the next generation of intelligent systems for application to even more Google products. To achieve this, were working on projects that utilize the latest techniques in Machine Learning (including Deep Learning approaches like Google Brain) and Natural Language Understanding.

Weve already been joined by some of the best minds, and were looking for talented Research Scientists that have applied experience in the fields of Machine Learning, Natural Language Processing and Machine Intelligence to join our team.

We do research differently here at Google. Research Scientists aren't cloistered in the lab, but instead they work closely with Software Engineers to discover, invent, and build at the largest scale. Ideas may come from internal projects as well as from collaborations with research programs at partner universities and technical institutes all over the world. From creating experiments and prototyping implementations to designing new architectures, Research Scientists and Software Engineers work on challenges in machine perception, data mining, machine learning, and natural language understanding. You stay connected to your research roots as an active contributor to the wider research community by partnering with universities and publishing papers.

There is always more information out there, and Research and Machine Intelligence teams have a never-ending quest to find it and make it accessible. We're constantly refining our signature search engine to provide better results, and developing offerings like Google Instant, Google Voice Search and Google Image Search to make it faster and more engaging. We're providing users around the world with great search results every day, but at Google, great just isn't good enough. We're just getting started.

Responsibilities
Participate in cutting edge research in machine intelligence and machine learning applications.
Develop solutions for real world, large scale problems.

Qualifications Minimum qualifications:
PhD in Computer Science, related technical field or equivalent practical experience.
Programming experience in one or more of the following: C, C++ and/or Python.
Experience in Natural Language Understanding, Computer Vision, Machine Learning, Algorithmic Foundations of Optimization, Data Mining or Machine Intelligence (Artificial Intelligence).
Contribution to research communities and/or efforts, including publishing papers at conferences such as NIPS, ICML, ACL, CVPR, etc.

Preferred qualifications:
Relevant work experience, including experience working within the industry or as a researcher in a lab.
Ability to design and execute on research agenda.
Strong publication record."
"Software Engineer, Data",Snap Inc.,"Snap Inc. is a camera company. We believe that reinventing the camera represents our greatest opportunity to improve the way people live and communicate. Our products empower people to express themselves, live in the moment, learn about the world, and have fun together.

Were looking for a Big Data Engineer to join Snap Inc! Were looking for someone who is passionate about applying quantitative information to solve tough problems and partnering with various teams across the company to help them harness the full potential of data. Youll help build out the data pipelines and data analysis tools that will enable us to take advantage of our amazing data set. Being able to gain valuable insights from data is a key part of this mission!

What you'll do:
Build data expertise and own data quality for the pipelines you build
Architect, build, and launch new data models that provide intuitive analytics to the team
Design, build, and launch extremely efficient and reliable data pipelines to move data (both large and small amounts) to our ridiculously large Data Warehouse
Design and develop new systems and tools to enable team members to consume and understand data faster
Use your expert coding skills across a number of languages from PHP, Python, and JavaScript
Work across multiple teams in high visibility roles and own solutions end-to-end
What we're looking for:
M.S. degree in Computer Science or related field
Proficiency in Python,Java,SQL (BigQuery, Oracle, Hive, etc)
2+ years of experience with dimensional data modeling and schema design in Data Warehouses
2+ years of experience in custom or structured (ie. Informatica/Talend/Pentaho) ETL design, implementation, and maintenance. Experience working with either a Map Reduce or a MPP system on any size/scale to write well-abstracted, reusable code components
Excellent communication skills, including the ability to identify and communicate data-driven insights
Experience building web applications and knowledge of web technologies (HTML/CSS/JS)
At Snap, we believe that having a team of diverse backgrounds and voices working together will enable us to create innovative products that improve the way people live and communicate. Snap is proud to be an equal opportunity employer, and committed to providing employment opportunities regardless of race, religious creed, color, national origin, ancestry, physical disability, mental disability, medical condition, genetic information, marital status, sex, gender, gender identity, gender expression, pregnancy, childbirth and breastfeeding, age, sexual orientation, military or veteran status, or any other protected classification, in accordance with applicable federal, state, and local laws. If you have a disability or special need that requires accommodation, please dont be shy and contact us at accommodations-ext@snap.com ( accommodations-ext@snap.com )."
Data Warehouse Engineer,Sonicbids,"As a Data Warehouse Engineer at Backstage, you will not only have the opportunity build and grow our data infrastructure to support the company in insights, analysis, and predictive product features, but also you will work with the latest tools and technologies to automate routine tasks and build new data applications. You will lead the effort to develop our unified data warehouse using state-of-the-art data infrastructure components to enable engineering, analytics, and product teams to incorporate data insights into our product and BI tools. Alongside this extensive and ongoing project, you will take part in the development of our existing website as well as new APIs and a green-field web+API project we plan to launch in a new vertical later this year.

This is a unique opportunity to work closely with an agile, quality driven team that is focused on using data to solve challenges in a stable and profitable company with a startup culture. Our team is experienced, small, nimble, and moves quickly, with minimal overhead. We emphasize shipping software, and we deploy to web daily. We go out of our way to make opportunities for everyone to learn and build new skills, but we don't go out of our way to have fun doing it -- that just happens naturally!

Key responsibilities
Collaborate with business stakeholders, other engineers, and product managers to define reporting requirements and design and develop operational reporting solutions and advanced analytics
Plan, analyze, design, develop and deploy solutions to ingest, transform and deliver business critical data each day from various sources
Build upon and improve our current Looker platform to give our business users deeper insight into their daily needs
Own the design and deployment of the new data warehouse and ETL processes
Provide production support and troubleshooting, manage and ensure availability of the reporting server architecture; monitor and communicate downtime and technical issues and respond quickly to failovers to maintain normal operations
Advise in capacity planning and service performance analysis and tuning
Produce scalable, robust, high-quality code
Must Have
5+ years software development experience
2+ years Python experience
Deep understanding of various database structures, concepts, uses and practices
Knowledge of postgresql, mysql and expert knowledge of SQL
Previous experience designing OLAP database schemas and building and maintaining a data warehouse
Strong understanding of Unix / Linux, with comfort of tweaking OS configurations to maximize program efficiency
Must be able to work in an agile environment with little supervision
Self-directed, ""get it done"" attitude
Must embody a passion for continuous improvement
Desire and ability to continuously learn new technologies and skills
Nice to Have
Knowledge of advertising technologies, techniques, pitfalls, opportunities.
Experience with Amazon Web Services (Redshift, S3, Data Pipeline, EMR, Kinesis).
Experience with system automation tools (Bamboo, Ansible, etc.)
Experience with payments and specifically recurring payments industry experience
Familiarity with Django, web development, and/or RESTful APIs
Experience with R, Pandas and other data processing tools / languages.
Programming polyglot with experience working in additional languages such as bash, awk, Javascript, Java, Objective-C or Swift, or others.
Backstage, LLC, is the leading platform in creative talent placement. We are currently composed of two brands: Backstage.com, for actors; and Sonicbids.com, for musicians -- with plans to launch a third vertical by the end of 2016. Nowhere else will you find the same combination of startup energy and entertainment industry creativity atop the solid foundation of an established but small and fast-growing business with decades of success in our industry.

We are a close knit, agile team that is disrupting the creative talent placement industry through technical innovation, product design and a solid business model. At the heart of our success and growth resides a group of hardcore technologists and product designers who share a tight bond and believe that collaboration creates a better experience for our users.

We offer benefits including health insurance (vision and dental available), commuter benefits, 401K, unlimited vacation, and tickets/tuition for select conferences/courses. Our DUMBO office is spacious and convenient to 3 subway lines, and we offer snacks and a couple days per week work-from-home."
Software Engineer - Data Services,eMoney Advisor,"At this level, you have a proven knowledge of our systems, customers, and business. Youve played a key role in the delivery of several projects and assisted bringing other team members on board. While learning for a software professional is never complete, you no longer require instructive pairing and oversight. Youve taken the role of dominant pair to transfer knowledge and reinforce our practices and values. Youve embraced the belief that great products come from the fusion of rock solid engineering, clear product vision, and a hypersensitivity to the user experience. At this level you dont just focus on the craft of programming, but also understand and actively participate in the collaboration between product management, design, and engineering. As is expected of all team members, you continue to refine your understanding of our business and continue to develop empathy for our customers.

Job Description
Embody and live the Scrum values of commitment, courage, focus, openness, and respect in all interactions, both within and outside of the team.
Be an active team member. Listen a lot. Ask questions to seek clarity. Take the floor when you have information, experience, or perspective to share. Communicate impediments as they arise.
A Scrum team collectively delivers on their sprint goals and commitments. All team members are expected to always be intimately familiar with, and working towards delivery of those commitments, never against.
In partnership with Product Management, actively seek to gain a deeper understanding of the business context under which your team operates. Leverage this understanding to present feature recommendation to your Product Owner.
In partnership with Design, continue to build empathy for the primary, as well as secondary, personas your team supports. Leverage this empathy to recommend small design changes to your Designer.
As a highly technical member of the team, actively seek to master of our tech stack, tools, and practices.
Complete modifications, refactoring, and bug fixes to existing functionality without the need for oversight.
Perform complex changes and complicated refactoring under the direction of an experienced team member.
While made up of many disciplines, the team delivers on commitments together. As such, its expected to perform documentation, design, and testing tasks as required by the team.
Actively participate and contribute to technical discussions and code reviews.
Evangelize and promote the successes and learnings of your team in formal ceremonies, such as sprint demos, as well as in casual conversation.
Write queries, scripts, and data upgrades.
Take point on supporting deployments and releases.
Other duties as assigned.

Requirements
3-5 years of professional experience. Completed one or more projects on commercially available applications.
B.S. in Computer Science, Computer Engineering, Software Engineering, or equivalent experience.
Self-motivation drive to get things done and deliver on team commitments.
Ability to be an active member of a Scrum team, embracing Scrum values.
Ability to know when to ask, take, and respond positively to direction and supervision.
Enthusiastic about company and programming company products.
Can plan tasks and estimate their completion. Communicating impediments as they arise.
Adapts well to changing conditions.
Communicates efficiently verbally and through written documentation.
Has a broad understanding of our systems, customers, and business goals as it pertains to the assigned team.

Skills
Fluent in C#. Highly knowledgeable of data structure, algorithms, and object oriented programming. Able to perform tasks effectively and pragmatically. Able to debug and diagnose complicated issues. Able to build from scratch implementations; following best practices.
Conversant and comfortable with design patterns.
Conversant and comfortable with working with legacy code.
Conversant and comfortable web development (Asp.Net Web Forms, Asp.Net MVC, JavaScript, Html, CSS, JQuery). Displays working knowledge. Able to perform tasks effectively with frequent use of guides, research, and mentorship. Can replicate from similar works.
Conversant and comfortable with Window and IIS.
Conversant and comfortable with relational databases (SQL Server).
Conversant with, embracing, and refining professional practices and methodologies (Test Driven Development, Pair Programming, Code Reviews, Refactoring Techniques, Agile/Scrum)
Conversant and comfortable of our development tools (Visual Studio, Resharper, PowerShell, BitBucket (Git), Jira, Confluence, Roundhouse, Octopus Deploy)"
Database Engineer,Applied Research Solutions,"Applied Research Solutions has a job opening for a Database Engineering.

Job Responsibilities:

 Designs, develops, builds, analyzes, evaluates and installs database management systems to include database modeling and design, relational database architecture, metadata and repository creation and configuration management.
 Uses data mapping, data mining and data transformational analysis tools to design and develop databases. Determines data storage and optimum storage requirements.
 Prepares system requirements, source analysis and process analyses and design throughout the database implementation.
 Maintains the operation of Oracle 12c Development, Test and Operational databases for multiple deliveries in a multi-vendor server based environment.
 Provides Oracle server troubleshooting, installation, upgrade, performance tuning, development and scripting support.
 Defines DBMS release upgrade planning. Maintains security and integrity controls and provides backup/recovery support.
 Installs and maintains COTS and custom database schemas.
 Works as a member of a team as well as independently.
 Meets deadlines is critical.
 Some travel may be required.

Qualifications:

 Active TS/SCI clearance
 Extensive Oracle RDBMS 11g or 12c experience including installation and tuning
 6 to 8 years of Oracle DBA experience
 Strong SQL (DML/DDL) and PL/SQL (stored procedures/triggers) experience
 Experience with major data transition activities
 Strong troubleshooting/debugging skills
 Familiarity with Linux and/or Solaris, to include shell scripting
 Familiarity with Agile and Scrum methodologies
 Organizational skills
 Ability to multi-task in a fast paced environment
 Excellent interpersonal and communication skills that will be used to interact with a distributed project team
 Experience with Oracle OEM CC 12c
 Experience with Oracle Spatial, Partitioning and VPD
 Experience with a database modeling tool (preferably ERStudio)
 Experience with a Cloudera, Elasticsearch
 Experience with the software development process

All positions at Applied Research Solutions are subject to background investigations. Employment is contingent upon successful completion of a background investigation including criminal history and identity check.

This contractor and subcontractor shall abide by the requirements of 41 CFR 60-741.5(a). This regulation prohibits discrimination against qualified individuals on the basis of disability, and requires affirmative action by covered prime contractors and subcontractors to employ and advance in employment qualified individuals with disabilities.

This contractor and subcontractor shall abide by the requirements of 41 CFR 60-300.5(a). This regulation prohibits discrimination against qualified protected veterans, and requires affirmative action by covered contractors and subcontractors to employ and advance in employment qualified protected veterans."
DATABASE ENGINEER,Old National Bank,"Old National Bank has an exciting IT opportunity in the greater Minneapolis, MN area for a Database Engineer. The Database Engineer is responsible for various aspects of the design, development, administration and implementation of database and application solutions. The Database Engineer will perform engineering and administration for multiple production databases and be responsible for fast-paced complex distributed database environments supporting primarily OLTP and OLAP. The Database Engineer is challenged with a high level of responsibility for engineering, research, development and maintenance of Old Nationals core information assets.

Core Responsibilities:

 Design, develop and implement data solutions, database programming and test plans.

 Test, validate and implement performance and resource optimization improvements.

 Maintain development and production environments.

 Monitor and maintain database security, application software, data integrity and quality.

 Perform root cause analysis of production-related database issues.

 Available for on-call production databases: daily maintenance, monitoring, problem resolution and internal customer and development support.

 Develop and maintain development standards, database policies, product documentation and support documentation.

 Review, design and develop data models (Business, logical and physical) in conjunction with the application development teams.

 Assist Enterprise Architect in the design and development of enterprise systems.

 Prepare documentations for database design, database objects, system configuration, code migration and change management.

Other Responsibilities:

 Create and maintain the enterprise data dictionary.

 Utilize strong query and database tuning techniques.

 Capacity monitoring and management capabilities.

 Evaluate industry trends in database systems and recommend improvement to Old Nationals database system management, protocols and procedures.

Qualifications:

 Bachelors degree in Computer Science, Computer or Electrical Engineering, Mathematics or a related field.

 Minimum of 5 years Database development experience with administering and supporting multiple databases for performance critical, highly available systems.

 Experience managing servers in large-scale, geographically diverse environments.

 Data modeling experience with conducting performance tuning, scalability/capacity planning, in addition to SQL, PL/SQL and UNIX shell scripting.

 Excellent troubleshooting, analytical and problem-solving abilities.

 Experience in creating automation tools for managing relational databases.

 Working knowledge of at least one compiled language (e.g., Java, C++, C#) and at least one scripting language (e.g., Python, Ruby, Perl) is a plus.

 Experience with other Data Persistence platforms: RDBMS, NoSQL, Hadoop is preferred.

 Experience with Data Information Lifecycle Management, Data Security, Big Data is a plus.

 Required knowledge of DBA forms, DB administration and DBA best practices (e.g., schema management, index optimization and use of hints).

 Must have an appetite for learning and passion for driving a positive change.

 Ability to work independently and with a diverse, challenging, dynamic work environment while maintaining a positive attitude.

Old National is proud to be an equal opportunity employer focused on fostering an inclusive workplace and committed to hiring a workforce comprised of diverse backgrounds, cultures and thinking styles.

As such, all qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, status as a qualified individual with disability, sexual orientation, gender identity or any other characteristic protected by law.

We do not accept resumes from external staffing agencies or independent recruiters for any of our openings unless we have an agreement signed by the SVP Talent Acquisition Manager, to fill a specific position."
"Data Engineer, Senior",Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years. Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Data Engineer, Senior
Key Role:
Perform data engineering activities, including working with data storage systems, ETL tools, custom script writing, data ingestion, and data visualization. Help to design, build, and maintain databases and scalable Big Data and NoSQL data stores. Perform data modeling and analytical exploration and examination of data from multiple sources of record, discovering patterns and identifying potential business rules. Build ETL pipelines to clean data, implement a variety of transformations, and ingest data in multiple end systems. Work with and integrate multiple types of data, including unstructured and structured, using SQL and API's. Deploy and support built data system as it goes operational. Work in a Scrumbased Agile teams operating in fastpaced environments.
Basic Qualifications:
-4+ years of experience in working with relational databases, including MySQL or PostgreSQL
-4+ years of experience with Big data systems, including HDFS, Hive, MongoDB, Elasticsearch, or Hbase
-4+ years of experience with objectoriented programming, including Java, C#, Scala, Python, or PHP
-4+ years of experience with ETL Tools, including Pentaho, Talend, or NiFi
-1+ years of experience with using Cloud services, including AWS
-Ability to obtain a security clearance
-BS degree
Additional Qualifications:
-Experience with a NoSQL database
-Experience with Agile software development
-Possession of excellent oral and written communication skills
-BS degree in CS, Computer Information Systems, Information Systems, or a related field
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet.
SIG2017"
"Data Loss Prevention Engineer, Senior",Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Data Loss Prevention Engineer, Senior
Key Role:
Design, develop, and recommend integrated security system solutions that will ensure proprietary or confidential data and systems are protected. Provide technical engineering services for the support of integrated security systems and solutions. Interface with the client in the strategic design process to translate security and business requirements into technical designs. Configure and validate secure complex systems and test security products and systems to detect security weakness.
Basic Qualifications:
-2+ years of experience with operations and maintenance of network and host-based DLP deployments, including policy creation and tuning, dashboard creation, and reporting for Windows and Mac OSs
-Experience in interfacing with customers daily
-Experience with weekly reporting of progress or remediation
-Ability to obtain a security clearance
-Security+ Certification or ability to obtain within 6 months of hire
Additional Qualifications:
-Ability to be self-driven to improve and automate processes
-Possession of excellent oral and written communication skills
-BA or BS degree
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet.
ESG1"
Senior Data Engineer,"Resolvit, LLC","RESOLVIT

Bringing Solutions That Make Business Better

Join Resolvit as a Senior Data Engineer and be part of a creative, forward-thinking team. Our success at deploying skilled, highly knowledgeable experts has landed us on the Inc. 5000 list of Americas fastest-growing companies four times  and were just getting started.

As the Senior Data Engineer, you will work with the team to create state-of-the-art data and analytics driven solutions, working across the company to drive business analytics to a new level of predictive analytics while leveraging big data tools and technologies. You will be responsible for all aspects of data acquisition, data transformation, and analytics scheduling and operationalization to drive high-visibility, cross-division aviation outcomes. You will also write complex SQL (100s of lines) that is able to transform, pivot, and stitch big data sets, both relational and non-relational. Additionally, you will:
Write custom scripts in Python, Java, R, or Spark to transform and pivot data
Integrate domain data knowledge into development of data requirements
Look across multiple systems and understand the purpose of each system and define data requirements by system
Identify downstream implications of data loads/migration (e.g., data quality, regulatory, etc.)
Develop automated testing for code deployment
Write data ingestion flows using message queuing
Operationalize and scale data science algorithms to extremely large datasets
Optimize existing data architectures and queries to scale across terabytes of data for visualization or application consumption
Architect and model to-be data structures based on customer requirements
Operate in Agile framework, creating user stories and tasks from customer requirements to track project progress
What Youll Need to be Successful:
A minimum of 3 years of experience as a data engineer
A minimum of 1 year of experience using Hadoop ecosystem (Spark, Hive, HDFS, etc.)
A minimum of 1 year of experience working on relational SQL databases (PostgreSQL, Oracle, etc.)
Bachelor's degree in Computer Science, Information Technology, or equivalent (STEM)
Demonstrated experience working in a large-scale, MPP/OLAP data warehouse environment (Greenplum, Teradata, etc.)
Great Additional Skills:
Hands-on experience with NoSQL data architectures (Cassandra, MongoDB, HBase, etc.)
Demonstrated experience using Data Analytics languages (R, Python and associated statistical packages)
Demonstrated experience using scripting languages (Pig, *Nix Shell, Perl, etc.)
Experience with Agile project delivery frameworks
Hands-on experience with streaming data architecture (Spark Streaming, message queues, etc.)
Exposure to and ability to execute analytics packages at scale in Python, R, and Spark/MLlib
Familiarity with Hadoop data flow tools (Sqoop, Flume, Nifi, etc.)
Familiarity with Hadoop workflow tools such as Oozie
Experience with enterprise data visualization technologies (Tibco Spotfire, Tableau, etc.)
Experience with Amazon Web Services data technologies (RDS, Kinesis, QuickSight, etc.)
Familiarity with text search and mining technologies (Solr, ElasticSearch, Lucene, etc.)
Excellent written and verbal communication skills, especially with product owners
Self-driven to learn new technology and build technical skill sets
Benefits:

At Resolvit, youll be given the support you need to grow in your consulting career. In addition to opportunities for advancement and ongoing training, we offer:
Medical, dental, and vision insurance
Life insurance coverage
Long-term and short-term disability coverage
401(k) retirement plan with matching
Professional support from a dedicated Business Services Manager
Networking opportunities with other consultants in your area
We currently have more than 100 open career opportunities across the country, so be sure to mention the appropriate Job Code with any correspondence!

About Resolvit:

Resolvit is an international technology consulting firm with industry-leading customers in the financial services, high tech, manufacturing, retail, life sciences, and government sectors. Through its partnerships, Resolvit delivers highly impactful, innovative solutions across five core areas: Infrastructure Modernization, Application Development Services, Enterprise Data Management & Analytics, Knowledge & Content Management, and Strategic Staffing."
Senior Data Engineer,CBTS,"Join CBTS Professional Services and take control of your future in one of the most exciting fields imaginable. Youll make an immediate impact on a growing business in an ever-evolving industry. Emerging, midsize, and enterprise businesses turn to CBTS Professional Services for help in augmenting their technology staffs with experienced, highly skilled professionals across all areas of IT.

CBTS is currently in search of a Sr. Data Engineer, here in the Cincinnati area.

Primary Responsibilities
Work with Data Product Managers and the Data Sciences organization to clarify requirements, specifications and methodology
Lead and mentor junior developers including peer code review and tutoring on higher level code quality constructs
Interpret complex mathematical formulas and other specifications provided by Data Science and transform raw data accordingly
Analyze and discover data trends and linkages that highlight errors or areas of potential opportunity for client utilization. Additionally, ensure that data warehouse standards are being maintained
Self-test and correct defects related to code developed. Correct security vulnerabilities
Skills, Education and Experience:

Required :
Bachelors degree in Computer Science or Engineering
Minimum of 3-7 years of experience in SQL, PLSQL
Experience with Oracle, PLSQL, Java, JavaScript/Dojo, Python, R
Experience in Unix scripting (Korn or Bourne)
Solid understanding of database design/modeling
Strong knowledge in Performance Tuning
Ability to apply analytical and problem-solving skills to solve complex design and coding problems
Knowledge of code repositories for maintaining code in development, test and production
Ability to apply software design and testing principles
Experience with Data Warehouse
Understand metadata and metadata management
Ability to manage multiple priorities and meet deadlines
Ability to work effectively as a team member, across project teams, and independently
Excellent interpersonal skills, as well as excellent written and verbal communication skills
Highly preferred :
ETL/ELT experience
Experience with ER/Studio
Experience with Big Data  Hive, MapReduce, Pig, Spark or other NoSQL technologies"
Analytic Data Engineer - (Hadoop),Highmark Health,"General Overview:
The Analytic Data Engineer architects and engineers solutions associated with analytic data for the organization (Advanced Analytics & Reporting) and, working closely with the IT teams, assists with the design, build, and upkeep for these solutions. This includes creating pathways for analysts to access operational, derived, and external data sets. The incumbent is responsible for the operation of Big Data Platforms as they are associated with analytic data discovery.
Essential Job Functions:
Receiving some direction, work closely with IT to architect and engineer solutions to provide views for the Analytic Data Warehouse. This would include working with the proper the teams, assisting with the design, building out the design, and providing upkeep for the solution.
Assemble, test, process, and maintain the Analytic Discovery Platform for the analytics organizations. This will include working to maintain pipelines with key analytic platforms throughout the organization.
Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
Complete tasks associated with a project. Meet with customers as part of a team lead by Lead or Senior.
Other duties as assigned.
Minimum Qualifications:
Bachelor's Degree in Computer Systems Analysis, Data Processing, Healthcare Informatics, Management Information Systems, or related field
3 - 5 years of Data Analytics Experience
Preferred Qualifications:
1 - 3 years of Data Warehousing Experience
1 -3 years of Database Administration Experience
1 - 3 years of Healthcare Industry Experience
SAS
Hadoop
Referral Award Payout Level: 3"
"Internship - BI / Data Engineer (Plano, TX)",Transamerica,"Job Description Summary
Internships are short-term work experiences for a designated period of time that allow college students the opportunity to observe and participate in the professional work environment, to explore how his/her interests relate to possible careers, and to gain practical experience in an industry relating to their field of study.
Job Description
Responsibilities

Summer of 2018 Internship - preferred location is Plano, Texas. To be eligible students must be currently enrolled in a college or university actively pursing a degree. Student who have graduated are not eligible for internships.

Responsible for the development and support of the Business Intelligence Analytics products and platforms including but not limited to data warehouse, data visualizations and interactive reports. Responsible for building a data pipeline from a variety of sources. Will work on a team with a visualization developer and with operations.
Continuously develops and maintains an understanding of Business Intelligence technology and architecture, including data warehousing, dimensional modeling, OLAP functionality, reporting tools, and other methods of information delivery.
Develop & maintains expertise with MSBI tools including: Excel/PowerPivot, SSRS, PerformancePoint, SSIS, & SSAS/BISM.
Develop database and/or data presentation solutions to deliver information to end-users accurately and efficiently
Extracts, transforms, & loads data using data management tools and T-SQL.
Analyze, understand, and document user needs to ensure accurate fulfillment of user story requests from internal business customers.
Contribute to, and possibly lead, project teams that are developing or modifying highly complex information solutions.
Administers & maintains the BI Portal, data warehouse, and existing BI solutions.
Continuously develops expertise in the data and data models within the data warehouse, as well as an understanding of supported business domains.
May construct data models, data dictionaries, and report glossaries.
May conduct user interviews and training sessions
May mentor teammates on BI tools and methods.
Conducts and may facilitate internal testing and user acceptance testing.

Qualifications:
Students must be currently enrolled in a college or university actively pursuing a degree in Computer Science or Computer Engineering or related field
Demonstrated ability to write SQL/TSQL queries to retrieve/modify data
Programming background in Java or Python

Preferred qualifications:
Experience with MSBI tools, including SSRS, SSIS, and/or SSAS
Moderate to advanced skills with MS Excel, including Office data connections and pivot tables
Conversational understanding of BI concepts & tools
Ability to abstract a concept into a series of lifecycle components and events.
Experience/education developing physical dimensional models, OLAP processes or data
visualizations OR
Experience/education in data modeling, data discovery and the application of data to solve business problems
Proven problem solving ability and a willingness to think outside-the-box.
Candidates must be detail oriented, have good written & oral communication skills, can work effectively in a team and be a self-starter

Working Conditions
Office environment"
Data Warehouse Engineer,Johns Hopkins Health Care,"Data Warehouse Engineer Requisition #: 152992
Location: Johns Hopkins Health Care, Glen Burnie, MD
Category: Information Technology
Work Shift: Day Shift
Work Week: Full Time (40 hours)
Weekend Work Required: Yes
Date Posted: Nov. 29, 2017

Johns Hopkins Health System employs more than 20,000 people annually. Upon joining Johns Hopkins Health System, you become part of a diverse organization dedicated to its patients, their families, and the community we serve, as well as to our employees. Career opportunities are available in academic and community hospital settings, home care services, physician practices, international affiliate locations and in the health insurance industry. If you share in our vision, mission and values and also have exceptional customer service and technical skills, we invite you to join those who are leaders and innovators in the healthcare field.

The Data Warehouse Specialist reports to the Data Warehouse Manager and assists in the design, development, documentation, and management of ETL (Exchange, Transfer, Load) processes for the data warehouses.

Responsibilities:
Assist with the design of technical and functional requirements for data integration projects
Advises and recommends best practices for design, development, and testing of ETL processes
Under direction of the reporting manager aligns the ETL environments with operational and business needs and strategies
Developing and maintaining ETL mappings for multiple development projects
Interact with business users and decision support staff as part of the support work for error resolution and design enhancements
Assist in the development of reporting and analytics solutions for the data warehouse
Participate in data quality initiatives to profile and cleanse data prior to loading into the warehouse
Develop ETL architecture standards, procedures and policies and assist in developing ETL processes for the organization
Requirements:
Strong SQL, analytical, and computational skills
Expertise and experience in the use of an ETL development tool (Informatica PowerCenter, DataStage, Business Objects Data Integrator, Microsoft SQL Server Integration Services); The ability to read data model (ER) diagrams
A Bachelors degree in Computer Science or related technology field; Masters Degree Preferred; Work experience may be substituted where appropriate
5+ years experience with ETL tools and ETL programming
3+ years experience designing ETL solutions for data migration or data integration
10+ years experience with IT projects and software development
1+ years of SQL Server experience
Citizenship Requirement: Please note that under the terms of a government contract, this specific position requires U.S. citizenship status to ensure compliance with the Department of Defense Personnel Security Program. We expect the team, at times, to work after hours as well as downtime weekends as necessary.

#LI-AH1 #LI- AH1

Johns Hopkins Health System and its affiliates are Equal Opportunity/Affirmative Action employers. All qualified applicants will receive consideration for employment without regard to race, color, religion, sexual orientation, gender identity,sex, age, national origin, disability, protected veteran status, and or any other status protected by federal, state, or local law.

APPLY"
Mission Data Engineer,DCS Corp,"The Mission Data Task Team has an immediate opening for an MD Engineer.

Essential Job Functions:

Develop mission data (MD) for advanced 5th generation fighter aircraft.

Develop complex electronic warfare models and program the CEESIM simulator.

Program mission data to provide enhanced situational awareness for pilots.

Train incoming mission data engineers in software and programming procedures.

Advise on RF antenna performance.

Requirements
Due to the sensitivity of customer related requirements, U.S. Citizenship is required for most positions.

Must have a current Secret security clearance

Bachelor's Degree in an Engineering discipline. Electrical Engineering degree is preferred.

Desired skills:

Familiarity with F-35 avionics.

Experience with programming mission data for the F-35.

Experience with developing SW supporting creating mission data reports. Experience with training on mission data SW and programming.

Experience with directing mission data tests in the lab.

Experience with electronic countermeasures to include: preparing V&V test plans, participating in CCBs with AESA, HFIM, and EW modulation on pulse."
Voice/Data Communications Engineer,AECOM,"AECOM is seeking a Voice/Data Communications Engineer for our D.C. Location. This opportunity is in support of our NAVSEA customer.

Plan, evaluate, and/or coordinate the installation of local and/or remote data and voice communications equipment. Provides technical direction and engineering knowledge for communications activities including planning, designing, developing, testing, installing and maintaining large communications networks. Ensures that adequate and appropriate planning is provided to direct building architects and planners in building communications spaces and media pathways meet industry standards. Develops, operates, and maintains voice, wireless, video, and data communications systems. Provides complex engineering or analytical tasks and activities associated with one or more technical areas within the communications function.

IND-SEIS

Minimum Requirements

Bachelors Degree (or equivalent experience) and 7-9 years of experience. Security+/Security+CE Certification and two years supporting COMSEC equipment STE, STU. Position requires a Security Clearance at the SECRET level.

Preferred Qualifications

10+ years of directly related IT experience supporting Avaya Comms Manager/Aurora OR a Bachelors Degree in a related discipline with 8+ years of equivalent IT experience; knowledge of MS Visio.

What We Offer

AECOM is a place where you can put your innovative thinking and business skills into high gear and work alongside other highly intelligent and motivated people. It's a place where you can apply your skills to some of the world's most challenging, interesting, and meaningful projects worldwide. It's a place that values the diversity of our areas of practice and our people. It's what makes AECOM a great place to work and grow. AECOM is an Equal Opportunity Employer.

At AECOM, employee's safety and security are our top Safeguarding core value. All employees are expected to set the highest level of safety expectation in their work, display the highest level of safe behavior, and actively participate in AECOM's Safety For Life Program. SH&E is a part of our company culture and participation is required for all employees.

_NOTICE TO THIRD PARTY AGENCIES: Please note that AECOM does not accept unsolicited resumes from recruiters or employment agencies. In the absence of a signed Recruitment Fee Agreement, AECOM will not consider or agree to payment of any referral compensation or recruiter fee. In the event a recruiter or agency submits a resume or candidate without a previously signed agreement, AECOM explicitly reserves the right to pursue and hire those candidate(s) without any financial obligation to the recruiter or agency. Any unsolicited resumes, including those submitted to hiring managers, are deemed to be the property of AECOM."
Data Engineer- Research & Development,Procter & Gamble,"Are you a relentless problem-solver? As a Data Engineer, you will help deliver faster decision making within R&D to enable faster launch timing (go-to-market) on initiatives and enable cost-savings efforts. Data wrangling, mapping, scrubbing/merging and formatting in a data-model that
'll power easy modeling and analytics will be the foundation of your role.

All of our roles provide competitive wages as well as the opportunity to engage in lifelong learning while developing innovative solutions for everyday problems. At this job, you will balance multiple projects at different stages of development simultaneously. In addition, you will need to maintain high level of curiosity to learn daily on many fronts. You should be self-motivated and able to drive technical insights into actions that improve business results.

Qualifications
If youre a good fit, youll have:
BS/MS in Computer Science or related engineering field
Hands on coding experience in one or more languages (C, C++, Java, R, Python, Javascript)
Strong technical and observational skills to develop and validate models with broad datasets
Technical mastery, technical curiosity, and a desire to do modeling work to enable faster innovation
Experience with managing and driving actionable insights from large data sets.

If youre a really good fit, youll have:
Previous experience in Operations Research
Experience with semi-structured and unstructured data sets including harmonizing multiple (schema and schema-less) datasets
Critical thinking, active listening and strong written and verbal communication skills
The ability to learn on the job in a fast-paced environment

Just so you know:
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, protected veteran status, disability status, age, sexual orientation, gender identity and expression, marital status, citizenship, HIV/AIDS status or any other legally protected factor
No immigration sponsorship is available for this position
Procter & Gamble participates in e-verify as required by law
Qualified individuals will not be disadvantaged based on being unemployed

Formal Title: Associate Scientist

Qualifies for Overtime: No

Job Research & Development
Primary Location United States
Other Locations US-Ohio-Cincinnati
Schedule Full-time"
Big Data / ETL Software Engineer,JP Morgan Chase,"JPMorgan Chase & Co. (NYSE: JPM) is a leading global financial services firm with assets of $2.6 trillion and operations worldwide. The firm is a leader in investment banking, financial services for consumers and small business, commercial banking, financial transaction processing, and asset management. A component of the Dow Jones Industrial Average, JPMorgan Chase & Co. serves millions of consumers in the United States and many of the worlds most prominent corporate, institutional and government clients under its J.P. Morgan and Chase brands. Information about JPMorgan Chase & Co. is available at http://www.jpmorganchase.com/ .

Commercial Banking (CB) serves more than 30,000 clients, including corporations, municipalities, financial institutions, and not-for-profit entities with annual revenues generally ranging from $20 million to $2 billion. Our Commercial Bankers serve these clients by operating in 14 of the 15 top U.S. major markets. Our professionals' industry knowledge and experience combined with our dedicated service model, comprehensive solutions, and local expertise to make us the #1 commercial bank in our retail branch footprint.

Commercial Banking IT is looking for a Big Data Lead/Architect/Developer with skills and experience with large-scale Hadoop-based data platforms who will be responsible for design, development and testing of a next generation enterprise data hub and reporting and analytic applications. This individual will work with an existing development team to create the new Hadoop-based platform and migrate the existing data platforms and provide production support. The current platform uses many tools including Oracle SQL, SQL Server, SSIS, and SSRS/SSAS. The candidate will be accountable for design, development, implementation and post-implementation maintenance and support. The candidate will develop and test new interfaces, enhancements/changes to existing interfaces, new data structures, and new reporting capabilities.

Bachelor's degree in a technical or quantitative field with preferred focus on Information Systems

Minimum 2+ Experience in a Big Data technology (Hadoop, YARN, Sqoop, Spark SQL, Nifi, Talend, Hive, Impala, etc.)

3-5+ years of Experience in Java Development

2+ years of Experience with Python is preferred

Experience performing data analytics on Hadoop-based platforms is preferred

Experience in writing SQL's

Experience in Map Reduce

Experience in implementing complex ETL transformations in Hadoop platform

Strong Experience with UNIX shell scripting to automate file preparation and database loads

Experience in data quality testing; adept at writing test cases and scripts, presenting and resolving data issues

Experience in implementing distributed and scalable algorithms (Hadoop, Spark) is a plus

Familiarity with relational database environment (Oracle, SQL Server, etc.) leveraging databases, tables/views, stored procedures, agent jobs, etc.

Familiarity with NoSQL database platforms is a plus

Experience to ETL tools is a plus

Strong analytical skills with the ability to collect, organize, analyze, and disseminate significant amounts of information with attention to detail and accuracy

Proficiency across the full range of database and business intelligence tools; publishing and presenting information in an engaging way is a plus

Experience with multiple reporting tools (QlikView/QlikSense, Tableau, SSRS, SSAS, Cognos) is a plus

Strong development discipline and adherence to best practices and standards.

Ability to manage multiple priorities and projects coupled with the flexibility to quickly adapt to ever-evolving business needs

Demonstrated independent problem solving skills and ability to develop solutions to complex analytical/data-driven problems

Must be able to communicate complex issues in a crisp and concise fashion to multiple levels of management

Excellent interpersonal skills necessary to work effectively with colleagues at various levels of the organization and across multiple locations

Financial Services and Commercial banking experience is a plus

Responsibilities:
Acquire data from primary or secondary data sources

Identify, analyze, and interpret trends or patterns in complex data sets

Transforming existing ETL logic into Hadoop Platform

Innovate new ways of managing, transforming and validating data

Establish and enforce guidelines to ensure consistency, quality and completeness of data assets

Apply quality assurance best practices to all work products

Analyze, design and code business-related solutions, as well as core architectural changes, using an Agile programming approach resulting in software delivered on time and in budget;

Experience of working in a development teams, using agile techniques and Object Oriented development and scripting languages, is preferred

Comfortable learning cutting edge technologies and applications to greenfield projects"
Visual Analytics / Big Data Engineer Job,SAIC,"Visual Analytics / Big Data Engineer (Job Number: 427890)

Description:
SAIC has an opening for a Visual Analytics / Big Data Engineer

who will be contributing to projects in support of Visual Analytics system development, improvement, and expansion for a prestigious research group within a government agency located in Columbia, MD The successful candidate will have the opportunity to work with a small group to develop cutting-edge concepts and solutions which will further research in cooperative intelligence analysis and visual analytics. This position will let the candidate explore analysis concepts, alternatives technologies, and operational solutions.

The candidate will:
Develops parallel data-intensive systems using Big Data technologies.
Works with the full open source Hadoop stack from cluster management, to data repositories, to analytics software, to schedulers.
Works in on-premises or public cloud environments to build scalable systems.
Determines the appropriate database given the data and analytics needs, whether file structures such as HDFS, relational databases including NewSQL, non- relational NoSQL databases including in-memory databases.
Optimizes the distribution of data across nodes and the performance of NoSQL repositories.
Identifies performance bottle-necks and evaluates scaling benchmarks.
Designs, develops, documents, tests and debugs application software.
Conducts analysis and collaborates with subject matter experts in the planning, design, development, and utilization of electronic data processing systems for information storage, processing, presentation, manipulation, display, or reporting.
End product may be special use, customized, or commercial software.
Determines computer user needs; analyzes system capabilities to resolve problems on program intent, output requirements, input data acquisition, programming techniques and controls; prepares operating instructions; designs and develops autonomous services, desktop applications, web applications, scripts, and utility programs.
Ensures software standards are met.
Qualifications:CLEARANCE REQUIREMENT:
Clearance required TS/SCI with Poly
U.S. citizenship required
REQUIRED EDUCATION AND EXPERIENCE:
Bachelors degree in Computer Science or closely-related discipline required and two (2) years or more experience of progressively responsible experience in software development, software engineering, data warehousing, Big Data analytics, or data visualization
Or; Masters and 0 years related experience
Experience is required in the following technologies and languages:
Java, JavaScript, HTML5, CSS3, XML, JSON, AJAX, and JQuery
Web frameworks, open source visualization technologies, interactive visualizations
Source code control systems
Relational databases
DESIRED EXPERIENCE:
Experience with WebGL, open-source container and resource management, web/proxy servers, and NoSQL databases.
Experience with development and use of government data analysis tools
Demonstrated success working in Agile development environments
Travel:
Minimal travel required for this position
SAIC Overview: SAIC is a premier technology integrator providing full life cycle services and solutions in the technical, engineering, intelligence, and enterprise information technology markets. SAIC provides systems engineering and integration offerings for large, complex projects. Headquartered in McLean, Virginia, SAIC has approximately 15,000 employees and annual revenues of about $4.3 billion.

EOE AA M/F/Vet/Disability

Job Posting: Jul 10, 2017, 4:43:57 PM
Primary Location: United States-MD-COLUMBIA
Clearance Level Must Currently Possess: Top Secret/SCI with Polygraph
Clearance Level Must Be Able to Obtain: Top Secret/SCI with Polygraph
Potential for Teleworking: No
Travel: Yes, 10% of the time
Shift: Day Job
Schedule: Full-time"
Data Scientist / Machine Learning Developer,Battelle,"Battelle is guided by a founding mission. We invest our knowledge, talents and resources, helping our customers achieve their most important goals. We apply scientific rigor and creativity, succeeding where others may fail and we invest in our communities, making the world better for generations to come. All of us share a common purpose: to solve the greatest challenges of today and tomorrow.

Our 22,000 employees work at the forefront of scientific innovation to tackle critical challenges in security, human health, manufacturing, energy and environmental management. Battelles work is grounded in the belief that science, technology and a passion for excellence can make industries more competitive and the world a better place.

We are currently seeking a Data Scientist / Machine Learning Developer . This position is in Columbus, OH . Come in for an interview to find out why Battelle is right for you.

JOB DESCRIPTION:
Do you like solving the hardest computing problems in the world, working on research projects you are passionate about, and competitive mini basketball, and bumper pool? Battelle may be the company for you.

Battelle Cyber Engineers solve the toughest Computer Science problems in the world. We work in small agile teams to push the bounds of computing technology. Our high-powered computer labs include specialized software and hardware, so our engineers have everything they need to invent new Cyber solutions.

Our team is casual. We usually wear t-shirts and jeans. We are a close-knit group and enjoy participating in social activities outside of work. Whether it is visiting local restaurants, bowling, Korean BBQ, or paintball we always have a good time.

Battelle is committed to its employees professional growth. We encourage new ideas with our large Internal Research and Development (IRAD) program where engineers work on projects they are passionate about.

JOB RESPONSIBILITIES AND EXPECTATIONS

Collaborate with the Cyber Innovations Team to design and implementation of machine learning software solutions including experience in machine learning methodologies such as regression/classification modeling, unsupervised/supervised/semi-supervised learning, and ensemble methods

Learn and improve your reverse engineering skills through project and team building activities

Collaborate with the Cyber Innovations Team to develop software systems that aid in data analytics, network-based applications, reverse engineering tasks, embedded system development, and integration of hardware.

Identifies and articulates strengths and weaknesses of solutions, conclusions and problem approaches during technical discussions.

Demonstrates awareness of deliverables and their role within the project plan. Identifies and executes steps necessary to complete less structured assignments with limited guidance from SMEs.

Works with internal and external stakeholders to prepare and present technical content tailored to the client's mission. Leads technical discussions, demonstrating command of the technology and adequately fielding questions which arise.

Contribute to Internal Research and Development (IRAD) studies and may lead small IRAD tasks.

Seeks out technical experts for collaboration and facilitates technical discussions with lower level staff.

Demonstrates understanding of business product offerings and contributes to marketing / business development by providing technical expertise during marketing engagements as well as supporting business development efforts led by others.

Forms technical approach and generates technical volumes for small proposals with minimal guidance and leads Work Breakdown Structure (WBS) creation and labor estimates

THE FOLLOWING REQUIREMENTS MUST BE MET TO BE CONSIDERED FOR THIS POSITION:

Bachelors degree with 4-9 years experience in Computer Science, Mathematics, Computer Engineering or related field of study, or equivalent experience.

Experience designing and implementing various machine learning solutions including unsupervised/supervised/semi-supervised learning, clustering, and learned feature extraction.

Experience with software design and development in C++/C# .NET and Script development using the Python scripting language.

Ability to demonstrate good organization, communication, problem-solving and teamwork skills.

Must be a sole US citizen with the ability to obtain and maintain a US government security clearance.

THE FOLLOWING IS DESIRED, BUT NOT REQUIRED TO BE CONSIDERED FOR THIS POSITION:

Masters degree with 2-6 years experience; or PhD with 0-2 years experience, or equivalent experience.

Experience designing and implementing novel machine learning techniques and leading machine learning software development teams.

Digital and/or analog signal processing experience.

Data mining experience.

Experience applying machine learning techniques to noisy, uncertain and/or dynamic data sets.

Active Secret government security clearance.

BENEFITS

Battelles competitive benefits program includes comprehensive medical and dental care, matching 401K, paid time off, flexible spending accounts, disability coverage, and other benefits that help provide financial protection for you and your family.

Battelle provides employment and opportunities for advancement, compensation, training, and growth according to individual merit, without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, marital status, age, genetic information, or disability. Our goal is for each staff member to have the opportunity to grow to the limits of their abilities and to achieve personal and organizational objectives. We will support positive programs for equal treatment of all staff and full utilization of all qualified employees at all levels within Battelle.

For more information about our other openings, please visit www.battelle.org/careers ."
Software Engineer - Data Infrastructure,Mesosphere,"Mesosphere's Data Infrastructure team is currently looking for several talented and highly motivated software engineers to develop data frameworks for next-generation web, mobile, and IoT Applications.

This team combines leading open source data frameworks (Spark, Kafka, Cassandra, Elasticsearch, and HDFS) with Mesosphere developed frameworks. Infinity customers include the most innovative names in tech, cloud, and financial services.

This position will give you the opportunity to collaborate with the brightest engineering minds in big data and datacenter computing. As a senior engineer, you should excel with minimal technical supervision, embrace time constraints, and work with team members to deliver high quality products and features.

Responsibilities:
-----------------

Design and implement new Mesos frameworks
Enhancing existing Mesos frameworks
Dive deep into Spark, Cassandra, Kafka, Elasticsearch, HDFS to fix bugs
Effectively estimate time to implement designs
Consistently make systems simpler
Basic Qualifications:
---------------------

BS or Masters degree in Computer Science, related degree, or equivalent experience
5+ years experience with OOP, and infrastructure design / coding skills
Self-driven and motivated, with a strong work ethic and a passion for problem solving
Experience in Java development / debugging and multithreaded programming
Able to debug, troubleshoot and resolve complex technical issues reported by customers
Preferred Qualifications:
-------------------------

Experience with Spark, Kafka, Cassandra, Elasticsearch, or HDFS
Experience designing, implementing and operating large-scale stateful distributed systems
Experience with RDBMS internals, JDBC and SQL
About Mesosphere
----------------

Mesosphere is dedicated to helping enterprises unlock the next generation of datacenter scale, efficiency and automation with Apache Mesos. Mesospheres open source product, the datacenter operating system (DC/OS), is a new kind of operating system that spans the entire datacenter, pools datacenter resources, and automates IT operations. Backed by Andreessen Horowitz, Khosla Ventures, Microsoft, HPE, Data Collective, and Fuel Capital, Mesosphere is headquartered in San Francisco with a second office in Hamburg, Germany."
Tracking (Data) Engineer,CBS Interactive,"# Tracking (Data) Engineer

**REF#: ** 29158

**CBS BUSINESS UNIT: ** CBS Interactive

**JOB TYPE: ** Full-Time Staff

**JOB SCHEDULE: **

**JOB LOCATION: ** Fort Lauderdale, FL

**ABOUT US: **

CBS Interactive is the premier online content network for information and online operations of CBS Corporation as well as some of the top native digital brands in the entertainment industry. Our brands dive deep into the things people care about across entertainment, technology, news, games, business and sports. With over 1 billion users visiting our properties every quarter, we are a global top 10 web property and one of the largest premium content networks online.

Check us out on [1] The Muse to get an inside look into #LifeAtCBSi through employee testimonials, office photos and company updates.

References

Visible links

1. https://www.themuse.com/companies/cbsinteractive

**DESCRIPTION: **

Role Details:
The Tracking Engineer possesses a deep sense of curiosity, a passion for building smarter tracking solutions to that drive decisions based on data throughout the CBS Interactive organization.

Key Projects include:
Working backward from analytical and experimental requirements to devise data and BI solutions, this engineer supports the design and audit of tracking across multiple platforms and devices.

Key Responsibilities:
Can implement translated business requirements into tagging specs across multiple tools and tracking approaches.
Builds tracking approaches for activity and user analytics, such as user segmentation or traffic trending across sites and business units.
Able to break down and communicate highly complex data problems into simple, feasible solutions.
Maintain ongoing upgrades to solutions and participate in training impacted teams.
Find answers to business questions via hands-on exploration of data.
**QUALIFICATIONS: **

What you bring to the team:
At minimum, BS degree in Statistics, Engineering, Informatics, or Computer Science degree with demonstrated experience in analytics.
Experience with Adobe Analytics (Omniture), and other Adobe products across platforms (mobile, desktop, video, app) and with large data volumes.
Experience with digital marketing strategy and analytics, including site, video, social media, SEM, SEO, and display advertising.
Basic knowledge of Java script. Should be comfortable debugging and modifying JavaScript and have the desire to expand their skills.
Familiarity with App Development (iOS, Android).
Familiarity with SQL skills for MySQL and Teradata.
Pro-active and strategic thinker, enjoys solving problems, thrives in ambiguous environment, can turn on a dime when dealing with multiple tight project deadlines, and strong communicator.
All candidates must successfully complete a background check prior to starting employment at CBS Interactive. CBS Interactive is an Equal Opportunity Employer and provides equal employment opportunities to all employees and applicants for employment without regard to race, color, religion, gender, sexual orientation, national origin, age, disability, or status as a veteran. CBS Interactive complies with applicable state and local laws governing non-discrimination in employment in every location in which the company has facilities.

**EEO STATEMENT: **

Equal Opportunity Employer Minorities/Women/Veterans/Disabled"
Data Engineering Analyst,BMO Financial Group,"This job description is in the processing of being loaded into My Hiring. Please proceed by completing this requisition and acquiring approval. Once your recruiter is assigned to this approved requisition, they will work with you to input the appropriate job description.

The Data Engineering Analyst will drive adoption of self-service analytics tools by working with prioritized users to engineer data flows and craft solutions. As a Data Engineer, you will collaborate within the Data Goveranance & Analytics team and across broad parts of the BMO. This position will support the Chief Data Officer and Data Governance Leaders with the following:

Work with Business & Technology teams drawing upon technical and data processing knowledge to design & build solutions

Develop & maintain Data Engineering framework and operating procedures

Develop expertise in data preparation & visualization tools, maintain/enhance training materials & contribute to/Lead formal training sessions

Work with Data Governance Leads, Advanced Analytics & Enterprise Architecture teams to engineer solutions that align to priorities and design

Execute key Data Governance processes intake/demand and prioritization process, issue management process, and monitoring processes

Analyze information and metrics for trends, risks and issues and report findings to senior leaders

Build and sustain the day-to-day relationships and communications with business partners at various levels of the organization, including Data Governance Leads, Line of Business partners, other Data Governance Analysts, as well as other internal and external resources

Impact on an organization that is rapidly growing, take on exciting challenges, scaling data infrastructure and improving the data platform to drive analytics capabilities

Participate in regular Data Governance team, project, and leadership meetings

Qualifications

Qualifications

Skills and Experience

Possesses a university degree in Technology, Business or related field, and/or 10+ years relevant work experience

5+ years in data engineering, data analysis, business intelligence/data warehousing experience or related experience

Hands on Experience in Hadoop, Python, R, Visualization tools (Tableau/Spotfire)

Possesses advanced knowledge and experience largescale data architecture, master data management or similar projects or programs of significant complexity and value ( > $5MM) with high risk.

Excellent problem solving and critical thinking skills to recognize and comprehend complex issues, policies, regulatory requirements and industry information affecting the business environment

Understanding of data concepts, tools, and SQL. Ability to learn new tools to analyze data & data quality issues

Ability to navigate a highly matrixed organization effectively

Good relationship management and influencing skills

Sound business and technical acumen, with demonstrated agility in learning and ability to quickly become comfortable with familiar businesses areas or of technologies

Able to work highly independently and lead multiple priorities

Desired

Have worked with at least one Enterprise Application (ERP) System or similar (Ex: SAP, Oracle Financials, etc).

US Banking or Financial Services Industry background

Has in depth knowledge of Data Governance / Data Quality processes and procedures

Were here to help

At BMO Harris Bank we have a shared purpose; we put the customer at the center of everything we do  helping people is in our DNA. For 200 years we have thought about the futurethe future of our customers, our communities and our people. We help our customers and our communities by working together, innovating and pushing boundaries to bring them our very best every day. Together were changing the way people think about a bank.

As a member of the BMO Harris Bank team you are valued, respected and heard, and you have more ways to grow and make an impact. We strive to help you make an impact from day one  for yourself and our customers. Well support you with the tools and resources you need to reach new milestones, as you help our customers reach theirs. From in-depth training and coaching, to manager support and network-building opportunities, well help you gain valuable experience, and broaden your skillset.

To find out more visit us at

https://bmoharriscareers.com

.

BMO Harris Bank is committed to an inclusive, equitable and accessible workplace. By learning from each others differences, we gain strength through our people and our perspectives. BMO Harris Bank is an Equal Opportunity Employer for all, inclusive of Minorities, Women, Veterans, and Persons with Disabilities.

Job

Information Technology

Primary Location

United States-Illinois-Chicago

Organisation

Data Governance & Analytics-X004874

Schedule

full-time

Job Posting

12/20/17

Unposting Date

Ongoing"
Software Engineer,Alorica,"Summary
The Software Engineer is responsible for developing new applications and supporting/troubleshooting existing solutions, leading and participating in the activities associated with identifying and analyzing user requirements to develop detailed design specifications, coordinating assigned projects for completion from beginning to end, and effectively communicating results to end user.
Essential Duties & Responsibilities
Working on highly technical, complex projects in order to develop solutions and services which are cost effective while meeting the business needs of the Company.
Lead activities throughout the entire software development life cycle providing consultative advice and direction to development team, as needed.
Develop technical specifications including the configuration of standard applications, modifications to standard reports, development of new reports, development of interfaces in and out of the application, and end user support for multiple geographies.
Participate in business meetings to identify and understand the company's goals and objectives in order to develop software that will meet the company's current and future business needs.
Analyze and review functional and performance requirements to support design architecture, providing alternative solutions as necessary.
Perform troubleshooting and issue resolution throughout project ensuring issues are brought to resolution and on-going communication with project team and management is maintained.
Coach and mentor project team members to ensure thorough understanding of project tasks and company goals.
Defines and implements strategies for integrating disparate operating environments.
Reviews progress and evaluates results. Recommends changes in procedures and processes.
Perform other duties as assigned by management. Qualifications & Requirements
Education:
Bachelor's Degree in computer science, computer engineering, MIS or a related field required.
Equivalent work experience in a similar position may be substituted for educational requirements.
Experience:
Minimum two years of a programming platform (e.g., Windows, Web, Unix/Linux, etc.) required.
Minimum five years of a standard programming language (e.g., JAVA, J2EE, .NET, C/C++/C#, SQL, Oracle, MySQL, PL/SQL, Informix, Data Comm, Visual Basic, C++, Angular JS, Bootstrap, HTML,XML/VXML, PERL, etc.) required.
Knowledge, Skills, Abilities & Other Characteristics:
Expert knowledge of Internetworking Operating Systems and call center technology.
Advanced project management experience required.
Possesses and applies comprehensive knowledge of principles, practices, and procedures of a particular field in order to complete assignments of highly advanced difficulty.
Exhibit strong attention to detail.
Excellent problem-solving skills.
Excellent interpersonal, written, and oral communication skills.
Ability to maintain the highest level of confidentiality.
Ability to work in a team fostered environment.
Position Scope
Management: May have oversight responsibility junior team members or project teams.
Relationships:
o Internal- Sr. Management, Operations
o External- Clients, Vendors
May focus on multiple high level projects for one or more disciplines.
Assignments are broad in nature and usually require originality and ingenuity."
Data Analyst,Walmart eCommerce,"Position Description
We are looking for a Big Data Analyst/Engineer/Scientist who wishes to jumpstart their career by helping SEM team drive continuous optimization of PPC ad spend on search engines by increasing transparency of outcomes of algorithm's decisions, by performing advanced visualizations of data to uncover insights and potential features to be used by the algo, by architecting, executing and reporting beyond doubt results of A/B tests of various bidding strategies and formalizing attribution methods to score impact of various capabilities of PPC ads on engines.

You have a deep understanding in A/B testing, and all the statistical theory supporting A/B testing. You will be comfortable working at the intersection of statistics, causal inference.

We are hiring a partner to both Digital Marketing and Data Science team to provide analytic support as they evaluate strategies or validate hypotheses using our extremely diverse sets of historical data. We are looking for someone who is as comfortable working on experimentation edge cases as well as the day to day. You love sitting down and helping solve problems and push the boundaries to drive quality and excellence in experimentation.

Minimum Qualifications
Minimum Qualifications

BS degree with 3+ yrs of relevant exp or Master's degree in Science, Technology, Engineering, or Math . PhD in quantitative fields is a plus.
Successful candidates typically demonstrate strengths in many of the following areas:
o Hands on experience in A/B testing, preferably in product/marketing analytics area
o Strong SQL skills and experience using several of Hive, Spark
o Experienced working with massive, messy data sets
o Confident applying appropriate analysis methods of causal inference in both experimental and non-experimental situations
Intellectual curiosity and attention to detail
Organized when analyzing data and conducting research
Strong verbal and written communication skills
Comfortable using the available data to help make decisions
Customer focused and passionate about using data to drive business growth
Clear and unambiguous when making commitments and always follows up on promises
Understands product development in leading technology companies
Additional Preferred Qualifications

Company Summary
Walmart Global eCommerce is comprised of Walmart.com, VUDU, SamsClub.com, and our technical powerhouse @WalmartLabs. Here, innovators incubate next gen e-commerce solutions in real-time. We integrate online, physical, and mobile shopping experiences for billions of customers around the globe. How do we do it? We continuously build and invest in new technology including open source tools and big data innovations. Data scientists, front and back-end engineers, product managers, and web and UX/UI teams collaborate alongside e-commerce experts to envision, prototype, and bring revolutionary ideas to life in a dynamic, flexible and fun work culture.

Position Summary
Position Summary

SEM Engineering team is a part of Growth at Walmartlabs and is in charge of optimizing paid and free search for walmart.com. We are tasked with optimizing ad spend on RoAS while making sure as many items from our catalogue as practical get conversion, while doing it at scale, meaning, the traffic quality should be maintained regardless of amount of ad spend. We use data science methods to dynamically modify keyword and product ad bids across top search engine providers. The team is responsible for building data pipelines and API integrations enabling us to do so, with concerns of SLA and data quality squarely in focus, as well as building internal tool to provide our business partners control and visibility of SEM operations. Our challenge is immense. Presently, the catalog is expanding by 10s of thousands of items daily and keyword universe is growing by 10s of thousands monthly."
Machine Learning Engineer,imo.im,"imo is a global messaging platform based in Palo Alto, California, and was founded and funded by one of the first ten employees at Google. Our current team consists of TopCoders, ACM ICPC World Finalists, and medalists of the International Olympiads in Informatics. We work on challenging projects that we choose from the ground up that have direct impact on our 200-million monthly-active users.

We are currently seeking machine learning engineers who are interested in working directly with our company founder and CEO building out and developing our machine learning initiatives. This is a great opportunity to be an early employee at a fast growing company.

Responsibilities:
Participate in cutting edge research in artificial intelligence and machine learning applications.
Develop solutions for real world, large scale problems.
Minimum qualifications:
PhD in Computer Science or related technical field or equivalent practical experience.
1 year of work or educational experience in Machine Learning or Artificial Intelligence.
Experience with one or more general purpose programming languages including but not limited to: Java, C/C++ or Python
Preferred qualifications:
Experience with one or more of the following: Natural Language Processing, text understanding, classification, pattern recognition, recommendation systems, targeting systems, ranking systems or similar.
Benefits & Perks:
Competitive salary, sign-on bonus and stock options
$500/month housing stipend for living within a five-mile radius of the office
Four weeks of paid vacation per year (plus paid holidays and sick days)
401(k), medical, dental and vision benefits
Access to a concierge medical group
Catered lunch or dinner daily and a fully-stocked kitchen
Data plan reimbursement
Gym membership at Equinox
Yearly conference stipend
imo-sponsored team events and outings"
Big Data Engineer - Intelligent Solutions,JP Morgan Chase,"JPMorgan Chase & Co. (NYSE: JPM) is a leading global financial services firm with assets of $2.6 trillion and operations worldwide. The firm is a leader in investment banking, financial services for consumers and small business, commercial banking, financial transaction processing, and asset management. A component of the Dow Jones Industrial Average, JPMorgan Chase & Co. serves millions of consumers in the United States and many of the world's most prominent corporate, institutional and government clients under its J.P. Morgan and Chase brands. Information about JPMorgan Chase & Co. is available at www.jpmorganchase.com .

JPMorgan Intelligent Solutions (JPMIS) transforms JPMC data assets to create and commercialize information and solutions that enable consumers, businesses and governments to make better decisions and achieve their objectives. JPMIS also uses Big Data Technology to improve efficiency and information use within JPMC.

JPMIS is looking to add an Engineer with experience in distributed systems design and build, with focus on Big Data. The vision is to create a reliable and scalable data platform, provide standard interfaces to query and support analytics for our big analytics related data sets that is transparent, secure, efficient and easy to access as possible by our varied applications.

Responsibilities will include:
Design, engineer and build data platform solutions using Big Data Technologies
Establish and communicate fit for purpose analytical platforms for business prototypes
Lead innovation by exploring, investigating, recommending, benchmarking and implementing data centric technologies for the platform.
Be a proactive coding engineer
Interfacing with vendors - manage POCs and RFPs  Full stack engineer, being able to lead and run conference calls, document and execute an engineered vision, ability to get on command prompt and troubleshoot / install.
Knowledge of various Big Data components, vendors and technologies including Hadoop Tableau, Gemfire, low latency solutions (networking / disk / etc).
Proven leadership skills to participate as a senior technologist in JPMIS
Prior experience with Hortonworks or Cloudera required
College degree required
Java (6+) years of solid development experience w/continuous integration tools and GIT
System Integration (5+)
Linux and Big Data Security Frameworks (3+)
Hadoop (2+) (e.g., NoSql, Security, Spark, ElasticSearch)
Web technologies experience"
Senior Data Engineer - Machine Learning,ThinAir,"About ThinAir

Based in Mountain View, CA, ThinAir is a first-of-its-kind enterprise security solution built for end-users. Our mission is to secure the world's data. We have built a ground-breaking platform utilizing some of the most innovative technologies available today to accomplish this mission. We give our customers complete visibility, context and seamless control over all their data.

We are a world-class team of operators and engineers from Apple, Spotify, IBM, Microsoft, Google, DropBox, VMWare, Amazon, U.S. military and intelligence communities. Join a diverse, passionate team where your voice and ownership are critical to success.

About The Position

As a Senior Data Engineer you'll be our first Machine Learning Specialist and help us discover the information hidden in vast amounts of data. You'll help us make smarter decisions to deliver even better products and apply data mining techniques and statistical analysis to build high quality prediction systems integrated with our products. For example, products that automate scoring using machine learning techniques, build recommendation systems, or build systems for automated fraud and anomaly detection. Your impact will be significant and will include building our initial machine learning infrastructure.

IN THIS ROLE YOU WILL:
Analyze source data and data flows, working with structured and unstructured data.
Manipulate high-volume, high-dimensionality data from varying sources to highlight patterns, anomalies, relationships and trends
Analyze and visualize diverse sources of data, interpret results in the business context and report results clearly and concisely.
Apply data mining, NLP, and machine learning (both supervised and unsupervised) to improve relevance and personalization algorithms.
Work side-by-side with product managers, software engineers, and designers in designing experiments and minimum viable products.
Build and optimize classifiers using machine learning techniques and enhance data collection procedures that is relevant for building analytic systems.
Discover data sources, get access to them, import them, clean them up, and make them model-ready. You need to be willing and able to do your own ETL ideally in an AWS environment.
Create and refine features from the underlying data. Youll enjoy developing just enough subject matter expertise to have an intuition about what features might make your model perform better, and then youll lather, rinse and repeat.
Run regular A/B tests, gather data, perform statistical analysis, draw conclusions on the impact of your optimizations and communicate results to peers and leaders.
Teach and mentor others in the use of AI/Machine Learning
Create an automated anomaly detection system and develop ways to constantly track it's performance.
IN ORDER TO BE SUCCESSFUL IN THIS ROLE YOU WILL NEED:
5+ years of Experience in one or more areas of machine learning such as pattern recognition, NLP, anomaly detection, recommender systems, sentiment analysis, clustering
The ability to work with loosely defined requirements and exercise your analytical skills to clarify questions, share your approach and build/test elegant solutions in weekly sprint/release cycles.
Development experience in Python or Java/Scala and pride in producing clean, maintainable code
Experience in one or more OSS machine learning libraries such as scikit-learn, Spark ML
Knowledge of different machine learning approaches such as GLM, Decision Trees, SVM, Neural Networks etc.
Practical experience in clustering high dimensionality data using a variety of approaches
In-depth experience in time series analysis and sequential data using ARIMA, Kalman Filters, HMM, RNN etc.
Real world experience in solving business problems by deploying one or more machine learning techniques
MS in Computer Science, Statistics, Financial Engineering or related quantitative field
Experience creating pipelines to analyze data, extracted features and updated models in production.
Independence and self-reliance while being a pro-active team player with excellent communication skills.
WHY JOIN THINAIR?
Innovative, customer-centric culture & bright, passionate teammates
Competitive compensation
100% paid health care
Catered meals & stocked kitchen
Perks program
Ability to make history while having fun doing it!"
Advanced Analytics Data Analytics Engineer,Travelers,"Company Information
Solid reputation, passionate people and endless opportunities. That's Travelers. Our superior financial strength and consistent record of strong operating returns mean security for our customers - and opportunities for our employees. You will find Travelers to be full of energy and a workplace in which you truly can make a difference.

Job Summary
Are you interested in becoming a member of a highly skilled team that provides support for cutting edge projects in an area that enables advanced analytics for our Research departments?

This position offers a technical, assertive, creative and inquisitive individual the opportunity to learn about our research work and teams here at Travelers and the products they support by working in an exciting and growing area of the company. You will partner with some of the best statisticians, modelers and technical staff in the company to improve the tools and processes used by the Advanced Analytic community. You will assist them in developing better advanced analytic models in an effort to impact our business more quickly.

As a member of the Enterprise Business Intelligence and Analytics (EBIA) team, the incumbent will support the Advanced Analytics Pillar by working closely with the Advanced Analytic Pillar Lead as well as leaders across the Enterprise and IT to execute on our modernization efforts. The incumbent will help position our advanced analytic platform and infrastructure to support the next generation of our advanced analytic models, enabling software capabilities like SAS, R, Python and other open source tools, and ensure our community has access to the appropriate hardware (servers, powerful desktops, GPU's, etc.) as well as latest Cloud (internal and external) technologies that support various advanced analytic initiatives.

Primary Job Duties & Responsibilities
Includes but not limited to partnering with our advanced analytic community and engineering teams to support the following within Travelers ecosystem:
Develop a strategy to identify, map and move current workloads to target state and assist customers with execution
Enable interoperability between our Big Data and advanced analytic ecosystems
Develop a framework to support enterprise use of custom built tools and scripts
Develop a framework to support regression test beds to test predictive models during ecosystem changes
Champion the effective use of storage technologies, metadata and third party data
Develop and champion best practices to support environment user guides, training and onboarding material
Develop and track server workloads, utilization and capacity metrics to drive future direction, purchases and needs
Develop and maintain a catalog of software products and their use across the platform

Minimum Qualifications
5 years of relevant experience with data tools, techniques, and manipulation required.

Education, Work Experience & Knowledge
Bachelors degree in MIS, computer science or equivalent.

Job Specific & Technical Skills & Competencies

Knowledge of advanced analytics modeling processes and practices
Knowledge of analytic ecosystems including: tools, ETL, metadata / data management, relational database management systems
Proven track record of success supporting analytic teams and environments
Strong analytical and problem solving skills
Strategic thinking skills demonstrated thought leadership in the analytics space
Strong communication, collaboration and team building skills with both business and IT disciplines
Ability to multitask and work on multiple projects concurrently
Customer service oriented with the enthusiasm to learn

Preferred Qualifications

Programming experience in a Linux environment using tools that include SAS, R and Python
Experience enabling projects related to advanced analytics, data warehousing and/or business intelligence

Physical Requirements
Operates standard office equipment - Continuously Sitting (Can stand at will) - Continuously Use of Keyboards, Sporadic 10-Key - Continuously

Equal Employment Opportunity Statement
Travelers is an equal opportunity employer."
Senior Data Engineer,Mozilla,"The target locations for this position are; Mountain View, San Francisco, Portland, Toronto, Vancouver, Berlin, Paris, London, Denver, or Remote in one of those countries.
---------------------------------------------------------------------------------------------------------------------------------------------------------------------------

At Mozilla we want to make Firefox a delightful product for our users. We are collecting and processing terabytes of data a day from hundreds of millions of users to improve Firefox, build data driven features, and foster a data-informed culture. We need your help!

As a Data Engineer at Mozilla, you'll be working with a talented group of engineers, product managers, and data scientists to make the Web a better place and impact the everyday lives of hundreds of millions of users. Our goal is to make Firefox better than ever.

------------------------------------
As a Senior Data Engineer at Mozilla
------------------------------------

You will work with cross functional teams to make data-informed decisions using data collected from users browsers (telemetry) and Mozillas services
You will collaborate with product managers and engineers to design field telemetry for existing and new features, and build data sets for efficient analysis
You will contribute to Firefoxs data platform by architecting, building, installing, testing, monitoring and maintaining highly scalable data management systems
You will pay attention to industry trends in data science and engineering, evaluating and learning new technologies
You will work with product managers, engineers and data scientists to experiment and build features into Firefox driven by data and algorithms (e.g. recommender systems)
---------------------

General Requirements:
---------------------

Proficiency with one or more of the programming languages: Python, Javascript, C/C++, Scala, or Java
Strong coding fundamentals: data structures, algorithms, etc.
A strong cloud and distributed systems background
Basic statistics: for example, T-tests, correlations, and sampling
Ability to work collaboratively with a distributed team
Ability to work with other teams across an organization
Ability to clearly communicate verbally and in writing
------------------------

Other Skills/Experience:
------------------------

Our team requires skills in a variety of domains. You should ideally have experience with some of the areas listed below, and be interested in learning new things. Were excited to see:

A strong foundation in database systems (SQL and/or non-relational)
A strong background in statistical foundations on experimental or observational data
Experience with tools and technologies were using: Kafka, Spark, Parquet, PrestoDB, Hive, Airflow, Re: dash (or similar)
Experience with product analytics
Experience with machine learning
Working knowledge of web development technologies: HTML, Javascript, CSS, HTTP.
If you have different experiences and skills from the ones listed above but have something else to contribute, please contact us.

Mozilla is a global organization, and this role is available in our offices in the U.S., Canada, or Germany, or home-based in the U.S. or Canada. You must have work authorization in one of these countries (we cannot hire anyone on a student visa at this time). We support flexible work hours, paid professional development, and have a wellness program that includes childcare assistance.

-------------
About Mozilla
-------------

Mozilla exists to build the Internet as a public resource accessible to all because we believe open and free is better than closed and controlled. Join us to work on products that promote choice and transparency and give people more control over their lives online. Mozilla is committed to Equal Employment Opportunity throughout our recruiting and hiring process and is dedicated to increasing diversity in our workplace."
Business Analyst/Data Engineer,Oracle,"Business Analyst/Data Engineer-17001E0S
Preferred Qualifications

Bare metal Cloud Services is seeking a Business Analyst/Data Engineer

We are building a new technology organization based in Downtown Seattle. This new entity is being constructed with an entrepreneurial spirit that promotes an energetic and creative environment. We are unencumbered and will need your contribution to make it a world class engineering center with the focus on excellence.

Oracles extensive enterprise customer base is looking for rock solid cloud solutions that provide the same reliability and effectiveness that they have come to expect from Oracle. As a trusted Enterprise vendor, Oracle is in the early stages to provide highly cost effective, highly performance compute, storage, & PaaS Cloud solutions to its customer base.

This position is responsible for analysis and reporting of complex business information to senior management across the Bare Metal Cloud Services organization.

The Business Analyst will integrate real time data through the use of business intelligence tools into useful information and metrics that will drive leadership strategic decision-making and allow department managers to immediately act on adverse trends . The analyst will provide transparency of data and targets through various performance tools such as dashboards and scorecards which seek to increase throughput, improve productivity, and gain operational efficiencies. This position requires a keen analytical ability to synthesize the data and correlation of metrics that will support achievement of organizational goals, operational and financial performance.

The position is located in Seattle, WA.

Additional Responsibilities
Develops key operational and strategic materials for senior management in support of major programs and processes.
Analyze data for performance trends, significant results (variances) and opportunities for improvement based on sound analytical methodology. Utilize analytical reporting/ Business Intelligence tools in designing reports, scorecards and dashboards to provide the appropriate detail information to diagnose reasons for performance issues and opportunities for improvement.
Review quality, financial and operational metrics and correlate metrics using statistical methods that will provide insight into causal effect on performance outcomes.
Leads significant analyses and fulfilled the most complex information requests. Determines optimum methodology for retracing and analyzing current and historical data.
Develops statistical models to analyze operational performance data and support ongoing reporting process.
Formulates and provides recommendations regarding appropriate performance metrics. Acts in an advisory capacity to management on metrics, measures and dashboards.
Conducts research, provides analysis and makes recommendations regarding management initiatives. Assists in the definition of problems and issues through the use of analytical techniques.
Own and drive data improvement projects.
Your Skills and Experience
Minimum of 4 years of business analysis experience in operations or customer support organization.
Outstanding analytical skills and demonstrated ability to exchange complex information effectively to reach agreement in ambiguous or difficult situations.
Advanced knowledge of Excel required with experience in other data analysis and Business Intelligence tools. Proficiency with Microsoft Office Suite.
Experience working with a variety of statistical and analytic platforms and packages.
Excellent written and oral communication skills.
Experience with Jira and SQL are preferred.
As part of Oracles employment process candidates will be required to complete a pre-employment screening process, prior to an offer being made. This will involve identity and employment verification, salary verification, professional references, education verification and professional qualifications and memberships (if applicable).

(Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law.)

Detailed Description and Job Requirements

Manage the development and implementation process of a specific company product.

Manage the development and implementation process of a specific company product involving departmental or cross-functional teams focused on the delivery of new or existing products. Plan and direct schedules and monitor budget/spending. Monitor the project from initiation through delivery. Organize the interdepartmental activities ensuring completion of the project/product on schedule and within budget constraints. Assign and monitor work of systems analysis and program staff, providing technical support and direction.

Duties and tasks are varied and complex, needing independent judgment. Fully competent in own area of expertise. May have project lead role and or supervise lower level personnel. BS degree or equivalent experience relevant to functional area. 5 years of project management, product design or related experience preferred.

Oracle is an Equal Employment Opportunity Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, disability and protected veterans status or any other characteristic protected by law."
Data Protection Engineer,BB&T,"Specific information related to the position is outlined below. To apply, click on the button above. You will be required to create an account (or sign in with an existing account). Your account will provide you access to your application information. Should you have a disability and need assistance with the application process, please request a reasonable accommodation by emailing BB&T Accessibility or by calling 866-362-6451. This email inbox is monitored for reasonable accommodation requests only. Any other correspondence will not receive a response.
Regular or Temporary:
Regular
Language Fluency: English (Required)
Work Shift:
1st shift (United States of America) Please review the following job description: Support information security activities across numerous systems/technologies in support of Corporate Information Security (CIS) and IT Services. Partner with key stakeholders in overall information security technology planning, bringing a current knowledge and future vision of information security technology and systems as related to the enterprise's competitive position. Understand the enterprise's core information security competencies and the value they bring to business processes for a competitive advantage. Interact with internal customers to ensure continuous customer satisfaction. Support long-term information needs for operating unit and participate in the development of an enterprise-wide strategy for systems development and hardware acquisition and integration.
Desired experience and skills:
Support engineering and integration of new security and data protection technologies and operational services support for the enterprise to ensure that the appropriate controls are selected, deployed, and are operating effectively. Support working with lines of business, management, SME leads, peers, and development teams to ensure data protection for systems are leverage to reduce risk across the organization. Analyze detailed feature and usability analysis based existing and future vendors platform capabilities against BB&T business and compliance needs. Support the review of data protection reports, metrics, and recommend enhancements and additional workstreams to further enhance our security posture. Analyze and support corporate data protection security projects and/or programs which include, but are not limited to, solution and service enhancements, new data protection technology evaluation, new data protection functionality testing, project requirement gathering and analysis, internal customer communications, data protection inquiries, and ensuring processes and services are documented.
Knowledge of financial services industry and all applicable regulations and industry standards;
You have experience protecting database environments, including Oracle, Sybase and MS-SQL;
You have experience with encryption solutions and encryption key management;
You have excellent communication and documentation skills; You have deep understanding of hardware required to support encryption infrastructure; Scripting skills in Python, PowerShell, Perl or a similar language;
Understanding and utilization of data protection technologies API use and integration across technologies;
Ability to diagram complex as-built systems using industry standard tools such as Visio and/or UML tools;
Support the as-built design, implementation, operation and maintenance of security applications and tools based upon the established security architecture for data protection;
Analyze data protection services to best fit a business need;
Assist in the review of data protection requirements of business functions and document the available solutions and processes; CASB, DLP, Tokenization, transparent encryption and other related data protection technologies;
Familiarity with solutions such as Vormetric, Thales, Venafi, Guardium or similar platforms.
Essential Duties and Responsibilities:
Following is a summary of the essential functions for this job. Other duties may be performed, both major and minor, which are not mentioned below. Specific activities may change from time to time.
Lead efforts related to designing, planning, enhancing and testing all information security technologies used throughout the enterprise, including base-lining current systems, trend analysis and capacity planning, as required, for future systems requirements; Lead technical assessment, research and testing of new technologies, as requested, using information security design, automation, and scripting best practices and procedures; Analyze information to determine, recommend and plan the use of new information security technologies, or modifications to existing equipment and systems that will provide capability for proposed project or work load, efficient operation and effective use of allotted resources; Lead the implementation of new information security technologies or integration of existing technologies, including initial configuration, installation, change management and operational handoff; Responsible for Tier III technical support of information security technologies, providing expert problem analysis and resolution in a timely manner; Participate in all areas of information security technology management, systems development, systems implementation and systems support, as assigned; Support training of less experienced personnel; Provide 24x7 on-call availability for supported information security technology infrastructure.
Required Skills and Competencies:
The requirements listed below are representative of the knowledge, skill and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Bachelors degree in Computer Science or relevant field, or equivalent education and related training;
5-7 years of experience in Information Security; Certified Information Systems Security Professional (CISSP) certification;
Extensive understanding of applied enterprise information security technologies including, but not limited to, firewalls, intrusion detection/prevention systems, network operating systems, identity management, database activity monitoring, encryption, content filtering and mainframe security;
Thorough knowledge of data flow, mainframe/client server systems, problem analysis and systems tuning; adept with network interfaces and technologies;
Knowledge of one or more programming/script languages with proficiency in development of systems and process automation ;
Good verbal and written communication skills; Demonstrated proficiency in basic computer applications, such as Microsoft Office software products;
Ability to travel, occasionally overnight.
INDBBTIT"
Data Integration Manager,BOEING,"Bellevue,Washington,United States
BEH8ML

Analytics & Information Management Services (AIMS) team is seeking a Data Integration Manager to lead a team of data technologists and software developers to maintain existing and build new data pipelines on the Boeing AnalytX platform. In this role, you will:

Partner with AI/ML and analytics team to establish a prioritized backlog to deliver data pipeline that includes data ingest, integration, curation and data services (APIs)
Work with software development, data science and analytics product management team to ingest data and provision data services through a reusable data api catalog in the AnalytX platform
Support long-range strategic business plans across AIMS and other teams.
Drive automation through innovation for ingestion, modeling and quality
Implement industry standard data model to keep data agnostic to changes in source systems
Innovate on monetizing data and build data products
Support current production system and future AnalytX development work statement.
Serve as the business integrator for production system applications. In this role, you will be the prime business contact for production system changes. You will integrate business requirements, prioritize work packages, commit to production deliveries on behalf of AIMS, and lead integrated data development projects across the organization.
Develop and maintain relationships with customers, stakeholders, peers, partners and direct reports.
Coach employees, and provide developmental opportunities and job assignments to enhance performance and expand capabilities.
Create a culture of continuous improvement by deploying enterprise best practices to improve cost, schedule, and quality.
In order to succeed in this role you should have demonstrated consistent performance at a high level, in a challenging, dynamic environment. This position will require the ability to manage multiple critical priorities simultaneously. You must possess a passion for customer service, and must clearly demonstrate outstanding leadership attributes. Additionally you should be comfortable working in an environment where urgency is the norm and providing consistent follow-through.

Boeing is the world's largest aerospace company and leading manufacturer of commercial airplanes and defense, space and security systems. We are engineers and technicians. Skilled scientists and thinkers. Bold innovators and dreamers. Join us, and you can build something better for yourself, for our customers and for the world.
Required Skills-

3 plus years of people management experience required.
3 plus years of experience with big data integration and Hadoop based data lakes.
3 plus years of experience with API gateway and systems integrations.
Experience solving complex business problems and delivering solutions via technology.
Preferred Skills-

Experience with Cloud native development
Experience with distributed data architecture
Experience building highly scalable enterprise grade platforms
Software Product Management is highly desired
Technical bachelor's degree and typically 11 or more years' related work experience or a Master's degree with typically 10 or more years' or a PhD degree with typically 8 or more years' related work experience or an equivalent combination of education and experience. A technical degree is defined as any four year degree, or greater, in a mathematic, scientific, or information technology field of study.

This position must meet Export Control compliance requirements, therefore a US Person as defined by 22 C.F.R. 120.15 is required. US Person includes US Citizen, lawful permanent resident, refugee, or asylee.

Yes, 10 % of the Time
CIO, Information & Analytics
Manager with Direct Reports
No
No
Standard
United States; The Boeing Company"
Data Engineer,Indeed,"Our mission:
As the worlds number 1 job site, our mission is to help people get jobs. We need talented, passionate people working together to make this happen. We are looking to grow our teams with people who share our energy and enthusiasm for creating the best experience for job seekers.

The team:
We are a rapidly growing and highly capable engineering team building the most popular job site on the planet. Every month, over 200 million people count on us to help them find jobs, publish their resumes, process their job applications, and connect them to qualified candidates for their job openings. With engineering hubs in Seattle, San Francisco, Austin, Tokyo and Hyderabad, we are improving people's lives all around the world, one job at a time.

Your job:
Create and manage data sources
Integrate with diverse APIs
Contribute to the ongoing development of the data warehouse ecosystem
Work closely with stakeholders on the data demand side (finance, analysts, and data scientists)
Work closely with stakeholders on the data supply side (domain experts on source systems of the data)
Build self-monitoring, robust, scalable interfaces and data pipelines for 24/7 global operations.
Design and build optimized OLAP and Star Schema data structures
Create highly reusable code modules and packages that can be leveraged across the data pipeline
Develop and maintain data dictionary for published data sources
Develop and improve continuous release and testing processes
Elicit requirements from a wide range of different teams
About you:
You are an experienced Software Engineer who is passionate about data-driven approaches. You enjoy exploring large data sets and get excited about learning new technologies and learning in a collaborative environment. You are skilled at eliciting requirements from a wide range of different teams.

Requirements

*

Bachelor's degree in computer science, computer engineering, or an engineering discipline
*

3+ years industry experience in software development and/or data engineering
*

Strong CS fundamentals and problem-solving skills
*

Strong software engineering skills
*

Expertise in an object oriented language (preferably Python or Java)
*

Ability to communicate effectively with stakeholders to define requirements
*

Strong SQL skills,
*

Familiarity working with relational databases
*

Familiarity with RESTful APIs
*

Ability to identify and resolve performance issues

Nice to have

*

Experience in corporate Data Engineering
*

Experience with Hadoop, Spark, and/or Scala
*

Experience in columnar data stores or NoSQL technologies"
Data Center Engineer - Principal Infrastructure Operations Analyst,BNY Mellon,"TECHNOLOGY SERVICES GROUP (TSG) is the central infrastructure services group with a proven track record of innovating to help BNY Mellon and its customers have most reliable, nimble and cost-effective solutions in the financial services market place. BNY Mellon BXP technology is leading edge with industry first PasS/IaaS fully integrated solution, combined with enterprise standards for developers.

TSG is building next generation scalable, efficient data center as a service using the latest innovation infrastructure. Building on top of the cloud deployments in BNY Mellon, we are aggressively automating and bridging the green field cloud with the existing virtual computing environments.

The Data Center Senior Engineer will be responsible for the data center infrastructure projects and day-to-day activities of the Data Center Services (DCS) group. The DCS team is responsible for the planning, installation, configuration, troubleshooting, and maintenance of all server hardware/devices. The position requires a professional with proven experience and success in managing the full life cycle of data center infrastructure projects from planning customer server installations, cabling, and designing structured cable plant systems. This individual will provide excellent customer service and report progress regularly. They will also assist the DCS team with troubleshooting server/network/storage systems. The work environment is fast paced and deadline-oriented where adaptability is key to our success as a team, thus having depth knowledge of working large-scale data center environments is necessary.

Key responsibilities include :
Ability to manage and delivery small to mid-size projects from various internal customers of competing priorities and work them through to an effective resolution

Ability to automate processes and create status reports in the projects as need. Report the progress to management and stakeholders regularly.

Responsible for all data center related hardware installations/repairs, configurations, troubleshooting (Intel/Linux/Network/Mainframe), upgrades (firmware) and optimization.

Strong understanding of HP & Cisco standalone servers, enclosures, & blades.

Experience with Data Center space planning, infrastructure, migration, relocation, consolidation.

Principal Infrastructure Operations Analyst- > Designs, implements, integrates, and provides full support for complex software in a multi-tiered, multi-platform environment. Advises specialists on areas to focus installation and support efforts. Works with the Senior Principal to ensure operations are consistent with LOB and organization wide goals. Identifies and solves highly complex and critical systems related issues to meet the objectives for the corporation. Participates in software strategic planning and consulting on platform selection, version implementation, software product recommendation, and usage of enhanced functionality. Tracks installations, modifications, and support efforts to ensure goals are being met. Implements, integrates and provides support for middleware software in a multi-tiered, multi-platform environment, dealing with development projects that have a broad impact across the organization. Provides consulting expertise in identifying and solving highly-critical middleware issues and serves as a resource on middleware issues related to policies and standards that impact middleware specialist and application support staffs across the corporation. Communicates with internal and external system users to address concerns and make sure that technical issues are dealt with appropriately. Manages ticket queues and handles highly complex escalated issues. Analyzes repeat incident patterns to identify opportunities for cost reduction and productivity enhancements. Manages relationships with vendors and works with them to ensure efficient incident resolution. Contributes to the achievement of area objectives. Responsible for budget activity.

#LI-LS Qualifications

Bachelor's degree in computer science or a related discipline, or equivalent work experience is required, advanced degree is preferred.

Ten to twelve (1012) years of related infrastructure experience is required.

Three or more (3+) years of related operations and/or service delivery experience in an enterprise environment.

Experience in the securities or financial services industry is a plus.

Experience in the areas of server installations in datacenters, surrounding process areas and capacity management (pipeline) of new hardware into the datacenters.

Strong knowledge and background in IT Support, Cabling and datacenter management.
For over 230 years, the people of BNY Mellon have been at the forefront of finance, expanding the financial markets while supporting investors throughout the investment lifecycle. BNY Mellon can act as a single point of contact for clients looking to create, trade, hold, manage, service, distribute or restructure investments & safeguards nearly one-fifth of the world's financial assets. BNY Mellon remains one of the safest, most trusted and admired companies. Every day our employees make their mark by helping clients better manage and service their financial assets around the world. Whether providing financial services for institutions, corporations or individual investors, clients count on the people of BNY Mellon across time zones and in 35 countries and more than 100 markets. It's the collective ambition, innovative thinking and exceptionally focused client service paired with a commitment to doing what is right that continues to set us apart. Make your mark: bnymellon.com/careers.

Client Technology Solutions provides our business partners with client-focused, technology-based solutions. These enhance their ability to be successful through world-class software solutions and leading-edge infrastructure. Client Technology Solutions provides employees with the tools and resources to enhance their professional qualifications and careers.

BNY Mellon is an Equal Employment Opportunity/Affirmative Action Employer.
Minorities/Females/Individuals With Disabilities/Protected Veterans.

Primary Location: United States-Tennessee-Nashville
Internal Jobcode: 45196
Job: Information Technology
Organization: Technology Services Group-HR06725
Requisition Number: 1716291"
Data Analyst II,Microsoft,"Are you someone with a passion for data, analytics, insights and technology? Do you want to be part of a team developing actionable insights to help the organization make impactful business decisions? If you excel in blending quantitative analysis with strategy development and want to influence the future of the Office business at Microsoft, then this position is for you.

The Office Insights, Data Engineering and Analytics Team is looking for a passionate, creative, analytical and experienced Relationship Marketing Data Analyst Leader who loves big data, curious to explore and unveil insights and create a business story and reporting to impact key business decisions.

Our team provides the data and insights needed to run and grow the Office business. This work includes:

Building and leading a marketing analytics team

Providing holistic, customer and business insights

Providing data and insights for modern marketing and sales

Managing RM campaign planning, baselining, and sizing analyses

Managing post-campaign lift analyses

Delivering data sets and exploration tools that democratize business and customer insights for the Office Product Group, marketing and our field. This includes common data models and architecture, and data governance.

Providing actionable intelligence and analytics that will make our products and services better and grow our business. This includes predictive modeling, machine learning models and recommenders

This position is a key role that drives insights about our business model and health, pre-sales and post-sales activity of our users, and acquisition and engagement efforts. The scope includes understanding and monitoring key metrics and ongoing analyses to inform overall strategy for acquisition, usage, and retention of our users. Also includes stakeholder engagement and close collaboration with and leveraging efforts of data scientists, data engineers and other data analysts of the OFFICE Insights, Data Engineering and Analytics team.

You are driven, self-directed, entrepreneurial and focused on delivering the right results. To be successful in this role, you must have strong skills in written and oral communications, a can-do attitude and the willingness to tackle hard problems in innovative ways. You also thrive in a team environment that values cross team collaboration and building on the success of others. You have a history of building healthy, high performing teams and strong partnerships.

Responsibilities:
Partner with different functions / domains of the Microsoft businesses (e.g. Relationship Marketing, Marketing Automation, Data Science/Engineering etc.) to drive effective campaign experiments

Provide actionable insights on campaign experiments for various products across its lifecycle via outcome and delivery health campaign reports

Create/Modify new and existing campaign metrics in order to measure the effectiveness across commercial, consumer and services business

Enable Relationship Marketers to take business decisions to personalize customer experience through the learnings from experiments

Measure the effectiveness of campaigns through A/B testing methodology which includes and not restricted to statistical significance test for one tail and two tail tests

Connect findings and recommendations to business initiatives and collaborate with key stakeholders at various management levels to drive actions maximizing revenue

Deliver in depth analysis on business performance (trends, funnels) and drivers, as well as forecasts and targets

Define, track and report key metrics to assess overall business health

Design and build dashboards based on business requirements to enable ongoing monitoring of campaigns and business health through Monthly Business Reviews (MBRs)

Partner with analytics, data science, and data engineering partners for data structures, data availability, and statistical model development

Work closely with data engineering to deliver requirements for data infrastructure

Guide and develop data accuracy via UAT

Basic Qualifications:
Min 3+ years analytics experience

Bachelor degree in Statistics/Math/Computer Science/Economics/Business or Engineering

Preferred Qualifications:
Advanced working knowledge of large data manipulation and data mining using SQL

Advanced Excel and PowerPoint skills, familiarity with Power BI preferred

3+ years hand on and management experience in analytics with proven quantitative orientation

Bachelors degree in computer science or a quantitative field required/Masters degree preferred

Experience in Relationship Marketing Sample Size Calculation, Segmentation Criteria, Organic Growth Analyses and Campaign Lift Analyses using statistics

Advanced working knowledge of large data manipulation and data mining using SQL/Cosmos

Advanced Excel and PowerPoint skills, as well as visualization tools (familiarity with Power BI preferred)

Prior experience in B2B marketing acquisition, campaigns, marketing automation, content marketing, and
digital marketing a plus

Exceptional problem solving, technical and data analysis skills

Effectively drive stakeholder management, project planning and team collaboration experience

Good written and verbal communication and presentation skills

Be self-driven, and show ability to deliver on ambiguous projects with incomplete or dirty data

Ability to lead in a team environment that promotes collaboration

Experience with marketing automation and CRM systems preferred

Experience with Data Lake infrastructures (Cosmos, Hadoop) and access preferred

Microsoft is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity or expression, religion, national origin or ancestry, age, disability, marital status, pregnancy, protected veteran status, protected genetic information, political affiliation, or any other characteristics protected by local laws, regulations, or ordinances. If you need assistance and/or a reasonable accommodation due to a disability during the application or the recruiting process, please send a request to askstaff@microsoft.com.

#OfficeDataScience"
Database Engineer,"Title Source, Inc.","Title Source is one of the largest providers of title insurance, property valuations and settlement services in the nation. The company is an authorized agent of the highest-rated title insurers in the industry, and its solutions power many of the nations largest residential lending institutions. Title Source is a preferred provider to five of the top 20 Fortune 100 companies and many of the largest residential mortgage lenders. The company is based in Detroit and retains regional operating centers in Ohio, California, Pennsylvania and Texas.
What You'll Do/Need
The Database Engineer uses versatile database technology expertise to build databases that are high-quality, scalable and high-performance to meet the demands of our rapidly growing and ever-changing environment. This team member works with Software Developers and Business Analysts to implement optimized database solutions to support front-end applications. Database engineers are responsible for database designing, supporting existing databases and performance tuning.
Responsibilities
Ensure quality data creation, presentation and data flow across systems
Responsible for database logical and physical design and performance tuning on queries
Ability to provide data integration solutions across systems
Assist in writing complex stored procedure and optimize execution efficiency in MS SQL Server environment
Participate in code and design review
Work closely with application teams and business owners
Strong understanding of business logic and processes
Have high-level knowledge on nonstructural data store concepts and data strategies
Research and evaluate third-party tools to increase efficiency
Participates in regular on-call rotation
Requirements
Experience with various version of MS SQL Server
Understanding of database design/architecture in a Microsoft SQL Server environment
Basic database administration operations experience in a Microsoft SQL Server environment
Knowledge of Transact SQL
Data modeling, query design and optimization, stored procedures
Experience with stored procedures, functions, scripting etc.
Ability to quickly understand large data models
Ability to work with and provide technical leadership to other team members
Whatll Make You Special
Azure and NoSQL technologies
Knowledge related to computer hardware, including networks and storage devices (SANS)
Experience creating SSIS packages for data integration
What You'll Get
Excellent benefits package that includes a 401(k) match, medical/dental/vision insurance and much more
Opportunities to participate in professional and personal development programs, including personal empowerment coaching, leadership training and ongoing personal growth training
Other incentives, contests and rewards, including trips, event tickets, cash prizes and more
Why Were Different
Meet the anti-corporate culture of Title Source, where there's no daunting hierarchy, ""boss"" is a four-letter word, and if you work hard, you're the one who'll reap the rewards, both personally and professionally. More than any other place you'll work, were dedicated to honing your skills, helping you grow and making sure you have plenty of fun while youre at it."
Big Data Engineer,CapTech Consulting,"Job Description

CapTech Big Data Engineers are tasked with designing and implementing big data solutions for our clients. CapTech employees enjoy a collaborative environment and have many opportunities to learn from and share knowledge with other CapTech developers, architects, and our clients.
Specific responsibilities for the Big Data Engineer position include:
Design, develop, document, and test big data solutions.
Understand the challenges being addressed by an engagement and collaborate with team members and clients to deliver a technical solution that meets the unique needs of our clients.
Create quality deliverables to communicate technical solutions to appropriate audiences.
Learn continuously, leveraging CapTechs training resources and self-directed training, sharing knowledge and skills with others.
Provide mentoring and leadership to more junior resources. Qualifications

Specific qualifications for the Senior Big Data Engineer position include:
Demonstrated growth over 5+ years experience working as a data engineer.
High level understanding of big frameworks.
Development experience with Big Data/NoSQL platforms, such as Hbase, MongoDB or Apache Cassandra.
Expert knowledge of SQL and NoSQL tools.
Knowledge of MapReduce and MapReduce generating tools like Pig or Hive.
Experience with message buses or real-time event processing platforms is a plus.
Java development experience.
Scripting language experience (Perl, Python etc.).
Understanding of NoSQL data modeling.
Knowledge of how to assess the performance of data solutions, how to diagnose performance problems, and tools used to monitor and tune performance.

Additional Information

We offer challenging and impactful jobs with professional career paths. All CapTechers can keep their hands on technology no matter what position they hold. Our employees find their work exciting and rewarding in a culture filled with opportunities to have fun along the way.
At CapTech we offer a competitive and comprehensive benefits package including, but not limited to:
Competitive salary with performance based bonus opportunities
Single and Family Health Insurance plans, including Dental coverage
Short-Term and Long-Term disability
Matching 401(k)
Competitive Paid Time Off
Training and Certification opportunities eligible for expense reimbursement
Team building and social activities
Mentor program to help you develop your career

At this time, CapTech cannot transfer nor sponsor a work visa for this position. Applicants must be authorized to work directly for any employer in the United States without visa sponsorship.

Candidates must be eligible to work in the U.S. for any employer directly (we are not open to contract or corp to corp agreements).
CapTech is an equal opportunity employer.
CapTech is a Drug-Free work place.
Candidates must have the ability to work at CapTechs client locations.
All positions include the possibility of travel.
CapTech has not contracted/does not contract with any outside vendors in its recruitment process. If you are interested in this position, please apply to CapTech directly."
Database Engineer,Quicken Loans,"Title Source is one of the largest providers of title insurance, property valuations and settlement services in the nation. The company is an authorized agent of the highest-rated title insurers in the industry, and its solutions power many of the nations largest residential lending institutions. Title Source is a preferred provider to five of the top 20 Fortune 100 companies and many of the largest residential mortgage lenders. The company is based in Detroit and retains regional operating centers in Ohio, California, Pennsylvania and Texas.
What You'll Do/Need
The Database Engineer uses versatile database technology expertise to build databases that are high-quality, scalable and high-performance to meet the demands of our rapidly growing and ever-changing environment. This team member works with Software Developers and Business Analysts to implement optimized database solutions to support front-end applications. Database engineers are responsible for database designing, supporting existing databases and performance tuning.
Responsibilities
Ensure quality data creation, presentation and data flow across systems
Responsible for database logical and physical design and performance tuning on queries
Ability to provide data integration solutions across systems
Assist in writing complex stored procedure and optimize execution efficiency in MS SQL Server environment
Participate in code and design review
Work closely with application teams and business owners
Strong understanding of business logic and processes
Have high-level knowledge on nonstructural data store concepts and data strategies
Research and evaluate third-party tools to increase efficiency
Participates in regular on-call rotation
Requirements
Experience with various version of MS SQL Server
Understanding of database design/architecture in a Microsoft SQL Server environment
Basic database administration operations experience in a Microsoft SQL Server environment
Knowledge of Transact SQL
Data modeling, query design and optimization, stored procedures
Experience with stored procedures, functions, scripting etc.
Ability to quickly understand large data models
Ability to work with and provide technical leadership to other team members
Whatll Make You Special
Azure and NoSQL technologies
Knowledge related to computer hardware, including networks and storage devices (SANS)
Experience creating SSIS packages for data integration
What You'll Get
Excellent benefits package that includes a 401(k) match, medical/dental/vision insurance and much more
Opportunities to participate in professional and personal development programs, including personal empowerment coaching, leadership training and ongoing personal growth training
Other incentives, contests and rewards, including trips, event tickets, cash prizes and more
Why Were Different
Meet the anti-corporate culture of Title Source, where there's no daunting hierarchy, ""boss"" is a four-letter word, and if you work hard, you're the one who'll reap the rewards, both personally and professionally. More than any other place you'll work, were dedicated to honing your skills, helping you grow and making sure you have plenty of fun while youre at it.
Title Source is an equal-opportunity employer.
Apply/Share Apply for this job onlineApply Share Refer this job to a friendRefer Share on your newsfeed Stay Connected Not ready to apply? Click here to create a profile and stay connected with Quicken Loans and our Family of Companies. Share this opportunity:"
Software Engineer,"Lexmark International, Inc.","Lexmark International is seeking an engineer to join our manufacturing execution software team. This position will require a strong ability to coordinate within a team organization with other programmers and engineers. Duties of this position will include:
Design and implementation of software control systems for new chemical plants.

Support and upgrade of existing process control software written primarily in C/C++ to ensure ongoing operation of the manufacturing plants.

Updating and modifying web based data reporting and recording systems, primarily written in PHP and Javascript.

Development and execution of continuous improvement projects.

One must have strong written and verbal communication skill to effectively interface with all levels of the organization and product documentation that is usable on the manufacturing shop floor. The ideal candidates will have experience in process control systems and understand development and execution of systems that aid in a high level of product quality and safety.

Required Skills:
B.S. in Electrical Engineering, Computer Engineering, Computer Science, or other technical degree.

Understanding of at least one of the following: Perl, PHP, Ruby, ASP.NET, C, C++, C#, Java.

Understanding of structured databases, such as MySQL, MariaDB, PostgreSQL, or MS SQL.

Experience with both Windows and Linux operating systems. Ability to administer systems and networks would be considered a plus.

Strong interpersonal and problem solving skills.

Preferred Skills:
Minimum 3+ years related experience working in an industrial manufacturing capacity.

Proficiency with multiple web-based programming languages (PHP, Perl, JavaScript), as well as HTML and CSS. Experience with development of MVC and Javascript frameworks in a plus.

Experience programming real-time systems in a UNIX/Linux environment for highly automated, state of the art manufacturing facilities; prior experience with C or C++ would be very desirable.

Familiarity with manufacturing process control system schemes. Understanding of electrical and/or P&ID drawings to provide support and improvement projects would be a strong attribute.

Ability and willingness to support up to 20% travel internationally (Juarez, Mexico) to support Lexmark manufacturing.

General Computing Skills Used in this Position:
C, C++, MySQL, PHP, HTML5, CSS3, Javascript, Linux scripting and administration, Motion Control, TCP/IP and Ethernet, Serial Communication (particularly RS422/485)"
Data Science Engineer,UPMC,"Description Job Purpose:
At UPMC Enterprises, we help create health care innovations that will impact the lives of patients in meaningful, lasting ways.
The Senior Natural Language Processing and Machine Learning (NLP/ME) Engineer will perform both exploratory and targeted algorithm development and research to develop cutting-edge NLP applications that help change the face of modern healthcare.

At UPMC Enterprises, we help create health care innovations that will impact the lives of patients in meaningful, lasting ways.
The Data Science Engineer will perform both exploratory and targeted algorithm development and research to develop cutting-edge NLP applications that help change the face of modern healthcare.

Design, develop, test, and maintain NLP applications using supervised and unsupervised methods.
Propose appropriate existing statistical language models/methodologies for specific NLP use cases
Propose accuracy measures and validation criteria for the language models
Implement and evaluate proposed model/methodology
Work with product managers, partners and peers to formulate the language analysis problems
Work with an existing, experienced inter-disciplinary healthcare NLP team to define and deliver new products
Communicate the results and methodology effectively within the team
Promote and participate in professional self-development to stay up-to-date with new technologies and development approaches within the industry
Participation in the full software development cycle using Agile methodology
Implementation of Packaged Solutions: Configuration & Testing
Technical competency, ability to find creative solutions to difficult problems and operate within a flexible, fast-paced development environment dedicated to converting advanced concepts in natural language processing and machine learning research into real-world functional solutions.
Qualifications Educational/Knowledge Requirements:
*

*
Educational background in relevant field (Computer Science, Applied Math, Statistics, Operations Research, Physics, or other quantitative fields) or has acquired core software development, natural language processing, and machine learning skills and knowledge via practical experience

*
Minimum 1-3 years of relevant experience in natural language processing and machine learning.

*
Fluency in R or Python, and Java.

*
Experience in statistical and machine learning NLP methods.

*
Ability to write well documented and clean code using software engineering best practices

Licensure, Certifications, and Clearances:
UPMC is an equal opportunity employer. Minority/Females/Veterans/Individuals with Disabilities"
Software Engineer,Google,"Google aspires to be an organization that reflects the globally diverse audience that our products and technology serve. We believe that in addition to hiring the best talent, a diversity of perspectives, ideas and cultures leads to the creation of better products and services.

Google's software engineers develop the next-generation technologies that change how billions of users connect, explore, and interact with information and one another. Our products need to handle information at massive scale, and extend well beyond web search. We're looking for engineers who bring fresh ideas from all areas, including information retrieval, distributed computing, large-scale system design, networking and data storage, security, artificial intelligence, natural language processing, UI design and mobile; the list goes on and is growing every day. As a software engineer, you will work on a specific project critical to Googles needs with opportunities to switch teams and projects as you and our fast-paced business grow and evolve. We need our engineers to be versatile, display leadership qualities and be enthusiastic to take on new problems across the full-stack as we continue to push technology forward.

Google is and always will be an engineering company. We hire people with a broad set of technical skills who are ready to take on some of technology's greatest challenges and make an impact on millions, if not billions, of users. At Google, engineers not only revolutionize search, they routinely work on massive scalability and storage solutions, large-scale applications and entirely new platforms for developers around the world. From AdWords to Chrome, Android to YouTube, Social to Local, Google engineers are changing the world one technological achievement after another.

Responsibilities
Design, develop, test, deploy, maintain and improve software.
Manage individual project priorities, deadlines and deliverables.

Qualifications Minimum qualifications:
BS degree in Computer Science, similar technical field of study or equivalent practical experience.
Software development experience in one or more general purpose programming languages.
Experience working with two or more from the following: web application development, Unix/Linux environments, mobile application development, distributed and parallel systems, machine learning, information retrieval, natural language processing, networking, developing large software systems, and/or security software development.
Working proficiency and communication skills in verbal and written English.

Preferred qualifications:
Masters, PhD degree, further education or experience in engineering, computer science or other technical related field.
Experience with one or more general purpose programming languages including but not limited to: Java, C/C++, C#, Objective C, Python, JavaScript, or Go.
Experience developing accessible technologies.
Interest and ability to learn other coding languages as needed."
Software Engineer,MIT Lincoln Laboratory,"The Advanced RF Techniques and Systems Group develops and demonstrates new algorithms, architectures, and hardware implementations for processing signals for a broad range of RF system applications, including advanced wireless communications and signals intelligence. The group's core competencies include developing algorithms and system concepts for communication, signal detection, localization, and classification in challenging environments, as well as hardware implementations and prototypes of these advanced RF systems. The group has expertise in adaptive antenna array processing, signal detection and estimation, pattern recognition, multichannel communications, wideband sampling techniques, antenna and RF subsystem design, and systems analysis. The staff members have advanced degrees in electrical engineering, physics, and applied mathematics. To support concept development and evaluation, the group conducts a significant amount of field tests and data collection campaigns, and collaborates with other groups within the division and throughout the Laboratory in real-time prototype implementation, experiment execution, and systems analysis.

Group 62 is seeking a candidate for a Software Engineer role. The individual will architect and implement software for prototype communications systems. The candidate's work will involve decomposition of newly developed communications algorithms into executable software tasks, oversight of small software teams for efficient implementation, and support of prototype system demonstrations. Occasional travel to field sites to participate in demonstrations may be required.

Qualifications: MS in Computer Science, Computer Engineering, Electrical Engineering, or similar technical field is required. In lieu of an MS, a BS with 3+ years of directly related experience will be considered. Knowledge of modern software development techniques is required. Familiarity with digital signal processing techniques for communications is desired.

MIT Lincoln Laboratory is an Equal Employment Opportunity (EEO) employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, veteran status, disability status, or genetic information; U.S. citizenship is required."
Software Engineer - Back-End Data Engineer,The Home Depot,"POSITION PURPOSE

The Software Engineer is responsible for joining a product team and contributing to the software design, software development, and overall product lifecycle for a product that our users love. The engineering process is highly collaborative. Software Engineers are expected to pair on a daily basis as they work through user stories and support products as they evolve.

In addition, Software Engineers may be involved in product configuration, performance tuning and testing as well as production monitoring.

As a Software Engineer, you will be part of a team with more experienced engineers to help build and grow your skills while you create, support, and deploy production applications.

MAJOR TASKS, RESPONSIBILITES AND KEY ACCOUNTABILITIES

60% - Delivery & Execution:
Collaborates and pairs with other product team members (UX, engineering, and product management) to create secure, reliable, scalable software solutions
Works with Product Team to ensure user stories that are developer-ready, easy to understand, and testable
Writes custom code or scripts to automate infrastructure, monitoring services, and test cases
Writes custom code or scripts to do ""destructive testing"" to ensure adequate resiliency in production
Configures commercial off the shelf solutions to align with evolving business needs
Creates meaningful dashboards, logging, alerting, and responses to ensure that issues are captured and addressed proactively

20% - Support & Enablement:
Fields questions from other product teams or support teams
Monitors tools and participates in conversations to encourage collaboration across product teams
Provides application support for software running in production
Proactively monitors production Service Level Objectives for products
Proactively reviews the Performance and Capacity of all aspects of production: code, infrastructure, data, and message processing

20% Learning:
Participates in learning activities around modern software design and development core practices (communities of practice)
Proactively views articles, tutorials, and videos to learn about new technologies and best practices being used within other technology organizations
Reviews and discusses code from more senior engineers to understand best practices and design patterns
Increases business acumen by learning about other parts of the business

NATURE AND SCOPE

Typically reports to the Software Engineer Manager or Sr. Manager.

ENVIRONMENTAL JOB REQUIREMENTS

Environment:
Located in a comfortable indoor area. Any unpleasant conditions would be infrequent and not objectionable.

Travel:
Typically requires overnight travel less than 10% of the time.

Additional Environmental Job Requirements:
MINIMUM QUALIFICATIONS

Must be eighteen years of age or older.
Must be legally permitted to work in the United States.

Additional Minimum Qualifications:
Experience in an object oriented programming language (preferably Java)
Must be legally permitted to work in the United States

Education Required:
The knowledge, skills and abilities typically acquired through the completion of a high school diplomas and/or GED.

Years of Relevant Work Experience: 0 years

Physical Requirements:
Most of the time is spent sitting in a comfortable position and there is frequent opportunity to move about. On rare occasions there may be a need to move or lift light articles.

Additional Qualifications:
Preferred Qualifications:
0-3 years of relevant work experience
Exposure to writing SQL queries against a relational database
Experience with Big Data solutions such as BigQuery
Exposure to REST and effective web service design
Experience in a modern programming language such as Java
Exposure to destructive testing methodologies and tools such as JUnit

Knowledge, Skills, Abilities and Competencies:
Cultivates Innovation: Creating new and better ways for the organization to be successful
Collaborates: Building partnerships and working collaboratively with others to meet shared objectives
Communicates Effectively: Developing and delivering multi-mode communications that convey a clear understanding of the unique needs of different audiences
Drives Results: Consistently achieving results, even under tough circumstances
Global Perspective: Taking a broad view when approaching issues; using a global lens
Interpersonal Savvy: Relating openly and comfortably with diverse groups of people
Manages Ambiguity: Operating effectively, even when things are not certain or the way forward is not clear
Nimble Learning: Actively learning through experimentation when tackling new problems, using both successes and failures as learning fodder
Self-Development: Actively seeking new ways to grow and be challenged using both formal and informal development channels
Situational Adaptability: Adapting approach and demeanor in real time to match the shifting demands of different situations"
Big Data/Hadoop Engineer,Capgemini,"Short Description

About Capgemini
With more than 193,000 people in over 44 countries, Capgemini is one of the world's foremost providers of consulting, technology and outsourcing services. The Group reported 2016 global revenues of EUR 12.539 billion. Together with its clients, Capgemini creates and delivers business and technology solutions that fit their needs and drive the results they want. A deeply multicultural organization, Capgemini has developed its own way of working, the Collaborative Business ExperienceTM, and draws on Rightshore, its worldwide delivery model. Learn more about us at http://www.capgemini.com . Rightshore  is a trademark belonging to Capgemini

Title: Senior Big Data/Hadoop Engineer

Location: Tampa, FL, United States

Description:
At least 7 Years of overall IT experience in data warehousing and 3+ years of experience in Big data implementation.

Job Requirements:
Good knowledge of Big Data architecture patterns, design patterns, estimation techniques, performance tuning and trouble shooting
Hand on experience with Cloudera technology stacks: Sentry, Navigator and Cloudera manager
Big data development and management experience on Hadoop platform including HDFS, Hive, Impala, HBase, Solr.
Experience of develop and maintain ETL jobs for data warehouses/marts
Experience of building solutions for streaming applications is a plus
Experience with data modeling, complex data structures, data processing, data quality and data lifecycle.
Experience in Unix shell scripting, batch scheduling and version control tools.
Knowledge / Expertise in Lambda architecture and real time / near real time implementation
AWS Lambda function implementation experience or Google BigQuery expertise will be a big plus
Willing to work on-call support over weekends

Capgemini is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship."
Machine Learning Engineer,eBay Inc.,"Looking for a company that inspires passion, courage and imagination, where you can be part of the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If youre interested in joining a purpose driven community that is dedicated to creating an ambitious and inclusive workplace, join eBay  a company you can be proud to be a part of. Description Looking for a company that inspires passion, courage and imagination, where you can be part of the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If youre interested in joining a purpose driven community that is dedicated to creating an ambitious and inclusive workplace, join eBay  a company you can be proud to be a part of. Work as a machine learning engineer in the Shopping Experience Applied Research organization at eBay. Work involves performing top down and bottom up applied research in the areas of information retrieval, natural language processing, data insights and analytic and graph processing. Work also involves building production facing user components which heavily rely on data mining and machine learning. The candidate is expected to be able to communicate with technical and non-technical audiences including leadership, quality engineering, software engineering and product as well as program management. Shopping Experience Applied Research Team at eBay is looking for a strong applied researcher / machine learning engineer. The team works on heterogeneous data sets (behavioral, transaction and crawled data) and focuses on solving applied problems using Natural Language Processing, Text Mining, Data Mining & Machine Learning. The ideal candidate will have a nice blend of science and engineering skills, proven track record of solving critical business problems through data science and strong analytical/quantitative and engineering skills. The candidate will be expected to be strong at communication and capable of cross group collaborations. Experience in several of Spark/Hadoop, information extraction, text mining, information retrieval, machine learning, NLP is highly desirable. eBay is one of the largest online marketplaces in the world servings 100's of millions of customers. These customers engage with the platform and buy the most diverse merchandise from sellers all over the world. The inventory ranges from a consumer selling her used t-shirt to some iconic merchandise sold by a few of the biggest brands on the planet. Due to the diverse nature of our sellers and corresponding inventory, we have a treasure trove of unstructured offers. The Shopping Experience Applied Research Team's charter is to conduct applied research in various domains of shopping experience. The problems span recommendation systems (search/browse recommendations, top picks, evidence signals), graph mining, product reviews classification and ranking, large-scale duplicate detection as well as competitive analytics and data-driven tools. What are we looking for? The ideal candidate has a nice blend of engineering and science skills. Previous experience with either IR, NLP, text mining, machine learning or big data mining is highly desirable. Passion for leveraging technical solutions aligned with long term strategy with incremental deliverable outputs would be appreciated. Strong interpersonal communication and collaboration skills Ability to work on data mining , data science projects with application engineering, quality engineers and product management. Ability to mentor other data scientists and engineers. Passion to stay on the cutting edge of Data Science eBay is a Subsidiary of eBay. This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies View our privacy policy View our accessibility info eBay Inc. is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, national origin, sex, sexual orientation, gender identity, veteran status, and disability, or other legally protected status. If you are unable to submit an application because of incompatible assistive technology or a disability, please contact us at talent@ebay.com . We will make every effort to respond to your request for disability assistance as soon as possible. For more information see: EEO is the Law Poster EEO is the Law Poster Supplement
R0021675"
Data Analyst,"Milestone Technologies, Inc.","Overview
Milestone is looking for a passionate Data Analyst to turn data into proof of concept work for our client's Facilities, Culinary, and Security (FCS) Operations group. The Data Analyst is able to support FCS turn data and information into insightful solutions that help align the client's business units and over all goals. The client group works on tools, technology, processes, budget, and planning. The ideal candidate takes ownership of the lifecycles and different stages of analysis and design, development of analysis, reporting, and continuous improvement efforts. This position requires excellent interpersonal skills, proven work experience in data analytics and the ability to work closely with a variety of key team members such as Developers and Engineers.

Responsibilities
Coordinate data analytics requests between Facility, Culinary, & Security (FCS) business partners and IT
Gather and document requirements from FCS teams and align on visualization expectations
Drive data analytic projects from initial ask, scope definition, testing, development, and deployment, and support
Develop and drive FCS self-service dashboards and analytics modeling
Generate monthly reporting and ad hoc analysis to support leadership decision-making
Drive data accuracy and consistency across teams by identifying gaps and issues in data sources and dashboards
Organize project charters, scorecards, and other documentation for the overall roadmap

Qualifications Experience:
3+ years experience in data analytics
Proficiency in Tableau, excel and other analytical applications
Ability to handle scope conflict and timeline requests
Extremely detail driven
Strong communication skills and experience working in a tech corporation
Excellent interpersonal skills, including relationship building and collaboration within a diverse, cross-functional team
Ability to apply logic and reason to solve complex problems
Understanding of real estate, facility, or transportation operations a plus
Experience in SQL (Oracle, Hive, MySQL) a plus
Education:
Bachelor's degree preferred."
"Data Engineer, Data Warehouse Team",The New York Times,"The New York Times is seeking an inventive and motivated data engineer to join the Data Warehouse team. In this role, you will build critical data infrastructure that surfaces data and insights across the company.
About the Team
The Data Warehouse team is part of the Data Engineering group at the New York Times. The team collaborates with analysts, product managers, the marketing and finance teams, and other specialists to deliver data solutions used for analysis and reporting. We are focused on key business goals like increasing our digital subscriber base, understanding the lifecycle of subscribers, and retaining our print subscriber base.
Currently, the team is re-thinking our subscription and financial data warehouses and integrating them with Spark, and Google Cloud services like Dataflow, Pub/Sub, and BigQuery, and more. We have fun, work hard, and we take our responsibilities very seriously.
Were part of the technology organization at the NYTimes. Check out the Open blog ( https://open.blogs.nytimes.com ), , which is written by engineers and technical team members, and follow @nytdevs on Twitter ( https://twitter.com/nytdevs ) to see what were up to.
About the Job
As a data engineer on the Data Warehouse team, you will:
Modernize our approaches to ingest, transform, store, and surface meaningful data across the company
Build fault tolerant and accurate pipelines
Design and develop data models
Transition existing data warehouses to Google Cloud Platform
Establish systems of record for print and digital subscription data
Provide production support
Document processes and standard operating procedures
About You
Were looking for someone who loves solving data problems, and can present their ideas to the team and take them to all the way to production. You are excited about data and motivated to learn new technologies.
You are comfortable collaborating with engineers from other teams, business owners, and data analysts. You are able to own and shape your technical domain area and move the related business goals forward.
You have an attention to detail, and are capable of dedicatedly analyzing changes to data architectures and reporting processes, and determining downstream effects and potential impacts to business owners. Overall, you take great pride and ownership of your work.
You are passionate about the New York Times and believe in its mission.
Qualifications
5+ years experience working as an expert in Java and Python
5+ years experience building batch and streaming data pipelines and proficiency with tools such as Spark, Airflow, and cloud-based data services like Googles BigQuery, Dataproc, and Pub/Sub
2+ years experience running and supporting a production enterprise-wide data platform
Experience with source control management tools, like Git and GitHub
Experience with automation, testing, build tools, and release engineering
Bachelors or Masters degree in Computer Science, software engineering, or a related field
Bonus: Working knowledge of machine learning
A few reasons why you would want to work at The New York Times:
Every few months, we hold a hack week that gives our developers the opportunity to explore ideas that might not otherwise make it on the product roadmap.
We are committed to career development. We offer a formal mentoring program as well as tuition reimbursement. We have frequent panel discussions and talks by industry leaders (Sheryl Sandberg, Melinda Gates and Ta-Nahesi Coates are a few recent examples).
We believe diversity fuels innovation and creativity, and we have a variety of employee groups dedicated to fostering a diverse and inclusive workplace.
We offer a generous parental leave policy, which was recently expanded in response to employee feedback. Birth mothers receive 16 weeks fully paid, adoptive parents and birth fathers receive 10 weeks fully paid. Similarly, we offer competitive health and dental insurance, as well as 401(k) matching.
#LI-SL1
If you are an active employee at The New York Times or any affiliates (excluding INYT), please do not apply here. Go to the Career Worklet on your Workday home page and View ""Find Internal Jobs"". Thank you!
The New York Times Company is an Equal Opportunity Employer and does not discriminate on the basis of an individual's sex, age, race, color, creed, national origin, alienage, religion, marital status, pregnancy, sexual orientation or affectional preference, gender identity and expression, disability, genetic trait or predisposition, carrier status, citizenship, veteran or military status and other personal characteristics protected by law. All applications will receive consideration for employment without regard to legally protected characteristics."
Senior Data Engineer,Northwestern Mutual Life Insurance Company,"At Northwestern Mutual, we believe relationships are built on trust. That our lives and our work matter. These beliefs launched our company nearly 160 years ago. Today, they're just a few of the reasons why people choose to build careers at Northwestern Mutual.
We're strong and growing. In a company with such a long and storied history, this may be the most exciting and important time to be a part of Northwestern Mutual. We're strong, innovative and growing.
We invest in our people. We provide opportunities for employees to grow themselves, their career and in turn, our business.
We care. We make a positive difference in our communities. Nationally, thousands have benefitted from our support of research and programs to fight childhood cancer. Each year, our Foundation, employees and financial representatives donate time, talent and financial support to causes they're passionate about.

We are an equal opportunity/affirmative action employer and all qualified applicants will receive consideration for employment without regard to race, color, religion, gender identity or expression, sexual orientation, national origin, disability, age or status as a protected veteran, or any other characteristic protected by law.
What's the role?
Accesses, combines, synthesizes, and stores data from a wide range of internal and external sources for analytical purposes. Works side-by-side with data scientists to assemble datasets required to develop advanced analytical models. Works side-by-side with analysts to derive business insights from data. Works to expand the companys analytical data universe. Accountabilities:
Achieves business value by translating raw data into usable business information.
Builds data equity by creating ad-hoc, managed, and production data stores used for reporting, dashboarding, analysis, and analytics.
Understands not only where data is and how to get it, but also what the information means and how to use it.
Establishes trust relationships with client partners (both within and outside of Enterprise Data and Analytics) based on credibility, reliability, and results.
Establishes trust relationships with data owners, experts and SMEs across a wide variety of Northwestern Mutual data domains, including continual expansion of this network to grow data expertise.
Understands Northwestern Mutual data and business, able to incorporate business understanding into design and approach to achieve current value and prepare for future benefit.
Documentation, communications, reporting, risk identification, accountability, ownership, collaboration Bring Your Best! What this role needs:
5+ years of experience in data engineering
Data systems experience, designing and building larger scale data engineering solutions
Able to design and build a data model, including both relational and dimensional models
Exceptionally strong SQL skills required.
ETL tool experience required, with a strong preference for MSBI
Can execute alone but is an awesome team player
Experience with different engineering eco systems (Hadoop, Netezza, relational, big data, complex data, mainframe) a plus
Industry background a plus
Cloud experience a plus
Analytics understanding is a must, past experience a plus
Consulting experience a plus  role includes client-facing meetings and communication
Project management experience a plus
Relevant bachelor degree a plus

Req ID: 19824
Position Type: Regular Full Time
Education Experience: Bachelor's Required
Employment Experience: 6-8 years

Licenses/Certifications:
FLSA Status: Exempt
Posting Date: 12/08/2017"
Software Engineer 2 (New College Grad),DELL,"Why Work at Dell?
Endless challenges and rewards. Opportunities on six continents. A team of colleagues fueled by collaboration. All this, and a company deeply committed to integrity and responsibility.
Dell EMCs Data Protection Division provides solutions protecting data everywhere from physical to virtual to cloud. Architected for the modern software-defined data center with unparalleled VMware integration and automation, our cutting-edge data protection solutions are purpose-built for all kinds of workloads and converged infrastructure accelerating data protection and availability across diverse computing and storage environments and protecting critical business data in a fast, secure, and easy-to manage way.

Were looking for a hands-on, talented, and innovative engineer to join Dell EMCs Data protection product development team building next generation integrated protection products. The work environment is fun, extremely challenging, and gives you a great sense of satisfaction!

Must have skills:
Bachelor degree in Computer Science & Engineering
Good programming skills in programming languages like C/C++, Java, Golang and debugging skills on Linux/Windows platforms.
Knowledge of OS/System level programming
Knowledge and usage of Multi-threading primitives
Excellent verbal and written communication skills

Major Pluses:
Knowledge of Virtualization products
Any prior experience in programming with Golang
Development of RESTful web services
Knowledge of storage stacks and filesystems
1-2 years of work experience
Dell is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Dell are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Dell will not tolerate discrimination or harassment based on any of these characteristics. Learn more about Diversity and Inclusion at Dell here .
Dell is committed to the principle of equal employment opportunity for all employees and to providing employees with a work environment free of discrimination and harassment. All employment decisions at Dell are based on business needs, job requirements and individual qualifications, without regard to race, color, religion or belief, national, social or ethnic origin, sex (including pregnancy), age, physical, mental or sensory disability, HIV status, sexual orientation, gender identity and/or expression, marital, civil union or domestic partnership status, past or present military service, family medical history or genetic information, family or parental status, or any other status protected by the laws or regulations in the locations where we operate. Dell will not tolerate discrimination or harassment based on any of these characteristics. Learn more about Diversity and Inclusion at Dell here ."
Sr. Data Engineer,Invesco,"Invesco Ltd. is a leading independent global investment management firm, dedicated to helping investors worldwide achieve their financial objectives. By delivering the combined power of our distinctive investment management capabilities, Invesco provides a wide range of investment strategies and vehicles to our clients around the world. Operating in more than 20 countries, the firm is listed on the New York Stock Exchange under the symbol IVZ.

About Invesco Technology

Invesco Technology is a global organization with 1300+ employees working together to serve our business to deliver a superior investment experience. We operate under a OneTech philosophy that guides our attitudes and business decisions and creates a truly collaborative environment. Our people are our priority because working together we can develop the innovative solutions that will propel us to becoming a leaders in the asset management space in the digital and data economy.

The thing I like best about working in IT at Invesco is the people. Every day, I am inspired by who I get to work with, not just in my local office, but around the globe.  IT Manager, Houston, USA.

Job Summary:

Is the intersection of finance and technology your preferred point of convergence? Do structured, semi-structured, and unstructured sound like the type of environments you like to work in? Can you look at a Jackson Pollock of data and see the Magic Eye image behind it? Then you may be a great fit for Invescos Disruptive Technologies group.
Were seeking a Senior Data Engineer to join a fast-paced agile development team building the latest in innovative and disruptive technologies. In this position youll get to flex your brain by imagining and developing cutting edge Financial Technologies (aka FinTech) that will ultimately yield significant benefits for Invesco. Youll be expected to tinker and experiment with the latest tools and technologies to introduce leading edge products and services to market, as well as foster new ways of working. Dream job? Maybe, but caveat emptor, before you submit your application, you should know the following:

Excellence is par at Invesco. The environment is demanding, and you will be challenged. We expect that you are fluent in all things data analytics, and you also understand the nuances of financial services and our investment capabilities. Youll need to know how to approach, interpret, and extrapolate from MASSIVE data sets, because big data just doesnt seem big enough. You will be expected to contribute immediately. Youll be a part of a dynamic, collaborative team that wants to hear your input because you know the leading methods, tools, and theories in data science.

You should come with a startup mentality. You must not be afraid to fail; in fact we encourage and celebrate failure, because failure leads to excellence. However, fail fast, learn fast, and move on to the next iteration. Be fearless in your wild ideas and vocal with your input. Bring solutions, not more problems, and remember that we can if, will always get you further than we cant, because

Key Responsibilities / Duties
Build robust data pipelines on public Cloud using AWS Kinesis, Kafka or other Lambda technologies
Responsible to collect, process, and compute business metrics from activity & persisted data using Python/Spark
Process, cleanse, and verify the integrity of data used for analysis; optimize data for consumption
Build scalable OLAP backend storage for data in PB scale.
Develop data set processes for data discovery, modeling, mining, and archival
Work with business analysts and data scientists to build new analysis tools and metrics for measuring product engagement & consumability
Serve as integrator between data architects, data scientists and other data consumers
Qualifications
Work Experience / Knowledge:
5+ years of experience with top-tier firms in big data analytics, management consulting, or comparable role in corporate setting
4+ years of experience with SQL
Experience with ETL, data warehousing, and columnar databases such as Redshift
Experience with data visualization tools, a plus
Experience with big data technologies such as Hadoop, Spark, Hive / Pig, and Java / MapReduce
Experience with NoSQL databases, such as MongoDB, Cassandra, HBase, DynamoDB
Experience with data and machine learning services using Azure, Amazon Web Services (AWS), and / or Google Cloud
Familiar with Agile software development (Scrum is a plus)
DevOps knowledge is a plus
Skills / Other Personal Attributes Required:
You can bring clarity to chaos and youre comfortable working with ambiguity (e.g. imperfect data, loosely defined concepts, ideas, or goals) and translating these into more tangible outputs
Strong analytical and critical thinking skills
Autonomous personality. Well help guide you, but we wont micromanage you. We expect integrity and results. Your work and deliverables will speak for themselves
Strong written and verbal communication skills
Enjoy challenging and thought provoking work and have a strong desire to learn and progress
Ability to manage multiple tasks and requests
Must demonstrate a positive, team-focused attitude
Ability to react positively under pressure to meet tight deadlines
You listen to the input of your team members and take diverse perspectives into account to approach challenges from multiple angles
Structured, disciplined approach to work, with attention to detail
Flexible  able to meet changing requirements and priorities
Maintenance of up-to-date knowledge in the appropriate technical areas
Able to work in a global, multicultural environment
Formal Education: (minimum requirement to perform job duties)
Bachelor's or Master's Degree of Statistics, Computer Science or other similar advanced degrees from a top tier educational institution

Working Conditions:
Normal office environment with little exposure to noise, dust and temperatures
The ability to lift, carry or otherwise move objects of up to 10 pounds is also necessary
Normally works a regular schedule of hours, however hours may vary depending upon the project or assignment
Hours may include evenings and/or weekends and may include 24 hour a day on call support by pager and/or cell phone
Able and willing to travel both domestically and internationally. Frequency and duration to be determined by manager. Estimate: 10-15%

FLSA (US Only): Exempt

The above information on this description has been designed to indicate the general nature and level of work performed by employees within this role. It is not designed to contain or be interpreted as a comprehensive inventory of all duties, responsibilities and qualifications required of employees assigned to this job. The job holder may be required to perform other duties as deemed appropriate by their manager from time to time. Invesco's culture of inclusivity and its commitment to diversity in the workplace are demonstrated through our people practices. We are proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, creed, color, religion, sex, gender, gender identity, sexual orientation, marital status, national origin, citizenship status, disability, age, or veteran status. Our equal opportunity employment efforts comply with all applicable U.S. state and federal laws governing non-discrimination in employment.

Job Technology
Primary Location North America-United States-Texas-Houston-11 Greenway Plaza
Other Locations North America-United States-New York-New York-1166 Avenue of the Americas, North America-United States-California-San Francisco-101 California Street, Suite 1800
Schedule Full-time"
Entry Level Software Engineer,General Dynamics Mission Systems,"Basic Qualifications Bachelor's degree in software engineering or a related technical field, or equivalent experience is required. Agile experience preferred.

CLEARANCE REQUIREMENTS:
Department of Defense Top Secret security clearance is required at the time of hire. Applicants selected will be subject to a U.S. Government security investigation and must meet eligibility requirements for access to classified information. Due to the nature of work performed within our facilities, U.S. citizenship is required.

Responsibilities for this Position General Dynamics Mission Systems has an immediate opening for a Entry Level Software Engineer. This position provides an opportunity to further advance the cutting-edge technology that supports some of our nations core defense/intelligence services and systems. General Dynamics Mission Systems employees work closely with esteemed customers to develop solutions that allow them to carry out high-stakes national security missions.

Do you like knowing that your work has life-saving value? Do you like the squeak of dry-erase markers on a giant whiteboard, or a lively debate about a users optimal experience, or the view of the South Side slopes, or the autonomy to accomplish your goals as you see fit, or board game lunches? Do you like the kind of day where you learn so many new skills that your brain feels like it just ate Thanksgiving dinner? Sound intriguing? Viz is looking for people like you: smart, creative, and fun.

We make software that interactively visualizes data. We make software that enables massive collaboration. We make software that supports our men and women in the military, that smoothes the way of emergency responders here at home, that offers insight to researchers and data analysts. Were a passionate, close-knit group of inter-disciplinary thinkers and doers that craft truly one-of-a-kind software used all over the world, and we do it from our cozy-casual office here in Pittsburgh.

In this role, you will:
Assist in software requirements analysis, design, development and testing for software enhancements and new products
Apply the appropriate standards, processes, procedures, and tools throughout the system development life cycle to support the generation of technical engineering products
Support and develop our technical culture by being engaged, intellectually curious, friendly, motivated to solve complex problems, and inspired to deliver superior user-centered design
Participate in internal reviews of software components and systems

Candidates should have demonstrated strengths and experience in some of the following areas:
User Interface development
Computer science theory (e.g., data structures, algorithms, computational complexity)
Appreciation for user-centric design
Attention to quality
Full use and understanding of software engineering concepts, principles, and theories
Effective communication and attention to detail

Programming Skills:
Strong fundamentals in data structures and algorithms
Solid object oriented programming and software design skills
Proficient in one of the following: Java, C#, C++, or JavaScript (using object oriented techniques)
Web Services knowledge (HTML, CSS, JavaScript, jQuery, XML and related AJAX technologies)

Other beneficial experience:
Experience with version control systems
Concurrent programming and building multi-threaded systems
Memory and CPU profiling
Data modeling and writing efficient Structured Query Language (SQL) statements
Experience building large-scale mission-critical applications
Experience with distributed systems
Mobile development
Direct-manipulation user interfaces
Information visualization
HCI coursework

PREFERRED DEGREE TYPES AND EXPERIENCE: Bachelors degree in Software Engineering, Computer Science or Computer Engineering

Company Overview General Dynamics Mission Systems (GDMS) engineers a diverse portfolio of high technology solutions, products and services that enable customers to successfully execute missions across all domains of operation. With a global team of 13,000+ top professionals, we partner with the best in industry to expand the bounds of innovation in the defense and scientific arenas. Given the nature of our work and who we are, we value trust, honesty, alignment and transparency. We offer highly competitive benefits and pride ourselves in being a great place to work with a shared sense of purpose. You will also enjoy a flexible work environment where contributions are recognized and rewarded. If who we are and what we do resonates with you, we invite you to join our high performance team!"
Database Engineer,Integrity Applications Incorporated,"Integrity Applications Incorporated (IAI) is a software and system engineering company headquartered in Chantilly, Virginia with offices in California, Colorado, Michigan, Massachusetts, Pennsylvania, Hawaii, and Maryland. IAI has been selected as one of the best companies to work for in America by The Great Place to Work Institute and as one of the top employers in the DC Metro area by the Washingtonian Magazine. We are always looking for bright, innovative and talented people to join our team of highly skilled professionals. IAI offers challenging work, competitive salaries, an incentive bonus program and top notch health and welfare benefits for you and your family.

Pacific Defense Solutions (PDS), a division of IAI, has a rapidly expanding presence in Colorado Springs that supports several on-going and anticipated new to be awarded DoD programs. We are looking for a Database Engineer to support our on-going engineering activities in Colorado Springs.

The Database Engineer will determine database (DB) storage requirements, architect DB solutions, and implement these solutions for DOD customers. This DB engineer will generate system requirements, query functions, data models and commensurate schemas for design throughout the database implementation. Supports troubleshooting of DB issues and works with the onsite personnel and customer to return systems to operations. Provides assistance to CM, SW and Test communities with system trouble shooting and detailed test analysis.

Requirements:
Bachelors degree in Engineering or Computer Science with ten years of experience, or, a Masters Degree in Engineering or Computer Science with five years of experience.
Relational Database administrative experience (Sybase, Oracle, or MS SQL Server preferred).
SQL programming experience.
Proficient in Microsoft Office Outlook, Word, PowerPoint, and Excel experience.
Demonstrated initiative and successful execution of projects of multi-member teams. \
Ability to interface directly with government customers.
No relocation expenses will be covered.
Desired skills:
Solid written and verbal communication skills.
Significant experience in SW development methodology, lifecycle and process including Agile SW development.
Significant experience in DB design, architecture, and implementation
Experience in software tools development
Security Clearance: Applicants selected will be subject to a government security investigation and must meet eligibility requirements to receive a TS/SCI clearance (will sponsor clearance for eligible candidates).

Integrity Applications Incorporated is an Equal Opportunity / Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, protected veteran status or any other factor protected by law."
Data Scientist Intern - New Ventures Campus,McKinsey & Company,"Qualifications
Be currently enrolled in an academic program with expected graduation date of Dec. 2018 through Aug. 2019
Exceptional numerical and statistical ability, with excitement for applying analytics to client challenges and significant experience using analytic / database software and languages such as SAS, SQL, SPSS, R, Python, etc.
Superior critical thinking and creative problem solving skillss
Basic business intuition and clear expertise in analyses with the ability to describe analytic processes, including when and why specific approaches are favored
Strong oral and written communication skills, including the ability to communicate effectively to non-technical audiences
Ability to deliver in deadline-driven environment
Team player with a passion for coaching colleagues and clientss
Desire to build and develop knowledge for the firm, and use analytics to solve client problems

Who You'll Work With Youll work with our New Ventures team in one of our North America offices over the summer for ~ 10 weeks.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for and expands our investment in our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and were hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.Jr. Specialists have a base of expertise in a function or industry. They are staffed on engagements (largely in full time roles) and are expected to leverage expertise to solve some of the most pressing and complex issues at clients.
What You'll Do You'll conduct hands-on rigorous quantitative analysis, including getting the data, cleaning it (when necessary / relevant), and exploring it for accuracy.

You'll deploy statistical modeling and optimization techniques most suited for the business problem (using R, Python, SQL, and/or other relevant tools). With support, you'll advise client teams on analytic methodologies and approaches to address their specific needs, including discussing data collection, architecture, associated costs and trade-offs, and recommendations.

You'll interpret outputs of statistical models and results to translate input from quantitative analyses into specific and actionable business recommendations and implications. This includes providing detailed documentation of modeling techniques, methodologies, assumptions made and process steps.

You'll manages delivery of analytical solutions to client via written and verbal presentations, including capability building as appropriate. You may advance McKinseys overall knowledge base by providing analytical rigor and problem solving to our proprietary knowledge investments.

You'll begin to own an analytics work stream, manage respective tasks and client project team members/experts, while building client relationships. You'll support analytics recruiting efforts (e.g., attends recruiting events, participates in WebExes, etc.)."
Senior Data Engineer,Expedia,"Expedia

CarRentals is a small but rapidly growing company within the Expedia Inc portfolio and the premier car rental booking company on the web. CarRentals powers three different brands across multiple points of sale on a single common platform, bringing suppliers and customers together to find the right car and the best price. CarRentals offers a great opportunity to work in a startup-like environment backed by the resources, benefits, and network of a large, multi-national online e-commerce company.

As a Senior Data Engineer on the Global Analtyics Platform team , you will create data solutions related to the heart of our car rental system, including data processing for searches, booking transactions, and connections to a vast array of suppliers for inventory. You will be designing and implementing data pipelines, ETLs, data warehouses, and data tools for the organization. You will be a key contributor in transforming the existing data systems into an extensible and optimized data platform.

This role will require you to have demonstrated data platform expertise, knowledge of microservice architecture, AWS services, and operational standards to maintain a 24/7 e-commerce platform. You will work in collaboration with partners including Product Managers, Analytics, and Marketing to help design, develop, and deliver projects. You will mentor others, and help promote the broader tech organization goals.

Key Responsibilities
Design and implement: data pipeline, ETL, data model, data warehouse
Gather and process raw data at scale (including writing scripts, calling APIs, write SQL queries).
Architect data platforms.
Building data models.
Implementing Machine Learning platforms.
Work closely with our engineering team to integrate with micro services stack.
Process structured and unstructured data into a form suitable for analysis.
Support business decisions with ad hoc analysis as needed.
Designing/implementing data solutions with AWS S3, EMR, Spark, Hive, Hadoop, Tableau.
Drive the teams design methodology and data quality practices
Serve as a mentor to junior members of the data engineering team
Communicate effectively up and down the organization including to the CTO and engineering leadership team

Qualifications
8+ years of experience developing production quality code including playing an active role in all aspects of software design and delivery
Deep experience in agile/scrum software delivery practices
Experience with server technologies such as Java, Node.js, or PHP
Excellent debugging skills to support high load cloud-based production systems
Experience with microservices architecture, design, and standard methodologies with an eye towards scale
Experience with AWS services and cloud development patterns
Strong working relationships with non-technical peers and a demonstrated ability to understand the workings of the overall business
History of technical depth and recognition within previous teams as a domain authority
Advocate for product-wide coding quality and a desire to find tractable new ways to improve the groups coding standards
Experience building APIs (REST), Java services, or Docker Microservices
Experience with data pipelines using Apache Kafka, Storm, Spark, AWS Lambda or similar technologies
Expertise in any of the following programming languages such as Python, R, Java, SQL
Expertise with ETL concepts and technologies
Experience with data warehouse using RedShift or similar data stores
Familiarity with Tableau, Power BI, or other analytics tools
Strong understanding of Hadoop, Hive, MapReduce or other similar technologies
Familiarity with: Machine Learning, Deep Learning, Statistical Analysis, Real-Time Data streaming
BS or MS in Computer Science or related field, or equivalent demonstrable hands-on experience

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

LI-MS1
Expedia is committed to creating an inclusive work environment with a diverse workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. This employer participates in E-Verify. The employer will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization."
IBM Watson Health Data Scientist Apprentice,IBM,"Job Description
IBM's Tech Re-Entry program develops talented technical professionals who are looking to restart their careers after an absence from the workforce of two years or more. This paid 12 week internship enables participants to work on projects that match their expertise, interests, and abilities and could lead to full-time employment. In the same way an internship offers a guided period of exploration, the IBM Tech Re-entry internship program provides participants with an opportunity to update their skills in a work environment that may have significantly changed since their last experience. Interns will have access to the latest tools and technologies available and work alongside a multi-disciplinary team of engineers and business professionals to hone their expertise and focus on the next generation of software. The program will also provide the opportunity to experience the breadth of IBM resources while developing new relationships. Internship positions are currently available for Data Scientist in Cambridge, MA

The Watson Health Innovation team is committed to the design and development of world-class data-driven solutions for the health ecosystem. It is composed of analytics professionals in healthcare data management, Big Data analytics, predictive analytics, and solution and application design, while also working alongside software developers, research scientists, and healthcare experts.

As an IBM Watson Health Data Scientist Apprentice you will:
Assist in architecting advanced data-driven solutions, composed of predictive modeling, statistical methods and cognitive computing components, to solve complex healthcare and wellness problems
Using a combination of judgment and experience in healthcare and life science, create hypotheses and experiments to identify hidden relationships and construct new analytics methods
Participate in analyzing and mining high volume and complex healthcare data, from a variety of data sources (clinical, administrative, social, device, and etc.)
Participate in hands-on implementation of various advanced analytics models, leveraging board spectrum of statistical, machine learning and cognitive computing methods
Prototype and implement analytics solutions in collaboration with an interdisciplinary team of designers, engineers and developers
Support sales, marketing and solution deployment effort.
In order to qualify for this apprenticeship, applicant must have been out of the work force for a minimum of 2 years

Required Technical and Professional Expertise

Basic knowledge, preferably 2 years experience, in statistical, and predictive modeling experience using open source tools, preferably Python and/or R.
Experience working closely with the business to define analytics use cases and explore a variety of analytics methods to solve the business needs.
1 year experience in data management and analysis, preferably using relational database or Big Data systems.
5 years technical experience prior to exiting the workforce
Must be out of the workforce for a minimum of two years

Preferred Tech and Prof Experience

Successful candidates will possess the following key attributes:
Masters degree in statistics, predictive analytics, operations research and related engineering degrees, doctorate preferred.
2 years machine learning experience
1 year experience in data-driven healthcare, wellness of life science analytics solution design
Excel in an agile and business value focus environment
Experience in analyzing and mining complex and large-scale healthcare data
Deep expertise in using statistical and predictive modeling methods to solve business problems
Strong communication skills and comfortable working in multi-disciplinary teams
Knowledge in Big Data tools and software development is a plus

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Database Engineer,naviHealth,"Position Summary
The Database Engineer is part of the engineering team. This position is primarily responsible for the planning, execution, and maintenance of naviHealth databases, both relational and NoSQL, while supporting development engineers to develop application query leveraging best practices and leading-edge database technologies. The person should be able to write SQL query to support business defined needs using Oracle based technology/tools, like SQL/PLSQL routines, in an agile development environment.
This is a hands-on development role at early to mid-level and the ideal candidate should have a history of building fantastic enterprise-grade products quickly and with quality. We have a culture where people do well based on their merits and their ability to contribute to the success of the larger team. The right candidate will have a key role in our challenging and fun mission that values a work-hard/play-hard model.
Principal Duties and Responsibilities
Create Oracle PL/SQL packages, complex stored procedures, partitioned tables, ad-hoc queries to support development and business needs.
Assist senior database engineers with the day-to-day activities, including databases maintenance such as installation, backup, restore and security control.
Collaborate with stakeholders to plan and deploy new database releases.
Analyze query performance and perform query tuning to assist development engineers in designing and optimizing queries.
Troubleshoot production support issues in the application environments.
Work as a part of a development team to implement product enhancements.
Minimum Qualifications
Technical Skills:
2+ years of experience with development in databases using traditional SQL databases like Oracle.
1+ years of experience with development in databases using NoSQL databases like MongoDB.
1+ years of experience in DBA, data modelling, and ER schema, star schema.
Fluency with ANSI-SQL and Oracle PL/SQL.
Familiarity with database design and architecture for relational/NoSQL database is a plus.
Understanding of data analysis, quality assurance and data quality principals as applied to database is a plus.
Soft Skills:
Critical thinking abilities to take complex, ambiguous, abstract requirements and break them into smaller components, patterns, views and features.
Strong ability to communicate technical and data concepts to non-technical audiences, and business concepts to technical audiences.
Strong analytical, organizational, verbal communication and business writing skills.
About naviHealth
naviHealth partners with health plans, health systems and post-acute providers to manage the entire continuum of post-acute care. We utilize evidence-based protocols to optimize care and bundled payment methodologies to align all stakeholders. The result: optimized care and outcomes, reduced inpatient days, reduced hospital readmissions, and increased patient satisfaction.
naviHealth Values
We care about the people we serve.
We care about each other.
We care about our communities.
We embrace innovation.
We like simple.
naviHealth  is proud to be an equal opportunity/affirmative action employer. We are committed to attracting, retaining and maximizing the performance of a diverse and inclusive workforce. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status."
Data Visualization Engineer,LGS Innovations,"LGS Innovations, delivering next generation solutions that solve the most complex sensing, networking and communications challenges facing our U.S. Federal Government Customers, is seeking a Data Visualization Engineer for our Florham Park, NJ location.

In this position you will work under limited direction, develop user-focused software that maximizes the usability, scalability, trainability, and sustainability of advanced Intelligence, Surveillance, and Reconnaissance (ISR) systems for LGSs Wireless Solutions division in support of government customers.

LGS Wireless Sensing and Communications is seeking candidates for its research and technology Labs in Florham Park, New Jersey. In addition to Florham Park, New Jersey, we have rapidly growing R&D organizations in these locations: Tampa (Florida), Herndon (Virginia) and Denver (Colorado).

We are seeking the best and brightest with a passion to solve tough challenges through innovation and the application of science and technology to catalyze impactful solutions for our customers.

The WSC lab performs advanced R&D to:
Create novel wireless communications and situational awareness systems using standards based and custom communications protocols.
Create unique small size, weight and power systems for use in air, land and maritime environments
Employ advanced signal and protocol processing, channel modeling and waveform design from HF to W band and optical frequencies as well.
Join a fast-moving, high-performing multidisciplinary team with a record of success and opportunity for advancement.

LGS Innovations is a U.S.-owned company headquartered in Herndon, Virginia, with offices nationwide. We provide our employees with competitive compensation packages and a full range of benefits, including vacation, medical, dental, life insurance, a 401(k) plan, tuition assistance, Employee Assistance Program (EAP), and paid parental leave.

Roles and Responsibilities:
Transform high level user and customer requirements for system functionality into streamlined, architected graphical user interfaces that are tailored to the specific end users operational needs
Develop dynamic, vibrant user interfaces to foster smooth operation and user insights.
Combine data visualization and data science expertise to form powerful user interface solutions
Design and implement custom messaging protocols using appropriate communication tools and libraries
Interface with existing third party software and hardware components and systems such as geospatial mapping and visualization tools and packages
Optimize solutions based on performance, power and unique mission constraints
Write user manuals and test documentation supporting hands-on field-testing of systems
Travel to other LGS Locations or Customer Sites as necessary
Understand and adhere to all LGS Ethical and Compliance policies
Proactively ensure a safe work environment and adhere to LGS EH&S policies and procedures
Perform other duties as required
Obtain/retain a government security clearance at the level required to perform the duties of the position
Qualifications

Basic Qualifications:

To be considered for this position, you must minimally meet the knowledge, skills, and abilities listed below:
Bachelors degree in Computer Science, Computer Engineering or Electrical Engineering and 4-6 years of related experience or an equivalent combination of education, skills and experience
Masters degree preferred
Development and integration of software command and control protocols between Windows GUIs and embedded sensors over Ethernet (TCP/UDP), Serial, and USB and serialization techniques including Google Protobuf, XML, JSON, and proprietary binary.
Experience with configuration and development management tools such as SVN, Bugzilla, JIRA, and Confluence.
Databases such as MySQL, PostgreSQL, and MSSQL.
Working knowledge of C#, Microsoft .NET framework, and Visual Studio development environment is desirable.
Development and test installation packages such as InstallShield and NSIS.
Additional languages such as Java, C/C++, Python, Objective C, Swift and associated development frameworks are highly desirable.
Experience with automated GUI test tools is highly desirable.
Responsive Web Application development experience using HTML/CSS, AJAX, Google Web Tool Kit, Leaflet, OpenLayers, Node.js, React/Angular, JavaScript, and SQL is highly desirable.
Ability to balance requirements and objectives from multiple disparate user bases
Preferred Qualifications:

Candidates with these desired skills will be given preferential consideration:
Advanced degree preferred"
Data Science Intern,Pluvio,"About the Data Science Intern position:
REMOTE OK

No recruiting agencies or recruiters please

Do you love bringing connected data to life and making data driven decisions? Pluvio is a data driven tech company looking for a Data Science Intern to support our growing data science team. You will work day to day with our CTO and CPO to bring world-class products into production.

What Were Working On

Recommendation and Personalization

Natural Language Processing and Query Understanding

Image Processing and Understanding

Text Understanding

Large-scale Machine Learning

What You'll Do:
Work with CTO, CPO, and CEO to understand business problems and define goals

Write platform tests (unit, functional, stress, performance, etc)

Conduct ad-hoc data analysis to characterize and evaluate product features and other areas of the company; translating the data that we are collecting into concise, clear, and actionable insights

Share insights into roadblocks or inefficiencies in data across teams and educate teams on best practices and aid in future growth

What We Look For:
Strong analytical and quantitative skills; you are familiar with and are currently enrolled in a program related to techniques in Machine Learning, Data Mining, Recommender Systems, Information Retrieval, Natural Language Processing, Computational Advertising, Deep Learning and Computer Vision or related fields

Demonstrated capabilities using applied statistics in a professional or other intensive problem-solving environment with complex datasets

Experience with Python and Python Flask framework and (or desire to learn) Hadoop/Spark related Big Data technologies

Knack for identifying and correcting problems in imperfect data, understanding where issues and biases may result in an analysis

You have demonstrated the capability to review and write technical papers

You have the ability to quickly prototype ideas and can solve complex problems by adapting creative approaches

You are a strong collaborator and communicator and you make the engineers around you grow and learn

ABOUT PLUVIO:
Pluvio is re-building the way in which customers and users are understood by eCommerce and digital media companies. Were re-imagining how customers interact with brands through knowledge graphs and real-time feedback loops in order to deliver in-store quality, contextually relevant customer experiences.

We are based in NYC."
Marketing Data Engineer,Vivint Smart Home,"If you are an active Vivint employee, please apply through Workday by searching ""Find Jobs"".
If this is your first time applying you will need to create a candidate account when you click on apply. Job Description
Our mission is to redefine the home experience through intelligently designed products and services delivered to every home by people who care.

Who Are We:
Vivint Smart Home is the leading provider of smart home services in North America. Vivint delivers an integrated smart home system with in-home consultation, professional installation and support delivered by its Smart Home Pros, as well as 24-7 customer care and monitoring. Dedicated to redefining the home experience with intelligent products and services, Vivint serves more than one million customers throughout the U.S. and Canada. Vivint is the largest tech employer in Utah, a certified Great Place to Work, and one of Fast Company s Worlds 50 Most Innovative Companies for 2017.

What We Stand For:
Honesty and Integrity Come First
Do the right thing
Customer Obsession is Our Advantage
A relentless passion to serve the customer
Innovation is Essential
Todays innovation is tomorrows lifeblood
We Win Together
Individuals win games: teams win championships
Exceptional is Expected
Talk is cheap: create value, not just motion
We Give Back
Helping people is core to our DNA

Perks/Benefits:
Free catered lunch/snacks/drinks; new menu daily
Paid holidays and flexible paid time away
Your choice between Mac or PC
Employee pricing on smart home products
Casual dress code
Onsite gym
Onsite health clinic
Medical/dental/vision/life coverage
401(k) Plan & Matching

The Role

As the Marketing Data Engineer on the Digital Marketing Team, you will be responsible for building the processes that support the ingestion and consumption of data at Vivint, specifically on the Digital Marketing Team. Working closely with our Data Ops, Data Analytics, and Digital Marketing teams, you will design, build, and maintain a data warehouse platform that provides timely, accurate, and reliable data to thousands of users. Your role will be critical in defining the appropriate architecture and processes needed to build a data warehouse that is flexible, agile, reliable, responsive, and scalable.

What you will be working on:
Help build and maintain an enterprise data warehouse platform with its associated ETL processes and data architecture requirements
Responsible for designing, building, and maintaining robust, high-performing ETL processes
Implement best practices and innovative ETL solutions to provide timely, accurate, & reliable data
Evaluate, recommend, and implement proper tools & technology to achieve a high performing data warehouse platform servicing thousands of users and a broad set of use cases
Build cross functional relationships with data analytic teams and business leaders to understand their requirements and data needs
Constantly evaluate and optimize deployment & ETL processes to achieve greater efficiency and reliability

Who you will work with and what we are looking for:
Must have a passion for data and helping the business turn data into information and action
2+ years of data warehousing architecture & design experience
2+ years of ETL development experience
Ability to initiate, drive, and manage projects with competing priorities
Ability to communicate effectively with business leaders, IT leadership, and engineers
Expert in SQL, databases, and ETL development processes & tools (SQL Server preferred)
Knowledgeable in scripting languages (.net, python, Perl, etc.)
Experience with Digital Marketing Data and Platforms (Google Analytics, Google Adwords, Facebook, Adobe Analytics, Bing, etc)
Experience with big data technologies (HDFS, Hadoop, Spark, Elastic Search, Redshift, Snowflake, etc...)
Experience with Tableau or similar data visualization tool
Experience with AWS or Azure product offerings and platform
Experience with machine learning technologies (R, SparkML, AzureML, etc.)

Find out more about what it's like to work here:
http://archive.sltrib.com/article.php?id=5360131&itype=CMSID
https://www.fastcompany.com/3067476/why-vivint-smart-home-is-one-of-the-most-innovative-companies-of-2
https://www.vivint.com/company/careers/culture

#LI-PH1"
Senior Data Engineer,HD Vest,"Picture yourself adding your creativity toward game-changing innovation. Our businesses operate at the intersection of tax preparation and financial planning, and our competitive advantage lies in their unique combination. With award-winning products and services through our TaxAct and HD Vest brands, Blucora is on the forefront of financial tech, turning taxes into opportunity for our consumers, small business customers and tax professionals. We have a culture of creating market disruption; a place where big ideas, and those who think them, can flourish. If you are interested in being part of a team that dreams large, talks straight, and helps millions of customers every year build their financial future, then we should talk!

Our Data Platform team is seeking an experienced Data Engineer with software engineering and system administration skills to join our team. Our data platform utilizes a polyglot persistence pattern using SQL Server, MongoDB, ElasticStack, Redis, Hadoop and other technologies to deliver a suite of data services to support our production systems and provide analytical capabilities to our analysts and data scientists.

In this role, you will be supporting multiple components of our data platform by: working with product owners and business partners to understand the data requirements for new projects/initiatives; collaborating with technical teams to develop data solutions to support business initiatives; building and maintaining the data architecture to store the data; and finally to administer and support the data platform to ensure performance targets and system SLAs are maintained.

Major responsibilities:
Design, implementation and delivery of complete analytic solutions in AWS
Own the data architecture and infrastructure
Architect, build and maintain high performing ETL processes, including data quality and testing
Define and build technical/data architecture for data warehouse, data marts and big data solutions (including data and dimensional modeling)
Develop analytics with a mind toward accuracy, scalability and high performance
Provide technical guidance and thought leadership to other members of the team
Establish lean data governance to drive data standards and data quality. Be an evangelist in the company for data-informed thinking and decision-making.
Education and Experience:
Experience implementing and operating solutions with one or more No-SQL technologies in a production environment.
Experience in architecting and building data warehouse systems and BI systems including ETL.
Expert knowledge in modern distributed architectures and compute / data analytics / storage technologies on AWS
Deep knowledge of RDBMS (SQL Server, MySQL, etc.) and NoSQL databases such as MongoDB, DynamoDB, Cassandra
Advanced knowledge of a programming language such as Java/Python/Scala
Outstanding analytical skills, excellent team player and delivery mindset.
Experience in performance troubleshooting, SQL optimization, and benchmarking.
Strong architectural experience in context of deploying cloud-based data solutions.
Good communication (oral and written) and interpersonal skills.
Enthusiastic attention to detail.
Demonstrated intellectual curiosity and ability to acquire new skills quickly
Ideal candidate will also have:
Experience with AWS Redshift,
Experience with Alteryx, Datameer & other data analytics toolsets
Experience with data visualization tools such as Tableau, QlikView
Experience in agile development methodologies.
Knowledge of statistical concepts and their application in reporting and data mart applications.
Demonstrated ability to champion projects from beginning through implementation, meet deadlines, handle multiple priorities and perform job responsibilities accurately with minimal supervision
We offer a competitive salary, outstanding benefits package that includes medical, dental, vision. Life insurance, paid vacation and sick days, paid holidays, tuition reimbursement, and 401(k) with company match.

Qualified applicants will receive consideration for employment without regard to their race, color, religion, national origin, sex, or disability."
Software Engineer,Verizon,"Responsibilities
The person in this position will be responsible for advanced software application development for new and ongoing telecommunications and data communications projects. These efforts will be aimed at customer retention and revenue expansion.

Major responsibilities include:
Design, code, and test custom software systems for telecommunications and data communications applications.
Analyze customer requirements and develop concepts for new applications.
Coordinate with project management and other engineering teams in determining overall system solutions.
Apply object oriented techniques and Agile methodologies.
Author technical documentation as needed.
Support QA team in developing test plans.
Qualifications
Minimum of 8 years in developing complex software systems.
Significant software development experience in LINUX, Java, C/C++.
Scripting languages such as Python or Perl.
Multi-threaded software development.
Experience working with IP protocols and packet processing.
Successful track record in forming strong partnerships with teams in the business organization such as project management, QA, and other engineering groups.
Outstanding communication and analytical skills.
BS/BA degree in Computer Science or relevant engineering field.
Must be able to pass an extensive background investigation as a condition of employment.
Preferred Qualifications
Strong background working with networking technology for telecommunications and data communications applications.
Web GUI development involving JSP/Tomcat, Struts, MySQL/PostgreSQL, CSS/HTML/JavaScript.
Java IDE, J2EE, and jQuery.
Usage of source version control products such as Perforce, Git, or SVN.
Experience with OODB and Big Data (Hadoop, Vertica, etc.).
Equal Employment Opportunity
We're proud to be an equal opportunity employer- and celebrate our employees' differences, regardless of race, color, religion, sex, sexual orientation, gender identity, national origin, age, disability, or Veteran status. Different makes us better."
Data Wrangler / Data Visualization Engineer,Equifax,"Data and Analytics is a high-energy, fast-growing organization within Equifax charged with building and supporting best-in-industry data assets and analytical solutions. We deliver solutions and insights to enable success for our customers, drive growth for our business, and create value for our shareholders. At Workforce Solutions, data driven decision making is at the heart of remaining nimble while still choosing the best path forward. This position plays an integral role in being able to help make those decisions. The Data Strategy and Execution Analyst will help to provide insights into our current processes but also drive what types of insights should be taken into consideration for the future.

This individual will develop a deep understanding of the analytical need and leverage knowledge of all Equifax data assets, big data technologies, keying and linking concepts to analyze Equifax Workforce Solutions data to create insights. They will interact with all levels of our business professionals including Data Stewards, Analysts, Statisticians, IT, and Sales. Responsibilities include gathering & refining requirements, interfacing with relevant business partners across the enterprise, and executing against the data strategy roadmap. The ability to work in a matrix environment to deliver complete solutions is required.

This individual will need to possess a breadth of technical skills across data management and data visualization domains. Technical responsibilities include the identification, evaluation, linking/matching, and transformation of disparate data assets into insights (data wrangling) and the visualization & communication of these insights across various distribution methods. Platforms & technologies include Python, SQL, Impala, Tableau, and Spotfire.

This individual must have a solid educational foundation and ability to balance delivery of near-term results that are aligned with execution of the long-term vision and roadmap. They must be detail-oriented and a fast learner in order to work with all Equifax's major data assets and a rapidly changing technology environment.

High-Level Responsibilities:
Collaborate with Product Development and Technology to extract KPIs from our existing business systems
Identify and establish metrics that drive current and future results
Work with Technology to implement data collection
Synthesize findings into actionable reporting for business leadership
Participate in the development of strategies, standards and best practices in the areas of data visualization, data access and data integration within Equifax's new big data analytical platforms.
Collaborate with Data & Analytics, Product Development and Technology leads to ensure the seamless transition of concepts and prototypes into production solutions.
Stay current with rapidly developing Big Data technologies landscape and share knowledge internally and with customers.

Requirements:
Knowledge of Equifax Workforce Solutions' differentiated data assets and technology delivery platforms or equivalent industry experience is a plus
5+ years of work experience in building, loading, transforming and analyzing data within and across database platforms is required. Big Data experience with Hadoop technologies such as Impala, Spark and MapReduce
Proficiency in one or more of the following technologies: SQL, Python
5+ years of work experience with data visualization tools and knowledge of data visualization concepts. Proficiency in one or more of the following tools: Tableau, Spotfire, Tibco.
Minimum of a Bachelors degree in Computer Science, Statistics, Economics, or equivalent quantitative field with heavy emphasis on programming required
Excellent communication skills
Creative and nimble with ability to overcome obstacles to solve the hardest problems
Ability to collaborate with peers and cross functional teams
Flexible and responsive with ability to adapt to rapid change in direction or business priority
Ability to work within the confines of data governance measures.
Limited travel required

Primary Location:
USA-St. Louis-Lackland

Function:
Function - Data and Analytics

Schedule:
Full time"
Senior Data Engineer,Sense360,"Sense360 is the leader in real-time, 360 insights which are revolutionizing the current state of market research, thats plagued by high-cost, static solutions that are neither timely nor accurate. Through analyzing and combining massive datasets of digital behavioral data with attitudinal surveys, Sense360 creates customized industry solutions that enable continuous measurement and optimization of its clients' business strategies and tactics . Sense360 was founded by successful repeat entrepreneurs and is funded by investors of Pinterest, Uber, HotelTonight, RiotGames and Twilio.

Sense360 has identified a massive market opportunity, and we are looking for a data engineer to help take us to the next level. You will have a large and immediate impact on the company, as you will be the 5th engineer on the team. We are looking for a scrappy engineer with solid CS fundamentals, who focuses on delivering value quickly. As a data engineer you will be responsible for architecting our data infrastructure that processes 1.4 TB data/day in an efficient manner. You will be working closely with our data science team to scale their processing algorithms. You will be working heavily with Apache Spark, AWS, Python and Apache Airflow.

Qualifications

7+ years experience

advanced knowledge of a distributed processing technology: Spark, Storm, Presto, Hadoop, Samza, Flink

intermediate knowledge with Python

experience working in a startup or an extremely strong desire to do so

solid CS fundamentals

excited to contribute in all phases of development; design, coding, testing, deployment and debugging

strong testing fundamentals

bias for delivering value over beautiful solutions

nose for data and data analysis a plus

Our Core Values

1. We are one team - We are one team with a single goal - to build an amazing company. If the company wins, we all win. We build the team by looking for amazing teammates who will elevate the team over individual genius.

2. Always improving - We believe that best people never stop learning and growing and that the only way to do that is to be humble and eager to be better. We crave feedback, we aren't defensive, and we internalize it. We are also equally as comfortable giving constructive and helpful feedback.

3. We are all owners - We empower everyone at the company to own their work. Owners want lots of responsibility, have authority and freedom to make real decisions, surface bad news quickly, and ask for help when they need it. A company of owners also means that ideas come from everywhere, that we all learn from each other, and titles and seniority don't matter.

4. Scrappiness - We are trying to do something transformative while also taking on the biggest and most resourced companies in the world. The only way we can pull off the impossible is to simplify everything to its core, only spend time and money on what truly matters, and work very, very, very hard.

5. We have fun - Building a company is hard work, but is also needs to be incredibly fun and rewarding. We don't just work together; we also make the time to have fun. This includes team outings, dinners, ping pong, halo and clipper games.

Our challenges

We have been solving hard problems from the beginning and we have many more to go!

We have built cutting-edge Android and iOS data collection SDKs. We have licensed our technology to companies with much bigger teams, who failed to build the same SDKs. Our data processing pipeline receives over 1.4 TB of sensor data/day. Cleaning, storing and processing that amount of data in a cost-efficient manner is a great challenge! We have built a complex data modeling pipeline, which evaluates the sensor data through multiple models and then fuses the results to come to the final conclusion. We also need to match our sensor data to a real world place (e.g. McDonalds, Nike store, laundromat). There are many datasets available but all have weaknesses. We are working to smartly fuse them.

If these types of problems excite you, you are the correct person for us!

Willing to relocate or sponsor visas."
NIS Data Engineer,Vivint Smart Home,"If you are an active Vivint employee, please apply through Workday by searching ""Find Jobs"".
If this is your first time applying you will need to create a candidate account when you click on apply. Job Description

JOB SUMMARY:
As the NIS Data Engineer on the Inside Sales Operations Team, you will be responsible for building the processes that support the ingestion and consumption of data at Vivint, specifically for Inside Sales. Working closely with our Call and Sales Analytics teams, you will design, build, and maintain a data warehouse platform that provides timely, accurate, and reliable data to many users. Your role will be critical in defining the appropriate architecture and processes needed to maintain our data warehouse.

JOB RESPONSIBILITIES:
Help build and maintain an enterprise data warehouse platform with its associated ETL processes and data architecture requirements
Responsible for designing, building, and maintaining robust, high-performing ETL processes
Implement best practices and innovative ETL solutions to provide timely, accurate, & reliable data
Evaluate, recommend, and implement proper tools & technology to achieve a high performing data warehouse platform servicing thousands of users and a broad set of use cases
Build cross functional relationships with data analytic teams and business leaders to understand their requirements and data needs
Constantly evaluate and optimize deployment & ETL processes to achieve greater efficiency and reliability

REQUIRED SKILLS:
Must have a passion for data and helping the business turn data into information and action
2+ years of data warehousing
2+ years of ETL development experience
Ability to initiate, drive, and manage projects with competing priorities
Ability to communicate effectively with business leaders, IT leadership, and engineers
Expert in SQL, databases, and ETL development processes & tools (SQL Server preferred)
Knowledgeable in scripting languages (.net, python, Perl, etc.)"
Data Analyst,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities Develop and maintain metrics to measure success of initiatives and programs. Present data and analysis in a clear and concise manner. Prioritize and plan research and other projects; establish project goals. With minimal supervision, provide accurate, timely, complex and sophisticated data analysis in support of initiatives and priorities. Determine and employ the most appropriate research design for data collection and analysis. Acquire, process, and refine large or complex data sets from various sources; utilizing computer programming where necessary. Analyze, evaluate, and assess quantitative data (using statistical software, computer models, software languages, mathematical models) to contribute to or develop software tools, analytic models, or reports. With Government direction, prepare mid-year and end of year reports. Extract data from various sources (Lotus Notes databases, Excel spreadsheets, Lawson via Cognos) as required. Compile, normalize, and analyze data to support specific project requirements. Provide interpretation of data in context of the project requirement. Other Data Analyst support functions, as tasked. Communicate effectively in both written and oral communication.

Qualifications Required:
BS degree or higher
5 years of experience with technical requirements, with at least one year in the IC
Familiarity with the Executive Office of the President, the Intelligence Community and other relevant communities.
Knowledge of Intelligence, Law Enforcement, and/or Homeland Security Community organization, mission, and goals.
Top Secret security clearance with SCI and CI Poly
Excellent oral and written communication skills, and demonstrated ability to actively participate/contribute in research, filter and synthesize data, and produce clear, logical, and concise products.
Very strong hands on experience with Microsoft Excel and Microsoft Office tools (Word, Powerpoint)
Demonstrated ability to document and present data analysis
Ability to translate complex, technical findings into an easily understood narrative (i.e., tell a story with the data) in graphical, verbal, or written form"
Geospatial Data Engineer,MONSANTO,"Monsanto produces leading seed brands in large-acre crops like corn , cotton , and oilseeds ( soybeans and canola ), as well as small-acre crops like vegetables . Monsanto also produces leading in-the-seed trait technologies for farmers that are aimed at protecting their yield, supporting their on-farm efficiency and reducing their on-farm costs. As a company, Monsanto remains committed to broadly licensing seed and trait technologies to other companies throughout the world. This approach ensures that farmers can access Monsantos products in the varieties that mean the most to their farm.

In addition to our seeds and traits business, Monsanto also manufactures the world's best-selling herbicide, Roundup  , and other herbicides used by farmers, consumers and lawn-and-garden professionals. The Geospatial Data Engineer will be involved in the design of big data solutions that leverage open source and cloud-based solutions within the Location360 enterprise initiative and will work with multiple teams across the organization (i.e. cloud analytics, data architects, business groups). The big data engineer will participate in the building of large-scale data processing systems, is an expert in data warehousing solutions and should be able to work with the latest (NoSQL) database technologies.

A Big Data Engineer should embrace the challenge of dealing with petabytes, or even exabytes, of data daily. A big data engineer understands how to apply technologies to solve big data problems and to develop innovative big data solutions. The big data engineer generally works on implementing complex big data projects with a focus on collecting, parsing, managing, analyzing and visualizing large sets of data to turn information into insights using multiple platforms. The geospatial big data engineer should be able to develop prototypes and proof of concepts for the selected solutions.

What you will do: - Design, build, and support cloud and open source systems to process geospatial data assets via an API-based platform - Provide up to date knowledge of public domain data sets that are relevant to Monsantos product pipeline - Partner with data science and commercial communities to brings needed data sets into GIS and Big Data analytical environments - Support integration of key environmental data into field management systems; provides leadership in advancing Monsantos understanding of environmental influences on field performance and risk factors Why you should join us: - Work with other top level talent solving a wide range of complex and unique challenges that have real world impact - We value the exploration of all relevant technology stacks to find the best fit for each dataset - We prioritize and pursue opportunities to present our work at relevant technical conferences - Our environment values your talent over a role or title. Strength of ideas trumps position on an org chart"
Python Data Engineer,Pamten,"Location: St Louis, MO Duration: 5-6 months H1s and sub vending is okay Job Details: Senior Level Python Data Engineer Skillset: Python, Flask, Pandas, API Design Project Overview: Proactive Outreach Manager : Client is in the process of developing a new interface for their Proactive Outreach Manager (POM) product. The development is in its beginning stages and the interface will be initially developed for corporate use with expansion to field business partners.
The goal is to enable reporting functionalities while limiting mistakes that can be frequently and easily made. Project Objectives:
Help create a more user friendly UI
Improve infrastructure of auto-dialer application
Assist with first phase of POM development
Develop functionality to record announcements, upload contacts, create campaigns and campaign strategies and change certain attributes

POM Team will be responsible for the following:
Assist with development of an integration layer with the POM sub system
Help develop proxy layer for UI development
Provide development within a Microservices environment
Leverage development tools and languages including Spring/Boot, Java, JavaScript, hibernate, HTML 5 and Bootstrap
Develop browser UIs for Chrome and Internet Explorer in a Citrix or Laptop environment
Support ETL activities using Teradata, Oracle, Python and Flask
Building micro services
Mixing and matching data from multiple dbs
Build reporting API w/ flask
APIs feed into internal application reporting and internal micro strategy environment (same API s for both)
Some automated data analysis and data transformations using Panda
Some data integration ( ETL ), uses panda as data transformation piece

Regards,

Kranthikumar
kranthi.bobbillapati@pamten.com
D: 6094231641"
"Senior Data Engineer (Ingestion, ETL & Audience)",Publicis Media,"Job Description

Software Engineers within Publicis Media concentrate on the development, implementation, and ongoing innovation of world class leading engineering products, business analytics, and next generation technologies. The Senior Data Engineer we are looking to have join our team will have strong experience running and managing large scale systems (e.g., Elasticsearch, Spark, MongoDB, JVM) along with a stellar understanding of data structures and algorithms. The Senior Engineer will be on point for design and development of data-centric applications and processes within the Publicis Media platform, utilizing various tools, technologies, and programming languages.

Your Day to day will include:
Design, create, maintain and optimize software applications for business intelligence solutions using a variety of languages, tools and technologies
Design, implement and maintain high performance data infrastructure/systems, data processing pipelines
Evaluate and prototype new technologies in the area of data processing
Think quickly, communicate clearly and work collaboratively with engineering, QA and operations teams
Embrace solid, agile development practices, such as unit-testing, code reviews and design documentation Qualifications

You have:
5+ years of experience designing and developing big data processing systems using distributed computing
Competent in at least one programming language. Scala, Java, and Ruby preferred.
2+ years of experience with MongoDB or ElasticSearch
Strong interest in emerging technologies: Spark, Hadoop, Hive, ElasticSearch, NoSQL
Comfortable with agile development practices, such as continuous integration and code reviews
Outstanding communication skills (oral, written and presentation) and strong interpersonal skills
High energy level, strong team player and good work ethic

Nice to Have:
Experience with Scala and Spark
Understanding of Amazon Web Services
Familiarity with API development and design
Digital Marketing and Advertising Domain Knowledge

Additional Information

All your information will be kept confidential according to EEO guidelines."
Big Data Software Engineer,Crestron Electronics,"Overview At Crestron Electronics, Inc we build the technology that integrates technology.

We are proud to be the largest and most recognized brand in automation and control solutions, and the premier technology partner for fortune 500 businesses globally. Our products are integrated into new high-tech commercial buildings to include some of the most exciting real estate throughout the world. Our clients include Google, Microsoft, Amazon

, Linked In and many others. We are the leaders in the most exciting and opportunistic industry in the world! Our automation and control solutions for homes and buildings allow our clients to control entire environments with the push of a button, integrating systems such as Audio Visual, Lighting, Shading, Security, Building Management Systems and HVAC to provide greater comfort, convenience and security.

We continue to experience rapid growth as we invest in resources and create new opportunities; as a result, we have exciting opportunities for a Big Data Software Engineer to join our Enterprise team in Rockleigh, New Jersey.

This individual will be responsible for building big data platforms/infrastructure and developing big data applications. In this role, you will also help us to build and enhance our cloud platform with new features for our customers. You will work on multi discipline projects. You must be creative and thrive on solving problems.

*LI-MT1

Responsibilities
Analyze product requirements to determine feasibility of design within time and cost constraints.
Work with discipline Leads and other engineers to create Module/Unit and Interface specification, implementation, integration and testing of product.
Rapidly architect, design, prototype, and implement architectures to tackle the Big Data and Data Science needs
Research, experiment, and utilize leading Big Data methodologies, such as HD Insight, Hadoop, Spark, Azure Data Lake, PowerBI, Azure Data Factory, Redshift and Microsoft Azure PaaS
Architect, implement and test data processing pipelines, and data mining / data science algorithms on a variety of hosted settings, such as Azure, client technology stacks, and Crestrons own clusters.
Work as part of an agile team.
Special projects as assigned
Qualifications
Bachelors degree is required. Area of study such as: Computer Science, Business Information Systems or other relevant field.
A minimum of 5 years of experience in architecting and building enterprise scale systems is required.
A minimum of 2 years of experience in C# .NET, with experience with source code management systems like SVN is required
Fluency with either Agile or SCRUM methodologies is required
Fluency in several programming languages such as Python, Scala, or Java, with the ability to pick up new languages and technologies quickly
Experience with large-scale, big data methods, such as MapReduce, Hadoop, Spark, Hive, Impala, or Storm is required
Experience in cloud and distributed systems principles, including load balancing, networks, scaling, in-memory vs. disk, etc.is required.
Ability to work efficiently under a Windows or Unix/Linux environment is required.
Experience in Azure Data Lake, PowerBI, Azure Data Factory is preferred.
Exceptional communication skills and the ability to work collaboratively in a fast paced team environment is essential

EOE/M/F/D/V

Benefits
At Crestron Electronics, we offer a competitive total compensation package including medical, Dental, Vision, Life Insurance and Short Term Disability. 401K with company contribution, Paid Vacation, Holidays and more!

We have new onsite state of the art fitness and wellness centers at our Headquarters in Rockleigh, NJ.

Must be able to work in the US without sponsorship

*No Solicitation*

Any agency submittal to any and all employees of Crestron Electronics, Inc by any method of communication will be deemed, the sole property of Crestron Electronics Inc."
Research Engineering/ Scientist Associate III - Hydrologic Data Analyst,University of Texas at Austin,"Purpose As part of a multi-disciplinary, integrated research team, the candidate will conduct research in hydrology (groundwater, surface water) related to energy and climate extremes using data analytics tools.

Essential Functions Conduct research in state of the art hydrologic issues using desired skills in big data analytics, artificial intelligence, and machine learning using different programming platforms. Assist research team(s) in related engineering and hydrologic analyses. Collaborate with team members on program development and fund raising. Present research results at local and national meetings. Provide service to professional societies, the Bureau of Economic Geology and the Jackson School of Geosciences.

Marginal/Incidental functions Other related functions as assigned.

Licenses: Class ""C"" Operator's Driver's License. Applicant selected must provide a current three year Driving Record from the current state of residence. If not currently a Texas resident, must obtain a Texas Driver's License within 30 days after entering Texas as a new resident.

Required qualifications Master's degree and 2 years or Bachelor's degree and four years of experience in Environmental, Civil, or Petroleum Engineering, Geology, or related field. Knowledge in data analytics, such as statistics and algorithm development. Demonstrated ability to work within an integrated team of geologists, engineers, and economists. Demonstrated ability to meet deadlines. Excellent interpersonal skills. Equivalent combination of relevant education and experience may be substituted as appropriate.

Preferred Qualifications More than the minimum required years of experience. Familiarity with basic concepts in fluid flow. Expertise with Geographic Information Systems (ArcGIS), Matlab, R, Python or similar programming language.

Working conditions Uniforms and/or personal protection equipment (furnished) May work around standard office conditions Repetitive use of a keyboard at a workstation Use of manual dexterity Occasional weekend, overtime and evening work to meet deadlines. Occasional interstate, intrastate, and international travel. Field work as necessary.

A criminal history background check will be required for finalist(s) under consideration for this position.

The retirement plan for this position is Teacher Retirement System of Texas ( TRS ), subject to the position being at least 20 hours per week and at least 135 days in length.

The University of Texas at Austin, as an equal opportunity/affirmative action employer, complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.

If hired, you will be required to complete the federal Employment Eligibility Verification form, I-9. You will be required to present acceptable, original documents to prove your identity and authorization to work in the United States. Information from the documents will be submitted to the federal E-Verify system for verification. Documents must be presented no later than the third day of employment. Failure to do so will result in dismissal.

UT Austin is a Tobacco-free Campus"
Database Engineer,Northrop Grumman,"Northrop Grumman is looking for a Database Engineer to support the Army Knowledge Online (AKO) program Data Center Operations. This position is located in Fort Belvoir, VA.

The Database Administrator designs, develops, builds, analyzes, evaluates and installs database management systems to include database modeling and design, relational database architecture, metadata and repository creation and configuration management. Uses data mapping, data mining and data transformational analysis tools to design and develop databases. Determines data storage and optimum storage requirements. Prepares system requirements, source analysis and process analyses and design throughout the database implementation.

A successful candidate will possess and apply expertise on multiple complex work assignments. These assignments may be broad in nature, requiring originality and innovation in determining how to accomplish tasks. Operates with appreciable latitude in developing methodology and presenting solutions to problems. Contributes to deliverables and performance metrics where applicable.

Roles and Responsibilities

Provide maintenance to all database components and maintain a secure configuration.

Provide lifecycle database management services including, but not limited to, design, development, provisioning, creation, modifying and managing data and schema, cloning and backup, troubleshooting, account management, and decommissioning.

Perform installation, upgrades, configuration, troubleshooting, maintenance, and optimization across all AKO databases and database servers.

Identify and resolve database problems in coordination with the service desk as required.

Coordinate with third-party vendors, and technology providers to troubleshoot and perform operations activities.

Develop database dictionaries for all AKO databases.

Provide IA system validation and assessments.

Maintain plans, designs, and architecture documentation for all AKO databases and database servers.

Maintain the Standard Operating Procedures (SOP) to support database server operations and administration.

Maintain the Certification and Accreditation (C&A) documentation for all AKO database servers.

Maintain the Configuration Management (CM) and change control documentation for all AKO database servers.

Maintain and create code in BASH and python for daily operations.

Review and provide recommendations to improve processes and procedures.

Provide reports on security vulnerability trends and analysis.

Monitor utilization requirements and conduct planning to ensure appropriate resource capacity management.

Provide ad-hoc reports as required.

Provide training and knowledge transfer as required.

Qualifications

Basic Qualifications:
1. Must possess a minimum of 5 Years' experience with Bachelors in a Science, Technology, Engineering or Mathematics field; 3 Years with Masters.
2. Must have a significant familiarity with Oracle Enterprise Database Server supporting a 24x365 environment.
3. Must have e xperience with Oracle Enterprise Database Server to resolve database outages and problems.
4. Must have e xperience with database design, integration and backup recovery.
5. Must have p articipated in capacity planning and benchmarking of application workloads.
6. Must have d atabase management programming skills.
7. Must have d atabase performance tuning experience.
8. Must have e xperience supporting Oracle Real Application Cluster (RAC) solution that spans multiple data centers.
9. Must have e xperience with writing and modifying BASH code.
10. Must have e xperience with writing and modifying python code.
11. Must have e xperience with SOAP.
12. Must have e xperience with LDAP queries and modifications.
13. Must have p erformed DISA STIG vulnerability remediation.
14. Must ha ve CompTIA Security+ or equivalent certification.
15. Must have Oracle Database Certified Professional certification equivalent or higher.
16. Must possess an active/current DoD Secret clearance.
17. Must be a U.S. Citizen.

Preferred Qualifications:
1. Experience supporting an Oracle Real Application Clusters (RAC) solution that spans multiple data centers.
2. Experience with Automatic Storage Management (ASM)
3. Experience with Data Guard replication
4. Experience with complex SQL queries and scripts
5. Experience in implementing scripting automation/orchestration tools.
6. Experience with Python libraries such as cx_Oracle, multithreading, pyladp, and suds.
7. Experience with AKO or the DoD Security Community.
8. Proven experience supporting a DoD program that consists of multiple Data Centers housing a multi-zone multi-network environment.
9. Strong oral presentation and written communication skills for providing documentation to customers.

Northrop Grumman is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer, making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class. For our complete EEO/AA and Pay Transparency statement, please visit www.northropgrumman.com/EEO . U.S. Citizenship is required for most positions."
Software QA Engineer 1 (Entry Level),WEX Inc.,"WEX Inc. is a leading and growing global provider of payment processing, information management and fleet card payment solutions. We are passionate about providing payment solutions with unparalleled security and control for corporate purchasing and transaction monitoring needs. We hire people who share the same passion for continuous innovation and client service that is unparalleled in our industry. We are employee centric offering value-based incentives and generous compensation and benefits packages. If you are looking for a growing career  come be part of WEX today!
General Purpose:

ESSENTIAL DUTIES AND RESPONSIBILITIES
Design, create, and execute high quality reusable manual test cases and scripts.
Experience delivering high quality products on time while working in Agile Teams and using an Agile or Iterative SDLC.
Individual tasks within a work team, using the ability to manage multiple priorities.
Leverage test case management and defect tracking solutions.
Create and validate data using SQL.
Works towards solving complex problems independently.
Manages individual career development; initiates career development discussions with manager.

Qualifications
Skills Required
Bachelor's degree in Computer Science or equivalent work experience.
Experience in the software industry (QA and/or Development).
Experience using test case management and defect tracking solutions.
Experience performing white-box and black-box testing activities in multi-tier solutions.
Experience testing client facing software.
Experience with HP/QC and/or Jira.
Experience troubleshooting technical issues with internal or external customers.
Demonstrates aptitude for analytical problem-solving.

Preferred Qualifications:
Experience with testing commercial client facing software.
Experience using HP/QC and/or Jira required.
Experience with testing Microservices a plus.

At WEX, we reward innovation, hard work and excellence. Benefits include:
 Health Insurance
 Dental Insurance
 Vision
 On Site Fitness Facility in South Portland location
 401(k) Plan
 PTO 
Life Insurance/AD&D
 Dependent Life Insurance
 Short- and Long-Term Disability Programs
 Extended Parental Leave
 Flexible Spending Accounts (Medical and Dependent)
 Employee Referral Award Program
 Employee Assistance Program
 Tuition Reimbursement
 Pet Insurance
 Bonus Plan

Equal Opportunity Employer/Vets/Disability

(#LI-GS1)

Primary Location : U.S.-ME-South Portland

Schedule : Full-time
Job : Information Technology - Corp"
Data Services Software Engineer - Foundational Services,Bloomberg,"Job Requisition Number: 63835

Engineering Data Services group is responsible for core data and analytics services that are used across Bloombergs application teams. Our service stack currently runs on over 900 machines and serves over 120 billion requests a day. With ever increasing data volumes and growth of Bloombergs Enterprise products this footprint has been growing at a rapid pace year over year. Our team also provides multiple foundational services such as meta data and id master for Bloomberg Query Language (BQL), the next generation analytics and screening engine for financial datasets.

We are building out a suite of tools that help developers support and optimize implementation of Bloomberg reference data and analytics. One such tool, the reference data integrity, is required to verify 50 billion unique data points daily to detect anomalies. This immense scale requires the use of Hadoop and Spark analytics framework. We are developing a set of brand new services for metadata, security & entity relationships and entitlement. Due to the large volume and explosive growth of underlying data we are looking to leverage Hadoop based storage for these services. We are also building new tools to manage our server farm more efficiently and leverage analytics to ensure high availability, efficient load balancing and fair scheduling.

We are looking for strong C++/Java software engineers who can design and develop low latency, high performance distributed systems. If you have strong programming skills, the ability to work collaboratively and a proven track record of delivering quality engineering solutions to real world problems, youll fit right in.

Our team is comprised of a small group of technologists passionate about data and pushing the boundaries of technology to deliver innovative solutions. We expect you to be proactive, able to work independently and take ownership in driving the vision and execution of the product.

Well trust you to:
Design, implement and own critical applications and components of our services stack

Participate in the full SDLC (Software Development Life Cycle) of various components and systems that are required to be highly efficient, robust and scalable

Enhance our infrastructure to fulfill mission critical SLAs, whether low latency or high throughput data retrieval

Get to know development teams across Bloomberg, understand their application requirements and data access patterns

You need to have:
3+ years of C++ or Java on UNIX/Linux environment

Solid understanding of algorithms and data structures, knowledge of object-oriented design and multi-threading

Excellent problem solving and communication skills, and ability to thrive in a highly collaborative and dynamic work environment

Experience with high volume, high availability distributed systems

Experience working in a test-driven development and agile environment

Wed love to see:
Knowledge of low-level Linux, UNIX networking and C system calls for high performance computing and debugging

Experience with systems level Java or Scala

Experience with Hadoop and/or using a distributed analytics infrastructure like Apache Spark

If this sounds like you, apply and well get in touch to arrange a technical phone interview if we think youre a match."
SQL Data Engineer,Progressive Leasing,"Job Description Progressive is blazing a path at the intersection of Technology and Consumer Leasing and we have an exciting new opening for a Data Engineer for our Data Science team in our Draper, Utah office.

As a Data Engineer you will support Progressives Data Science team and will be responsible for creation and maintenance of databases, tables, data flows, and reporting dashboards covering every aspect of the companys activities. You will understand the nuances of the data and ensure that each field in each table is properly named, populated, documented, and used. You will maintain high data quality and integrity by identifying and eliminating ambiguity, duplication, data errors, and inefficient data flows. The data that you maintain is a key strategic and competitive advantage for Progressive, and will form the backbone of analysis that drives the companys business decisions.

What You'll Do:
Build, manage, maintain, and optimize the data infrastructure of Progressive Leasing
Curate structured, semi-structured, and unstructured data and take part in proper system and database design for each type
Collaborate with data scientists and other team members on project work including algorithm development and business analysis
Be a critical part of a scrum team in an agile environment, ensuring the team successfully meets its deliverables each sprint
Ensure the appropriate level of system and service monitoring and instrumentation is in place at all times
Provide clear, accurate, timely, and 100% transparency to the status of service related efforts, issues, upgrades, etc.
Continuously innovate, learn new skills, develop your talents, and exercise a passion and drive to deliver excellence
Minimum Experience & Qualifications:
A successful candidate will have hands-on experience in a multitude of domains; including, but not limited to database design, data warehousing, business intelligence, big data, database tuning, application optimization, security, virtual computing and storage, incident tracking, and general database administration
B.S. degree in Computer Science, Information Technology, or equivalent experience (required)
Three or more years experience including enterprise data warehouses, business intelligence, and MDM
Knowledge and familiarity with concepts in predictive analytics
Expert level knowledge of Microsoft SQL Server
Expert level knowledge of SQL administration, engineering, and monitoring tools
Expert level knowledge of designing, constructing, administering, and maintaining data warehouses
Solid experience working with SSIS and SSRS or similar tools
Solid experience years working with various Business Intelligence tools
Solid experience with change control and agile methodologies
A passion for efficiency, stability and creativity
Experience with Performance Tuning
Compensation, Perks & Benefits:
Competitive Compensation
Full Health Benefits - Medical/Dental/Vision
401k, Paid Time Off and Tuition Reimbursement
Full Service Gym, Game and Lounge Area, Basketball Court
Free Healthy Snacks and Refreshments
Subsidized Public Transit
Fun and Relaxed Work Environment
WHO IS PROGRESSIVE LEASING?

Founded in 1999, Progressive Leasing, a wholly-owned subsidiary of Aarons Inc. [NYSE: AAN], is a steadily growing company, already surpassing $1B in revenue. Our scalable customer payment software products provides lease-purchase technology solutions through 16,000+ retail locations in 46 states.

Progressive Leasing does not discriminate in any aspect of employment on the basis of race, color, religion, national origin, ancestry, gender, sexual orientation, gender identity and/or expression, age, veteran status, disability, or any other characteristic protected by federal, state, or local employment discrimination laws where Progressive Leasing does business. All new hires must pass a pre-employment criminal background check and drug test"
Machine Learning Engineer,Nielsen,"1406
Analytics - USA New York, New York
The Nielsen Marketing Cloud is the leading provider of data technology powering the digital marketing ecosystem. Our customer data cloud provides the industrys only unified customer profiles, which connect identities across all channels and devices  including display, video, audio, offline, mobile and smart TVs  enabling marketers to engage individuals and households with personalized messages to drive performance at scale. The Nielsen Marketing Cloud's data marketplace is the largest 3rd-party cross-device data set in the world with over 5B unique users and devices.

Nielsen brings to bear the markets most powerful proprietary data assets, and together we are helping the worlds biggest brands make better and faster marketing and media decisions, we are permanently disrupting the landscape of audience measurement. For more information, please visit nielsen.com, or follow us: @Nielsen.

The marketing cloud team has built a thriving business by accumulating thousands of data points about the online consumer.

This hands-on position reports to our Director, Machine Learning and will take a leading role in developing and levveraging the team's machine learning assets.

Responsibilities:
Build turnkey solutions for machine learning applications, data engineering pipelines, & model deployment, via
Machine learning engineering
Data engineering
Full stack engineering
Algorithm & technology research

Requirments: E=essential, P=preferred.

E: Minimum of 3+ years relevant machine learning work experience with internet scale data
E: Familiarity with sophisticated data mining & modeling solutions
E: Expert knowledge of an object oriented and functional language such as Python, Java, C++, and/or Scala
E: Experience in full stack development in frameworks such as Django and Javascript
E: Experience with AWS
E: Extensive experience with the Hadoop ecosystem, streaming and parallel computing, with a strong knowledge of Spark, Flink, and Kafka
E: High energy and enthusiasm to develop the next big thing in internet advertising; someone that can think out of the box, solving new and unique analytical challenges
E: Advanced degree in statistics, mathematics, computer science, or machine-learning related fields required
P: Familiarity with dev ops tools and CI/CD workflows including github, Jenkins, Travis CI, and Docker Swarm
P: Familiarity with marketing analytics, including marketing response modeling and customer segmentation
P: Experience with online media and targeting a big plus
P: Extensive experience with relational database management system (RDBMS), with strong knowledge of SQL

#LI-MJ1

About Nielsen:
Nielsen N.V. (NYSE: NLSN) is a global performance management company that provides a comprehensive understanding of what consumers Watch and Buy. Nielsens Watch segment provides media and advertising clients with Total Audience measurement services across all devices where content  video, audio, and text  is consumed. The Buy segment offers consumer packaged goods manufacturers and retailers the industrys only global view of retail performance measurement.
By integrating information from its Watch and Buy segments and other data sources, Nielsen provides its clients with both world-class measurement as well as analytics that help improve performance. Nielsen, an S&P 500 company, has operations in over 100 countries that cover more than 90 percent of the worlds population. For more information, visit www.nielsen.com
Nielsen is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action-Employer, making decisions without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability status, age, marital status, protected veteran status or any other protected class.
Job Type: Regular
Primary Location : New York , New York
Seconday Locations: , , ,
Travel: Yes, 10% of the Time"
R&D Yield Enhancement Data Analysis Engineer,Micron,"Req. ID: 92192
Micron Technology, Inc. is seeking a Yield Enhancement Data Analysis Engineer to be responsible for providing yield analysis for next-generation Micron products. In this important role, you will provide analysis of probe, parametric, and process data involving electrical and/or visual characterization of failing wafers and devices. You will provide analysis of process experiments, assist with identification of yield excursions, and identify correlations to root-cause issues during processing. You will also help support the transfer of data-analysis processes into production. This includes supporting yield enhancement and analysis teams around the world and training them in the latest techniques and methods of production.

The Yield Enhancement Data Analysis Engineer will also:
Work with other engineering teams to resolve testing issues
Support and develop new tools and techniques to localize and identify device failures
Recognize, identify, and raise awareness of process issues to appropriate teams
Attend necessary meetings and present results to process and integration teams
Possibly travel to manufacturing sites during transfer of next-generation processes, using the latest data analysis techniques and methods

The right candidate will have an intermediate understanding of device physics as well as familiarity with semiconductor manufacturing and statistical analysis of experimental data.
This role requires good analytical and problem-solving skills, high motivation, and a strong work ethic so you will be able to work independently once trained. Because you will conduct training, you will need strong communication and presentation skills. Good computer skills, including a working knowledge of Windows, JMP statistic software, and Microsoft Office are also required.

Additional skills preferred, but not required include one-to-three years work experience with volatile/non-volatile memory; familiarity with parametric testing and reporting; and programming skills to aid in automation or script creation.

A bachelors degree with five years of industry-related experience is required OR a master of science or Ph.D. is required in one of the following: electrical engineering, chemical engineering, materials science engineering, physics, or related field.

About Micron
Micron Technology is a world leader in memory and semiconductor technology. We offer the industrys broadest portfolio of silicon-to-semiconductor solutionsstarting with foundational DRAM, NAND, and NOR Flash memory, and extending to SSDs, modules, MCPs, HMC, and other semiconductor solutions. For more than 37 years, Micron has dedicated itself to collaborating with customers and partners to engineer technology that drives innovation and transforms whats possible. We continually rethink, recast, and advance new ideas to make it easier for customers to try new things and gain competitive advantages in their markets.

We recruit, hire, train, promote, discipline and provide other conditions of employment without regard to a person's race, color, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, pregnancy, veterans status, or other classifications protected under law. This includes providing reasonable accommodation for team members' disabilities or religious beliefs and practices.

Each manager, supervisor and team member is responsible for carrying out this policy. The EEO Administrator in Human Resources is responsible for administration of this policy. The administrator will monitor compliance and is available to answer any questions on EEO matters.

To request assistance with the application process, please contact Microns Human Resources Department at 1-800-336-8918 (or 208-368-4748).

Keywords: Boise || Idaho (US-ID) || United States (US) || Technology Development || College || Regular || Engineering || *LI-BB1 ||"
Database Engineer,"New Directions Technologies, Inc.","Duties and Responsibilities:

 Design, develop, enhance and support new and existing software and business intelligence systems primarily using Microsoft technologies including TSQL, SSIS, and IBM Cognos BI tools.
 Develop business intelligence systems and Data Warehousing (Microsoft/Kimball) including dimensional modeling, schema design, Extract Transform and Load processing using SQL Server Integration Services, and Reporting/OLAP analysis with both SQL Server subsystems and IBM Cognos.
 Create database components that include Stored procedures, Functions, Scheduled Jobs.
 Domain expert on Navy ERP and Defense Civilian Personnel Data System (DCPDS) mapping these external data sources to NSWCPHD internal processes and requirements.
 Resolve issues by finding root cause of the issue and doing programming changes.
 Perform the detailed design of application and technical architecture components.
 Apply Information Assurance (IA) requirements, including database Security Technical Implementation guides (STIGs).
 Assist in defining and reviewing the technical requirements for the application, including security, integration, performance, quality, and operations requirements.

Education and Work Experience Requirements:

 Associates degree from an accredited college or university with major course works in Computer Science, Management Information Systems, or a closely related field a plus or Associates degree and 2 years experience.
 CompTIA Security + CE, MCTS SQL Server 2008 BI or equivalent.
 Three years of IT related experience.
 Experience with Microsoft SQL Server 2008 or later, including SQL Server Integration Services (SSIS) and Data Warehousing. Integration with SharePoint 2013, IBM Cognos and/or BMC Remedy application systems a plus.
 Work requires certification at IAT Level II in accordance with DoD 8570.01 and SECNAV 5239.2
 Experience in Navy Security requirements and implementations: SECNAV 5239.2, NIST SP 800-53, DoDI 8500.2, DoDI 8510.01 and DoDI 8510.bb.
 Sufficient systems administration experience with Windows 2008 or Windows 2012 to enable training as an IBM Cognos administrator (Installation, configuration, management, etc.)

Security Clearance:

Secret"
"Data Engineer, Prime Video Content Analytics",Amazon.com,"Amazon has an immediate opening for a Data Engineer to support the Prime Video Content Analytics team in Seattle, WA.

In this role you will:
- Design, develop, implement, test, document, and operate large-scale, high-volume, high-performance data structures for business intelligence analytics in support of prime video content analytics.
- Implement data structures using best practices in data modeling, ETL/ELT processes, and SQL, Oracle, Redshift, and OLAP technologies.
- Gather business and functional requirements and translate these requirements into robust, scalable, operable solutions that work well within the overall data architecture.
- Analyze source data systems and drive best practices in source teams.
- Participate in the full development life cycle, end-to-end, from design, implementation and testing, to documentation, delivery, support, and maintenance.
- Produce comprehensive, usable dataset documentation and metadata.
- Evaluate and make decisions around dataset implementations designed and proposed by peer data engineers.

Basic Qualifications - 3+ years of relevant experience in business intelligence role, including data warehousing and business intelligence tools, techniques and technology, as well as experience in diving deep on data analysis or technical issues to come up with effective solutions
- BS degree in math, statistics, computer science or equivalent technical field
- Experience in data mining structured and unstructured data (SQL, ETL, data warehouse, Machine Learning etc.) in a business environment with large-scale, complex data sets
- Proven ability to look at solutions in unconventional ways. Sees opportunities to innovate and can lead the way

Preferred Qualifications - 5+ years experience in Oracle and Redshift including complex querying, analytical functions, and database tuning for optimal query performance with large data sets
- Proven communication (verbal and written) and interpersonal skills
- 5+ years experience in Datanet or other ETL technologies
- 5+ years experience in Tableau including advanced dashboarding

Amazon is an Equal Opportunity-Affirmative Action Employer  Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
Data Scientist,The Johns Hopkins Applied Physics Laboratory,"Are you passionate about providing real impact to the countrys toughest national security problems?
Are you searching for engaging work with an employer that prioritizes continual innovation?
Our team is looking to provide large-scale analytics contributions to national and homeland security programs. To succeed, we need talented data and computer scientists. We are looking for individuals with curiosity and drive to develop new approaches and technologies in data science to solve our nations most difficult challenges.
The Johns Hopkins University Applied Physics Laboratory (APL), a national leader in scientific research and development, is located midway between Baltimore and Washington, DC is a data scientist with strong computer science and software development skills.
Job Summary:

The Large-Scale Analytics Group develops software systems that incorporate machine learning algorithms on big data platforms, natural language processing algorithms, and visual analytics to find information hidden deep within large and complex data sets. We support multiple agencies within the US Government by applying innovative analytics to uncover activities such as illegal activities, international trade fraud, illicit manufacturing of weapons of mass destruction, and cybercrime. We are looking for data scientists, computer scientists, applied mathematicians, statisticians, software developers and systems engineers to join our team and provide our sponsors with new solutions to old problems. The successful candidates should be creative thinkers, motivated problem solvers, team players, and life-long learners that want to strengthen the safety and security of our country.

Duties (Listed in order of importance with the estimated amount of time spent at each task):

1. Develop advanced algorithms for analyzing large-scale and complex data.(30%)
2. Create software applications and perform analytics on complex data. (30%)
3. Design and develop large-scale graph architectures using technologies like Hadoop and Spark to support analytic algorithms. (30%)
4. Present results to both JHU/APL and Sponsor leadership. (10%)

Note: This job summary and listing of duties is for the purpose of describing the position and its essential functions at time of hire and may change over time.

Qualifications
: Required Qualifications:
B.S. in Computer Science, Information Science, Mathematics, Statistics, Physics, Operations Research, Data Science, or related field. 1-5+ years of experience. Experience with statistics, machine learning algorithms, or general algorithm development. Working knowledge of modern large-scale data systems and architectures. Solid software development and programming skills. Excellent interpersonal skills, the ability to work independently, outstanding written and oral communications skills, and good organizational skills. Applicants selected will be subject to a Government security investigation and must meet the eligibility requirements for access to classified information. Eligibility requirements include US citizenship. Must be eligible for DoD clearance requiring background investigation and/or polygraph examination. Preference for current SSBI.

Desired Qualifications:
M.S. or Ph.D. in the disciplines listed above. Working knowledge of state-of-the-art large-scale data scienceapproaches and architectures, the ability to manage and manipulate large data sets, and the ability to utilize various machine learning techniques. Experience with software engineering processes and devops approaches.

Special Working Conditions [Travel, working in closed areas, extended hours]: Some local travel to sponsor sites may be required.

Security: Applicant selected will be subject to a government security clearance investigation and must meet the requirements for access to classified information. Eligibility requirements include U.S. citizenship.

Benefits: APL offers a comprehensive benefits package including a liberal vacation plan, a matching retirement program, significant educational assistance, a scholarship tuition program for staff with dependents, and competitive salaries commensurate with skills and experience."
Data Analyst,LexisNexis,"We are looking for a passionate data analyst who can help us build world class products relied on by countless legal professionals every day around the globe. This position reports into the global technology organization. You will be on the front line making critical recommendations through your data analysis that directly impact marketing and sales organization. You will be responsible for analyzing, investigating, and hypothesizing data to effectively communicate with internal, management and functional areas by presenting problem resolution, engineer data solutions and status reports in a professional manner. You will work with marketing analysts, product development team and software data engineers, to make recommendations on data processing, cleansing, storage and analysis of marketing prospect data. This role reports directly to the Director of Enterprise Customer Master and Global Marketing Warehouse

Accountabilities:
1. Analyze marketing prospect data, develop recommendations for mastering (Master Data Management) strategy

2. Develop analysis frameworks and repeatable processes to source, cleanse, enrich and ingest data into marketing warehouse

3. Leverage state of the art record linkage and machine learning methods to improve existing clustering, match/merge and hierarchy resolution algorithms

4. Partner with marketing analysts, data engineers and product owners to design data storage and governance models for marketing prospect data attributes

5. Define data quality metrics, SLAs and automated processes for ongoing measurement

6. Influence and support fellow analysts with problem resolution and creation of documentation.

7. Requires experience effectively identifying, researching, and coordinating the resources necessary to troubleshoot/diagnose data issues; possesses ability to probe, research and gain knowledge and understanding of internal and external business and data flow processes.

8. Professionally maintain relationships across technology and business stakeholders.
9. Demonstrate understanding of marketing knowledge and customer requirements to ensure thorough analysis and data solutions align with business needs.
10. Shows drive for personal and career development by investing in self-development.
11. Initiate and implement projects autonomously.

12. Requires exceptional oral and written communication skills, including articulating ideas and information.

Qualifications:
1. Demonstrated knowledge of marketing analytics within software applications.
2. Knowledge of statistics and experience using statistical packages for analyzing datasets (R, Python, SQL, Excel etc)

3. Knowledge of and experience with reporting packages (Business Objects/Tableau etc), databases (Oracle SQL etc), programming (XML, Javascript, or ETL frameworks).
4. Demonstrated analytical skills with the ability to collect, organize, analyze, troubleshoot and disseminate significant amounts of information with attention to detail and accuracy.
5. Adept at writing SQL queries, report writing and presenting findings.
6. BS in Mathematics, Economics, Computer Science, Information Management, Statistics or equivalent.
7. Knowledge of Legal, Business or Government data a plus..

articulating ideas and information; possesses ability to collaborate effectively with personnel at all levels including various levels of management, staff, vendors, and clients.

8. Preferred yrs experience 3+ yrs. experience

LexisNexis Legal & Professional (www.lexisnexis.com) is a leading global provider of content and technology solutions that enable professionals in legal, corporate, tax, government, academic and non-profit organizations to make informed decisions and achieve better business outcomes. As a digital pioneer, the company was the first to bring legal and business information online with its Lexis and Nexis services. Today, LexisNexis Legal & Professional harnesses leading-edge technology and world-class content, to help professionals work in faster, easier and more effective ways. Through close collaboration with its customers, the company ensures organizations can leverage its solutions to reduce risk, improve productivity, increase profitability and grow their business. Part of RELX Group plc, LexisNexis Legal & Professional serves customers in more than 100 countries with 10,000 employees worldwide. LexisNexis, a division of RELX Group, is an equal opportunity employer: qualified applicants are considered for and treated during employment without regard to race, color, creed, religion, sex, national origin, citizenship status, disability status, protected veteran status, age, marital status, sexual orientation, gender identity, genetic information, or any other characteristic protected by law. If a qualified individual with a disability or disabled veteran needs a reasonable accommodation to use or access our online system, that individual should please contact 1.877.734.1938 or accommodations@relx.com."
Big Data Software Engineer,TransReach,"Seeking Big Data Engineer to join our Software Engineering Team. We are focused on making a difference in healthcare and developing solutions that will help shape the future of medication management. Our team thrives on the spirit of innovation and is dedicated to solving our customers most pressing needs. We are looking for an enthusiastic experienced team oriented Big Data Engineer who has a passion for data and creating insightful analytics so that we can transform healthcare in the US.

As a Performance Center team member, you will work hard with a team of engineers on our cloud data platform that streams data from a variety of health care software and hardware systems in real-time to creeate transformational recommendations for our customers. Our solutions help to drive improved financial performance, compliance, and better patient outcomes. Each day you will make an impact.

Responsibilities:

Defines technology roadmap in support of product development roadmap
Architects complex solutions encompassing multiple product lines
Provides technical consulting to multiple product development teams
Develop custom batch-oriented and real-time streaming data pipelines working within the MapReduce ecosystem, migrating flows from ELT to ETL
Ensure proper data governance policies are followed by implementing or validating data lineage, quality checks, classification, etc.
Act in a technical leadership capacity: Mentor junior engineers and new team members, and apply technical expertise to challenging programming and design problems
Resolve defects/bugs during QA testing, pre-production, production, and post-release patches
Have a quality mindset, squash bugs with a passion, and work hard to prevent them in the first place through unit testing, test-driven development, version control, continuous integration and deployment.
Ability to lead change, be bold, and have the ability to innovate and challenge status quo
Be passionate about solving customer problems and develop solutions that result in a passionate customer/community following
Conduct design and code reviews
Analyze and improve efficiency, scalability, and stability of various system resources
Contribute to the design and architecture of the project
Operate within Agile Development environment and apply the methodologies

Required Skills and Experience:
Advanced knowledge of data architectures, data pipelines, real time processing, streaming, networking, and security
Proficient understanding of distributed computing principles
Good knowledge of Big Data querying tools, such as Pig or Hive
Good understanding of Lambda Architecture, along with its advantages and drawbacks
Proficiency with MapReduce, HDFS Experience with integration of data from multiple data sources

Basic Qualifications:

Bachelors Degree in Software Engineer or similar degree
12+ years experience in software engineering
Experience developing ETL processing flows using MapReduce technologies like Spark and Hadoop
Experience developing with ingestion and clustering frameworks such as Kafka, Zookeeper, YARN
Experience with building stream-processing systems, using solutions such as Storm or Spark-Streaming
Experience with various messaging systems, such as Kafka or RabbitMQ

Desired Experience:

Experience with DataBricks and Spark
Ability to solve any ongoing issues with operating the cluster
Management of Spark or Hadoop clusters, with all included services
Experience with NoSQL databases, such as HBase, Cassandra, MongoDB
Experience with Big Data ML toolkits, such as Mahout, SparkML, or H2O
Understanding of Service Oriented Architecture
Technical writing, system documentation, design document-management skills"
"Senior Data Engineer, Scala",Capital One,"West Creek 8 (12080), United States of America, Richmond, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Senior Data Engineer, Scala

As a Capital One Data Engineer, you'll be part of an Agile team dedicated to breaking the norm and pushing the limits of continuous improvement and innovation. You will participate in detailed technical design, development and implementation of applications using existing and emerging technology platforms. Working within an Agile environment, you will provide input into architectural design decisions, develop code to meet story acceptance criteria, and ensure that the applications we build are always available to our customers. You'll have the opportunity to mentor other engineers and develop your technical knowledge and skills to keep your mind and our business on the cutting edge of technology. At Capital One, we have seas of big data and rivers of fast data.

Who you are:
You yearn to be part of cutting edge, high profile projects and are motivated by delivering world-class solutions on an aggressive schedule
Someone who is not intimidated by challenges; thrives even under pressure; is passionate about their craft; and hyper focused on delivering exceptional results
You love to learn new technologies and mentor junior engineers to raise the bar on your team
It would be awesome if you have a robust portfolio on Github and / or open source contributions you are proud to share
Passionate about intuitive and engaging user interfaces, as well as new/emerging concepts and techniques.

The Job:
Collaborating as part of a cross-functional Agile team to create and enhance software that enables state of the art, next generation Big Data & Fast Data applications
Building efficient storage for structured and unstructured data
Developing and deploying distributed computing Big Data applications using Open Source frameworks like Apache Spark, Apex, Flink, Nifi, Storm and Kafka on AWS Cloud
Utilizing programming languages like Java, Scala, Python and Open Source RDBMS and NoSQL databases and Cloud based data warehousing services such as Redshift
Utilizing Hadoop modules such as YARN & MapReduce, and related Apache projects such as Hive, Hbase, Pig, and Cassandra
Leveraging DevOps techniques and practices like Continuous Integration, Continuous Deployment, Test Automation, Build Automation and Test Driven Development to enable the rapid delivery of working code utilizing tools like Jenkins, Maven, Nexus, Chef, Terraform, Ruby, Git and Docker
Performing unit tests and conducting reviews with other team members to make sure your code is rigorously designed, elegantly coded, and effectively tuned for performance

Basic Qualifications:
Bachelors Degree or military experience
At least 5 years of professional work experience in data warehousing / analytics
At least 4 years of experience in open source programming languages for large scale data analysis
At least 3 years of Java development for modern data engineering
At least 3 years of data modeling development
At least 1 year of experience working with cloud data capabilities

Preferred Qualifications:
Master's Degree or PhD
5+ years Java development experience
4+ years of experience in Python, Scala, or R for large scale data analysis
4+ years' experience with Relational Database Systems and SQL (PostgreSQL or Redshift)
4+ years of UNIX/Linux experience
2+ years of Agile engineering experience
2+ years of experience with the Hadoop Stack
2+ years of experience with Cloud computing (AWS)
1+ years of experience with supervised machine learning

What we have:
Flexible work schedules
Convenient office locations
Generous salary and merit-based pay incentives
A startup mindset with the wallet of a top 10 bank
Monthly innovation challenges dedicated to test driving cutting edge technologies
Your choice of equipment (MacBook/PC, iPhone/Android Device)

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Automation & Machine Intelligence Engineer,Apple,"Imagine what you could do here. At Apple, great ideas have a way of becoming great products, services, and customer experiences very quickly. Bring passion and dedication to your job and there's no telling what you could accomplish.

Key Qualifications
Strong acumen in Industrial Automation with a focus on design, development, qualification and deployment of custom automation work-cells for automated assembly and inspection
Experience working with system integrators on custom automated machinery design, development, qualification and roll-out
Experience in equipment life-cycle from concept development, prototyping, acceptance testing and qualification for mass production
Working knowledge of automation sub-systems viz. optical (cameras and laser scanners), motion (servos and steppers), robots (SCARA, 6-axis and others) and grippers (mechanical and otherwise)
Strong understanding of GD (geometric dimensioning) and TA (tolerance analysis) and translation into machine qualifications via GRR
Experience in programming of industrial automaton controllers (PLC and PC-based) and robot programming with an understanding of field-bus protocols and data exchange methods (OPC-UA)
Strong working knowledge of applied statistics (descriptive and inferential) with proven understanding of process capability analysis and factory physics (line balancing and KPIs such as OEE, UPH, Yield)
Strong working experience with variability analysis techniques such as PCA, ANOVA and ANOM for analyzing manufacturing process and product data (JMP or the equivalent)
Adept in the application of Artificial Intelligence, Machine Learning and Neural Networks - from model building, training, tuning, validation and scaling in an enterprise environment
Strong working knowledge of ML methods such as regression, classification, and SVMs along with development of CNN/RNN neural networks
Experience in applied ML in the domains of anomaly detection, image classifications, and leading indicator predictions
Knowledge in processing large data sets from querying (SQL based), collating and organizing towards development of a fully automated data analysis pipeline
Strong understanding of open source ML libraries such as Tensorflow, Keras and Scikit and data science libraries such as pandas, seaborn, dplyr (R), ggplot (R).
Adept at OO programing principles and software development for data automation in R, Python, Java and C/C++/Obj-C
Understanding of software development life-cycle and adept at prototype building from idea through concept and demonstration
Strong working knowledge of the Unix/Linux OS and shell scripting. Understanding of text parsers such as sed/awk are a bonus.
Experience working within a high-volume manufacturing production environment a plus
Able to communicate effectively in a worldwide environment
Willing to travel domestically within the US and internationally amounting to a 30-35% travel in a year
Description
Responsible for ideation, concept development, and scaling of an enterprise data automation system with specific focus on software integration with automated equipment, development of automated data analysis modules for the analysis of manufacturing process, product attributes and operational efficiencies in the manufacturing of Apple products. Perform research and development on new technologies necessary to solve complex analysis tasks that have a direct impact to the quality and delivery of Apple products.

Develop new and or improved software data automation modules for the standardization of machine data, integration with assembly or inspection equipment and collection thereafter
Train, educate, guide and qualify automation system integrators on machine software standardization for data generation, collection and integration with Apples data automation infrastructure
Develop data analysis modules through the application of theoretical and practical data science methods
Assemble large data sets for analysis either through direct SQL-based querying or development of scripts and code-modules to collate distributed and disparate data sources
Analyze large data sets towards the goal of identifying anomalies (pattern detection) and variabilities in a measure of interest
Proof-of-concept application of ML methods and Neural Networks for a wide range of prescriptive/predictive applications
Develops software components in Python, Java and/or C/C++/Obj-C towards roll-out of a data automation system
Works with software management tools for requirements definition, prototyping, version control, testing and release management
Works with automated process equipment vendors to prescribe data standards (formats and structures) that lend themselves well to automation up the data pipeline
Engage automated process equipment vendors for software code reviews, design reviews and development proposals
Travel to the machine supplier and contract manufacturer for equipment bring-up, qualification, acceptance testing and support during manufacturing build events
Drive failure analysis and machine issue resolution relating to the data automation software, working with the machine supplier and contract manufacturer
Communicate and collaborate effectively with the multiple, globally distributed cross functional teams
Education
Bachelors in Engineering with 7+ years, Masters in Engineering/Data Science with 5+ years or a PhD in Engineering with 3+ years in a data science or analytics research environment"
"Data Engineer, Zoro Tools",Grainger,"Data Engineer- Zoro US

Zoro is seeking a Data Engineer. The Data Engineer develops and maintains data architecture for the company to enable knowledge worker productivity, improve customer experience and business decision making with accurate, timely, and consistent information. The role works with solution and cloud architects, business partners, and external service providers to ensure that new capabilities comply with the data architecture and meet the data needs of the company.

Job Responsibilities Include:
Creates the data architecture and data management strategy for the organization
Ensures that data architecture and data management strategy creates a foundation for future investment in business intelligence and collaboration
Collaborates with business, service, and solution architects to understand the implications of respective architectures on data architecture and maximize the value of data across the organization
Maintains a holistic view of data assets by creating and maintaining blueprints that illustrate how data is stored, processed, and accessed
Responsible for the development of logical data models and physical database designs applied across multiple computing environments
Provide oversight and direction for the design and development of the data warehouses
Lead the design and implementation of new business intelligence solutions. Design and implement ETL processes
Monitor performance and advise any necessary infrastructure changes
Ensure optimal end-user performance by establishing and consistently executing overall system performance assessment processes and associated action plans
Develop standards, process flows, and tools that promote and facilitate the mapping of data sources, documenting interfaces and data movement across the enterprise.
Define governance and best practices around meta-data to ensure an integrated definition of data for enterprise information, and to ensure the accuracy, validity, reusability, and consistent definitions for common reference data.

Skills & Responsibilities:
Expert in data architecture development, data policy formation, data asset management, data modeling, and data taxonomy creation
Must be able to draw insights from structured and unstructured information
Competent in information systems design and information visualization techniques
Excellent verbal and written communication skills
Ability to challenge and convince the various stakeholders involved in any project
Working knowledge of usability design and data warehousing techniques
Ability to perform business domain analysis and business process modeling
Proficient understanding of distributed computing principles
Proficient with cloud data warehouse technologies, such as AWS RedShift and/or Google BigQuery
Experience with MySQL and PostgreSQL databases
Good knowledge of data warehouse querying tools, such as SQL, AWS QuickSight, and Google Data Studio
Good knowledge of business intelligence tools, such as Jaspersoft and/or Tableau
Ability to solve any ongoing data issues that arise throughout the data processing lifecycle
Knowledge of various ETL techniques and frameworks, such as Matillion and/or Talend
Experience with integration of data from multiple data sources
Experience with integrating machine learning toolkits into data infrastructure
Experience with Big Data technologies to support future growth

Success Criteria
Advanced Analytical Thinking and Problem-Solving skills
Solid experience in architecture, advanced reporting, and dashboards
Experience working with data warehouses is required
Strong SQL skills and experience with performance tuning are required
Get it done attitude
Superior Communication and Business-Technical Interaction skills
Good understanding of data modeling concepts and data relationships

To qualify, you must possess the following skills:
Bachelors degree in computer science, management information systems, or a related discipline
More than five years of experience in data architecture or minimum five to seven years of experience as data analyst, business intelligence analyst, or equivalent roles
Proven experience with ETL tools
Professional or educational experience in software development
Excellent communication and data analysis skills
Excellent knowledge of SQL, R, and Python for performing data transformation and analysis

Additional Information
Zoro is an Equal Opportunity Workplace and an Affirmative Action Employer.
All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or protected veteran status.

See if we are normal at: www.zoro.com/careers
Meet some of the skulk at: www.zoro.com/bios"
Big Data Engineer II,Amazon.com,"Amazon EMR ( http://aws.amazon.com/emr ) is an AWS service that makes it easy for customers to run their big data workloads. EMR supports well-known big data platforms like Hadoop and Spark, and multiple applications that are part of this ecosystem, like Hive, Presto, Pig and Hue.

The EMR team is looking for experienced, passionate and talented engineers to innovate in the rapidly growing area of big data and integrate the latest in big data technology applications into a massively scaled distributed service.

Utilize your domain expertise to tackles the complexities of build, integration and innovation while providing a reliable, quality stream of releases for today and emerging applications (Spark, Tez, Presto, Flink, etc.) in the big data space.

You:
 Are passionate about distributed processing and massive scaled applications
 Enjoy using and participating in development of open source applications
 Enjoy working with the latest software technologies and paradigms
 Are eager to be an early adopter of emerging big data applications
 Savor the challenges of making diverse technologies work through the entire stack
 Are comfortable with continuous integration workflows and tools like git, jenkins and gradle
 Are comfortable with Java and digging into any programming language in order to understand the internals of an application
 Are at ease with Linux troubleshooting and bash scripting and can do wonders with a single command
 Are comfortable with OS-specific packaging such as rpm/dpkg

Basic Qualifications  BS Computer Science or other relevant technical degree and/or related experience.
 Experience with Java or Scala build systems: maven, ant, sbt and gradle.
 Java or similar language development experience.
 Strong understanding of distributed systems and distributed computation.
 Deployment automation experience with scripting, chef, puppet, etc.
 Linux and RPM packaging experience.
 Experience with Hadoop, Spark or related projects
 Experience with virtualized environments and cloud services such as AWS
 Ability to communicate comfortably, at different levels, with different stakeholders
 Excellent communication and collaboration skills
 Worked on applications or web services deployed in at scale

Preferred Qualifications  Experience with Apache Hadoop ecosystem applications: Hadoop, Hive, Oozie, Presto, Hue, Spark, Zeppelin and more!
 Strong Java development experience.
 Experience building and operating at scale on AWS
 Commits or contribution via code or technical guidance to Apache Hadoop, Spark or related big data projects.
 Significant contribution to development for applications or services with a large user base"
Database Engineer,CGI,"CGI Federal is looking for a mid-level Database Engineer who will establish database management systems, standards, guidelines and quality assurance for database deliverables, such as conceptual design, logical database, capacity planning, external data interface specification, data loading plan, data maintenance plan, and security policy.

Your future duties and responsibilities: Adapts business requirements, developed by modeling/ development staff and systems engineers, and develops the data, database specifications, and table and element attributes for an application.
Documents and communicates database design.
Evaluates and installs database management systems.
Codes complex programs and derives logical processes on technical platforms.
Builds windows, screens, and reports.
Assists in designing user interface and business application prototypes.
Participates in quality assurance and develops test application code in client server environment. Provides expertise in devising, negotiating, and defending the tables and fields provided in the database.
At more experienced levels, helps to develop an understanding of client's original data and storage mechanisms.
Determines appropriateness of data for storage and optimal storage organization.
Determines how tables relate to each other and how fields interact within the tables for a relational model.

Required qualifications to be successful in this role: Bachelors Degree is required plus 2-5 years of relevant experience (Master's Degree a plus)
Knowledgeable of federal laws and regulations, directives, instructions, policy, standards, and guidance, so that requirements support security and privacy compliance. Knowledgeable of FedRAMP requirements and guidance.
Leads, directs or is responsible for implementation and compliance with information security/information assurance policies, privacy policies, access controls, data integrity, and database systems.
Experienced in database management and data administration support for medium-to-large databases. Advises/presents on best practices and provides support for database management and administration tasks such as standardizing data names, definitions, usage, and structures.
Leads and directs database specialists to manage databases, manage database projects, and provide database administration functions.
Experienced in database structures including design, normalization, documentation, file organization, and indexing and tuning of the design to make sure applications can make efficient use of the data contained within the database.
Leads, directs or responsible for creating, mirroring, backup, tuning data warehouses and implementing, managing and tuning data mining tools , makes recommendations on set up, selection, security, and management thereof.
Provides design and development of interface capabilities between database systems and other management systems to support access and usage of programmatic data. Manages access and usage of databases including data security.
Works with business analysts and clients to make sure functional requirements are embedded supported by the database and application developed during the SDLC.
Develops or directs the development of database documentation such as logical and physical database design, normalization diagrams (i.e., Entity Relationships), and any other documentation necessary to use for the SDLC phases of design to coding.
Leads, advises or oversees the critical database usage standards, procedures and conventions. Advises on new technologies and best practices in this area.
Presents technical briefings on designs, recommendations, project progress, status, issues, and solutions on data administration and database development projects.
Advises, evaluates, tests recommendations for newer technologies that can improve database management, access, security, of traditional datacenter -based databases, distributed and cloud based databases.
Understands and can lead, direct, develop, and set up SQL programs. Understands and can work with different database structures including hierarchal, relational, and object-oriented systems in a centralized or distributed environment.
Must have excellent oral and written communication skills

Desired Certifications in: Database software, PMP, etc.

What you can expect from us: Build your career with us.

It is an extraordinary time to be in business. As digital transformation continues to accelerate, CGI is at the center of this changesupporting our clients digital journeys and offering our professionals exciting career opportunities.

At CGI, our success comes from the talent and commitment of our professionals. As one team, we share the challenges and rewards that come from growing our company, which reinforces our culture of ownership. All of our professionals benefit from the value we collectively create.

Be part of building one of the largest independent technology and business services firms in the world.

Learn more about CGI at www.cgi.com.

No unsolicited agency referrals please.

CGI is an equal opportunity employer.

Qualified applicants will receive consideration for employment without regard to their race, color, religion, sex, gender Identity, sexual orientation, national origin, age, disability, veteran status, pregnancy, or other status protected by law. CGI will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with CGIs legal duty to furnish information.

Skills:
Database

Have you been referred by a CGI Member for this position?* Yes No"
"ASIC Digital Design Engr, Sr II",Synopsys,"We are looking for a dynamic ASIC Verification Engineer to work as part of the DesignWare Verification R&D team at Synopsys. You will be expected to use your knowledge of SOC ASIC, Custom IC and/or IP cores design to specify, design and implement state-of-the-art verification environments for the DesignWare USB family of synthesizable cores. You will work closely with RTL designers and be part of a team of expert Verification Engineers.

Requirements
BS in EE/CS with 10+ years of relevant experience, or MS with 8+ years of relevant experience in developing and debugging complex SoC designs and/or testbench environments. Candidate must have verification experience and debug skills of IP cores and/or SOC RTL designs. Candidate must have experience in developing system verilog, UVM, VMM or similar HDL based test environments. Developing and implementing test plans, extracting verification metrics, developing bus functional models, checkers and similar verification components is a Strong plus. Experience in the development of complex software projects, fluency in scripting, and a strong background in data structures and algorithms is preferable . It is essential that the individual is self-driven, has good communication skills and is able to demonstrate good analysis and problem solving skills. Working knowledge and experience of USB3.0/3.1 and AMBA protocol is a strong plus."
Data Security: Application Security Engineer,Time Inc,"The Data Security: Application Security Engineer will join a team of other Information Security professionals reporting directly to the Global Director of Application Security and Architecture. The qualified candidate will have data security and AWS experience. The Application Security Engineer will join an elite team of some of the smartest minds in the business that have been tasked with performing deep technical security assessments for our most critical applications to ensure that they are highly resilient against security threats. You will work on some of the most cutting edge digital technologies and provide value by solving real world problems that our industry as a whole is facing.

Your primary key stakeholders will be the Data Architecture, Engineering and Operations team. You will work directly with them on day-to-day security engineering and development tasks as well as various initiatives and projects focusing on data security.

The position will possess the primary skills in performing data security reviews, application security testing, and technical assessments working with the technical and digital development organization. You will be a key member of the team in communicating security weaknesses, exploits, and vulnerabilities to the business and technical teams, and you will be responsible for providing mitigating controls and security solutions.

The qualified candidate will be working out of the Time Inc.s Los Angeles, CA, Seattle, WA or New York, NY offices.

You Will:
Perform data security reviews with a strong understanding of PII data
Identify, document, measure and communicate technical Information Security risks using blended toolsets and exploitation techniques to identify attack surfaces
Determine data compliance requirements and identify applicable security controls
Provide data governance and security best practice principles as it relates to data security
Perform secure architecture reviews for database and data storage architectures

You Are:
3-5 years of combined data security, application security, data governance, data storage and management, and security vulnerability experience.
Strong knowledge in data security and the associated data compliance requirements.
Strong knowledge in data storage and management.
Strong knowledge in AWS architecture and services such as S3, Redshift, DynamoDB, etc.
Strong knowledge in RDBMs such as Oracle, SQL, etc.
Strong knowledge in transformation systems such as EMR, ETL, Lambda, etc.
The ability to conduct basic application penetration testing using a wide variety of exploitation techniques, tools, and procedures.
An understanding of coding and scripting, and ability to provide best coding practices.
A self-starter mentality. You are able to work under pressure and with limited supervision. Must be able to successfully prioritize and manage to completion multiple complex tasks and deliverables. Must be able to speak clearly to conduct teleconferences.

JR0004970

#LI-LD1"
Senior Data Engineer,Rover,"Who we're looking for:

Having just acquired our biggest competitor and expanded internationally, Rover now has more data in its systems than ever before! This exciting growth has created a lot of interesting new challenges. The types of requests we are getting to our Data Engineering Team have become more complex, and our rapidly growing data volume will require us to look into alternative solutions in the near future.

As our data systems expand so must our Data Engineering Team. We are looking to grow the team that builds and operates the data infrastructure that:

1. Rovers data scientists and analysts use when building machine learning algorithms,
2. powers Rovers reporting, analytics and experimentation systems, and
3. backs our the production systems for the Rover.com web site and mobile apps

This role is not a traditional BI role - these systems are a key piece of our production systems.

We're looking for a Senior Data Engineer to join our Data and Reporting Infrastructure team. In this role, you'll primarily be responsible for helping design, develop, implement, and operate Rover's data and reporting systems. Ideally, you'll also have experience with Software Engineering and Infrastructure Operations in the context of consumer-facing websites. You'll succeed in this role if you can help us further our culture of data self-service by designing and implementing tooling and infrastructure to satisfy our organization's rapidly growing data needs. The ideal candidate is detail-oriented, has excellent communication skills, and a knack for problem solving.

Who we are:

Rover.com connects pet parents with the nations largest network of pet sitters and dog walkers. On Rover, pet parents can easily discover, book, and manage personalized care for their dogs including pet sitting, dog walking, and doggy day care. Founded in 2011 on the belief that everyone should have the opportunity to experience the unconditional love of a dog, Rover improves and simplifies life for pet parents and the dogs they love.

We're headquartered in downtown Seattle. We're an agile, fast-growing startup, and our leadership comes from some of the country's most respected tech companies. We've been named among Seattle's ""Best Places to Work"" in Seattle Met magazine, Seattle Business Magazine, and the Puget Sound Business Journal."" We're also a recipient of Seattle Business Magazine's 2015 Tech Impact Awards. Needless to say (but we're going to say it anyway)we love dogs, and we wouldn't dream of going to work without them by our sides.

Your Responsibilities:
Design and develop software, tooling, and infrastructure to satisfy our organization's growing data needs
Lead the troubleshooting of complex problems in development and production environments, ensuring timely resolution of issues
Design, develop, implement, and operate Rover's data and reporting systems
Provide mentorship, guidance and best practices as a Senior Engineer to a growing team that is hungry to learn
Educate data consumers in our organization as to what data sources are available
Monitor, maintain, and improve the data pipeline
Collaborate on the architecture and design of the overall system
Your Qualifications:
8+ years experience in Software Engineering
3+ years experience working with production data systems
Strong Python skills in both data and engineering contexts
Strong SQL Skills
Redshift experience (or similar)Docker experience
Python workflow tooling - Luigi/Airflow/etc.
Excellent communication skills
Experience with an agile software development process
Ability to work cross-collaboratively with different business units within Rover with the ability to drive projects to completion as a Senior member of the team


Why we'll hire you:
You're qualified for the position and your values align with Rover's Core Values
You're a great Engineer who wants to get even better, and you have an ongoing thirst for new knowledge and ideas, and a passion for Software Engineering
You have a strong sense of prioritization and execution against critical deliverables, coupled with a sense of personal ownership for key projects / issue resolution.
You play nicely with others and believe in being successful as a team
You can take loose requirements, figure out what needs to be done, and make it happen!
You have experience using our tools or similar ones
You can be your own QA team, producing quality code the first time around
You can pitch your own ideas and shepherd them through to completion
You know when to call it 'good enough' and ship, and when to put in the extra time to polish your code
You like to work hard, but have fun while you're doing it!
This position is based at Rover HQ in Downtown Seattle. Candidates must be eligible to work in the U.S."
Machine Learning Engineer,JP Morgan Chase,"As an experienced Machine Learning Engineer within our Software Engineering group, your mission is to help lead
our team of innovators and technologists toward creating next-level solutions
that improve the way our business is run. Your deep knowledge of design,
analytics, development, coding, testing and application programming will help
your team raise their game, meeting your standards, as well as satisfying both
business and functional requirements. Your expertise in various technology
domains will be counted on to set strategic direction and solve complex and
mission critical problems, internally and externally. Your quest to embracing
leading-edge technologies and methodologies inspires your team to follow suit.
And best of all, youll be able to harness massive amounts of brainpower
through our global network of technologists from around the world.

The Machine Learning Software Engineer will be an early member
of a growing team responsible for designing and developing highly scalable
machine learning solutions that impact many areas of our business. The
individual in this role will collaborate with and mentor other Machine Learning
Engineers and Data Scientists.

An ideal candidate will have a background in software
engineering and data science; with expertise in machine learning, statistical
analysis and distributed systems. The candidate will be involved in all phases
of a project including ideation, design and implementation.

This role requires a wide variety of strengths and capabilities,

including:
Expertise in application, data and infrastructure architecture
disciplines

Advanced knowledge of architecture, design and business processes

Keen understanding of financial control and budget management

Ability to work collaboratively in teams and develop meaningful
relationships to achieve common goals

Study and analyze problems, propose solutions and design
experiments

Design and build machine learning solutions to solve complex
problems

Enhance our machine learning capabilities and expand our
infrastructure

This position is anticipated to
require the use of one or more High Security Access (HSA) systems.
Users of these systems are subject to enhanced screening which includes
both criminal and credit background checks, and/or other enhanced
screening at the time of accepting the position and on an annual basis
thereafter. The enhanced screening will need to be successfully
completed prior to commencing employment or assignment.

A post-graduate
degree (MS or PhD) in a quantitative discipline (e.g. Computer Science,
Mathematics, Operations Research, Data Science).

2+
years of hands-on industry experience developing machine learning applications.

4+
years of experience in a highly quantitative or analytical field.

Familiarity
with machine learning APIs and computational packages (TensorFlow, Scikit-Learn,
NumPy).

Experience
working with large scale distributed systems (Hadoop, Spark).

Experience
programming in Python, C++, Java or Scala.

Experience
translating analysis results into business recommendations.

Excellent
communication skills.

Our Consumer & Community Banking
Group depends on innovators like you to serve nearly 66 million consumers and
over 4 million small businesses, municipalities and non-profits. Youll support the delivery of award winning
tools and services that cover everything from personal and small business
banking as well as lending, mortgages, credit cards, payments, auto finance and
investment advice. This group is also focused on developing and delivering
cutting edged mobile applications, digital experiences and next generation
banking technology solutions to better serve our clients and customers.

When you work at JPMorgan Chase &
Co., youre not just working at a global financial institution. Youre an
integral part of one of the worlds biggest tech companies. In 14 technology
hubs worldwide, our team of 40,000+ technologists design, build and deploy
everything from enterprise technology initiatives to big data and mobile
solutions, as well as innovations in electronic payments, cybersecurity,
machine learning, and cloud development. Our $9.5B+ annual investment in
technology enables us to hire people to create innovative solutions that will
not only transform the financial services industry, but also change the
world.

At JPMorgan Chase & Co. we value the
unique skills of every employee, and were building a technology organization
that thrives on diversity. We encourage
professional growth and career development, and offer competitive benefits and
compensation. If youre looking to build
your career as part of a global technology team tackling big challenges that
impact the lives of people and companies all around the world, we want to meet
you.

Ready
to use your expertise and experience to drive change? Apply today."
Data Analyst,Aaptiv,"About Aaptiv

Aaptiv is the fastest growing mobile fitness product on the marketwith nearly 200,000 paid subscribers in less than 2 years of business. We are the only service that turns your phone into an on-demand fitness studio with all your favorite classes and music. That drive you feel to run, push, or pedal harder in a class or when working with a personal trainer? Aaptiv recreates the experience by synchronizing the voice of a trainer with a playlist of all the music you love, to deliver fun and motivating guided workouts straight to your earbuds.

At Aaptiv, we're building a fitness platform that enables us to develop long-term and personal relationships with users based on their data profiles. By analyzing user behavior patterns, we are able to both create and recommend content customized to specific fitness levels and interests.

Want to join our team? We're looking for people who are passionate about building a world-class fitness experience. There are over 80 million Americans who value fitness - and we believe every one of them should be an Aaptiv user.

About the Role

Aaptivs product consists of native mobile applications for iOS and Android as well as a full featured API and content management system. These systems all generate user behavioral, events, and transactions information. Were looking for a strong data analyst to add to our Data team who can help us structure, organize and analyze data across the organization to gain insights in our business and operations.

Our ideal candidate has a background in test engineering, software development and exposure to AWS / cloud systems. This role is one for a true generalist who loves to continue to learn new technologies and techniques. A can-do attitude and excitement to work across a diversely skilled team is critical.

What You'll Do

Working closely with all business stakeholders to analyze, organize, and visualize data
Developing processes to define and measure KPIs
Running ad hoc analyses to answer strategic questions that can help move our fast growing business forward
Creating and maintaining data models in Looker to support reporting as a service
Automating data pipelines from multiple sources in order to run analyses and derive insights
Working collaboratively in an agile environment with engineers across mobile / API / data to ensure data precision is enforced
Who You Are

Strong SQL skills (PostgreSQL)
Ability to provide discoveries using data
Excellent communication skills
3+ years of scripting experience (Python, Bash)
Experience with ETL process / software
Experience with data analytics / visualization tool (Looker)
Experience with AWS tools (RDS, Redshift, EC2, Data Pipeline, etc.)
Understanding of data access patterns and performance impacts in relational and dimensional models
Exposure to mobile application development life cycle"
Data Visualization Engineer - Machine Learning focus,Capital One,"114 5th Ave (22114), United States of America, New York, New York

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Data Visualization Engineer - Machine Learning focus

Data Visualization makes data tangible for our company and customers, thereby empowering us all by transforming whats unknown into something thats known. We enable these outcomes by digging into data sets, large and small, to manifest it in enlightening, understandable ways to fuel decision-making.

We're looking for engineers to help build this new practice within our Design organization. We partner with our Center for Machine Learning to find meaning and understanding in previously inaccessible areas. You can expect your work to be applied to the domains of cybersecurity, fraud, anti-money laundering, and explainable AI among others. These efforts are at the forefront of Capital One and drive our goals and strategy.

WHY WE CARE
Because were aiming to Change Banking for Good, which means being brave enough to change how people interact with us AND their money, for the better
Because we want to know things thatll enable us to make strategic decisions with the greatest degree of confidence, driven by real-time data
Because insights should be accessible to us, both as employees of Capital One, and as customers who are trying to make sense of their money

WHO ARE YOU? LIKELY SOMEONE WHO
Is deeply creative and driven by playfulness, wonder, and a thirst for understanding
A problem solver with a knack for uncovering elegant solutions
Asks questions to get at the deeper-level why because you prioritize relevance and meaning over the artifact or visualization itself
Better understands a complex ML algorithm by visualizing it
Is outcome-focused and has an exceptional track record to prove it

HERE'S WHAT YOU CAN EXPECT IN THIS ROLE

As a Capital One Data Visualization Engineer, you may work on internal and customer facing tools, platforms, and libraries. Heres what you'll be doing:

Seek
You will sit with product owners and Machine Learning Engineers to discuss their problem space. Rather than take orders on what to build, you'll engage users in conversations to uncover how they do their work, what tools they use, how they understand their work, and how they currently solve their problems. You will work to explore pain points and areas for meaningful improvement.

Design
You'll spend the majority of your time finding the shape of data. This entails immersing yourself in and exploring the data. You'll rabidly pull apart and tease out the nuance of the data to understand the breadth, depth, extents, outliers, relationships, and other factors that reveal its structure as a whole. Your sketchbook is your constant companion. It picks up where your tools fail you.

Build
You'll bring solid experience in technologies such as: D3.js, WebGL, Processing, Three.js, Unity, and Python to name a few. Using these technologies and others, you'll construct meaningful, interactive, responsive, frictionless, and succinct visualizations for a variety of subject matters and domains.

Basic Qualifications:
Bachelors degree in Computer Science, or military experience
At least 3 years of experience engineering data visualizations and interactive experiences
At least 3 years experience building data visualizations using D3, WebGL, Processing, Three.js, or Unity
At least 1 year experience working on a cross-functional team

Preferred Qualifications:
Masters degree in Data Visualization, Computer Science, or related field
At least 1 year of full-stack web development experience using React, Node, AWS, and SQL
Experience with Kafka or Spark
Conversational knowledge of ML, AI, NLP methodologies; and related Python libraries
Experience engineering NLP, AI, and ML models in Python is a big plus

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Data Engineer / Administrator,Visa,"Job Description

Information security is an integral part of Visas corporate culture. It is essential to
maintaining our position as an industry leader in electronic payments, and it
is the responsibility of each and every employee to safeguard information,
protect it from unauthorized access, and ensure regulatory compliance. Information security has a significant effect on privacy, consumer confidence, external reputation, and/or the bottom line,
and it is a priority on everyones agenda.
Successful candidate will part of Visa Security Analytics program. Visa Security Analytics
(VSA) program has an enterprise security focus, and is responsive to evolving
threat landscape, regulatory compliance, IT security requirements and
technology architecture. The VSA program is part of cyber security organization,
handles long-term retention for security system logs and security sensor logs.

The successful candidate will:
Will be a part of the Security Analytics group and involved
with Data Engineering activities on Hadoop and Greenplum Data warehouse (GPDB) ecosystem.

Experience on working with very large data sets and
knowledge of building programs that leverage Massively Parallel Processing
(MPP) Data warehouse platforms.

The engineer will have significant knowledge of Big Data
technologies and tools with the ability to share ideas among a collaborative
team. Some of the responsibilities include loading data from several disparate,
data sets, documentation, performance testing and debugging applications.

Will represent the team in the project meetings. Qualifications

Masters degree in Information Security or Information
Systems

2+ years of Data Engineering/ETL/Administration experience

Fluent in at least one scripting language
(Shell/Perl/Python/Java etc.)

Experience with GreenPlum database administration &
development is a must

Experience with no-sql database administration &
development like mongoDB is a must

Good Understanding of cyber security threat landscape

Experience with Hadoop eco-system (MapReduce, Streaming,
Pig, HIVE, Spark).

Ability to create and manage big data pipeline using kafka,
flume & Spark

Experience building large-scale distributed applications
and services

Experience with Scala programming is plus

Experience with agile development methodologies

Knowledge of industry standards and trends

Nice to have: Data Science modeling

Additional Information

All your information will be kept confidential according to EEO guidelines."
Entry Level Software Engineer,APCON,"Career Opportunity: To write software in a team environment which includes research, design, debug, and test for next generation network management software systems while contributing to the existing product line.

Responsibilities:
Designs, implements, debugs, and tests software for cloud computing platforms like AWS and Azure.

Partners with the Cloud Services Team to develop innovative solutions for monitoring data in the cloud.

Creates proof of concepts for validating design.

Works with product management on creating, analyzing, and implementing solutions that can be deployed with different cloud providers.

Responsible for creating written documentation for his/her own software development.

Works in both an independent and team member environment to meet multiple product release cycles.

Requirements:
Proven ability to use any OOP language (C++, Java)

Experience with software testing tools and procedures

Good understanding of object oriented design and design patterns.

Prefer experience interacting with JSON, XML, SOAP, web services and web applications.

Strong analytical and trouble-shooting skills.

Good oral and written communication skills.

Education and/or Experience:
BS degree (BSEE or BSCS strongly preferred, MSCS a plus) or the equivalent in training and experience.

Coursework or experience relating to software development practices.

Coursework or experience in Java/C/C++ and/or any scripting language (Python, Perl, Shell, Ruby, PHP).

Coursework or experience with computer networking, Ethernet, IP, VLANs, or telecom a plus.

About APCON: For more than 23 years, APCON has been on the forefront of cutting edge, innovative network monitoring technology. Our core values have and continue to be a benchmark for everything we do as a company. All qualified applicants will receive consideration for employment without regard to race, color, sex, sexual orientation, gender identity, religion, natural origin, disability, veteran status, age, marital status, pregnancy, genetic information, or other legally protected status."
Senior Data Visualization Engineer,Akuna Capital,"Storytelling through data is both an art and a science, we are looking for a Senior Data Visualization Engineer to do just that by driving our Data Visualization platform to the next level. This role will focus on ensuring our desktop and tablet trading applications are highly performant both today and tomorrow through all market conditions. You will work with Senior Trading Platform and Data Engineers, and Trading & Tech Management to identify priorities, requirements and resourcing in order to execute tasks accordingly. We are looking for a candidate who is passionate about data visualization, problem solving and driving Akuna from competitive to dominant.

What youll do as a Senior Data Visualization Engineer at Akuna:

Build an intuitive data visualization platform that allows Akuna to seamlessly trade through high volatility markets and provide traders with easy access to information at a glance
Support the growth and scalability of the company by building a Data Visualization platform that works in both current and future markets. This will be key as platform performance is our number one metric for this position
Develop innovative and intuitive new tooling for trading desktops and tablets
Construct a holistic view for data visualization road-map
Oversee and provide technical design leadership to Data Visualization team
Drive and promote your vision- interfacing with our existing applications requires you to have a big system engineering mindset
Collaborate on vision with other Senior Engineers who own various parts of our trading or data systems
Choosing the appropriate technologies for the various problems that need to be solved
Qualities that make great candidates:
Bachelors degree in Computer Science, a related field or equivalent work experience required
6+ years of experience in UI/ UX
Ability to support current C# platform and evolve to next generation (not necessarily in C#), we are open to using new languages for this
Must be an experienced developer in C++, Python, C# or Java or similar
Experience with Big Data and high throughput low-latency systems
Ability to communicate information clearly and efficiently through UIs
Experience with web development technologies such as React
Ability to build a system that takes data from a variety of sources and present it to the user in a cohesive manner with no perceptible delay"
Data Analyst,Inovalon,"Inovalon, Inc. is seeking Data Analyst for multiple openings.

Responsibilities
Collaborate with business and technology teams on data analysis and modeling;
Translate business requirements into technical requirements for any specific feature or function;
Design, code, test and debug business logic for different product lines using Microsoft T-SQL;
Conduct User Acceptance Testing per defined business rules and specifications using custom queries;
Conduct root cause analysis on any regular ad-hoc client requests;
Diagnose data related problems on the test environment; and
Communicate test progress and results to the relevant stakeholders and management team.

Qualifications
Bachelor's degree in Computer Science, Engineering, Business Analytics, or related field of study, plus at least 2 years of experience in the job offered or at least 2 years of experience as an Engineer, Lead, Analyst, or a related role. In lieu of a Bachelor's degree in Computer Science, Engineering, Business Analytics, or related field of study plus at least 2 years of experience, Employer is willing to accept a Master's degree in Computer Science, Engineering, Business Analytics, or related field of study, or related plus 1 year of experience in the job offered or 1 year of experience as an Engineer, Lead, Analyst, or a related role.
All applicants must have experience with: (1) SQL/T-SQL, SQL Server 2008/SQL server 2012; (2) SQL Server Reporting Services and/or any BI reporting platform; and (3) MS Office, including MS Project, Excel and Visio."
Financial Data Engineer,TGS Management Company,"Interested in an opportunity to work in a robust, challenging, data-focused environment with an outstanding team of technical and research professionals? At TGS, financial data is at the heart of our business. The Financial Data Engineering role is mission critical - requiring uncommon reliability, a passion for writing great software and tools, experience working with a wide variety of data (preferably financial data, but not necessarily), and the ability to support large-scale production systems. Its an exciting, challenging role that provides clear opportunities to contribute to the success of an exceptional organization.
In addition to writing programs and working with data, the successful candidate will work closely with our research team and is likely to interact with a variety of external resources such as data providers, brokers, dealers, and software vendors.
To be considered for this role, you will need to demonstrate expertise in a number of the following general areas:
Programming: experience developing data management tools and/or applications using languages such as Java, Perl, Python, or C/C++
System tools: experience with scripting languages (Perl, Python, shell scripts, etc.), and Unix-based operating systems (especially Linux)
Production support - willingness and ability to support large, complex production systems in an on-call or rotational capacity
Analysis of large data sets: experience developing programs to parse, process, analyze, and comprehend large data sets
Applications: experience designing, developing, and maintaining software applications, tools, and systems
Vendor interaction: working with external resources to solve problems, acquire data, and improve relationships
Financial data: familiarity with financial terms and experience working with data from a variety of vendors and sources

About Us

For over two decades, the TGS team has built quantitative trading systems that have produced exceptional results across a range of financial markets. We use scientific methods and engineering discipline to solve challenging problems and develop technology solutions. Our Irvine office is as unique as our Southern California location, combining elements of high tech, finance, and applied research in a collegial atmosphere and beautiful workspace.

As an employer, we are small, discreet, and highly selective. We look for talented people with proven track records of performance and achievement, and are far more interested in aptitude and potential than expertise in any particular technology, tool set, or professional domain. If you're inspired by the idea of working on interesting problems with talented colleagues, we invite you to share your resume and explore the possibility of joining our team."
Senior Data Engineer,"athenahealth, Inc.","The Opportunity:
Do you love to build solutions to big, challenging problems? Do you embrace the challenge of using unstructured data to deliver real-time actionable insight to a leading healthcare organization?
As a Senior Data Engineer, you'll be responsible for contributing to the design of the next generation of athena's distributed data pipeline, which will drive instantaneous access to insight throughout our organization. In addition, you will be a technical mentor to other Data Engineers, helping them to make architectural and technology decisions.
You'll partner with the Data Science and Software Engineering teams to create and architect innovative solutions to scale our ability to learn from our growing network.

Responsibilities may include, but are not limited to:
Technical Execution
Produce accurate, unambiguous technical design specifications to the appropriate detail
Deliver customer value in the form of high quality software components and services in adherence with RPR policies on Security, performance, longevity and Integration testing
Estimate the size of development tasks in story points with the goal of 80% accuracy; begin to challenge others during estimation
Understand and follow coding conventions, architectures, and best practices
Write, debug, and deploy code to production; deliver timely fixes
Adhere to DOD (story definition of done) as part of the sprint, including unit tests, functional testing, code reviews, no regressions, bug fixes, documentation and adhere to best coding practices
Perform peer code reviews in order to ensure quality standards.
Contributions to the Team
Master the domain knowledge for area of assignment
Take ownership of what the team builds after the release; consider customer adoption as success criteria
Contribute to agile ceremonies to improve team performance
Volunteer for work in the backlog and commit to quality delivery; Coordinate efforts across te team to ensure completion and effectiveness
Participate and contribute to scrum meetings i.e. daily stand-up, sprint planning, readouts and retrospectives
Drive self-organization; help determine how the team functions in collaboration with your peers
Cross functional Coordination and Communication
Work collaboratively across the Technology and Product organizations to ensure alignment towards business goals
Builds strong relationships with cross-functional team members
Share business and technical learnings with the broader engineering and product organization, while adapting approach for different audiences

Education, Experience, & Skills Required:
4-6 years of experience in an engineering role
Experience in an Agile environment preferred
Bachelor's Degree or equivalent
Strong software engineering skills and computer science experience
Knowledge of at least one modern programming languages such as: C#, C++, Perl, Java, Python, and javascript/jQuery
Familiarity with Univ/Linux, Big Data, SQL, NOSQL, and various other technologies
Exposure to object oriented programing, relational database technologies, distributed computing tech (Hadoop, spark), RESTful API, WebUI (HTML 5) and Modern JS frameworks
Understanding of iOS and Android environments

Behaviors & Abilities Required:
Ability to learn and adapt in a fast paced environment, while producing quality code
Ability to work collaboratively on a cross-functional team with a wide range of experience levels
Ability to prioritize both individual time and the time of the team
Ability to write code that is technically sound, performant, scalable, and readable
Ability to collaborate with business owners to understand and refine business requirements
Ability and willingness to demonstrate ownership of an area of athena's technology

Location of Role Watertown, MA"
Senior Data Engineer,Concur,"Are you a Software Engineer with a passion for data? Do you want to work with teams that are fundamentally improving travel for millions of people? Do you want to help define an infrastructure for data collection and analysis? This position is the founding member of a Data Engineering function within Concur Technologies Traveler Services organization, the same folks that brought you TripIt and now ExpenseIt.

This position will work alongside the Data Science & Services team in Concur, and focus on the data and logic that applies to TripIts 10+ million users and a growing user base for ExpenseIt. The successful candidate will combine a passion for software engineering with a love of data, the ability to synthesize business priorities into working systems, and great communication skills.

Position Requirements

Responsibilities:

Design and develop internal Traveler Services Data Platform
Collaborate with Concurs Data Sciences team to integrate and/or utilize data and tools
Create services to power TripIt and ExpenseIt product features based on data science
Design key performance metrics with Product and Engineering stakeholder input
Provide the Customer Insights & Analytics team with data specific to Traveler Services

Position Requirements: Education, Experience & Training required:

BSCS, BSEE, or equivalent years of work experience
6+ years in a related software development role
3-5 years of experience in Analytics, Consulting, or similar quantitative roles
Familiarity with one or more relational databases (e.g. MySQL, Postgres, etc) and solid knowledge of SQL
One or more server side scripting languages (e.g. Python, Ruby, PHP, Javascript).
Experience with the AWS ecosystem tools such as EC2, Lambda, Redshift, Cloudformation, EMR
Development experience in Apache Open Source / Big Data projects (e.g. Hadoop, Hive, Spark etc.) preferred
Comfortable working in a GNU Linux/Unix environment
Effective communication skills, focusing on presentation of technical information
Strong team-working skills with ability to work independently, and work well with colleagues at all levels in the organization
Previous experience with travel industry a plus.

Value Competencies:

Displays passion for & responsibility to the customer
Hires, develops & rewards great people
Displays leadership through innovation in everything you do
Displays a passion for what you do and a drive to improve
Displays a relentless commitment to win
Displays personal & corporate integrity

EEO:

Concur is an Equal Opportunity Employer and applies this policy to all applicants and employees. We are committed to hiring and valuing a global diverse work team. (M/F/D/V)

Concur is a dynamic, growing and fast-paced organization. As such, successful employees are able to work in a fast-paced environment, managing multiple priorities often times under tight deadlines. This typically requires working a 40+ hour work week to accomplish performance objectives. With that, Concur offers flexibility as to the specific working hours that may be required or available depending on your role.

Concur is a SaaS company. Employees must be technically savvy with the ability to use the computer/keyboard and telephone to conduct business. The ability to creatively problem solve to our core value of Leadership through innovation in everything we do. Many positions within Concur are customer facing so written, verbal and interpersonal communications skills are required for a majority of opportunities with Concur.

Confidentiality and our core value Personal and corporate integrity are critical components being that Concur is a publicly traded company and working towards building a great, enduring company.

E-Verify:

Concur participates in E-Verify to confirm work authorization.
*LI-WL1 IND123 #DCE

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class."
Senior Data Engineer,Talkdesk,"Together at Talkdesk, were building a future of brilliant customer interactions. We have the cleverness, curiosity, and grit to make us a team of customer heroes: thinkers, achievers, and dreamers who believe that our world-class SaaS platform can influence a new kind of customer interaction.

Talkdesk started from a hackathon win and within five years has become one of the fastest growing companies in the world, a Forbes Next Billion Dollar Startup, and has enabled 100,000,000+ customer interactions using our platform. With backing from DFJ, Salesforce Ventures, and Storm Ventures, and supported by the successes of our 1,200+ customers from DoorDash, Box, Betterment to Shopify, Talkdesk is disrupting a $22 billion stagnant market.

Were now looking for the new members of the Talkdesk family - those ambitious, driven, and collaborative individuals who thrive in a fast-paced environment and will push us to do even greater things together. If you are a world-class Senior Data Engineer and would like to help us shape the future of Talkdesk, come along with us on our journey - your dream job is waiting!

The Senior Data Engineer will steer development of a highly scalable and extensible real time and batch oriented data pipeline for collection, storage, processing, and analysis of large sets of data from multiple data sources including our real-time communications platform. You will be responsible for choosing an optimal solution to use for these purposes, then maintaining, implementing, and monitoring them. You will have an opportunity to work with and grow a team of other data enthusiasts.

Responsibilities

Selecting, integrating and building any Big Data tools and frameworks required to provide requested capabilities: ex Storm, Hadoop, etc

Implementing ETL process

Monitoring performance and advising any necessary infrastructure changes

Defining data retention policies and capacity planning

Work closely with product managers, data analysts, user experience experts, and quality engineers to build new features to empower our business through data

Skills and Qualifications

Proficient understanding of distributed computing principles

Proficiency with building stream-processing systems, using solutions such as Storm or Spark-Streaming

Proficiency with Hadoop, MapReduce, HDFS

Good knowledge of Big Data querying tools, such as Pig, Hive, and Impala

Experience with integration of data from multiple data sources

Experience with NoSQL databases, such as MongoDB, Cassandra, HBase

Strong SQL knowledge

Knowledge of various ETL techniques and frameworks, such as Flume

Experience with various messaging systems, such as Kafka or RabbitMQ

BS or MS in Computer Science, Computer Engineering or other related discipline

Nice to Have:
Experience with Big Data ML toolkits, such as Mahout, SparkML or H2O

Good understanding of Lambda Architecture, along with its advantages and drawbacks

Experience with Cloudera/MapR/Hortonworks

Management of Hadoop, Storm clusters with all included services

Ability to solve any ongoing issues with operating the cluster

The Talkdesk story hinges on empathy and acceptance. It is the shared goal among all Talkdeskers to empower a new kind of customer hero through our innovative software solution, and we firmly believe that the best path to success for our mission is inclusivity, diversity, and genuine acceptance. To that end, we will hire, promote, work along, cheer for, bond with, and warmly welcome into the Talkdesk family all persons without regard to ethnic and racial identity, indigenous heritage, national origin, religion, gender, gender identity, gender expression, sexual orientation, age, disability, marital status, veteran status, genetic information, or any other legally protected status."
RDC Data Engineer,Credit Human Federal Credit Union,"About Credit Human:

Chartered in 1935, Credit Human is a $2.7 billion credit union serving 250,000 members in 43 states across the U.S. Headquartered in San Antonio, we are a not-for-profit, member-owned financial cooperative offering competitive products and services from 16 San Antonio locations, one in Houston and nationally through our manufactured home lending division.

With Credit Human, you gain more than a job. You gain a career with competitive compensation, comprehensive benefits, and a clear sense of community. Credit Human is the place where true craftsmanship and potential are seeking to meet in the financial services industry, focusing on serving and building meaningful relationships with members. We are looking for employees who share like values and, more importantly, are dedicated to a purpose-driven mission.

We are seeking a Data Engineer for the Retail Delivery Channel.

The role is accountable for consolidating Retail Delivery Channels multiple and disparate data sources into one central system (Retail Database). The role collaborates with and interprets RDC's Team desired business data needs and working these details into an understandable approach/road map, and then translating these plans into workable technical solutions. The role is accountable for creating the architectural design of conceptual, logical and physical models of the Retail Database (RDB). The role leads and coordinates discussions with Consultants, and the Retail Delivery Channels team in an effort to devise the appropriate Retail Database (RDB) structure in an effort to enhance the teams risk assessment skills.

The role is required to maintain strong knowledge of and employing database building techniques/practices. The role is accountable for engineering the architecture, modeling and design of the RDB to include generation of necessary bus matrix, data dictionary/library, dimensional modeling, star/snowflake schemas, ETL process, integrity validation, ongoing maintenance and integration of the RDB with related business intelligence front-end applications (e.g. Excel/Tableau/PowerPivot/PowerBI/SharePoint). The role is also accountable for supporting and serving as technical liaison with IT and 3rd party vendors for existing and new systems.

If you have the passion for building databases and turning raw data into meaningful insight you should apply right away!

Highlights:
Import data from various data sources; manipulate quickly thousands of data records by using code (XML/SQL)
Perform data cleaning to rid the system of old, unused, or duplicate data for better management and/or quicker access
Establish data standards and procedures, data modeling, systems architecture design to ensure access to and integrity of data assets
Perform initial data preparation in the form of depth matching, splicing, merging and other data manipulation.
Improve and stream line processes regarding data flow and data quality to improve data accuracy, viability, and value
Design database architecture schema, data model and extract, transfer and load of data into RD
Schedule: Monday-Friday, 8:30am-5:30pm

Level: 2C

Credit Human provides employees with many benefits from insurance coverage to college tuition reimbursement.

Employees who work at least 25 hours per week qualify for:
Vacation and Sick Leave Programs
12 Paid Holidays per Year (Full-Time Employees)
Medical, Dental and Vision Insurance
Voluntary Insurance Products  Life, Accident, and Cancer
Basic Life/AD&D Insurance - Employer Paid
Long-term Disability Insurance - Employer Paid
Flexible Spending Accounts (FSAs) - Medical/Dependent Care
401(k) Plan
Retirement Plan  Defined Benefit Plan
Employee Assistance Program (EAP)
Travel Assistance Program
Credit Union Membership with Free Checking and Service Fee Discounts
College Tuition Reimbursement
Learning and Development Opportunities
Credit Human conducts employment background checks that may be used for decisions related to employment with Credit Human. Standard background checks performed on final candidates include NCUA Administrative and Prohibited Orders, CHEX systems, past employment verifications, criminal history check on convictions and outstanding arrest warrants within the past seven years. Degree verification are conducted if listed on the employment application. Additional background checks relevant to the role may include a motor vehicle registration check, credit check, and/or fingerprint card.

Credit Human is an Equal Employment Opportunity Employer. Qualified applicants will receive consideration for employment without regard to race, color, religion, sex, national origin, sexual orientation, gender identity, genetic information, disability or protected veteran status."
Data Engineer - Analytics Platform,Grubhub,"About The Opportunity Welcome! Hungry for a new venture?

We work with awesome people every day. We dont mean awesome like how other people use awesomein some conjunctive form with sauce or balls. We really mean it. We work with some of the greatest people in the tech industry here at GrubHub. Were looking for more.

GrubHub leads in the mobile food-ordering industry with a portfolio of brands that includes GrubHub, Seamless, MenuPages and Allmenus. Were dedicated to connecting hungry diners with local restaurants with our internal delivery network. With offices in New York City, Chicago and London, GrubHub supports every order with 24/7 customer service. We want you to enjoy where you work. Who you work with. What you work on. At GrubHub, you can order your cake and eat it too.

Some Challenges Youll Tackle
Develop compelling PoCs for data solutions using emerging technologies for real-time and big data ingestion and processing
Contribute to designing, building, and deploying high-performance production platforms/infrastructure to support data warehousing, real-time ETL, and and batch big-data processing; help define standards and best practices for enterprise usage
Design, build, and maintain processes and components of a streaming data/ETL pipeline to support real-time analytics (from requirements to data transformation, data modeling, metric definition, reporting, etc)
Focus on data quality - detect data/analytics quality issues all the way down to root cause, and implement fixes and data audits to prevent/capture such issues
Collaborate with data scientists to design and develop processes to further business unit and company-wide data science initiatives on a common data platform
Translate business analytic needs into enterprise data models and ETL processes to populate them

You Should Have
Bachelors in Technology, Science, Statistics or Math degree preferred but equivalent experience will be ok
8-10 years of data engineering experience in traditional data warehousing/ETL and/or big data pipeline & processing environments
Strong data modeling and SQL experience (Dimensional/Star, Transactional/3NF)
Strong programming experience with any of: Python, Scala or Java
Experience with emerging big data processing technologies (Spark, Storm, Kafka, Flume, Pig, Hive, Sqoop, Hadoop/MapReduce, etc)
Experience with columnar storage and massive parallel processing data warehouses (Redshift preferred)
Experience modeling and querying for NoSQL databases (Cassandra preferred, HBase acceptable)
Experience working within the Amazon Web Services (AWS) ecosystem (S3, EC2, etc)
Experience working within agile software engineering methodologies
Strong analysis and communication skills required
Got These? Even Better:
Experience with one or more ETL/Data Integration frameworks (e.g., Talend, Informatica, Pentaho, etc.)
Exposure to BI/analytics platforms (e.g., Tableau, Microstrategy, etc.)
Familiarity with statistical methods and experimentation (A/B testing)

And Of Course, Perks!
Unlimited paid vacation days. Choose how your time is spent.
Never go hungry! We provide weekly GrubHub/Seamless credit.
Regular in-office social events, including happy hours, wine tastings, karaoke, bingo with prizes and more.
Company-Wide Initiatives encouraging innovation, continuous learning and cross-department connections.
We deliver favorites every day. Join us as we move eating forward.

Grubhub is an equal opportunity employer. We evaluate qualified applicants without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, and other legally protected characteristics. The EEO is the Law poster is available here: DOL Poster. Grubhub is committed to working with and providing reasonable accommodations to individuals with disabilities. If you need a reasonable accommodation because of a disability for any part of the employment process, please send an e-mail toTalentAcquisition@grubhub.com and let us know the nature of your request and your contact information."
DATA ANALYST  BUSINESS INTELLIGENCE,"Honda R&D Americas, Inc.","We are seeking a Data Analyst Engineer to join our team.

The team leverages several advanced platforms to deliver on high priority strategic initiatives, working closely with internal business teams in a highly collaborative, cross-functional and fast-paced environment, ensuring the programs and projects are aligned with enterprise timelines and business objectives.

It is essential that this person be a self-starter and able to work well both in a team environment as well as independently. This role is expected to be a substantive contributor to the outcomes described and must ensure high quality deliverables and end user satisfaction are attained in all areas. In addition, the coordinator must consistently display good judgment, effective interpersonal communications. Confidentiality and discretion are required consistently.
Essential Job Requirements:
Business Analysis  Capture and interpret the business needs/requirements through appropriate means (interviews, document analysis, workshops, etc) and translate them into application and operational requirements.
Process Management and Improvement  Actively participate and assist in the development and adherence to project management processes and be a champion of ongoing process improvement initiatives to implement best practices.
Team Building  Promote active participation and empowerment of the team, ensure youre fully engaged in the project and making a meaningful contribution, with a sustainable pace and high-level of quality.
Critical Thinking & Problem Solving - Recognizes problems and/or opportunities that are new or without clear precedent. Evaluates alternatives and finds solutions using a systematic, multi-step approach with the goal of providing improvements or innovations to enhance overall business performance.
Data Visualization  Organize and develop easy to use and interpret reports/dashboards for use by all levels of the organization. Employing basic user experience (UX) methods and delivery techniques to enable quick understanding, what-if analysis and decision making.

Responsibilities:
Works closely with management and lead BI Analyst to understand key business behaviors and decision-making processes, identifying and developing analyses that improve the business
Determines the true underlying need of an information request. Analyzes and verifies requirements, ensuring that requirement statements are complete, consistent, concise, clear, traceable, feasible, and verifiable, and that they conform to appropriate standards set by the organization. Advises business on options, risks, and costs/benefit.
Combines data from multiple sources to create integrated views that can be used to drive decision making
Has a thorough understanding of the data analysis process and is able to set expectations for analyses (timing, delivery, issues, etc.). Communicates requirements for the design and implementation of analytics solutions to technology teams, representing the business process perspective during design reviews, and executes the Quality Assurance (QA) review prior to implementation
Acts independently to identify opportunities for improved analysis or efficiencies
Actively learns about and evaluates new technology, methodologies and data transformation techniques that are relevant to achieving the goals of the organization.
Provides input into our company data strategy & governance practices and works to incorporate them in the ongoing development and process

Qualifications
BA or BS in MIS, CIS or other similar programs or and a minimum of 1 of year experience or equivalent combination of education and experience
Strong interpersonal skills including mentoring, coaching, collaborating, and team building
Strong analytical, planning, and organizational skills with an ability to manage competing demands
Strong knowledge and understanding of business needs with the ability to establish/maintain high level of customer trust and confidence
Solid understanding of and demonstrated experience in using appropriate tools: Jira, Microsoft Project, Visio, and all Office Tools
Superior analytical skills with the ability to present sophisticated and quantitative messages in simple terms
Understanding of data visualization concepts and best practices
Excellent oral and written communications skills and experience interacting with both business and IT individuals at all levels including the executive level
Creative approach to problem-solving with the ability to focus on details while maintaining the big picture view.
Ability to travel and work overtime may be required."
Software Engineer,Teradata,"Software Engineer-175086

Description
Teradata empowers companies to achieve high-impact business outcomes through analytics. With a powerful combination of Industry expertise and leading hybrid cloud technologies for data warehousing and big data analytics, Teradata unleashes the potential of great companies. Partnering with top companies around the world, Teradata helps improve customer experience, mitigate risk, drive product innovation, achieve operational excellence, transform finance, and optimize assets. Teradata is recognized by media and industry analysts as a future-focused company for its technological excellence, sustainability, ethics, and business value.
The Teradata culture isnt just about one kind of person. So many individuals make up who we are, making us that much more unique. Its what sets apart the dynamic, diverse and collaborative environment that is Teradata. But even as individuals, theres one thing that we all share our united goal of making Teradata and our people, the best we can be.
Our Team
Teradata Labs is where cutting-edge innovations in data management turn into business value. Our outstanding team of database architects and software engineers work together to understand and advance emerging technologies to produce the next wave of big data analytic solutions. Teradata Database is the core of Teradata Massively Parallel Processing (MPP) systems that run on-premises and in hybrid clouds to manage and optimize sophisticated workloads.

The heart of Teradata Database is its best-in-class query optimization engine. We work on query optimization techniques in database and analytics engines, machine learning algorithms, scalability and elasticity issues, and many other exciting challenges related to performance, usability, accessibility and integration.

Your Opportunity
The Database Query Optimization group at Teradata Labs has an opening for Software Engineer III  Grade 11. As a Software Engineer III in the Database Query Optimization group, you are expected to contribute to the design, development, and testing of new enhancements and advanced features for the the Teradata Analytical Platform.

Responsibilities / Interesting Work Youll Do
 Perform competitive analysis of other database systems
 Implement, validate, and test new database and novel query optimization features in an Agile form
 Contribute to the delivery and continuous support of robust, resilient, and quality database products

Qualifications
Qualifications / Experience
 Bachelors Degree in Computer Science, with at least five years of related research or industry, or
 Masters Degree in Computer Science or related discipline, with at least three years of related research or industry experience, or
 Ph.D. in Computer Science or related discipline
Ideally, you will also have the following
 Top-notch programming skills in C++, Python, R, SQL
 System development experience
 Passionate, self-motivated, risk taker, pro-active, initiative taker, good communicator (written & verbal), creative, and team-oriented
 Familiarity with various database technologies
 Experience using Agile software development methods and tools
Location
The work location for the open positions is El Segundo, California (primarily) or Rancho Bernardo in San Diego, California.

Benefits Youll Enjoy
*Our total compensation approach includes a competitive base salary, 401(k), strong work/family programs, and medical, dental and disability coverage.

Teradata is an Equal Opportunity/Affirmative Action Employer and commits to hiring returning veterans.

Job : Engineering
Schedule : Full-time
Primary Location : Americas-United States-California-El Segundo
Other Locations : Americas-United States-California-San Diego
Organization : Database Engineering"
Software Development Engineer - Data Collection and Tools,"A2Z Development Center, Inc.","We are a smart team of doers that work passionately to apply cutting edge advances in robotics and software to solve real-world challenges that will transform our customers experiences in ways we cant even imagine yet. As a Software Development Engineer, you will be working with a unique and gifted team developing exciting products for consumers and collaborate with cross-functional engineering teams, including Amazon Robotics.

We are looking for passionate, hard-working, and talented engineers who can solve problems in software, system design, data science and machine learning. In this role, you will:
- Build the tools and infrastructure required to collect large volumes of data that will power our algorithms.
- Have the ability to gets hands-on involved to solve hard engineering problems in different disciplines.
- Work with different teams to understand the algorithms, and their data collection needs. And come up with a plan and budget proposal to service these needs.
- Investigate and develop new innovative solutions to help with data collection and ground truth.
- Help to build and maintain data collection labs.
Basic Qualifications
Bachelors degree in Computer Science, Computer Engineering or related field, or 4+ years of relevant work experience
3+ years of professional experience in software development building production software systems
Computer Science fundamentals in object-oriented design, data structures, algorithm design, problem solving, and complexity analysis
Proficiency in at least one modern programming language such as C, C++, C#, Java, PHP or Python
Experience working with embedded devices
Experience in Android and/or embedded operating systems.
Understanding of machine learning
Preferred Qualifications
Experience investigating, designing, prototyping, and delivering new and innovative system solutions
Good understanding of machine learning and computer vision
Excellent judgment, organizational, and problem solving skills
Comfortable taking initiative and working across teams
Excellence in technical communication with peers, partners, and non-technical co-workers
Entrepreneurial spirit"
Data Operations Engineer,healthgrades,"Healthgrades is seeking a Data Operations Engineer to join our Data Team in Madison, WI. Your expertise will be utilized to perform routine maintenance and operations on established customer databases. Ultimately, you ensure data meets quality standards and routinely work with the customer data contacts. Possessing the drive to collaborate, gather feedback, solve problems, and tackle challenges through trial and error is highly valuable in this position.

Primary Responsibilities:
Updates company databases upon receipt of customer data
Completes all routine database maintenance processes in accordance with database operations policies and standards
Identifies and troubleshoots database maintenance issues
Communicates with customers to discuss any issues with received data and helps them identify and fix data issues
Communicates all major delays to team leader and/or account team
Elicit, analyze, and validate client data from ingestion to production
Analyze data trends to identify issues
Monitors all data update processes and output to ensure data quality
Escalates significant data quality issues for resolution
Regularly provides guidance to internal users who access the database
Accountable for processes that guide the performance of the data load
Support and enhance implemented solutions
Creates new processes to streamline data loading process, increasing performance, reducing effort, and increasing data quality
Requirements / Preferred Skills:
High school diploma, boot camp, or equivalent experience
1-3+ years experience in database development and maintenance
Proficiency in writing SQL queries, Linux/ Unix scripting, and database maintenance
Familiarity with Python experience preferred, but not required
Database experience within SQL, Oracle and Greenplum (not required)
Experience with scheduling or automation software such as Automic
Healthcare data experience and QA skills preferred, but not required
Demonstrated ability to absorb information, apply conceptual skills in practical applications, and achieve desired results in a highly technical, operations environment
Effectively deal with rapid technology and business change
Why Healthgrades?
At Healthgrades, we recognize that our people drive our greatest achievements. We are passionate about maintaining a fulfilling, rewarding and energetic work environment while setting the stage for your continued success.
Meaningful Work  helping millions of Americans connect with their healthcare providers
Changing the Game - evolving, fast paced culture with career advancement opportunities
Community Builders- partnering with local charity organizations and wellness initiatives
Robust Perks  generous PTO, 401k contributions, tuition assistance, entertainment discounts & more!"
B-1 Bomber Mission Data Engineer II,Jacobs,"Jacobs is one of the worlds largest and most diverse providers of full-spectrum technical, professional and construction services for industrial, commercial and government organizations globally. The company employs over 54,000 people and operates in more than 25 countries arou nd the world.

The successful candidate will assist the 53d Electronic Warfare Group (53 EWG), 53d Wing, at Eglin AFB, Florida. The 53 EWG is the technical focal point for all electronic warfare (EW) support of warfighter systems for the Combat Air Forces (CAF). The mission of the 53 EWG is to develop and test mission data (MD) to defeat enemy radar and infrared guided missile systems, thus enhancing aircrew and aircraft survivability in combat. This mission includes operational EW testing, MD development/validation/verification, force development evaluation execution and facilitating foreign materiel exploitation.

Knowledge,
skills and attributes associated with this position(s) include:
EW and the electromagnetic (EM) spectrum, computer software, threat warning, radio frequency (RF) jammers, electro-optical/infrared (EO/IR) jammers, expendables, threat analysis, foreign/US/radar/weapon systems, airborne EW computer software, avionics, systems integration, electronics engineering concepts, principles and practices applicable to a broad range of engineering assignments.
Candidates must be analytical, methodical and detail-oriented.

The successful candidate will conduct B-1 Bomber EW research, MD development and MD testing. The successful candidate will support EW system programming/reprogramming, work with bomber EW system engineers to coordinate programming/reprogramming requirements, prepare validation and verification test plans, and organize and participate in MD configuration control boards. The successful candidate will assist with the collection, recording, and post-test analysis of data generated during MD testing. The successful candidate may be required to travel.

Qualifications
Education and Experience:

Candidates must have at least a Bachelor of Science degree with at least 5 years of experience as a B-1 aircrew member and/or bomber EW test experience.

They must also have a demonstrated ability to recognize and analyze problems, conduct research, summarize results, and make appropriate recommendations. Applicants must have a working knowledge of computer systems and an understanding of Windows-based personal computers and Microsoft Office software and possess the ability to communicate effectively both orally and written.

Candidate must possess attributes to include a current active Secret security clearance and previous experience in RF integration, EM spectrum management, antenna design, and digital signal processing. Also desired is knowledge of EW weapons systems development, test and evaluation, and systems engineering. Highly desirable attributes include experience in threat analysis and threat definition.

Additionally it is highly desirable candidate possess a Top Secret security clearance. Candidates without an active Top Secret security clearance must be able to obtain and maintain a Top Secret clearance.

Essential Functions:

Physical Requirements
Work may involve sitting or standing for extended periods of time. Position may require filing, typing, and reading from a computer screen. Must have sufficient mobility, included but not limited to: bending, reaching, turning and kneeling to complete daily duties in a timely and efficient manner. There is a possibility that due to parking availability and location of work, walking moderate to long distances may be required.

Work Environment
An inside and outside environment (considerations: closed quarters, close together, lighting). Needs to be able to work well with co-workers and all levels of management. No hazards on job or unusual environmental conditions.

Equipment and Machines
Ability to operate office equipment such as a personal computer, printer, copy machine, telephone, fax machine and other equipment including desk supplies and other work related tools as required.

Attendance
Regular hours will be between Monday-Friday 7:00 a.m.  5:00 p.m. Regular attendance of scheduled hours is extremely important, but not critical, except for deadlines and meetings. Punctuality and regular attendance are necessary to meet deadlines. Must exhibit flexibility of work hours to adjust to surge situations based on critical mission requirements.

Other Essential Functions
Must demonstrate professional behavior at all times when dealing with customers, management and co-workers. Must have clear, concise and accurate communications skills in English, both verbal and written. Grooming and dress must be appropriate for the position and must not impose a safety risk to employees or others. Must maintain a positive work atmosphere by behaving and communicating in a professional manner. Independent personal transportation to office or work site is required. Travel to and from customers locations and the test fields may be required to support projects. This may involve airline travel. In some cases, accommodations can possibly be made for POV, if necessary. When operating any vehicle for work purposes, must wear seat belt and in addition, no cellular devices are to be used when vehicle is in motion.

The Jacobs System Lifecycle Group (SLG) enables the success of Department of Defense (DOD) and other Governmental organizations by providing unmatched systems life-cycle solutions. As a partner to our clients, we focus on innovative and cutting edge solutions that drive value. We take pride in sustaining a strong culture by remaining true to our Core Values: Profitable Growth is an Imperative, People are the heart of our business, Clients are our valued partners, and Performance excellence is our commitment. Jacobs offers a partnership in which you can grow personally and professionally with the advantages of strong leadership, competitive compensation and rewarding career paths. Come join the team whose work is destined to have a long-range effect on future generations.

cjpost
Jacobs is an Equal Opportunity Employer and employment selection decisions are based on merit, qualifications, and abilities. Jacobs does not discriminate in employment opportunities or practices on the basis of: race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability, veteran status, or any other characteristic protected by country, regional, or local law."
Analyst Relations Manager- Analytics and AI,IBM,"Job Description
IBM is looking to hire an Analytics and AI Analyst Relations Manager in our San Francisco office. This person will join our team of AR professionals as we build relationships with key influencers to help IBM further lead the market in AI platforms and solutions. The Analyst Relations Manager plays a critical, cross-functional role by working with experts at many different levels of the organization across IBM's Analytics and AI businesses. The candidate will work closely with product management, engineering, sales and marketing teams across IBMs global offices to build and nurture strong internal relationships that will be leveraged to engage with leading analyst firms and influencers.
We are entering the era of artificial/augmented intelligence, and IBMs clients are leveraging Artificial/Augmented Intelligence (AI) across the enterprise and its applications. IBM is leading the industry in AI investment, both in our current products and in IBMs legendary Research division. A big part of that is IBMs Analytics and Artificial/Augmented Intelligence strategy. IBMs Analytics and AI solutions are purpose-built for today and tomorrows AI and machine learning applications, providing superior performance over all advanced workloads.
The Analyst Relations Manager should have a strong blended background with a combination of two or more skills in product management, engineering, business development, analyst relations, market research, communications, data analysis, and/or competitive positioning. The candidate must have the ability to learn, understand and articulate Analytics and AI product details and strategy. Experience in the analytics or AI industries is a plus. This role requires strong analytical skills and experience evaluating qualitative and quantitative research in the IT or related industries. This candidate must enjoy growing and nurturing relationships and influencing analysts on IBM competitive strengths and differentiators.

RESPONSIBILITIES
Develop, manage and execute a targeted Analyst Relations plan for IBM Analytics and AI; educate the most influential industry analysts who work with existing or prospective IBM customers. Embed with the business unit to influence strategy and sales and improve analyst perceptions and relationships year-over-year.
Influence analyst research on IBM, its competitors and the markets that IBM competes in; shape the perception of IBM and the value it brings to its customers; improve analyst rating in influential reports and evaluations.
Monitor the competitive landscape and strategic developments in the area of focus; share relevant analyst research and implications with senior management to shape BU strategy and offerings.
Connect key IBM executives with expert analysts for advisory sessions to help define and influence offering strategy.
Collaborate with colleagues in marketing, sales, offering management, communications legal and BU leadership to ensure that IBM is getting the best representation with the analyst community.
Manage and procure paid strategic research projects on behalf of IBM to ensure that key analyst firms are actively engaged with IBM offerings
Develop a keen understanding of how to build credible and impactful relationships with key influencers and leading industry analysts.

This role is located at IBM offices in San Francisco, CA.

Candidate must be willing to travel up to 50% to meet in person with Business Unit executives and key global analysts and influencers.

Applicants must have the ability to work in the US without current/future need for IBM sponsorship.

Required Technical and Professional Expertise

8+ years of diverse work experience in more than one functional area such as: analyst relations, product management, market research, consulting, operational, financial and/or strategy roles.
Executive presence to counsel to senior management regarding the market, competitive landscape and analyst outlook. Candidates must have the tenacity, determination and patience to be able to challenge conventional thinking and processes.
Excellent verbal and written communication skills.

Preferred Tech and Prof Experience

Experience as a product manager, engineer or strategist in an Analytics and AI function at a leading company.
Experience as a senior analyst at a leading industry research firm with a focus on Analytics.
Existing relationships with leading industry analysts/influencers focused on the Analytics and AI market.
Advanced Degree in the Engineering/Sciences, Statistics or Business
EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Data Scientist,Amazon.com,"Interested in Amazon Echo? Were building the speech and language solutions behind Amazon Echo and other Amazon products and services. Come join us!

As a Data Scientist in our Applied Modeling team, you will be responsible for data-driven improvements to our spoken language understanding models. Your work will directly impact our customers in the form of products and services that make use of speech and language technology.
You will:
Ensure data quality throughout all stages of acquisition and processing, including such areas as data sourcing/collection, ground truth generation, normalization, transformation, cross-lingual alignment/mapping, etc.
Clean, analyze and select data to achieve goals
Build and release models that elevate the customer experience and track impact over time
Collaborate with colleagues from science, engineering and business backgrounds.
Present proposals and results in a clear manner backed by data and coupled with actionable conclusions
Work with engineers to develop efficient data querying infrastructure for both offline and online use cases

Basic Qualifications
Masters or PhD in a relevant field
2-5 years experience with various data analysis and visualization tools
Experience in Perl, Python, or another scripting language; command line usage
Experience with various machine learning techniques and parameters that affect their performance

Preferred Qualifications
Track record of diving into data to discover hidden patterns and of conducting error/deviation analysis
Ability to develop experimental and analytic plans for data modeling processes, use of strong baselines, ability to accurately determine cause and effect relations
Understanding of relevant statistical measures such as confidence intervals, significance of error measurements, development and evaluation data sets, etc.
The motivation to achieve results in a fast-paced environment.
Experience with statistical modelling / machine learning
Strong attention to detail
Exceptional level of organization
Comfortable working in a fast paced, highly collaborative, dynamic work environment
Ability to think creatively and solve problems
Fluency in a foreign language

Amazon.com is an Equal Opportunity-Affirmative Action Employer  Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation"
Senior Data Engineer - AWS,Getty Images,"Who You Are:
You are motivated by the technical challenges that come with structured and unstructured data at an enterprise level. Even more, you are energized by bringing solutions and innovations that help the business move forward. You are passionate about building data platforms, frameworks and driving insights from complex multi structured datasets.

Your Next Challenge:
Were looking for an experienced Data Engineer to continue to evolve and mature our enterprise data platform. In this role, you will partner closely with product, analytics, and technology stakeholders to continue to grow and innovate the platform by: Design, im plement and deliver cloud or on- prem based analytical solutions. Design and develop high-throughput, low-latency data processing pipelines including data quality and testing Identify approaches to extract valuable insights from massive datasets Research, evaluate and utilize new technologies, tools and frameworks Write well designed and efficient code Ensure scalability and high performance of the platform What Youll Need:
5+ years hands-on experience with designing and implementing solutions using distributed technologies. Expert knowledge in modern distributed architectures and compute / data analytics / storage technologies on AWS Cloud. Hands-on working experience on AWS Redshift Experience in reporting and visualization tools Understanding of architectural principles and design patterns / styles using parallel large-scale distributed frameworks such as Hadoop and Spark; Advanced knowledge of a programming language s such as Scala, Java or Python Experience in SQL Server BI stack (SSIS, SSAS, SSRS) E xperience with Hadoop, Spark, Kafka and hive will be a plus Demonstrate broad knowledge of technical solutions, design patterns, and code for medium/complex applications deployed in production on Hadoop & Spark clusters. Outstanding analytical skills, excellent team player and delivery mindset. Experience in performance troubleshooting, SQL optimization, and benchmarking. Thorough understanding of service-oriented architectures and data processing in high-volume applications. Knowledge of working in UNIX environment with shell scripting. Experience building solutions using agile methodologies Strong bias for action; ability to juggle multiple priorities and create a sense of urgency in a fast-paced, dynamic environment
#LI-EB1

Who We Are:
Getty Images is the most trusted and esteemed source of visual content in the world, with over 200 million assets available through its industry-leading sites www.gettyimages.com and www.istock.com . The Getty Images website serves creative, business and media customers in almost 200 countries and is the first place people turn to discover, purchase and share powerful content from the world's best photographers and videographers. Getty Images works with over 200,000 contributors and hundreds of image partners to provide comprehensive coverage of more than 130,000 news, sport and entertainment events, impactful creative imagery to communicate any commercial concept and the world's deepest digital archive of historic photography.

Visit Getty Images at www.gettyimages.com to learn more about how the company is advancing the unique role of still and moving imagery in communication and business, and enabling creative ideas to come to life. For company news and announcements, visit our Press Room , and for the stories and inspiration behind our content, visit Stories & Trends . Find Getty Images on Facebook , Twitter , Instagram , LinkedIn , Pinterest and Tumblr , or download the Getty Images app where you can explore, save and share the world's best imagery.

Getty Images is an equal opportunity employer and strongly supports diversity in the workplace."
Senior Data Engineer,The Climate Corporation,"Position Overview:
The Climate Corporations mission is to help the worlds farmers sustainably increase their productivity with digital tools. The Data and Analytics team is focused on creating competitive advantage for Climate and our customers through novel data infrastructure, metrics, insights and data services. We are a small but rapidly growing analysis and engineering team that builds and leverages state-of-the-art analytics systems. Our work informs decisions and direction for our business, while also impacting our products. We are looking for a Data Engineer to not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's products, data consumers and analysts. We are looking for a candidate with a deep knowledge of data warehouse and strong experience with ETL/ELT tools and streaming technologies.

What You Will Do: Help design and build a Business Analytics Warehouse. Build and maintain the core data model, ETL/ELT, core data metrics and data quality. Rapidly prototype new analytics views and work directly with stakeholders across multiple functions (Science, Marketing, Sales, Risk, Finance, Product) Champion data warehousing best practices Build systems to answer business questions in a timely fashion and expand our product features Architect, build and launch new data models that provide intuitive analytics to business users Develop infrastructure to inform on key metrics, recommend changes and predict future results Work closely with other departments to gather new data and leverage existing data to make our products better for us and our users Build data expertise and own data quality for the pipelines you build Design and develop new systems and tools to enable folks to consume and understand data faster Provide expert advice and education in the usage and interpretation of data systems to the business users
Basic Qualifications: B.S. or B.A. in computer science, math, economics, engineering or other technical field 5+ years of SQL and dynamic or static programming languages experience as applied to ETL/ELT tools (Informatics, Kettle, Talend, etc.) Experience with relational databases or NoSql Some experience with streaming technologies such as Kafka or Spark

Preferred Qualifications: 3+ years of experience with dimensional data modeling & schema design in Data Warehouses. 3+ years of scripting experience Experience with massive scale relational databases (MPP) is a big plus (Vertica/Redshift/Teradata/MemSQL). Excellent communication skills including the ability to identify and communicate data driven insights

What We Offer:
Our teams are composed of industry experts, top scientists, and talented engineers. The environment is extremely engaging and fast-paced, with dozens of specialties coming together to provide the best possible products and experiences for our customers.
We provide competitive salaries and some of the best perks in the industry, including: Superb medical, dental, vision, life, disability benefits, and a 401k matching program A stocked kitchen with a large assortment of snacks & drinks to get you through the day Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hackathons to encourage participation and growth in both community involvement and career development
We also hinge our cultural DNA on these five values: Inspire one another Innovate in all we do Leave a mark on the world Find the possible in the impossible Be direct and transparent

Learn more about our team and our mission: The Climate Corporation - The Technology Behind Making A Difference
https://youtu.be/c5TgbpE9UBI or visit https://climate.com/careers

#LI-EF1"
Analytic Data Engineer - (Hadoop),Highmark Inc,"Description

Company : Highmark Inc Job Description :General Overview:
The Analytic Data Engineer architects and engineers solutions associated with analytic data for the organization (Advanced Analytics & Reporting) and, working closely with the IT teams, assists with the design, build, and upkeep for these solutions. This includes creating pathways for analysts to access operational, derived, and external data sets. The incumbent is responsible for the operation of Big Data Platforms as they are associated with analytic data discovery.

Essential Job Functions:
Receiving some direction, work closely with IT to architect and engineer solutions to provide views for the Analytic Data Warehouse. This would include working with the proper the teams, assisting with the design, building out the design, and providing upkeep for the solution.
Assemble, test, process, and maintain the Analytic Discovery Platform for the analytics organizations. This will include working to maintain pipelines with key analytic platforms throughout the organization.
Work with alternative analytic data systems to incorporate them into the operational data flow for the Analytics Teams. This may include products purchased by the organization that must be ingested or modeled/derived data maintained by analytic teams.
Complete tasks associated with a project. Meet with customers as part of a team lead by Lead or Senior.
Other duties as assigned.

Minimum Qualifications:
Bachelor's Degree in Computer Systems Analysis, Data Processing, Healthcare Informatics, Management Information Systems, or related field

Preferred Qualifications:
3 - 5 years of Data Analytics Experience
1 - 3 years of Data Warehousing Experience
1 -3 years of Database Administration Experience
1 - 3 years of Healthcare Industry Experience
SAS
Hadoop

Referral Award Payout Level: 3
----------------------------------------------------------------------------------------
Highmark Health and its affiliates prohibit discrimination against qualified individuals based on their status as protected veterans or individuals with disabilities, and prohibit discrimination against all individuals based on their race, color, religion, sex, national origin, sexual orientation/gender identity or any other category protected by applicable federal, state or local law. Highmark Health and its affiliates take affirmative action to employ and advance in employment individuals without regard to race, color, religion, sex, national origin, sexual orientation/gender identity, protected veteran status or disability.
EEO is The Law
Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled/Sexual Orientation/Gender Identity ( http://www1.eeoc.gov/employers/upload/eeoc_self_print_poster.pdf )

We endeavor to make this site accessible to any and all users. If you would like to contact us regarding the accessibility of our website or need assistance completing the application process, please contact number below.

For accommodation requests, please call HR Services at 844-242-HR4U or visit HR Services Online at HRServices@highmarkhealth.org"
Data Engineer,"InterWorks, Inc.","We know that there are all kinds of data engineers, with varying levels of knowledge about different platforms and data sources. At InterWorks, were more interested in candidates who are hard-working, smart and love what they do. We care more about overall data and programming skills rather than experience with a certain database platform. Whether you have worked primarily in SQL Server, Postgres or something else, we only ask that you have a strong understanding of building data pipelines and can apply your skills within our fast paced and agile approach.

While some companies may hide their data engineers away in some dungeon, data engineers at InterWorks are social, friendly creatures. That is why being able to communicate with clients day in and day out is especially valuable. Our data engineers must be able to work closely with users in order to understand their needs and help them as best as possible.

You might find it difficult to do the same thing each day at InterWorks. Data Engineers can expect to work on diverse projects ranging from a few days to several months. Many of these projects include solving data acquisition, integration and management problems for some of the largest organizations in the world. Projects also include working with disparate data sources (Relational databases, flat files, Excel, HDFS/Big Data systems, high performance analytical databases, etc.) to unify client data, creating ETL processes based on client needs and managing client expectations.

InterWorks prides itself on having an exciting work environment, with a unique company culture. InterWorks employees are some of the most intelligent and friendly people you will ever meet, and they are the primary reason for the success of InterWorks. Joining the InterWorks team as a Data Engineer means joining a family and our family is growing faster than ever.

Skills and Requirements

Here are some basic requirements that we look for in our Data engineers:

Required:
Excellent SQL skills

Programming ability (Python, Java, C#, PHP, etc.)

Strong ETL skills using GUI based tools or code based patterns

Understanding of data modeling principles

Excellent verbal and written communication skills

Business acumen

Strong problem solving skills

Easily adaptable and flexible to changing situations

Passion for delivering compelling solutions that exceed client expectations

Highly Desired:
Experience with software engineering practices

Experience with modern data engineering practices and frameworks

Experience with integration from API sources

About InterWorks

InterWorks is a business-to-business I.T. consulting firm based out of Stillwater, Oklahoma with staff in Dallas, Atlanta, Chicago, San Francisco, San Diego, Washington D.C., New York and in Europe. We opened our doors in 1996 to focus on I.T. networking with an emphasis on customer service. Still focusing on the needs of the customer, we now serve clients by offering network architecture, software development, web strategy and business intelligence services on a national and global scale.

About the Business Intelligence Team

At InterWorks, were on the front lines of the big data revolution. Companies around the world continue to seek us out to meet their big data needs. That is why InterWorks assembled the best and brightest business intelligence team there ever was, wielding powerful data tools like Tableau and Vertica. Our BI team provides solutions to some of the largest organizations in the world, including several Fortune 500 companies, government organizations, and respected universities."
Software Engineer - Data Privacy,Square,"Job Description

Information privacy and security has become a critical issue for consumers as well as companies.
GDPR requires substantial investment by companies with European customers Doing so in a manner that minimizes disruption to product development and risk analysis is critical to Square's future in the EU.
As the founding engineer for the GDPR effort, youll be the nexus for all GDPR related engineering efforts. You will influence the roadmap for products affected by GDPR and review designs and implementations from teams to validate GDPR conformance. As the GDPR and other privacy efforts increase you may be responsible for managing a team of engineers. Qualifications

You will:
Work directly with teams at Square building products for use in the EU.

Help product teams understand the effects of GDPR and how to minimize disruption from the requirements.

Work with the Information Security team to insure appropriate safeguards are in place for private data.

Work with the Chief Privacy Officer and legal team to understand the ongoing requirements and impacts of GDPR

Communicate and collaborate with others in the industry to stay on the leading edge.

Attend conferences to understand current industry best practices as well as represent Square's interests.

You have:
At least 3-6 years of software development experience.

Programming experience in one or more object-oriented languages, including C, C++, Objective-C, Java, Ruby, Scala, Go, and/or Python.

Experience working with MySQL or similar DB

Experience with data warehousing technologies is a plus

A demonstrated ability to learn new technologies quickly

An ability to communicate well with a wide variety of people and product areas

Unwavering commitment to customer experience and product quality

Eagerness to share your own ideas, and openness to those of others

A record of self-directed side projects, research, or open source participation are a plus!

Additional Information

At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance."
Software Engineer,Odyssey Systems Consulting Group,"Job Details Description
PRIMARY FUNCTION(S)
The Ground Based Space Surveillance Division (SMC/SYG) executes sustainment program management, sustaining engineering, maintenance and supply support, and small modification execution for SSA ground sensors.  These sensors provide timely and accurate metric and space object identification (SOI) data on near Earth and Deep-Space (DS) objects.  These sensors also provide data for SSA catalog maintenance and support for special events (e.g. space launches, satellite breakups, and maneuvers). The Division also provides hot-back-up to the Joint Space Operations Center (JSpOC), and provides Command and Control for the Space Surveillance Network.
The scope of this contract is to execute the following activities:  sustainment engineering, sustainment management, product support/life-cycle logistics, user interface functions, communications network management for long-term sustainment. These tasks are to be executed laterally with SYG depot and development contractors.  Tasks fall into five areas: Product Support, Sustainment Engineering & Sustainment Management, Technical Order Management, Technical User Interface, and System Design, Engineering and Integration.
This position provides Software Engineering support to the SYG office at Peterson AFB in Colorado Springs, CO.
Specific Responsibilities include but are not limited to:
Provide technical and programmatic software engineering support to SYG programs. This support includes, but is not limited to: General systems engineering, sustaining engineering, architecture development and decision making, integrated logistics planning and execution, acquisition assistance (includes integration with contractual, financial, and scheduling activities), and overall operations and sustainment support as required. The scope of this effort includes providing support, as required, to division or branch level managers.
Develop, maintain, and coordinate life-cycle sustainment plans (LCSPs) and, as appropriate, logistics planning documents related to software, for government approval.
Ensure for the planning and execution of all software related product support elements in the development, modification, upgrade and sustainment of new and existing programs within the SYG Enterprise portfolio.
Support, review and provide comments/input to, the development of overarching system design documentation related to software to include: technical requirements documents (TRD); capability description documents (CDD); capability production documents (CPD); interface control documents (ICDs); and system specifications.
Support, review and provide comments/input to the development of training related tasks such as the Training System Requirements Analysis (TSRA), system training plans, and other training materials. Coordinate for government approval of training materials and training systems.
Collect, develop, and review program specific information to support the development of training related tasks for the supported weapon systems.
Lead and resolve program deficiencies and facilitate effective integration of software related support system technical and product support requirements and efforts with other acquisition and sustainment activities within SYG, with outside agencies and industry partners.
Provide recommendations to lower the Life Cycle Cost per program and improve system readiness.
Develop, plan, and implement lifecycle support strategies and analysis for Quick Reaction Capabilities, technology demonstrations, prototyping, and system test and evaluation activities.
Develop strategies, plans and packages for classified and unclassified Depot Source of Repair (DSOR) submissions and facilitate associated life cycle cost planning and benefit analyses, partnering, acquisition of data rights, and provisioning.Staff packages through DSOR process and follow through final approval.
Track development/sustainment risks in support of Government activities.
Prepare briefings, reports, informational analyses in support of SYG programs.
Coordinate and lead resolution of system issues, deficiencies, software bugs, etc. with prime contractors, depot contractors other support contractors.
Identify and prepare consolidated lessons learned reports from applicable related programs for Directorate and SMC reviews, update processes required by the contractor, and make recommendations to improve government processes and procedures.
Participate in configuration management and data management processes associated with software configuration items and deliverables.
Develop and maintain software related technical requirements documents and architecture documentation to execute programs, upgrades, system modifications and enhancements to fielded and deployed systems.
Develop and update roadmap activities including incremental upgrade planning, scheduling and funding for fielded and deployed systems.
Coordinate with prime contractors, depot contractors and other support contractors on software issues.
Provide software related input to acquisition planning activities, upgrades and modifications and studies and interface with contractor logistics support personnel and product support integrators.
Provide software related inputs to the development and management of Technical Orders
  MINIMUM QUALIFICATIONS
Bachelors degree in Software Engineering, Electrical Engineering, or related field
5 years of experience in providing acquisition related support to the DoD specifically in the area of software
Knowledge of the theories, concepts, principles, practices, standards, methods, techniques, and materials of multidisciplinary professional engineering to effectively apply advanced engineering theories, principles, and concepts to new applications, experimental theories, and other areas of responsibility.
Knowledge of planning and program management to execute major projects, provide expert advice to colleagues, and present innovative recommendations for advancing programs and/or methods.
Knowledge of safety, security, personnel management, and EEO regulations, practices, and procedures.
Ability to plan, organize, and direct the functions and mentor, motivate, and appraise the staff of an organization.
Ability to research, evaluate, interpret, and apply rules, regulations, and procedures to a variety of situations and to recommend timely and economical solutions.
PREFERRED QUALIFICATIONS
Ability to communicate clearly, concisely, and with technical accuracy, both orally and in writing, as well as work in a professional manner with peers, management, contractors, academia, and other agencies.
Strong written and verbal skills including writing reports and presenting results to customers
Secret clearance
Masters degree in Software Engineering, Electrical Engineering, or related field
10 years of experience in providing acquisition related support to the DoD specifically in the area of software
Knowledge of the Space Surveillance Network and its systems and mission
Familiarity with SMC/SYG and its predecessor organizations
Able to quickly obtain TS/SCI accesses due to prior adjudication or recently in an SCI billet
WORKING LOCATIONS & ADDITIONAL INFO
Peterson AFB, Colorado Springs, CO
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractors legal duty to furnish information."
Data Engineer II,AmerisourceBergen,"Data Engineer II - 00001JGL
Description
AmerisourceBergen is one of the world's largest pharmaceutical services companies, currently ranking 12th on the Fortune 500 and with over $146 billion dollars in annual revenue.

AmerisourceBergens Enterprise Data & Analytics team works closely with business stakeholders and information technology services team members on analytics-enabled business transformation and innovation initiatives. Our Business Analysts/Data Visualization Specialists work directly with business stakeholders to drive business analytics solutions and communicate insights via creative visualizations, our Data Engineers translate analytics requirements into the underlying technical solution architecture and code components, and our Data Scientists turn data into critical information and knowledge that can be used to make sound organizational decisions.

POSITION SUMMARY:

Individuals in this role understand how information is turned into knowledge and how this knowledge supports and enables key business processes. They must have a solid understanding of logical data warehousing design principles and data access requirements for business analytics and exploration. Also required are analytical skills, the ability to establish and maintain effective working relationships with team members, as well as an innate curiosity around wanting to understand business processes, business strategy and strategic business initiatives to help drive incremental business value from enterprise data assets.

Individuals in this role will work in a team with other engineers and researchers to develop innovative data driven solutions that integrate distributed sources of data, perform large scale learning and reasoning, and integrate results in collaborative environments to support the business analytics and data provisioning needs of the enterprise.
This role will be responsible for technical leadership of junior engineers and will also be responsible for hands on design and coding of solutions. The ideal candidate will be well-versed in state of the art data management and processing technologies and experienced in applying them to large scale commercial, scientific or industrial applications. The candidate should be comfortable working with cloud and hybrid solutions.

PRIMARY DUTIES AND RESPONSIBILITIES:
Performs development in support of enterprise analytics and data provisioning needs.
Ensures a consistent and coherent technical delivery of solutions.
Participates in testing design and execution, peer code reviews, etc.
Identifies opportunities to drive down service costs though technology or process enhancements.
Optimizes platform performance.
Keeps abreast of new technologies in the Big Data arena.
Adheres to development and governance standards
Works on moderately to complex tasks in support of one or more projects as a project team member, or independently on small projects.
Increased skill in multiple technical environments and possesses knowledge of a specific business area.
May participate in project planning processes.
May identify project tasks and assists with project task effort estimations.
Works with business analytics team members to develop business requirements.
Leverages knowledge of underlying data sources and builds up specific business data domain expertise.
Helps translate business analytics needs into semantic data access requirements.
Translates business requirements into conceptual, logical and physical data model.
Develops and maintains an integrated logical and physical data model.
Assists in creating a framework for representing the data elements including the entities, relationships and attributes.
Works with Delivery Management team to understand potential impacts to physical/virtual infrastructure and assists in remediation plans as needed.
Recognizes and resolves conflicts between models, ensuring that data models are consistent with the enterprise model (e.g. entity names, relationships and definitions).
Works closely with other IT groups to use the data models and/or HDFS data assets throughout the whole life cycle.
Develops technical design of data sourcing, transformation and aggregation logic.
Works with Information Delivery team to determine data sourcing options and recommend implementation approach.
Leverages enterprise standard tools and platforms to develop data transformation and aggregation logic, typically working with and/or leading a small team.
Works with business stakeholders, data visualization specialists and/or data scientists to determine analytics data consumption alternatives and recommend optimal approach based on analytics requirements.
Verifies that data access mechanisms satisfy analytics consumption needs (may include testing various approaches).
Transfers knowledge of data access/consumption mechanisms to business stakeholders, data visualization specialists and/or data scientists.
Collects, analyzes and summarizes data to support business decisions.
Identifies patterns and trends regarding data quality.
Helps to maintain the data dictionary.
Ensures that the information is named properly and defined consistently across the organization.
Provides data that is congruent and reliable and is easily accessible by the user.
Identifies opportunities for reuse of data across the enterprise.
Validates data sources.
Manages the flow of information between departments.
Leverages master data as needed by business processes.
Consults Delivery Management team on batch/bulk data load scheduling to optimize performance
Works with data stewards to gather requirements on merging, de-duping, cleansing rules.
Ensures and maintains a high level of data integrity by using tools to monitor and mass update data changes.
Develops data profiling and preventative procedures to improve data quality.
Monitors and resolves daily exception reports.
Analyzes data inaccuracies and recommends process improvements or system changes to enhance overall quality of the data.
Communicates data integrity accuracy to the business, and escalates/communicates issues when necessary.
Identifies opportunities and supports the development of automated solutions to enhance the quality of enterprise data.
Analyzes data issues and works with development teams for problem resolutions.
Monitors data dictionary statistics, analyze reports of data duplicates or other errors to provide ongoing appropriate data reports.
Identifies problematic areas and conducts research to determine the best course of action to correct the data, identify, analyze and interpret trends and patterns in complex datasets.
May define test plans and system documentation to monitor testing for the implementation of business analytics solution enhancements, modifications and new releases.
Conducts unit testing to ensure business analytics solutions meet user specifications
May participate in integration testing.
Interfaces with testing teams to incorporate plans into the testing process.
May analyzes existing data platform to identify weaknesses and develop opportunities for improvements, when assigned.
Provides technical coaching and mentoring to less-experienced team members.
Qualifications
EXPERIENCE AND EDUCATIONAL REQUIREMENTS:
Bachelor's degree in Programming/Systems or Computer Science or other related field. Or equivalent work experience.

MINIMUM SKILLS, KNOWLEDGE AND ABILITY REQUIREMENTS:
3-7 years of experience in technologies relevant to data/information management, such as SQL, RDBMS, Data Lake/Hadoop, MDM, Metadata, Data Quality, etc.

Demonstrated knowledge of data management concepts as well as an outstanding command of the SQL standard.
Knowledge and exposures to cloud or on premises MPP data warehousing systems (e.g. MS APS, Teradata, Snowflake, Azure SQL DataWarehouse)
Experience in using and tuning relational databases (MS SQL Server, Oracle, MySql).
Knowledge and experience with Informatica, Talend or another ETL environment
Familiar with Lambda and Kappa architecture implementations
Familiar with DataLake implementations and design patterns
Object Oriented Programming proficiency using Java EE and/or .Net technology stack
Knowledge of Test Driven Development, Continuous Integration, Agile/Scrum
Experience with code versioning tools and a command of configuration management concepts and tools.
Demonstrated ability to quickly learn and adapt to new technologies and coding techniques.
Experience designing, developing and testing applications using proven or emerging technologies, in a variety of technologies and environments.
Knowledge in cloud resource provisioning and management on Azure is a plus.
Familiar with Streaming and with streaming concepts and patterns is a plus
Experience with analytics model management and analytics workflow tools (e.g. SAS Model Manager, Knime, ML Studio, Alteryx) is a plus
Scientific computing experience (R, Matlab, Python, etc.) is a plus.
Knowledge of NoSql data management systems (MongoDB, HBase, Cassandra) is a plus.
Theoretical and practical background in data mining and machine learning is a plus.
Self-driven, possesses the ability to work on multiple tasks and adapt to change.
Outstanding interpersonal and communication (written & verbal) skills.
Strong collaboration skills and ability to thrive in a fast-paced environment.
Successful candidate must be able to work with controlled technology in accordance with US export control law.

Organization : IT
Job : Information Technology Jobs
Schedule : Full-time
Primary Location : United States-Texas-Frisco

Equal Opportunity Employer/Minority/Female/Disability/Veteran"
Python Software Engineer,NRG Energy,"NRG is the leading integrated power company in the U.S., built on the strength of our diverse competitive electric generation portfolio and leading retail electricity platform. A Fortune 500 company, NRG creates value through best-in-class operations, reliable and efficient electric generation, and a retail platform serving residential and commercial businesses. Working with electricity customers large and small, we implement sustainable solutions for producing and managing energy, developing smarter energy choices and delivering exceptional service as our retail electricity providers serve almost three million residential and commercial customers throughout the country.

Summary:
At NRG, we apply advanced analytics and modeling to address challenging business problems. Within our Texas mass markets retail business, which includes Reliant Energy, we aim to promote customized offerings: the right product offered through the right channel, with the right message at the right time for each current or prospective customer. To accomplish this, we leverage our data via predictive modeling, statistical analyses, and optimization. We are looking for a talented software engineer to develop Python packages to streamline our analytics work; enable common, scalable analytical computing environments on demand; and deploy our reports and predictive models into production. This position is part Software Engineer, part DevOps Engineer, and part Data Engineer. If you like variety and starting projects from scratch, this is the job for you!

Responsibilities:Essential Duties/Responsibilities:
Develop custom Python packages to help our analysts connect to various data sources; provide convenient methods of joining and transforming data in ways common to our workflow
Prepare a common analytics environment that analysts can deploy on demand via internal cloud or Amazon Web Services, and access via Jupyter Notebooks
Mentoring code development best practices. This is an awesome opportunity to develop your leadership skills.
Automate data pipelines
Deploy predictive models and reports into production
Manage Git Repositories
Manage AWS Cloud Infrastructure

Qualifications:Education:
Degree (BS, MS, or PhD) in Computer Science, Electrical and Computer Engineering, Software Engineering, or Information Systems preferred; any STEM BS, MS or PhD with appropriate work experience will be considered.

Experience:
5+ years of professional Python software design and development
2+ years automating jobs on Amazon Web Services
2+ years administering an open source database such as PostgreSQL or MySQL
Experience with Hadoop/Spark a plus
Linux administration experience a plus
Technical Skills:
Expert-level Python programming
Test-driven development
Monitoring systems and data pipelines
Proficient with the following Python packages: SQLAlchemy, flask, pandas, sphinx, and pytest
Linux server administration
Git
SQL
Continuous Integration Deployment (Gitlab-ci, Ansible, Jenkins, or equivalent)
Containerized Environments (Docker, LXC)
Additional Knowledge, Skills and Abilities:
Ability to design Python software from scratch
Desire to automate everything
Good communication skills
Independent and self-driven worker
Creative problem solver
NRG Energy is committed to a drug and alcohol free workplace. To the extent permitted by law and any applicable collective bargaining agreement, employees are subject to periodic random drug testing, and post-accident and reasonable suspicion drug and alcohol testing. EOE AA M/F/Protected Veteran Status/Disability
EEO is the Law Poster (The poster can be found at http://www.eeoc.gov/employers/upload/poster_screen_reader_optimized.pdf )

Level, Title and/or Salary may be adjusted based on the applicant's experience or skills.
Official description on file with Human Resources"
Software Engineer: Data Services,Workday,"Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description
If you have experience in handling large volumes of data (ETL, enterprise implementations, data warehousing), you are a team player and mentor, and you would like to add an important component into Workday's leading cloud-hosted architecture, please contact us.
Required Experience:
* 3+ years working in enterprise scale implementations* Super proficient in Java and SQL
* Web Services (SOAP and REST)
* Automated Testing in a continuous build environment
* Bachelor's Degree in Computer Science or similar
Desirables:
* Data Tooling (transformation, cleansing and validation)
* Data Warehousing (architecting for scale, performance and security)
* Cloud Architectures and APIs
* NoSQL Databases, Hadoop
* Web UI
* Eclipse plugin development
* Agile development methodologies
*LI-LT1"
Software Engineer,University of Maryland,"The Software Engineer maintains, implements, and integrates both on premise and hosted systems using primarily Java and web-based technologies. The position is hands-on, agile, team-based development.

The Software Engineer is responsible for:

Implementing software application within a development team

Following a formal development methodology and working within scheduled timelines

Participating in peer code reviews, documentation, and formal testing

Position may require some travel.

Minimum Qualifications:
Bachelors Degree, preferably in computer science or related field, or equivalent combination of education and work experience.

Experience with Java as a language and platform

Knowledge of object oriented principles; software design; use of design patterns

Understanding of established software engineering standards and methodologies including some or all of the following: creation and use of Use Cases, UML modeling, naming conventions, source control facilities, documentation, and unit testing

Excellent written and verbal communication skills, paired with ability to express complex technical concepts effectively

In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification document form upon hire.

Preferences:
Experience working productively in a software engineering team in the design, development, and implementation of a software module/component

Familiarity with java web application environments (Servlets, JSP , HTML , CSS )

Familiarity with Spring frameworks, databases (preferably Oracle), and data binding tools ( ORM , XML )

Additional Certifications:

Additional Information:

Posting Date:
09/18/2017

Closing Date:

Open Until Filled
Yes

Best Consideration Date
10/31/2017

Physical Demands
None.

Diversity Statement:
The University of Maryland, College Park, an equal opportunity/affirmative action employer, complies with all applicable federal and state laws and regulations regarding nondiscrimination and affirmative action; all qualified applicants will receive consideration for employment. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, religion, sex, national origin, physical or mental disability, protected veteran status, age, gender identity or expression, sexual orientation, creed, marital status, political affiliation, personal appearance, or on the basis of rights secured by the First Amendment, in all aspects of employment, educational programs and activities, and admissions.

Applicant Documents Required Documents
Resume
Cover Letter
List of References (no emails sent from system)
Optional Documents"
Lead Platform Engineer - Big Data,McKinsey & Company,"Qualifications
Bachelors degree or Masters Degree in IT, Computer Science, Information Systems, Math, Engineering or related discipline
6-10 years of experience in the field of business intelligence, application development, database development and ETL and/or data analysis domains with strong Hadoop, Hive and/or Spark knowledge
Extensive coding and application development abilities
Experience in real time scalability and highly available solutions
2+ years experience in Amazon Web Services cloud technologies such as EC2 , Cloud HSM, KMS, Route 53, SES, Inspector, SNS, S3, Elastic Load Balancing, CloudWatch, CloudTrail, Trusted Advisor, Security Group Management
Experience with scripting languages such as Bash, Perl, PowerShell or Python
Proficient in Linux (CentOS / Redhat & Ubuntu)
Proficient with Ansible
Familiar with Atlassian JIRA and Confluence solutions
Familiar working with Agile operations
Ability to translate business requirements to functionality, with estimates and implementation plans
Experience in the healthcare industry either payor or provider focused is a plus
Able to travel 20% for working sessions and meetings

Who You'll Work With You'll work with the McKinsey Healthcare Analytics team in New York. Healthcare Analytics is a part of McKinsey Solutions and McKinsey's New Ventures.

Our Healthcare Analytics group leverages a big-data analytics platform that includes 100 terabytes of integrated claim, encounter, clinical, consumer, and other data. The team consists of 100 dedicated experts and 250 affiliated professionals with industry, advanced analytics, statistics, clinical, and software expertise, all of whom work to design, deliver, and operate advanced analytic tools to help healthcare clients around the world.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for and expands our investment in our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and were hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.

What You'll Do

You will be responsible for taking ownership of a Healthcare Analytics & Delivery platform component. You will also drive the process of defining technical requirements by engaging with business colleagues (product & delivery) and challenging business requirements for applications and services. You will develop technical solutions/processes using the Hadoop and AWS infrastructure and will identify opportunities to improve the platform and apply new technology to automate and standardize the work of our business domains.

You will be expected to develop and maintain Ansible scripts for data lake deployment and management in cloud environments. Leveraging AWS technologies on a daily basis, you will achieve efficient, highly available, scalable cloud environments. You are responsible for data governance using Apache Ranger.

You will develop knowledge and expertise in specific areas of the HA&D platform and will help to organize the work of junior colleagues during the development of complex products, and mentor them on development best practices."
Senior Data Engineer,Aruba Networks,"Aruba, a Hewlett Packard Enterprise Company, is a leading provider of next-generation networking solutions for the mobile enterprise: http://www.arubanetworks.com/company/about-us.

The company designs and delivers Mobility-Defined Networks that empower IT departments and #GenMobile, a new generation of tech-savvy users who rely on their mobile devices for every aspect of work and personal communication. To create a mobility experience that #GenMobile and IT can rely upon, Aruba Mobility-Defined Networks automate infrastructure-wide performance optimization and trigger security actions that used to require manual IT intervention. The results are dramatically improved productivity and lower operational costs.

We are looking for a Senior Data Engineer who is responsible for designing and implementing data pipelines at Big Data scale.

Your Responsibilities
Implement parsers and validators for new Log sources
Implement ETL transformers to reformat and enhance the data
Implement ETL correlators to update the data from multiple data sources
Work on tools and APIs to visualize the backend data
Troubleshoot performance and data related problems
Work with the Analytics team in defining the schema for new data sources

Our Minimum Requirements for This Role Are
4+ years java and/or Python development experience
Experience working with Hadoop or Big Data (HDFS, Parquet, HBASE)
Experience working with Large scale databases like Cassandra
Experience working with Map Reduce or Spark, ElasticSearch, Kafka
Experience working with Databases like Postgres, SQL

Education

Bachelor's or Master's degree in Computer Science, or equivalent and typically 4-6 years experience.

Benefits youll enjoy

At Aruba, a Hewlett Packard Enterprise Company, we offer an exciting and fun work culture, driving innovation, collaboration, and growth. We place our customers first, deliver some of the most innovative technologies to the market, and have fun doing it all! Come join our team and be part of an exciting organization poised for success!

Thanks for taking the time to review our job, if you think it is a match to your experience and interests please apply today  we are eager to learn more about you! We have dozens of openings, so encourage your friends to apply as well!

#ArubaNetworks #GenMobile #ArubaNetworksJobs #HPE #HewlettPackardEnterprise

Please note the above statements describe the general nature and level of work only. They are not a complete list of all required responsibilities, duties and skills. Other duties may be added, or this description amended at any time."
"Machine Learning / AI Engineer, Data Stream",Workday,"Join our team and experience Workday!
It's fun to work in a company where people truly believe in what they're doing. At Workday, we're committed to bringing passion and customer focus to the business of enterprise applications. We work hard, and we're serious about what we do. But we like to have a good time, too. In fact, we run our company with that principle in mind every day: One of our core values is fun.
Job Description
We are the Anomaly Detection / Fraud prevention team (ML Applications, Services, and Platform) team at Workday. We collaborate with teams at Workday delivering practical application of machine learning, statistical analysis and distributed computing to prevent fraud.
As a Machine Learning / AI Engineer, you will work closely with engineers and scientists from our team to design and build the infrastructure, engines, APIs required to apply Anomaly Detection to solve real-world challenges at Workday scale utilizing our massive computing resources and extensive data-sets. You will work with our product teams to deploy our solutions across our customer-facing services line up.
If you have industry experience working on a range of anomaly/outlier detection including data streams we want to talk to you right away! We will challenge you to bring these skills and applying them in very creative ways to solve to some of the most exciting business prediction problems that exist on the web.
Excited? Do you want to be a part of a world-class team? Here's more. In this role, you shall:
Lead/Contribute towards the design, planning, development, deploy and support of Fraud Prevention/Detection Services
Propose statistical or M/L based model/methodologies to tackle different problems
Communicate the results and methodology effectively within the team and to external stakeholders
Work with our ML platform team to design and implement end-to-end solutions for Workday customers
Own continuous support, performance, and enhancement of our ML products
We would love to discuss your:
Experience solving real-world problems such as anomaly detection, image classification, or ranking using machine learning (e.g. deep learning) techniques.
Proficiency in Python.
While having an experimental bias and enthusiasm to get hands dirty, Making architectural and implementation tradeoffs to meet project specifications while adhering to the project milestones and timeline objectives
Proficiency in at least one high-level programming language like Java, Scala, C++ or Python (NumPy, SciPy, Pandas)
Proficiency in at least one statistical modeling tools from among R, Matlab or Weka
Experience with Hadoop/YARN/Mesos/Spark/Elasticsearch/Kafka and building applications end-to-end
Educational background in a relevant field (Computer Science, Applied Math, Statistics)
Operations Research, Physics, Computational Biology, or other quantitative fields with relevant experience in Applied Machine Learning
What we offer:
Opportunity to make an impact on industry-leading projects
Work with a highly motivated and dedicated team
Full medical, dental, vision for FTE roles
Employer-matching 401k
Discounted employee stock purchase plan
Paid baby bonding / parental leave
Adoption reimbursement
Backup child and elderly daycare
Dog-friendly offices (Pleasanton-only)
Commuter benefits
Unlimited PTO
Healthy snack program
Fitness rooms"
Machine Learning Intern,Mitek Systems,"What Youll Do (Role Description) Join Mitek Labs, delivering computer vision and machine learning solutions for capturing and verifying mobile document images such as IDs, Passports, and Checks. Use cutting-edge technology to solve complex classification, registration, data extraction, and document verification problems. Work with an amazing team of scientists and engineers delivering the technology at the very center of all of Miteks products.

The goal of this project is to experience working in an Agile development team delivering cloud technology. You will work on projects including, but not limited to:

 Improving Miteks machine learning based classification toolset

 Measuring and improving data extraction algorithms

 Creating document verification algorithms

Team members will work closely with developers, business domain experts, and software engineers to prototype, design, develop, test, and document proposed solutions to be used in our science libraries.

As we work in an Agile environment, we value the following characteristics: collaborative, reliable, team player, insatiably curious, passionate, diligent, and likes ping-pong.

The ideal candidate will have a strong object-oriented programming background with an interest in building cloud technologies. The cloud platform will be used by some of the largest organizations around the world to verify identities for financial transactions, customer onboarding, and more.

Value for Students Incredible opportunity to work with cutting-edge technology while getting hands-on experience using computer vision and deep learning in products that are used by millions of people.

Technologies and Tools We Use
Languages and Technologies: C++, OpenCV, Python, Caffe
Development tools: Visual Studio, Git, VSTS (repository, testing, CI/CD), VersionOne (scrum process management)
Who You Are (Soft Skills, Attributes)
Self-starter and entrepreneurial mindset
Thrives in a fast-paced start-up team-focused culture and adapts to a changing environment
Data-driven, strategic mindset
Logical and creative problem-solving
Excellent interpersonal and relationship management skills
Planning, organization, and facilitation skills
Ability to manage and influence others (both within and outside your own direct work-group)
Ability to summarize complex issues simply and effectively
What You Need (Skills/Experience/Abilities)
Seeking Computer Science, Computer Engineering, Electrical Engineering, Mathematics or related majors
Coursework/projects in Machine Learning and Python
Computer Vision and C++
Nice-to-Haves
Experience with OpenCV
Experience with Caffe
Experience working with Images and Image Processing
About Mitek ""Accelerating the digital transformation of business through intelligent mobile capture.""

Mitek (NASDAQ: MITK) is an innovator in Artificial Intelligence and Machine Learning that simplify everyday tasks.

We are a global leader in mobile capture and digital identity verification software solutions. Miteks ID verification allows an enterprise to verify a users identity during a mobile transaction, enabling financial institutions, payments companies and other businesses operating in highly regulated markets to transact business safely while increasing revenue from the mobile channel. Mitek also reduces the friction in the mobile users experience with advanced data prefill. These innovative mobile solutions are embedded into the apps of more than 5,900 organizations and used by tens of millions of consumers for mobile check deposit, new account opening, insurance quoting, and more.

We have a track record of breakthrough achievements that have helped to transform mobile banking and the identity authentication markets. As a result, we have grown globally with offices in the UK and the Netherlands and are listed on the NASDAQ.

Were looking for team members that live our core values of Delivering, Learning and Caring."
"Data Engineer, (Big Data Oriented)",Dailymotion,"Job Description

As a Data Engineer, you will design, build and test highly scalable data pipelines and user profiling systems using cutting-edge big data technologies.

Responsibilities:
Develop reusable tools for the management and manipulation of hundreds of terabytes of data
Build a user profiling system for ad profiling
Analyze and improve efficiency, scalability, and stability of data collection, storage, and retrieval processes
Interface with other teams to incorporate their innovations and vice versa
Work closely with data analysts to construct creative solutions for their analysis tasks
Optimize our infrastructure at both the software and hardware level
Ingest and store terabytes/ petabytes of data Qualifications

Extensive experience working with large data stores
Hands on experience with Spark, Spark Streaming, Kafka, Flume is very useful.
Advanced SQL query writing abilities and data understanding
Solid understanding of Distributed Computing Systems
Strong experience in developing and maintaining reliable ETL processes
Deep understanding of storage appliances and storage software internals
Experience with filesystems , server architectures, and distributed systems
Experience / Knowledge in Cloud Architectures
Ad Tech experience is a huge plus
Masters Degree in Engineering or equivalent work experience required

Additional Information

We offer great health / dental / vision insurance
401k (company Safe Harbor contribution) summer hours, paid vacation, sick time, paid holidays, company sponsored Training Opportunities (upon eligibility) and much more."
Cloud Data Engineer,"MacAulay-Brown, Inc. (MacB)","MacAulay-Brown, Inc. (MacB) is looking for a Cloud Data Engineer that has experience with a full open source stack but with data requirements, data models, schemas, and the design of databases and data integration solutions experience as well. Will need to design, develop and analyze complicated and difficult database processes and events for computer based systems plus develop, monitor and operate the database management used in a cloud environment. The candidate will perform systems modeling, simulation and analysis. They will collaborate with hardware and software design engineers on machine characteristics that affect software systems and work with them to resolve incompatibilities. As required, will be asked to provide inputs for documentation of new or existing programs. Candidate must have a working knowledge of data migration tactics used to import and export within cloud environments. Candidate should have an inquisitive nature, responsiveness, and excellent testing skills. Must also possess strong troubleshooting skills and the ability to work under pressure with multiple deadlines. Patience in working with non-technical end users is essential.

Requirements:

Security Clearance:
A current TS/SCI government clearance is required
Required Skills:
Bachelor's Degree in Computer Science, a related field or equivalent experience.
10+ years of experience with database management systems and at least 2 years with cloud storage solutions (i.e. Hadoop, S3, Glacier, Snowball, MongoDB, etc.)
Strong background in the challenges of big data, Automation, memory usage optimization, and systems engineering is needed.
Training and/or experience with the Linux operating system, Ozone Widget Framework (OWF), Agile process, and the latest front end libraries.
Experience with the agile software lifecycle.
Must be willing and able to learn new things and stay on top of the latest and greatest technologies.
Desired Skills:
Scripting experience with Ruby, Bash, Python or Puppet is a huge plus.
Knowledge of DoDAF documentation is a plus.
Experience with Java, GitHub repositories, and Apache Maven.
Hadoop Certification with Cloudera or Hortonworks is a plus.
AWS Certified Solutions Architect (Associate)
For more than 37 years, MacAulay-Brown, Inc. (MacB) has been solving some of the Nations most complex National Security challenges. Defense, Intelligence Community, Special Operations Forces, Homeland Security and Federal agencies rely on our advanced engineering services, cyber security, and product solutions to meet the challenges of an ever-changing world. Join MacB where you will work with a team of highly experienced professionals in the areas of Intelligence, Cybersecurity, Research Development Test and Evaluation, Information Operations, fabrication, IT solutions, Logistics and Acquisition Management. MacB has won multiple workplace awards, offers generous salaries and benefits, and has outstanding growth opportunities.

#CJ"
Sr. Data Engineer,TechStyle Fashion Group,"Job Description
TechStyle Fashion Group is currently looking for a Sr. Data Engineer

How Do You Fit In?
As the Sr. Data Engineer , you will take ownership of Development oversight, guidance, and direction setting for the data pipeline as well as the Data Management platforms. The right candidate would be a self-motivated, highly detail-oriented team-player with a positive drive to strategize and implement BI Solutions that enable the business derive valuable insights. You will join a tight knit group of key contributors who are actively working together to achieve aggressive goals, and meet timelines to drive the business forward. This role is critical in laying the foundation for the key Decision support systems that will form the backbone of our Big Data ecosystem.
RESPONSIBILITIES:
Work in a dynamic, agile environment managing multiple high-impact and high-visibility priorities simultaneously
Interface with other technology teams to develop/maintain the ETL footprint and quality from a wide variety of in-house and 3rd party data sources
Collaborate within the team, across other teams including, application development, DBAs and product/project management to effectively follow through on the data pipeline workflows
Participate in cross-functional meetings to review business requirements/use stories, assist in fit/gap analysis and provide detailed technical design documentation
Develop workflows to extract, transform, clean and move data from the business systems into an MPP Data Warehouse using enterprise ETL tools like Informatica/Talend
Develop detailed ETL specifications based on business requirements
Partner with business users, senior architects, product managers, engineering teams, and other teams to deliver a robust data services platform
Profile and understand the large amounts of source data available, including structured and semi-structured/web activity data
Provide cross organizational business stakeholders operational support on existing and newly developed data pipeline
Triage, identify, and fix scaling challenges
Use your expert coding skills across a number of languages from Python, Java, Ruby, and PHP
Triage, identify, and fix scaling challenges
Recommend ways to improve data reliability, efficiency and quality
Interact with Data science team and product management team to understand requirements
Educate product managers, analysts, and engineers about how to use our tooling to answer hard questions and make better decisions
Diagnose ETL and database related issues, perform root cause analysis(RCA), and recommend corrective actions to management
Work with Data / Data Warehouse Architect on logical and physical model designs
Work with Data Modeler to develop data warehouse models, design specifications, metadata process and documentation
Mentors the team on the industry standards and best practices for effective use of Informatica data integration and data quality technologies and exception handling
Experience working in a matrix organization and able to provide reports to the functional and project managers with regards to the project schedule tasks and deliverables
Constantly Monitor, refine and maintain system performance and provide statistical reporting
QUALIFICATIONS:
6+ years of data engineering experience with high performance Big Data platforms including cloud based Data Warehousing on large scale development efforts leveraging industry standard ETL tools
5+ years of experience with ideally Java, or at least one of the OO programming languages (Python etc.)
4+ years of Data Warehouse development for 100s of gigabytes of data and billions of records (Redshift and Hive is preferred)
4+ years of expert level implementing ETL development using tools such as Informatica PowerCenter, Talend Open Studio, additionally open-source tools like Airflow, Luigi a plus
2+ years minimum experience with a Big Data cluster i.e. Hadoop ecosystem on HDP, Cloudera, AWS
2+ years of experience with data technologies such as Spark, Kafka, Hadoop, and distributed datastores
Experience developing ELT pipelines using Terradata,Vertica, Redshift, or similar data warehousing technologies
Experience implementing streaming pipelines (Kinesis, Kafka, Storm, Spark, Flink)
Experience working in an environment that ingests large amounts of raw data (web logs, Click stream, data feeds)
Experience with source code management using Git or Subversion and release processes.
Experience with scalable systems in a load balanced environment and experience conducting load tests
Ability to extend your scope into the Analytics domain and partner with that team to optimize the output of the Analytics function
Ability to create and interact with very large data processing pipelines, distributed data stores, and distributed file systems
Ability to learn quickly and multi-task in a fast-paced, dynamic environment
Understanding of data warehouse architectures (Kimball a plus) & Strong metadata modeling experience
Conduct training programs and knowledge transfer sessions to junior developers when needed
Experience with EDA (Exploratory Data Analysis) and Data Visualization a plus
Experience with Spark Streaming, Spark SQL in a production setting is a plus
Professional programming experience in Java or JVM-based languages such as Scala a plus
E-commerce or retail experience a plus
TechStyle is an Equal Opportunity Employer: M/F/PV/D (minority, female, protected veteran, disability)"
"Software Engineer, Online Data Storage",Square,"Job Description

The Online Data Storage team (ODS) is responsible for OLTP systems within Square. ODS provides and maintains a reliable and secure platform to host databases. The team develops tooling to automate the creation and maintenance of databases, provides multiple levels of redundancy, and gives application owners tools to better manage their databases.

You will:
Optimize datastore architecture and configurations to improve performance efficiency.

Write software and tools to automate the lifecycle of databases.

Collaborate with other platform teams to develop the infrastructure of Squares database servers.

Collaborate with other products teams to optimize their database usage. Qualifications

You have:
5+ years of industry experience developing highly scalable production services.

Strong experience writing software applications in Ruby, Java, or Go.

Strong experience in MySQL or Redis administration, operation, and performance tuning on Linux/Unix.

Familiarity with scripting languages such as Ruby and Bash

Additional Information

At Square, we value diversity and always treat all employees and job applicants based on merit, qualifications, competence, and talent. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. We will consider for employment qualified applicants with criminal histories in a manner consistent with the requirements of the San Francisco Fair Chance Ordinance."
Fullstack Software Engineer (Angular | React | Data Viz),Kinetica DB,"Job Description

Kinetica is looking for an outstanding mid to senior level full-stack engineer with hands-on experience developing data visualization applications. In this role, you will join and collaborate with a team of passionate engineers who excel in turning complex problems into intuitive user experiences. You will have the opportunity to build and work on features that are critical to both internal and external customers.

Key responsibilities:
Build and develop our database admin application including working with core database team to implement new GUI to support new features.

Work on enhancing our data visualization application which gives our customers insight into their data using rich charts and interactive mapping/geospatial tools and filters.

Responsibility for both admin and visualization applications include end-to-end deliveries including, but not limited to, UI design, graphic design, client-side and server-side coding, and database/SQL modeling.

Assist and support Sales Engineering team in creation and design of data visualization dashboards for both demos and production implementations.

Become the SME for both the admin and visualization tool. Qualifications

To be considered:
Technical Degree with strong knowledge of Computer Science fundamentals, data structures, and algorithms

At least 4+ years of FE development experience in JavaScript, Angular.js, and React.js

At least 4+ years of BE development experience in Java, and/or Python.

Understanding of how to operate and develop within a Linux environment.

Experience building data visualization and web mappings applications using tools such as OpenLayers, D3.js, or Mapbox ( preferred )

Solid verbal and written communication

Local to the DMV area ( preferred but open to relocating an exceptional engineer)

Additional Information

All your information will be kept confidential according to EEO guidelines."
Senior Data Engineer,Funding Circle,"Funding Circle brings together small businesses and investors in a way that is truly revolutionary. Our mission is to foster an environment where small business can thrive. Our online platform provides a marketplace where investors receive better returns and small businesses find lower rates. The driving force behind our product is our engineering team; we are building elegant, sustainable, and scalable infrastructure on a global scale, and we want you to be a part of it! Our mission: to build a better financial world.

Prospectus :
Would you describe yourself as a data fanatic? Do you have a passion for integrating and analysing data sources to help your company make better decisions faster? If you answer yes to these questions then we're looking for you to join our team! Funding Circles Data Team are looking for a Senior Data Engineer to help build and transform FC UKs data warehouse.

The right person will be:
An enthusiast - you understand what a good data warehouse can bring to a business and what it takes to build one.
A communicator - you can communicate effectively to engineers as well as business users.
A builder - you are experienced in all stages of warehouse development, from data modelling to building out comprehensive ETL.
A pathfinder - you are interested in solving problems today while helping shape strategic direction for the months and years ahead.
A thinker - you have an inquisitive mindset and the desire and ability to turn business requirements into working software.
An owner - youll take pride and ownership in the quality of the work you and the team produce.
The UK data team are part of a global team tasked with building and maintaining the data platform that supports analytics and reporting for FC. Our stakeholders are from every area of the business - from teams tracking performance, through BI specialists to data scientists. We have identified data as one of our key strategic assets and so this role is one in which you can make a visible impact on the business.

The ideal candidate will have:
7+ years in a development and data engineering role preferably in tech, consulting, or finance.
Extensive hands-on experience with SQL (we use Postgres).
Strong programming skills (Python, Java, Ruby, Scala, Clojure; we love them all).
Familiarity with AWS stack including EMR, Lambda & EC2.
Proficiency using large data sets and relational and dimensional modeling.
Expertise in a Unix environment.
Experience with messaging and streaming platforms (Kafka/RabbitMQ/JMS/etc.).
Exposure to big data/NoSQL systems and the issues that arise from working with large data sets.
A self-starter attitude with an enthusiasm to work in a fast-paced, team-oriented, start-up environment.

Bonus points for:
Engineering/CS or Finance degree.
Experience in virtualized environments (Mesos, Marathon, Chronos, Docker, etc.).
Skills working with structured and unstructured data sets.
Microservice architecture development experience.

Why Join Us?
Happy employees are productive employees, thats why we offer a hearty benefits package. From learning and development and commuter stipends, to a competitive salary, equity, and health benefits, weve got you covered! That being said, have you heard about what we're doing?! Our mission is what really motivates us to come to work each day:
We're supporting small business, the engine of economic growth.
We're helping facilitate higher yields for investors and lower interest rates for borrowers.
We can fund loans extremely quickly, all online!
We have a clear competitive advantage globally in areas like domain expertise and regulatory processes.
Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records.
Funding Circle provides equal employment opportunity to all individuals regardless of their race, age, creed, color, religion, national origin or ancestry, sex, gender, disability, veteran status, genetic information, sexual orientation, gender identity or expression, pregnancy, or any other characteristic protected by state, federal or local law."
Data Design Engineer,Plenty,"Plenty is re-imagining the way humanity grows food and its digital systems are making agriculture smarter. The computer scientists at Plenty are plant and human-centric: we build outwards from the way our people and the grow spaces work every day, crafting tools that understand, coordinate, and optimize the farms. Our weaving of custom-built physical devices, large scale data processing, user-centered design, and machine learning both automate systems and facilitate the broader Plenty team. Join us in making digital tools that help grow plants!

We are also looking for this position as Junior, Mid-Level, or Senior placement. We encourage folks to apply wherever they are within their journey through design and CS.

What You'll Do:
Data flows through everything we do at Plenty from farm operations to plant science research. Our Data Design Engineers are the caretakers of those data and the automation that runs on top of them; they make sure everything is flowing securely, efficiently, and reliably.
You'll be working on software that can do heavy lifting of tracking plants through our farms, reporting on internally built devices that report on how the farm is doing, and sending commands to software written by our IoT folks that tells robots and embedded devices what to do and when. All of this data and automation keeps the plants healthy and happy at scale: running complex nutrient systems, maintaining environmental control, managing people operations, etc.
That said, you will be not just a engineer but a designer, working on a team to architect for these large scale systems and designing around and learning from plant science, data science, farm operations, business development, and mechanical engineering.
What We're Looking For:
Experience with Java or Python.
Experience with Flask or DropWizard a plus but not required.
Experience with Heroku, Google Cloud, AWS, IBM Cloud, or Azure a plus but not required.
Experience with Spark and Hadoop a plus but not required.
Familiarity with distributed systems concepts like sharding or map reduce a plus but not required.
Familiarity with Sentry, Log4J2, and PagerDuty a plus but not required
Familiarity with (or willingness to learn) software architecture concepts like design patterns.
We are on the lookout for great people and understand that the most excellent contributors won't fit into boxes. So, we encourage folks to apply even if they do not meet all of these recommended items."
Digital Engineer  Data Scientist,Siemens AG,"Division: Power Generation Services
Business Unit: Power and Gas
Requisition Number: 218050
Primary Location: United States-Florida-Orlando
Assignment Category: Full-time regular
Experience Level: Senior level
Education Required Level: Bachelor's Degree
Travel Required: 15%

Division Description:

Power Generation Services (PS)

Siemens is the leading service partner for ensuring high reliability and optimal performance of rotating power equipment within the utility, oil & gas, and industrial processing industries worldwide  as well as for wind turbines. Siemens extensive national network of service technicians is able to quickly and comprehensively offer expert service to maximize the lifecycle of power generation equipment, helping to ensure reliability and prevent downtime.

For more information, please visit: http://www.siemens.com/businesses/us/en/power-generation-services.htm

Job Description:

Digital Engineer  Data Scientist
Orlando, FL

Tasks & Responsibilities
 Working within the Digital Engineering group across several locations (Berlin, Germany and Orlando, FL and Pune, India) to develop commercial software applications
 Collaborates closely with our customers and engineering teams to develop innovative solutions for our digital services portfolio
 Analyzes data from power plant assets, infrastructure assets, and total asset management systems
 Formulates data recording requirements and communicates these to our engineering teams
 Present results of data analytics projects to our customers and management
 Codes prototypes of data analytics solutions and integrates them into our productive platform
 Preparation of invention disclosures

Requirements
 Bachelors degree in experimental physics, applied computer science, mechanical or electrical engineering, or similar studies
 Proven experience with applied data analytics and machine learning
 Strong understanding of physics processes and engineering of mechanical systems
 Basic understanding of interpretation of data statistics
 Experience with sensor technology, singleboard computers or microcontrollers (e.g. Raspberry Pi or Arduino) are a benefit
 Object oriented programming with Python
 Experiences with machine learning libraries (e.g. scikitlearn, Keras, TensorFlow, Apache Spark MLlib or similar)
 Practical experience with data analytics in AWS, e.g. with Apache Spark
 Profound knowledge of querying data with SQL
 Experiences with Docker are a benefit
 Demonstrated ability to adapt to new technologies and learn quickly
 Fluent English required
* 8+ years of relevant experience"
"Software Engineer (PHP, Python), Enterprise Applications",Tableau,"What youll be doing

At Tableau, we are relentlessly focused on customer success. In the Enterprise Applications group, we are builders, writing software and stand up services that help our customers buy our software, manage their profiles, download software keys and login to our services. We also build internal tools that help our business run faster and more efficiently.
With your help, we can build systems that scale as our business grows. We have a collaborative team of engineers working in technologies like PHP, Python and JavaScript on platforms including AWS and Salesforce. As a member of the Enterprise Applications Engineering team, you'll build systems that empower our customers and energize our business.
At Tableau, we believe in efficiency, simplicity, freakishly friendly customer service, and making a difference in the world. Come join us!

Who you are
Performer. Ability to learn on the fly and a passion for solving challenging business problems.
Results Oriented. Enjoy working with and learning about different areas of the organization to develop more effective solutions.
Strong organizational skills and good verbal and written communication skills.
Education. B.S. Computer Science or equivalent.
Experienced:
3+ years of software development experience
Proficiency in at least one modern programming language such as Python, PHP, Java or Javascript
Understanding of object oriented design and development
Strong problem-solving experience with a desire to learn
Familiar with:
Web services technologies: JSON, REST, XML, SOAP
Database management: MySQL, PostgreSQL
Web dev: HTML, CSS, jQuery, Anuglar or React
Git or similar version control system
Linux, Apache
Unit tests, performance tests
Agile methods
System hardware architecture (servers, redundancy, backups, performance analysis)
Docker
UX design
Knowledge of Amazon Web Services
You are a Recruiter! Tableau hires company builders and, in this role, you will be asked to be on the constant lookout for the best talent to bring onboard to help us continue to build one of the best companies in the world!
Tableau Software is a company on a mission. We help people see and understand their data. After a highly successful IPO in 2013, Tableau has become a market-defining company in the business intelligence industry. Our culture is casual and high-energy. We are passionate about our product and our mission and we are loyal to each other and our company. We value work/life balance, efficiency, simplicity, freakishly friendly customer service, and making a difference in the world!

Tableau Software is an Equal Opportunity Employer."
Associate Software Engineer-Data Services Parser Team,eMoney Advisor,"The Associate Software Engineer is responsible for the collection, movement, and aggregation of data via techniques such as web scraping and API manipulation. Requires strong critical thinking and problem solving skills, and the ability to define strategies that lead to the efficient acquisition of data that is complete and of the highest quality. Ideal for a team player with a passion for learning and continuous improvement. Entry level software engineering position that provides a nurturing culture where you can further build on your software development skills.
Job Description
Address support tickets associated with existing Web Scraping Parsers
Analyze Financial Institution web-sites
Strategize and define best method for efficient and quality data collection
Implement and maintain C# code for web scrapers
Write basic SQL as needed to verify proper data ingestion
Test and verify developed solutions
Deploy solutions to testing and production environments
Develop a general understanding of the eMoney aggregation API and infrastructure
Grow your skills with regards to software craftsmanship, software design, design patterns, best practices, etc., assisted by mentoring provided by engineers on the team
Requirements
Bachelor's degree in Computer Science (or equivalent work experience)
1+ years of relevant work experience or internships
Skills

Technical Skills Required:Intermediate level:
C# or Java
Object Oriented programming

Basic Level:
HTML
JavaScript
Data Structures
SQL - queries

Software Design Skills Required:Intermediate level:
Critical Thinking and Problem Solving Skills

Basic level:
Debugging skills
Testing skills
Learning Software Design aspects such as Design Patterns

Personal Attributes:
Quality focused
Self-Motivated
You strive for continuous improvement in how you think and code.
You strive to gain a deep knowledge of the underlying systems on which you work. You want to be the Subject Matter Expert!
You love data and writing code.
You are energetic, smart, get things done, and are a good teammate.
You welcome collaborating with others and distributing your knowledge amongst the team.
You invest personal time to strengthen your skills and to grow new skills as needed to keep up with the fast paced technology field."
Principal Engineer / Analyst,Estee Lauder,"Position Summary

This position in the Technical Package Engineering group will assist in package engineering functions including design, engineering and development of primary packages and delivery systems for cosmetics and beauty care products.

Key Roles & Responsibilities
Design and engineer new-to-the-world packaging components in 3D CAD, which meet manufacturing, functional, and performance requirements.

Develop and communicate design intent using 3D CAD files, physical prototypes, and 2D drawings: Revise and maintain existing package designs and modify engineering drawings to ensure compliance to best standards of package performance.

Communicate directly with representatives from multiple-discipline project teams including Finance, R&D, Marketing, Manufacturing, Purchasing, and Consumer Knowledge.

Select and recommend materials and manufacturing methods for final and prototype components.

Actively participate in innovation processes and generate innovative packaging and product solutions.

Assess conceptual, creative, and aesthetic designs for manufacturability; document engineering direction for form, fit and function using 3D CAD.

Use prototypes to improve designs and gain agreement and approval from stakeholders.

Use predictive engineering tools such as Mold Filling Analysis (MFA) and Finite Element Analysis (FEA) to improve and validate part designs.

Provide direction related to design, engineering, fit, and performance specifications in Design FMEA processes.

Collaborate with Designers, Package Developers, Manufacturing Engineers and Suppliers to agree development and production approvals for packaging and consumer device components.

Collaborate with Manufacturing Engineering to ensure efficient handling of package components on filling lines.

Evaluate and approve suppliers dimensions and tolerance stacks for component drawings of primary packaging components.

Analyze pre-production parts, Tool Qualification data, and Troubleshoot packaging issues prior to final approvals.

Qualifications
Engineering degree required, with 5 years in a product and/or part engineering and/or development role. Mechanical, Chemical Engineering preferred.

3D CAD modeling, surfacing and drawing creation (SolidWorks preferred) for part / product design.

Must be fluent in 2D technical drawing review, tolerance stack-up analysis and decision making.

Must understand, leverage, and generate intellectual property.

Capable of designing and developing dynamic product delivery systems for handling solids, liquids, and powders.

Technical knowledge in the areas of plastics injection molding, tool / mold design, and troubleshooting.

Knowledge of Health and Beauty product manufacturing and packaging techniques.

Well-versed in packaging component conversion processes & materials (such as plastics, metals, glass) used in the cosmetics industry.

Familiar with packaging test procedures and ASTM traceable test methods.

Experience with Minitab (or similar) is preferred, but not required.

Must be able to effectively use standard Office productivity tools such as Outlook, Word, Excel, PowerPoint.

Job: Manufacturing / Operations / Packaging

Primary Location: Americas-US-NY-New York

Job Type: Standard

Schedule: Full-time

Shift: 1st (Day) Shift

Travel: No

We are an equal opportunity employer. Minorities, women, veterans, and individuals with disabilities are encouraged to apply. Job Number: 173560"
Senior Big Data DevOps Engineer,Apple,"At Apple, we work every day to create products that enrich peoples lives. Our Advertising Platforms group makes it possible for people around the world to easily access informative and imaginative content on their devices while helping publishers and developers promote and monetize their work.

Our technology and services power advertising in Apple News and Search Ads in App Store. Our platforms are highly-performant, deployed at scale, and setting new standards for enabling effective advertising while protecting user privacy.

The Ad Platforms TechOps Team is seeking a seasoned DevOps Engineer for the Big Data Operations team.

Key Qualifications
5+ years of working experience in Big Data technologies (Hadoop, Solr, Spark Kafka, Flume, NoSQL databases)
Proven track record of architecting, designing and implementing data pipelines and related operations
Big Data operations and support experience in Hadoop, Spark, Kafka, and NoSQL databases
Hands-on experience with DevOps tools, automating engineering and operational tasks
Highly knowledgeable and experienced with scripting languages like Puppet, Python, Perl, etc.
Experience with source control management such as Git and/or SVN
Expertise in troubleshooting complex OS, database, file system, network configuration, and application & web server issues
Development experience in object oriented programming languages such as Java & Scala is strongly preferred
Must have extensive experience evangelizing DevOps best practices and influencing cross functional teams to solve production issues upstream
Strong verbal and written communication skills
Data analysis experience is a bug plus
Willingness to participate in a 24x7 on-call rotation for escalations

Description
As a senior DevOps engineer for our Big Data environment you will add tremendous value in developing strategies, defining new processes and driving initiatives to support our infrastructure globally. You will work on collaborative projects such as building large scale data pipelines and analytical solutions between our team and others businesses at Apple. You must be a self starter, an outstanding communicator, and a solid technical contributor.

Education
Bachelor's degree in Computer Science or equivalent is required. Master's degree preferred."
Data Science Intern,ZocDoc,"I get to collaborate with smart people across the company on interesting problems that have a meaningful impact on the business. It doesnt get better than that!

Courtney , Data Science
About the position

Would you like to revolutionize the business of healthcare through data science? Do you want to be part of a rapidly growing highly talented data science team who works closely with business, product and engineering units to leverage our big data infrastructure to drive everyday business decisions?

Join Zocdoc as a 2018 Summer Data Science Intern! Youll have the chance to be a valued member of a talented team of world class data scientists and machine learning engineers. Youll be immersed in highly challenging products an d operational questions, work in a fully modern Big Data stack, and create insights that impact millions of patients!

What youll do:
Kick off your internship at ZocU, a week long engineering class where you will be introduced to commonly used technologies and Zocdocs internal tooling
Learn data engineering best practices, like how to write clean, readable SQL and Spark data flows, interact with various data storage engines, and commit code to a collaborative repository
Become fluent in AWS based data systems, such as S3, Redshift and Spectrum
Be an integral member of a small, agile squad, solving exciting problems everyday as you collaborate closely with our team of brilliant data scientists, engineers and product managers
Work on a real production level machine learning system and/or dive deep into user behavioral data to answer important research questions
Present your summer project to our entire technology and leadership team at the end of your internship
Whats required:
A sound understanding of data science fundamentals such as machine learning and statistical hypothesis testing
Experience writing clean code in Python, R o r Scala, and working knowledge of SQL
An eagerness to immerse yourself in Zocdoc tech, problem solve on a daily basis and be challenged to grow everyday
You are a smart communicator and can effectively discuss even the smallest nuance of your analysis with the rest of the team
An infectious commitment to learning through data an enthusiasm about working on a product that is revolutionizing healthcare
At least one semester of school left post internship at Zocdoc; i.e. a graduation date between December 2018 & May 2019 with a BS or MS in Math, Statistics, Computer Science or similar degree
What youll get:
A mentor who will help you hone your analysis and engineering skills, provide you with constructive feedback and guide you to optimizing your internship
A competitive compensation package and generous monthly housing stipend
Applicable reading materials & a package of custom Zocdoc branded swag
Engaging and fun team events where you will build relationships with your peers and explore all NYC has to offer
Catered lunch every day in our sunlit SoHo office and a fully stocked snack room
An incredible team of smart and supportive people combined with the chance to create a better healthcare experience for millions of patients
Potential for full time employment post-graduation based on excellent internship performance"
RF Engineer- Data Analyst,General Atomics and Affiliated Companies,"Job ID#: 14590BR
Company: General Atomics Aeronautical Systems
Job Title: RF Engineer- Data Analyst
Job Category: Engineering
City: San Diego
State: California
Full-Time/Part-Time: Full-Time Salary

Job Summary

General Atomics Aeronautical Systems, Inc. (GA-ASI), an affiliate of General Atomics, is a world leader in proven, reliable remotely piloted aircraft and tactical reconnaissance radars, as well as advanced high-resolution surveillance systems.

We recognize and appreciate the value and contributions of individuals with diverse backgrounds and experiences and welcome all qualified individuals to apply.

We have an exciting opportunity for a Data Analyst to join Mission Systems, in support of the Signal Processing team, in Rancho Bernardo, CA. This employee will work closely with tactical teams of system developers including antenna engineers, RF/electrical engineers, software developers, and algorithm developers, to collect system data and analyze performance, suggest new tests, and contribute to all phases of the program lifecycle. Candidate should have willingness to be involved with all phases of the product development cycle. The ideal candidate will have experience with Signal Processing, Modeling and Simulation, Data Analysis, Array processing, and RF Waveforms.

DUTIES & RESPONSIBILITIES:
Develop Matlab scripts to visualize laboratory and field data.
Use a broad array of data analysis techniques to identify systems issues, characterize performance, and find trends within measured data from a wide array of RF systems ranging from HF to UHF.
Under general directions with limited review, this position is responsible for applying innovative approaches to resolving unusual or complex engineering problems. Assignments are normally outlined in terms of overall objectives and reviewed largely at completion.
Initiates the planning of engineering projects and develops innovative designs or modifications in order to meet project goals.
Independently performs difficult/complex engineering work with instructions as to the general expected results.
Documents findings, communicates results to engineering staff, makes technical presentations, and may represent the organization as the prime technical contact on small contracts or single component projects.
Provides direction to design or technical staff and may lead a team of moderately experienced professional staff.
Essential Functions Include: 1.Supports operation of engineering systems and documents any problems. 2.Devises new approaches to resolve unusual or complex engineering problems. 3.Troubleshoot conflicting design/development requirements, difficult coordination requirements, and special material requirements. 4.Provides documentation and makes technical presentations as required. 5.May represent the organization as the prime technical contact on small contracts or single component projects. 6.Provides direction to design or technical staff and may lead a team of moderately experienced professional staff. 7.Maintains the strict confidentiality of sensitive information. 8.Performs other duties as assigned. 9.Responsible for observing all laws, regulations and other applicable obligations wherever and whenever business is conducted on behalf of the Company.
Expected to work in a safe manner in accordance with established operating procedures and practices.

Job Qualifications

Typically requires a bachelors degree, masters degree or PhD in engineering or a related technical discipline from an accredited institution and progressive engineering experience as follows; six or more years of experience with a bachelors degree, four or more years of experience with a masters degree, or two or more years with a PhD. May substitute equivalent engineering experience in lieu of education.
Must have a thorough understanding of engineering concepts, principles, codes, and theory; experience demonstrating a broad application of those concepts; and, expanding knowledge of principles, concepts, theory, and practices in related technical specialties.
Must possess the ability to develop and communicate new concepts; apply them accurately throughout an evolving environment; organize, schedule, and coordinate work phases; and, determine the appropriate approach at the task level or, with assistance, at the project level to provide solutions to a range of complex problems.
Must have strong communication, computer, documentation, presentation, and interpersonal skills, ability to work independently and as part of a team; able to perform complex tasks in one or more engineering areas; capable of representing the organization as a prime technical contact and, lead a team of moderately experienced professional employees on single component projects.
Must be customer focused and able to work on a self- initiated basis or in a team environment and able to work extended hours and travel as required.
A Professional Engineering License and original work published in professional engineering journals are desirable.
Experience using core signal processing techniques (FFTs, digital filters, digital down conversion, PSK FSK waveforms)
Ability to clearly visualize and plot data in meaningful ways
Problem solving skills  especially in the context of data analysis
Matlab Modeling, Simulation, and Data Analysis

Desired Skills include:
C, C++ programming
Experience with RF direction finding techniques
Experience with array processing techniques
Experience with Broadband RF and Bi-Static or Multi-Static Radar
Laboratory experience with spectrum analyzers, signal generators, network analyzers
Presentation (oral and written) skills
A Professional Engineering License, original work(s) published in professional engineering journals, invited to present one or more original works to an engineering symposium, and invited and/or participated on an engineering review panel are desirable.
Must be able to obtain and maintain a DoD Security Clearance. (Clearance requires U.S. Citizenship and no criminal or credit issues.)
Travel Percentage Required 0% - 25%

Clearance Required? Desired
14590"
"Software Engineer, Python",Spoken Communications,"Experience building large scalable and distributed systems
Experience working with python web frameworks and building RESTful systems.
Provide ongoing support, maintenance and enhancement of systems
Understanding of secure coding principles
Perform peer code reviews and Code audit - Analyze existing code and remediate performance issues within existing applications
Unit test modules, integration testing with other modules to deliver quality code
EDUCATION AND EXPERIENCE NECESSARY:
Bachelor's Degree in Computer Science or related field
5+ years professional experience in software development and building successful production software systems
Knowledge of Computer Science fundamentals (object-oriented design, algorithm design, data structures, problem solving, and complexity analysis)
Candidates must have experience developing distributed software services and an understanding of design for scalability, performance and reliability
5+ years professional Python software development in Linux environment
Demonstrable and clear understanding of Python multi-threading concepts and best-practices
Excellent verbal and written communications skills
Commitment to excellence"
Software Engineer,"ARRIS Group, Inc.","ARRIS is a premier broadband, wireless and video technology company that transforms how service providers worldwide deliver entertainment and communications without boundaries. Its powerful end-to-end platforms enable service and content providers to improve the way people connect - with each other and with their favorite content. The Company
's vision and expertise continues to drive the industry's innovations as they have for more than 60 years. Headquartered north of Atlanta, in Suwanee, Georgia, ARRIS has R&D, sales, and support centers throughout the world.

The ARRIS telco gateway engineering team designs residential gateways for telecommunication companies around the world. Our DSL, GPON and LTE gateways provide high speed communication to homes supporting Internet, TV and telephony services. Our gateway software is Linux based. This position is based in Silicon Valley(Santa Clara, CA).

The software engineer develops embedded software for telco gateway running Linux operating system. She/he will work under guidance of more senior engineers. The candidate responsibilities include:

* Design, implement, unit-test and debug software for ARRIS telco gateway software.
* Create required design documents. Participate in code and design review and generally adhere to high quality software development practice.
* Communicate project status, issues, and requirements both verbally and in writing.
* Conduct technical presentations/demonstrations if necessary.
* Perform other duties as required.

Qualifications
Required Education and Job Experience:

* Requires a Bachelors Degree in Computer Science, Computer Engineering, Electrical Engineering or related field.

* 0-3 years of related experience.

Required Knowledge, Skills and Abilities:

* Proficient in C, C++

* Working knowledge of HTML, Javascript, CSS highly desirable

* Experience with writing Linux user-space applications and scripts

* Understanding of Linux kernel and boot process. Experience with configuring and building the Linux kernel

* Solid knowledge of Network protocols (TCP/IP, ARP, DNS, etc)

* Hands-on experience with compilers, debuggers and build systems

* Strong problem solving, software debugging and hands-on trouble-shooting skills

* Solid foundation in data structures and algorithms

* Excellent written and verbal communication skills"
Software Engineer,Ambarella,"Description

Microcode engineers at Ambarella are responsible for the design, development and implementation of Imaging and computer vision algorithms on proprietary SOC. You will be working with industry recognized scientists and architects to bring out best performance to the market.

Join a team of highly innovative scientists and engineers to research, architect and implement new technologies related to Automotive Camera Solutions. This domain requires deep knowledge of imaging and computer vision.

We want to create efficient platform where new algorithms are developed and ported by customers for various applications.

Responsibilities
Develop deep understanding of Ambarellas DSP architecture for efficient code development.
Efficient use of hardware resources like CPU cycles and memory are core to scaling imaging and computer vision algorithms.
Debug low level software in various environments such as; pre-silicon simulation model, silicon bring up and production.
Drive development activities in pre and post silicon environments.
Give feedback on tools for development that will speedup customer development.
Understand features from customer point of view and review those with application and system team. Enable customers to port their algorithms on to AMBA SOC.
Develop and debug in test environment for unit testing of different functional areas of microcode such as CFA, filters, video CODECs, CNN etc.
Qualifications:
You must possess a Bachelors degree in EE, computer engineering, CS or equivalent. Masters degree with same disciplines preferred.
Must be comfortable with assembly coding and C programming. Minimum qualifications:
3+ years of experience in developing in C.
1+ year experience in assembly code development and debugging.
Solid knowledge of scripting languages such as perl.
1+ year experience with computer architecture which include registers, interrupts, threads, on chip memory and dram. Must be able to resolve data synchronization issues with computer architecture.
Must be comfortable with modern software development methodologies such code review, unit testing, peer reviews etc."
Software Engineer,"A2Z Development Center, Inc.","We are looking for a passionate engineer to join our team and drive the development of web applications that power consumer electronic devices. Your job will be to work with a team of engineers (hardware and software) and other team members (PM, Product, Marketing) to design and develop new ways of providing exciting and compelling experiences. You'll measure your success by the quality of your execution, the pace of innovation you help drive, and the increased satisfaction you deliver. If youre ready to make a difference, if you want huge challenges to tackle, we would love to talk to you.You will have an enormous opportunity to make a large impact on the design and architecture of cutting edge products used every day, by people you know.
Will work in an Agile/Scrum environment to deliver high quality software against aggressive schedules
Will establish design principles, select design patterns and mentor junior team members
Will help to define and push for the best possible end-user experience

In addition the role includes:
Solving complex technical challenges

'Continuous Deployment of consumer facing products used by millions of customers daily

Designing solutions for a wide range of Web and Mobile devices; Tablets, Phones and FireTV, Echo (Inc.Voice Search)

Basic Qualifications
Bachelors Degree in Computer Science or Software Engineering

Computer Science fundamentals in object-oriented design, data structures and algorithm design, and complexity analysis

4+ years professional experience in software development using C++, C# or Java
Preferred Qualifications
Experience developing cloud software services and an understanding of design for scalability, performance and reliability
Development experience defining, developing and maintaining web service API's
Demonstrated ability to mentor other software developers to maintain architectural vision and software quality.
Excellence in technical communications with both technical and non technical peers"
Software Engineer,Yale University,"Work as part of a computational immunology research laboratory to support research activities in bioinformatics involving immune system profiling and computational biology applications. The position involves a combination of bioinformatics programming and analysis, along with database/system administration supporting research and collaborative projects with basic and clinical researchers.

Essential Duties
1. Develops programs or systems of small to moderate size and complexity. Modifies existing systems at all levels of difficulty. 2. Provides informational input into decisions concerning the development and delivery of applications, programs and systems. Defines and analyzes requirements to meet the expectations of stakeholders and intended end user needs, scheduled timeline, and budgetary targets. 3. Analyzes, defines and designs new systems and applications. Writes code in support of business solutions. 4. Responsible for the creation, definition, communication, and management of project plans which includes architectural design, technology selection and methodologies to apply. 5. Troubleshoots problems and provide ongoing maintenance and support for applications and systems. 6. Prepares documentation, user manuals and develops formal proposals for new systems and modifications to existing systems. Mentors technical staff and provide training for end users. 7. Contributes in the development of policies or modifications to exiting policies. 8. Applies and keeps current with existing and emerging technologies and methodologies. Provides ongoing input to the establishment of programming standards, procedures, and methodologies. 9. May perform other duties as assigned.

Required Education and Experience
Bachelor's Degree in a related field and two years of related work experience or an equivalent combination of education and experience.

Required Skill/Ability 1:
Ability to support collaborative projects by performing bioinformatics analysis of large-scale experimental/clinical data. Ability to define objectives by analyzing user requirements and expectations; envisioning system features and functionality.

Required Skill/Ability 2:
Ability to collaborate with team members, design and implement computational pipelines for analysis of high-throughput biological data (e.g., microarray, next-generation sequencing (RNA-seq, Rep-seq, etc.), flow cytometry).

Required Skill/Ability 3:
Ability to participate in design and development of bioinformatics application; determine design methodologies, tool sets; and, program
using languages and software. Ability to design, conduct tests and making recommendations. Assist drafting methods sections of grants and manuscripts.

Required Skill/Ability 4:
Ability to investigate different software and technical research tools in order to determine general strengths/weaknesses as they relate to research issues, and prepare reports on their applicability to research.

Required Skill/Ability 5:
Ability to maintain public repository and various lab server and software support.

Drug Screen
No

Health Screening
No

Background Check Requirements
All candidates for employment will be subject to pre-employment background screening for this position, which may include motor vehicle, DOT certification, drug testing and credit checks based on the position description and job requirements. All offers are contingent upon the successful completion of the background check. Please visit www.yale.edu/hronline/careers/screening/faqs.html for additional information on the background check requirements and process.

Posting Disclaimer
The intent of this job description is to provide a representative summary of the essential functions that will be required of the position and should not be construed as a declaration of specific duties and responsibilities of the particular position. Employees will be assigned specific job-related duties through their hiring departments.

Affirmative Action Statement:
Yale University considers applicants for employment without regard to, and does not discriminate on the basis of, an individuals sex, race, color, religion, age, disability, status as a veteran, or national or ethnic origin; nor does Yale discriminate on the basis of sexual orientation or gender identity or expression. Title IX of the Education Amendments of 1972 protects people from sex discrimination in educational programs and activities at institutions that receive federal financial assistance. Questions regarding Title IX may be referred to the Universitys Title IX Coordinator, at TitleIX@yale.edu, or to the U.S. Department of Education, Office for Civil Rights, 8th Floor, Five Post Office Square, Boston MA 02109-3921. Telephone: 617.289.0111, Fax: 617.289.0150, TDD: 800.877.8339, or Email: ocr.boston@ed.gov.

Note
Yale University is a tobacco-free campus"
Software Engineer,Centrify,"Do you have a passion for quality and innovation?
If so, you'll be right at home at Centrify. We are always looking for talented, motivated individuals who would like to help us design, build, sell and support the next generation of integrated software solutions as we redefine security from a legacy static perimeter-based approach to protecting millions of scattered connections in a boundaryless hybrid enterprise!
Do you have a passion for quality and innovation?
If so, you'll be right at home at Centrify. We are always looking for talented, motivated individuals who would like to help us design, build, sell and support the next generation of integrated software solutions for a boundary-less hybrid enterprise environment.
Centrify provides unified identity management across data center, on premise, cloud and mobile, resulting in one single login for users and one unified identity infrastructure for IT.
You must be an energetic self-starter with a desire to learn new things quickly. In this position, you will work on products that securely manage and audit access to privileged servers and services.
RESPONSIBILITIES:
Contribute to feature design and implementation to bring the product to the next level.
Participate in continuous and iterative engineering cycles with emphasis on code quality, supportability, scalability and performance.
Develop unit test cases and perform comprehensive unit testing.
Diagnose and fix product issues found internally or in the field.
Interface with Support to handle customer escalation issues.

CRITICAL SKILLS:
4+ years experience in developing system applications for Unix/Linux using C/C++
Development experience in using REST APIs and JSON.
Experience with Go programming language and C# a plus.
Working knowledge of networking implementation.
Proficient understanding of SQL and relational databases.
Working experience with SaaS, Azure and/or AWS a plus.
Solid understanding of computer security best practices a bonus.
Demonstrated strong software engineering discipline as well as ability to complete highly detailed tasks with strict attention to detail, quality and timeliness.
Excellent analytical and troubleshooting skills.
Excellent oral and written communication skills.
EDUCATION/EXPERIENCE:
4+ years hands-on software development experience, with the most recent experience preferably in cloud/SaaS environment.
4 Years College Education or equivalent work experience
Centrify offers competitive salaries and an excellent benefits package, including medical, dental, vision and life insurance. We also provide an Employee Assistance Program, paid holidays, paid time off, flexible spending account, stock options, 401k, and more. This is just part of what made Centrify one of Glassdoor's ""Best Places to Work in 2016""!
Centrify Corporation is an Equal Opportunity Employer, committed to providing a diversified work environment, free of discrimination and harassment.
For more information regarding our company, or to search for all available job openings, please visit our website at www.centrify.com.
Centrify does not accept unsolicited resumes from individual recruiters or third party recruiting agencies in response to job postings. No fee will be paid to third parties who submit unsolicited candidates directly to our hiring managers."
Big Data Intern,MedeAnalytics,"Want to work on something that really matters and impacts everyone in the US? MedeAnalytics sits at the intersection of healthcare, technology, big data, and national policy. Mede is made up of inspired people with vast industry knowledge. We are all committed to a mission of providing clear, human-powered insights to transform healthcare. Our goal is to lower the overall cost of healthcare using data analytics and workflow applications that reduce inefficiencies, facilitate communication and increase transparency. Think Money Ball for hospitals. Over 1000 healthcare organizations use our cloud based, Software-as-a-Service (SaaS) products every day and were looking for smart people to help us double that number and change the healthcare industry. Our company is privately held and has a high-energy work environment that rewards innovation and represents a tremendous personal growth opportunity for the right professional. We are proud to say that Mede is one of Modern Healthcares 100 Best Places to Work in Healthcare.

Role Profile:

The Big Data Interns will work closely with the engineers, architects and business analysts to build and maintain the big data systems for large scale, high performance, and rock-solid reliability. This is a hands-on position that will require performing system development and maintenance activities for the exciting and dynamic medical fields.

Essential Duties and Responsibilities:
Collect, clean and transform data from disparate sources for big data analytics.
Responsible for creating and maintaining systems that leverage big data and traditional BI technologies.
Work with the product owners and business analysts in analyzing business requirements.
Work on engineering our big data platform for scale, performance, and rock-solid reliability.
Responsible for the setup, creation and maintenance of MS SQL database, Vertica, MongoDB clusters and servers.
Work with engineering and product teams distributed across multiple locations.
Effectively document and communicate results
Stay up to date on current trends in big data analytics and healthcare
Other duties as required by business conditions or as assigned
Essential Education, Experience and Interests:
Ability to work effectively in a team.
Results-oriented, self-motivated and quick-learner.
Good interpersonal (both oral and written) communication skills.
Knowledge with .Net, SQL and databases.
Knowledge with Linux.
Technical degree (Bachelors of Science in Computer Science, Information Technology or equivalent) strongly desired; advanced degree (MS) preferred.
Desired Experience
Knowledge with Vertica and MongoDB
Familiarity with Scrum methodology
Knowledge with SSAS, SSIS or SSRS
Working Environment:

The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

While performing the duties of this job, the employee regularly works near office equipment (telephone, computer) and other employees. The employee works in normal office conditions where there is no physical discomfort due to temperature, dust, noise, etc. Verbal and written communication, telephone usage, filing, sitting, typing, driving, reading and carrying required to perform the essential functions of this job."
Software/Data Engineer - Design Technology,TransReach,"Qualifications

Minimum requirements are a Bachelor's degree in Computer Science with a minimum of minimum of 2 years of Single Page Application ( SPA) development experience, preferably in engineering tool development.

The ideal candidate will have experience with the following:

Python

HTML 5

Java script

CSS IDE development platform

NO SQL datbase

CAD systems

Matlab

Heuristic Evaluation

Responsibilities
Design, code, and test engineering computing systems
Modify, test, and troubleshoot existing tools and utilities
Apply knowledge of engineering processes to develop computing systems
Monitor and perform maintenance on existing systems
Produce and maintain documentation on system processes."
"Enterprise Solutions Engineer (C++, Python, SQL)",Bloomberg,"Job Requisition Number: 63720

Our Team:
The Enterprise Products and Services (EPS) team is a new and growing area of Bloomberg. We build state-of-the-art software to support product implementation and client-facing tools that allow clients to manage their data distribution, users, applications and respective permissions. Our product has the potential to have a tremendous impact on the market. Our solutions offer cost effective, customizable and innovative alternatives to clients. We are solving old problems in a new way and bringing healthy competition to a well-established market.

Our day-to-day includes a wide range of development opportunities, from front-end to back-end to data warehousing. As part of our team, you will be exposed to a variety of domains and technologies, learn continuously and contribute new ideas starting day one.

We'll trust you to:
Create C++ and Python services on Linux

Work with Oracle and SQL Server databases

Script complex stored procedures

Build UIs in JavaScript

Work across multiple teams in Engineering

Communicate and interact with product managers and end users

You'll need to have:
5+ years of programming experience with Object-Oriented languages

5+ years of experience developing stored procedures, performing optimization and query tuning and designing databases

We'd love to see:
Familiarity with C/C++, Linux, T-SQL, Python, Shell scripting, data structures, algorithms and optimizations, Object-Oriented design or JavaScript

Experience working with UIs

An under- or post-graduate degree in Computer Science, Engineering, Finance or related field (or equivalent expertise)

Ability to learn on the fly and the desire to share ideas and be a team player

Strong verbal and written communication skills"
"Data Infrastructure Engineer, Siri Search",Apple,"The Siri Search team is creating groundbreaking technology for algorithmic search, machine learning, natural language processing, and artificial intelligence. The features we create are redefining how hundreds of millions of people use their computers and mobile devices to search and find what they are looking for. Siris universal search engine powers search features across a variety of Apple products, including Siri, Spotlight, Safari, Messages and Lookup. As part of this group, you will work with one of the most exciting high performance computing environments, with petabytes of data, millions of queries per second, and have an opportunity to imagine and build products that delight our customers every day.
Key Qualifications
5 years minimum of experience writing high-performance server software on Linux/Unix
3 years minimum of experience working with large datasets
Mastery of two of following languages: C++, Python, Java, Perl, Go
Practical experience with Hadoop and extensive experience w/ SQL and NoSQL databases
2 years minimum experience with data transformation pipelines
Experience with large scale search and machine learning systems a plus
Thorough knowledge of HTTP
Description
Apple's Internet Software & Services organization is seeking a Sr Software Engineer who will play a central role in the delivery of Big Data, Machine Learning & Search based Internet services. As a member of our fast-paced group, youll have the unique and rewarding opportunity to shape upcoming products from Apple. We are looking for people with experience in creating high performance Linux/Unix server side software and data processing pipelines.

This role will have the following responsibilities:
Architect & develop Internet-scale server software for Linux systems
Develop methods and software to process petabyte-scale datasets
Work alone or as part of small team to deliver complete systems
Work closely with operations and project management teams
Education
Master's Degree in Computer Science or Computer Engineering, or equivalent work experience"
Data Scientist,McAfee,"Job Description

With the mission of capturing the biggest market share in the area of cyber security, network security, endpoint security, threat research, malware research, cloud security, we work together for a common goal of shaping the companys future by designing and building the best in class robust and scalable security products for consumer and enterprise customers. As industry top performers, we aim to develop optimized high performance system software solutions with high availability and reliability.

This is where you come in. We are looking for a new team member who defined by his/her unique and innovative skills, style or point of view. You can be an architect, scientist, threat researcher, or a coder as long as you design things that matter.

The Data Scientist will develop and enhance advanced data mining algorithms and machine learning, which will help drive data analysis for a wide range of information and product features.

Essential Duties and Responsibilities
Collaborate with other engineers to formulate innovative solutions to experiment and implement advanced data mining algorithms in cloud-based application security space
Hands-on in application/development of machine learning techniques on the large datasets to solve business problems
Communicate complex concepts and the results of the analyses in a clear and effective manner to senior management
Big Data experience, ideally using Hadoop technology stack
Professional Competencies
Passion for working with big data and professional experience in data mining, statistical analysis, machine learning, predictive modeling and data manipulation preferred
Working knowledge of Big Data processing, e.g., Map-Reduce and Spark
Proven ability to work with a team as well as work independently
A can-do attitude to carry out a project from scratch to production on a timely basis
A great communicator, execution focus, and superb attention to details
Required Experience and Skills
Master's Degree in Computer Science, Engineering, or related fields
7+ years experience with a Masters Degree as Data Scientist or 3+ years experience with a Doctoral Degree as a Data Scientist
2+ years programming experience in a distributed systems environment using one or more of Java, Python or Scala.
2+ years model development experience using one of R or Python (scikit-learn)
2+ years experience exploring data in using Hive, SQL or SprakSQL.
Experience and knowledge of Security domain, particularly Anomaly Detection and Behavior Modeling
Strong understanding of one or more of Bayesian Statistics, Deep Learning, Time Series Analysis, Unsupervised Learning and State Space Models.
Experience in optimizing big data pipeline a plus
Excellent problem solver, analytic thinker and quick learner
Qualifications

_"
Data Content Product Engineer,Esri,"Are you prepared to munch, crunch, and digest spatial data before breakfast? Were looking for a talented data engineer to integrate spatial data into our geocoding products and solutions.

This is an opportunity to work with massive spatial data from both commercial and open sources. Youll be asked to master data specifications, automate processes for acquiring and loading spatial datasets, transform those datasets into normalized data models, and work with product teams to ensure continuous quality improvements driven by the best available content.

If data is your thing, consider this position where you can contribute to making geocoding successful for the ArcGIS platform.

Responsibilities
Analyze, verify, and import data from various sources into normalized databases
Write and maintain scripts for data processing and loading
Investigate data issues reported by users
Create tools that allow team members to view and modify database contents
Develop automated test procedures and data integrity checks for ETL processes

Requirements
3-5 years working with large normalized databases
3-5 years of Python or another scripting language
3-5 years of experience using MS SQL Server or Oracle, specifically with spatial data
Experience writing complex SQL scripts and stored procedures to read/write/transform data
Prior experience loading/transforming data of different formats such as flat tables
Understanding of data models and specifications
Ability to manage priorities and tasks as needed in a fast-paced work environment
A keen attention to detail and drive to resolve any issues encountered
Bachelor's, masters, or Ph.D. in computer science, information systems, GIS, or related field, depending on position level
1+ years working with C, C++, or C# reading and writing to databases
Experience working with commercial data from vendors such as HERE and TomTom
Experience working with open-source data such as OpenStreetMap
Prior experience with ETL workflows and software such as Safe Software FME
Experience using spatial databases such as PostgreSQL
Prior experience with geocoding, geocoding data, or address data
Knowledge of Esri products or similar GIS or spatial software
Familiarity with data such as Shapefiles and FileGDB as well as different geometries and projections
Prior experience working with the Esris File Geodatabase API
Knowledge of Agile software development using Scrum
Prior experience with software development and release of commercial software products
#LI-CK1"
Software Engineer,Comtech EF Data Corporation,"SUMMARY
Under limited supervision, performs engineering software development and applied research, design of new products. This classification includes engineers who are capable of handling moderately complex engineering assignments.

ESSENTIAL DUTIES AND RESPONSIBILITIES include the following. Other duties may be assigned.

Design and implement major enhancements to Network System Management products, a multi-threaded, multi-user system comprised of Windows services, a core infrastructure, and kernel mode drivers, including a virtual environment.

Author functional and design specifications, build prototypes, and use other appropriate techniques to achieve product definition and design goals.

Design, implement and maintain system software components, principally in C/C++.

Apply expertise in Windows Internals and Kernel Mode Development to deliver new product features, enhancements and product support.

Work with Product Design/Product Management to ensure that the architecture and design supports key end-user product requirements such as ease of use, extensibility etc.

Knowledge, Skills and Abilities
Moderate knowledge of systems software engineering: operating system internal file systems, kernel mode development, accompanied by broad knowledge in a number of other technical areas (e.g. networking, security, etc.)

Very strong knowledge of Windows 2012 internals is required; other OS internals experience is a plus.

Very strong C/C++ knowledge is required; ability to read and understand lower level languages is a strong plus.

QUALIFICATIONS
To perform this job successfully, an individual must be able to perform each essential duty satisfactorily. The requirements listed below are representative of the knowledge, skill, and/or ability required. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
Must be a US Citizen or Permanent Resident due to contract requirements.

EDUCATION and/or EXPERIENCE
Bachelor of Science degree or equivalent or four (4) to ten (10) years related experience and/or training or equivalent combination of education and experience.

LANGUAGE SKILLS
Ability to read, analyze, and interpret common scientific literature and technical journals. Ability to effectively present information to colleagues and/or management. Ability to respond to common inquiries or complaints from customers, regulatory agencies, or members of other departments.

MATHEMATICAL SKILLS
Ability to apply advanced mathematical concepts such as exponents, logarithms, quadratic equations, and permutations. Ability to apply mathematical operations to such tasks as frequency distribution, determination of test reliability and validity, analysis of variance, correlation techniques, sampling theory, and factor analysis.

REASONING ABILITY
Ability to apply principles of logical or scientific thinking to a wide range of intellectual and practical problems. Ability to deal with nonverbal symbolism (formulas, scientific equations, graphs, etc.,) in its most difficult phases. Ability to deal with a variety of abstract and concrete variables.

PHYSICAL DEMANDS
The physical demands described here are representative of those that must be met by an employee to successfully perform the essential functions of this job. Reasonable
accommodations may be made to enable individuals with disabilities to perform the essential functions.

WORK ENVIRONMENT
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions."
Software Engineer,"Tokyo Electron America, Inc.","Driving success happens daily here. Tokyo Electron America, Inc. employees foster strong partnerships with TELs US customer base. We market a wide range of semiconductor production equipment (SPE) products and provide outstanding service and support to U.S. semiconductor device manufacturers. TEL
's world-renowned products and technologies support diverse customer needs for the manufacture of increasingly sophisticated semiconductors.

Position Summary: In this dynamic role, you will join a small team of highly motivated and skilled scientists and engineers to develop software/algorithm for semiconductor equipment control and data analysis for our new products and make improvements to existing projects.

Responsibilities:

 Software development tasks/projects to include concept design, specification, implementation, unit testing, debugging, build release, deployment and documentation.

 Potential tasks include data acquisition and communications, GUI design, digital signal processing and data analysis.

 Deliver high-quality prototype and/or production SW on time.

 Create and maintain design documentation and customer documentation (e.g. SW requirements, release notes, user guides).

 Troubleshoot issues with previously released software and document root causes and fixes.

 Collaborate with applications engineers to provide effective and timely support to end users.

Qualifications:

 BS, MS, or PhD degree in computer science/engineering/physical science or related fields

 2+ years industry SW design and development experiences are a plus

 Technical experience in the area of digital signal processing, machine control for metrology, inspection, and algorithm development is highly desired

 Demonstrated industry experience in developing applications under Windows/Linux based operating systems

 Ability to travel on occasion, as required

 Proficient in C/C++, C#

 Knowledge of SQA processes

 Strong problem solving skills

 Ability to work independently and/or as part of a team

 Good written and verbal communication skills in English

Equal Opportunity Employer/Minorities/Females/Disabled/Veterans"
Software Engineer,pony.ai,"sought by PONY AI, INC. in Fremont, CA to design software for
autonomous vehicle product. Requirements: Bachelor Degree (or foreign equivalent) in
Computer Science, Computer Engineering or related field, and 5 years of experience in
the job offered or in a computer-related occupation. Position requires education or
experience with: 1. C++ programming skills and Python skills; 2. Data structures and
algorithms; 3. Machine learning algorithm.

Mail resume to PONY.AI Inc., Attn: HR Job #B5, 46791 Fremont Blvd, Fremont CA
94538."
Data Visualization Software Engineer,IBM,"Job Description
The Data Visualization Software Engineer will work directly with client and internal team members to develop and support interactive data visualizations used across the enterprise, and conduct data analysis with outcomes that include delivery of actionable insights and business intelligence.

Required Technical and Professional Expertise

Build real-time interactive mapping solutions using spatial data (raster and vector) for use on the web and internal tooling
Build real-time interactive visualizations and APIs in JavaScript, HTML5 to make data easy to understand and access across the organization.
Responsible for developing data visualization based on streaming and stored data points.
Work closely with data scientists, product managers and other engineers to ideate and translate concept into concrete products.
Develop software solutions by studying information needs; conferring with users; studying systems flow, data usage, and work processes; investigating problem areas, following the software development lifecycle.
Update job knowledge of self and team by studying state-of-the-art development tools, programming techniques and computing equipment.

Preferred Tech and Prof Experience

Advanced programming skills in JavaScript (3+ yrs), HTML5 (1.5+ yrs)
Experience in visualizing data to illustrate spatial data via interactive mapping libraries
Can demonstrate broad experience in Data Analysis and Business Intelligence
Hands-on development experience in frontend / data visualization
Experience in visualizing data to illustrate business-analytic intents
Highly developed organizational skills and attention to detail
Solution oriented and able to problem-solve effectively and proactively
Demonstrated ability to effectively coordinate multiple tasks and able to react to spontaneous changes in priorities

EO Statement
IBM is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. IBM is also committed to compliance with all fair employment practices regarding citizenship and immigration status."
Data Integration Engineer,OneGlobe,"Support an organization performing data standardization, Ingest data from various structured and unstructured data sources into Hadoop and other distributed Big Data systems.
Support the sustainment and delivery of an automated ETL pipeline, using a suite of COTS, GOTS, Java, and other tools.
Validate data that is extracted from structured and unstructured data inputs, databases, and other repositories, using scripts and other automated capabilities, logs, and queries.
Enrich and transform the extracted data, as required.
Create monitoring and reporting of the data flow through the ETL process. Perform data extractions, data purges, or data fixes in accordance with current internal procedures and policies.
Track development and operational support via user stories and decomposed technical tasks in a provided issue tracking software, including GIT, Maven, and JIRA.
Basic Qualifications:
5+ years of experience with software development throughout the SDLC
5+ years of experience with Java development
Experience with Map Reduce and the Hadoop ecosystem
Experience with Linux, including CentOS or RedHat
Experience with working on Scrum or another Agile methodology
Additional Qualifications:
Experience with ingesting data into Hadoop or Accumulo-Experience with Apache NiFi
Experience as an Open Source Software contributor in github, code.google.com, and apache.com
BS degree in CS or a related field preferred;
MS degree in CS or a related field a plus.
Experience with Apache Spark, Apache Storm, or Kafka is relevant.
Clearance: Must have an active TS/SCI clearance"
"Data Engineer, Servicing",SoFi,"We are seeking a Data Engineer to work in a fast-paced and fun environment in our Helena, Montana office.
By joining SoFi, you're joining a new kind of finance company based around speed, transparency, and alignment with our members interests. Our goal is to be the center of our members financial lives. We created student loan refinancing, addressing the biggest financial challenge this new generation has through a new approach to lending. We expanded into other types of loans, and then into insurance and wealth management with similarly inventive products. As the company has grown, weve been able to help more people with these tools.
SoFi has achieved significant growth, with big plans ahead. In just the last year, we've more than doubled our member base (300,000 strong), doubled loan volume ($8 billion in 2016), and grown our team from under 200 people to more than 1,000. We're preparing to go global, with expansion to Australia and Canada planned this year. And we're well capitalized to power all this growth, having raised $1.9 billion in equity backing. But we'll only be able to continue this growth with great talent, and that includes you.
The ideal candidate will be forward thinking, innovative, and ready to dive in and help us grow and achieve greater efficiencies within our servicing product offering. We are growing quickly, and with this growth are seeking great talent to help us meet the needs of our internal customers and our members.
Desired Skills
Ansi SQL, Stored Procedures, Relational Database Experience

Key responsibilities include:
Understand and interpret customer requirements and design solutions to meet those requirements
Develop new entity relationship models, and modify existing schemas to assist the team in meeting customer requirements
Undertake or support data conversions and integrations with third parties
Write SQL queries to meet customer requirements and analyze/optimize queries written by other developers
Create/maintain procedural code to satisfy business needs and automate processes
Troubleshoot data inconsistency issues and write scripts to cleanse data
Maintain the security, data integrity, and availability of data in production environments
Execute System and Integration Testing
Support the development team by assisting with requirements gathering, design, and development of new solutions to meet ever-changing needs in the industry
Requirements:
Bachelor's degree (or equivalent work experience)
Experience with PL/SQL or other database stored procedures
Experience working with relational databases; such as Oracle, Postgres, or MYSQL
Self-motivated
Ability to bring new ideas and promote process improvement
Solid Ansi-SQL skills
Plusses:
Experience designing data models
Knowledge of and experience with Python
Expertise in conversions or batch processing
Experience analyzing and optimizing SQL
Experience in Tableau or other business intelligence platforms
AWS exposure
Experience with ETL processes and data warehouses
Experience in financial industry
Experience in designing, implementing, and managing large databases, including query tuning and performance enhancements
Personal Attributes of the Ideal Candidate Include:
Ability to thrive in a fast-paced growing company.
Ability to drive a project from inception to completion.
Enthusiasm for solving challenging problems.
Team attitude: a willingness to roll up your sleeves, work with others and achieve results.
Benefits:
Catered lunches, a fully stocked kitchen, and subsidized gym membership.
Competitive salary packages, bonuses, and stock options.
A flexible vacation policy allows you to truly relax and reboot.
Comprehensive health, vision, dental, and life insurance as well as disability benefits.
100% of health, vision, and dental premiums paid by SoFI for employees and their dependents.
401(k) and education on retirement planning.
Tuition reimbursement on approved programs, up to $5,250 a year.
Monthly contribution to help you pay off your student loans.
Applicants must be authorized to work in the United States without visa sponsorship."
Machine Learning Data Engineer,Capital One,"7900 Westpark Drive (12131), United States of America, Tysons, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Machine Learning Data Engineer

Are you a high performing software engineer and/or data scientist looking to take part in some of the most cutting edge research and production projects? Do you enjoy reading and investigating advancements in various applied machine learning architectures and solution white papers? Would you like to take part or drive the creation of publishable advancements in machine learning across various disciplines? You could be a great match for a Machine Learning Data Engineer role at Capital One's Center for Machine Learning (C4ML).

As a Machine Learning Data Engineer in C4ML, you will contribute to building fast data and machine learning solutions to create and improve some of the most interesting use cases in the financial services industry. Capital One maintains a full stack of technology solutions including streaming big data, state of the art machine learning, micro-service architecture, distributed computation engines, and intuitive visualizations in the cloud. To manage this, we are working with several cutting-edge technologies and are actively developing and contributing to the open source community. We are highly technical with strong backgrounds in our fields to support use cases ranging from cyber threat prevention to sophisticated NLP understanding in an always on 24/7 service architecture. We have the highest executive support for acting as a catalyst of machine learning across Capital One providing our researchers extraordinary diversity in topics.

What you will bring to the role:
Excellent communication skills evidenced by multiple white papers (internal proprietary or externally published).
Demonstrated ability to build full stack systems architected for speed and distributed computing.
Demonstrated ability to quickly learn new tools and paradigms to deploy cutting edge solutions.
Experience mentoring junior engineers.
Adept at simultaneously working on multiple projects, meeting deadlines, and managing expectations.

What you will do in the role:
Act as an advisor to various lines of business to help create or improve projects.
Develop both deployment architecture and scripts for automated system deployment in AWS.
Code new machine learning paradigms, sometimes from first principles, for integration into production systems.
Learn and work with subject matter experts to create large scale deployments using newly researched methodologies.
Construct data staging layers and fast real-time systems to feed machine learning algorithms.
Create white papers, attend conferences, and contribute to open source software.

Basic Qualifications:
Bachelors Degree or Military Experience
At least 2 years of experience designing and building full stack solutions utilizing distributed computing.
At least 2 years of experience integrating with larger code bases.
At least 2 years of experience working with Python, Scala, or Java.
At least 2 years of experience with leading distributed file systems or multi-node database paradigms.

Preferred Qualifications:
Masters Degree
At least 4 years of experience in designing and building full stack solutions utilizing distributed computing.
At least 6 years of experience integrating with larger code bases.
At least 4 years working with Python, Scala, and Java.
At least 4 years of experience with leading distributed file systems and multi-node database paradigms.
At least 2 years leading teams in code development and balancing feature requests with feasibility constraints.
A history of publications and conference attendance.

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
iOS Location Analytics Software Intern,Apple,"Shape the next generation of location-aware iOS applications by working on bleeding-edge solutions for the Contextual Analytics, Location Technologies and Motion Sensors team. The Location Data Analysis team is looking for server side computer engineer / Data Scientist with a passion for data mining, algorithm and server side development. The candidate should be creative, hands on server technologies, working with huge amount of data and team oriented.
Key Qualifications
Implement and support pipelines to analyze huge amount of data
You know Java Programming (design & architecture, algorithms)
Hands-on knowledge of prototyping and scripting languages, such as Python or MATLAB
Experienced in designing and implementing various machine learning models and deep understanding of core machine learning and estimation concepts including classifiers, clustering algorithms, anomaly detection techniques and filtering.
Preferred to have experience with application servers, Hadoop, RDBMS, NoSQL DB and other backend technologies
Excellent written and verbal communication skills
Ability to work in a team
Proven capability of solving challenging problems
Strong attention to details and excellent analytical capabilities
Passion for developing and testing clear, robust code.
Description
This is a great opportunity for data scientists/algorithm engineer who are interested in big data, backend technology and algorithms developments.You will be a key member of a team that is pursuing future products in location technologies. In that role, you will be expected to think out-of-the box, investigate a set of research hypotheses and shape them into a meaningful definition that can be demonstrated with a prototype and subsequently implemented in a production-grade code. The problem space may include various estimation and machine learning algorithms.
Education
Working towards M.S. or PhD in Computer Science or related field such as Mathematics, Statistics, Physics or Electrical Engineering
Additional Requirements
Demonstrated ability to complete milestones under pressure with tight deadlines.
Excellent communication skills and good team player.
Familiarity with location technologies, RF signals, recommendation systems and similarity functions is a plus
Familiarity with Obj-C / C++ is a plus
Exposure to Hadoop, Spark and Kafka preferred"
Big Data - Senior Manager,"J.C. Penney Corporation, Inc.","Senior Manager - Big Data
J.C. Penney Company, Inc.
Plano, Texas

JCPenney is one of the nations largest apparel and home furnishing retailers with more than 800 stores and jcp.com. We are a diverse community of people, all working together to bring sensational style, sensible prices and the best service possible to our customers. Were looking for talented individuals who want to work in an energetic, respectful, collaborative environment. With a wide array of jobs, internships, training and more, there are countless opportunities for you to grow your career with us.

We are looking for a Senior Manager to join our Digital Platform team working on jcp.com and Omnichannel initiatives, with a focus on Big Data technology. Reporting directly to the Senior Director/Architect of the engineering team, the Senior Manager works on challenging, mission critical projects and will work in conjunction with other engineering and business partner teams to drive and develop cutting edge technology solutions in an Agile environment.

Responsibilities Include: Lead and architecting scalable, maintainable and reliable services that process very large quantities of structured and unstructured data using Big Data platform and relevant technologies Lead architecture planning and design for deploying new data ecosystems that are cloud-based Hadoop implementations Lead engagement solution, planning, and serve as the main point of contact projects Define deliverable strategy, resource planning, structure and success criterion Manage the solution architecture and delivery of projects, while directing a multidisciplinary project teams of project managers, analysts, architects and technology specialists/engineers Analysis and identification of source data, architecting and designing the data extraction and loading approach to create the digital analytics and business intelligence data foundation leveraging Big Data Platform and relevant technology Architecting the Big Data layer for data mining & analysis, developing and fine tuning algorithms based on data insights in Big Data Platform Architecting, application design, software development and implementation of digital analytics, reporting tool to create executive dashboard and key performance indicators by leveraging data stored in big data platform. Build various algorithms to drive personalized and relevant customer experience; lead integration of digital platform with Big data platform Performance engineering of data storage and retrieval of data from Big Data platform for timely data analysis, reporting and also drive real time digital customer experience Key deliverables include architecting, designing, data analysis, processing, integrating, performance tuning, monitoring and alerting modules that fit into a unified and reliable Big Data infrastructure Evaluating various technologies, cloud implementation approach to support Big data related omnichannel initiatives. Manage, coach, and develop the technical skills of team members Direct and participate in cross-functional delivery of large-scale, complex projects often involving multiple internal and external teams

Core Competencies & Accomplishments: Understanding of software engineering best practices, object oriented analysis & design, and design patterns & machine algorithms Prior success leading Big Data/engineering team for ecommerce is required Knowledge and experience in building relevant algorithms using customer data and sales data Implementation experience of customer data foundation, customer relationship management and loyalty system Knowledge in Cassandra or Hadoop platform is critical. Hands on experience with data modeling, data loading, optimization, and warehousing techniques and technologies in either of the following: Teradata, Hive, Pig, NoSQL, or Cassandra Expertise in data loading, data mining & analysis and building algorithms based on data insights in Big Data platforms. Excellent ability to think in terms of how to think MapReduce, and write massively parallel algorithms using the MapReduce paradigm, and build applications that use chained MapReduce jobs. Expertise in Java and, Good understanding of the design of Hadoop Proven ability to lead a team of highly technical software/systems engineering staff and earn trust of team members and stakeholders Familiarity with the following Hadoop components: Hadoop Common, HDFS, HBase and YARN Ability to troubleshoot problems with MapReduce applications, diagnose performance bottlenecks in MapReduce Knowledge of in-memory high-speed cluster computing technologies such as Apache Spark or Storm and cloud computing

Our corporate office supports the JCPenney stores nationwide and offers a competitive benefits package including medical/dental/vision, term life insurance, paid vacation/holidays, 401(k) savings plan with company match, and a merchandise discount plan to JCPenney. Amenities include free covered parking, full cafeteria services, fitness center, and on-site childcare.

For more opportunities to join our team please visit our careers page .
Follow us and see whats new: Instagram Facebook Twitter LinkedIn Media Room jcp.com"
Software Engineer,Cogo Labs,"Cogo Labs provides the companies we incubate with access to a centralized tech platform that they can use to bootstrap their businesses. We are looking for a Software Engineer who can help us further develop this platform, making it the best possible toolchain for running marketing campaigns and launching new websites. Youll collaborate with and learn from experienced engineers while making contributions that directly impact the success and profitability of teams and companies that were building.

Our tech:
When it comes to bits and bytes, our guiding philosophy is that using the right tool for a job is more important than rigid adherence to any particular programming language or tech stack. From humble origins in Python (we still use Python to write quick prototypes and one-off scripts), the majority of our core systems have evolved into highly concurrent Golang services. We also use some embedded Lua and dabble with low level network protocols. RabbitMQ, Ledis, MySQL, PostgreSQL, and Amazon Redshift are some of our favorite data wrangling tools; Munin, Nagios, and Icinga keep us alerted when something's going awry.

As a Software Engineer you will:
Add new features to tools and services that support businesses under incubation.
Streamline existing processes that are painful or time consuming.
Upgrade existing systems to take advantage of new technologies.
Support analysts who use your tools in production workflows.
Build monitoring infrastructure that makes sure systems and campaigns are running smoothly.
Things that would help you succeed in this role:
BA/BS in Computer Science preferred.
Fluency with at least two programming languages.
Familiarity with SQL and database design.
Working knowledge of HTML, Javascript and CSS.
Experience working with macOS or Linux development tools.
A desire to work with and solve problems related to the warehousing of large data sets.
Ability to thrive in an agile, team oriented environment.
About Cogo Labs:
Were a venture accelerator. We employ data-driven, analytical methodologies to create profitable web businesses.
Our work is challenging and dynamic; our environment is fun and supportive.
Our company is profitable and established. This means startup culture without startup anxiety.
We offer competitive salaries and benefits, plus an equity share that actually means something.
Our employees have lives outside of work. We are musicians, athletes, filmmakers, gamers, authors, and whatever you are."
Database Engineer,Ukpeagvik Iupiat Corporation/Bowhead Family of Companies,"Description

DATABASE ENGINEER (ERDC-17-1844-Z):
Bowhead seeks a Database Engineer to join our team in Vicksburg, MS. The Database Engineer will join the team in a real estate system environment which includes data design, database architecture, metadata and repository creation. In this position, you will administer complex enterprise databases and applications while applying knowledge of advance design to optimize performance and guide the team in the implementation of practices in database maintenance and administration. You will establish and maintain security policies and procedures as it pertains to content and servers and will work with management systems to determine a logical way to organize and store data.

Other essential functions will include:
Responsibility for troubleshooting the ERDC real estate systems and databases by conducting research into new database applications to determines viability for adoption. This may include creating new or enhancing existing components of the database.
Determining correct user requirements, set up, testing and coordinating determined changes.
Development, maintenance, and support of complex enterprise data systems and corresponding data marts by establishing standards and evaluating existing areas stored in the database. You will have to incorporate existing areas into an enterprise model and communicate requirements, transaction rates, volume analysis and other data to required personnel in a concise and effective manner.
Responsible for keeping the various ERDC real estate systems and databases updated and to create reports for users and customers to view the data to track and manage funds by projects. Will report errors to the Real Estate Program Management Office (PMO).
Qualified candidates must have experience working with the following:

Administer, install, update/patch and configure Linux 6-7, WebLogic, Apache and IIS
Windows 2008-2012
Unix Solaris 11-12
The ability to write/modify jobs, procedures, scripts and tables in Oracle
Java, Perl, ASP
KSH/Bash shell scripting
JavaScript, query, HTML, CSS
Requirements
Must have a BA/BS degree in a related field and eight plus (8+) years of experience
An active CompTIA Security + certification is required. Highly qualified candidates may be considered but will be required to obtain certification before starting this position
Intermediate to advanced level skills in Microsoft Office software suite - Word, Excel, Outlook, PowerPoint
Ability to communicate effectively with all levels of employees and outside contacts
Strong interpersonal skills and good judgment with the ability to work alone or as part of a team
SECURITY CLEARANCE REQUIRED: Must be able to obtain/maintain a security clearance at the SECRET level however candidates with an active Secret clearance are highly encouraged to apply. US Citizenship is a requirement for Secret clearance at this location.

Applicants may be subject to a pre-employment drug & alcohol screening and/or random drug screen, and must follow UICs Non-DOT Drug & Alcohol Testing Program requirements. If the position requires, an applicant must pass a pre-employment criminal background history check. All post-secondary education listed on the applicants resume/application may be subject to verification.

Where driving may be required or where a rental car must be obtained for business travel purposes, applicants must have a valid driver license for this position and will be subject to verification. In addition, the applicant must pass an in-house, online, driving course to be authorized to drive for company purposes.

UIC is an equal opportunity employer. We evaluate qualified applicants without regard to race, age, color, religion, sex, national origin, disability, veteran status, and other protected characteristics EOE/AA/M/F/D/V. In furtherance, pursuant to The Alaska Native Claims Settlement Act 43 U.S.C. Sec. 1601 et seq., and federal contractual requirements, UIC and its subsidiaries may legally grant certain preference in employment opportunities to UIC Shareholders and their Descendants, based on the provisions contained within The Alaska Native Claims Settlement Act.

All candidates must apply online at www.uicalaska.com, and submit a completed application for all positions they wish to be considered. Once the employment application has been completed and submitted, any changes to the application after submission may not be reviewed. Please contact a UIC HR Recruiter if you have made a significant change to your application. In accordance with the Americans with Disabilities Act of 1990 (ADA), persons unable to complete an online application should contact UIC Human Resources for assistance (http://www.uicalaska.com/contact-us/human-resources/).

UIC Government Services (UICGS / Bowhead) provides innovative business solutions to federal and commercial customers in the areas of engineering, maintenance services, information technology, program support, logistics/base support, and procurement. Collectively, the fast-growing Bowhead Family of Companies offers a breadth of services which are performed with a focus on quality results. Headquartered in Alexandria, VA, we are a fast-growing, multi-million-dollar corporation recognized as one of the top 25 8(a) companies for government contracting.

^ CUT/PASTE this LINK to APPLY:
https://rn21.ultipro.com/UKP1001/jobboard/NewCandidateExt.aspx?__JobID=19199
#DI"
Backend Engineer- Python,AppZen,"AppZen has developed the worlds first artificial intelligence (AI) solution for back office automation. AppZens AI for business solution uses patent-pending Natural Language Processing machine learning algorithms to analyze data and automate back office functions. The data science-based technology automatically detects accidental as well as intentional fraud, and provides real-time compliance to IRS rules, FCPA regulations, and company policies.

We are looking for a Backend Engineer with exceptional skills to work on the backend of our AI-powered solution developed in Python. The ideal candidate will have at least 2-3 years of experience working at a Cloud/SaaS company.

Responsibilities:
Working on the backend tasks ranging from API development to interacting with our Big Data platform in Kafka and Spark
Manual and automated testing
Architecture and design
Help take engineering practices to the next level
Must-haves:
2-3 years of experience in a Cloud/SaaS company
Expert in Java
Working knowledge Kafka and Spark
Working knowledge of Java Spring
Development using OWASP principles
Experience with RestFul APIs
Self starter who can be productive from the first day
Excellent communication skills
Bonus points for:
Working knowledge of Celery, Kafka and Spark
Knowledge of Secure SDLC and development using OWASP principles
Expert in Python
Candidates who require visa sponsorship or who are currently on OPT with not be considered."
Data Engineer,Crux Informatics,"Job Description

As a Data Engineer, you will be a key contributor to the Client Success team where you'll be solving real world problems with experts in the field. This includes building and using tools to gather, clean, and transform disparate data so its structurally suited to client needs.
Other responsibilities include creating tailored solutions for wrangling data and automating ETL and database solutions. This will involve accessing and extracting data through various channels (web services, web crawlers, direct access to source database, etc) and data source formats (e.g. PowerPoint, PDF, Word, Text, Cloud Storage, SQL, etc.). You'll be working in a collaborative environment where you'll have the support of your team behind you as you apply advanced analytic methods to solve client problems.

Responsibilities include:
Wrangling data from a wide variety of complex, large volume data into cleaned, normalized, and enriched datasets.
Performing data discoveries to understand data formats, source systems, file sizes, etc. This will include engaging with internal and external business partners in the discovery process.
Delivering data acquisition, transformations, normalization, mapping, and loading of data into a variety of data models.
Anticipating client needs and formulate solutions to client issues.
Reviewing deliverables for accuracy and quality.
Building and enhance the ETL codebase for added efficiency and capacity.
Working on normalization and performance tuning of analytics processes.

About you:
You like to be involved in all stages of data wrangling and development, from early days MVPs to large scale production architecture.

You love working in high performing teams, collaborating with members to improve the team's ability to deliver.

You are motivated by the desire to solve challenging technical problems and find learning about the data fascinating.

You understand server, network, and hosting environments as well as web service standards such as REST. A good grasp of common data distribution/hosted storage solutions is also appreciated. Qualifications

Required:
5+ years of data engineering or data science experience in a professional environment, but skill fit is a higher priority than just work experience.

Proficiency in Python and/or Perl as was as other languages in the context of data processing or data science.

Experience with ETL and/or other Big Data processes.

Expert level proficiency in SQL.

Experience working independently.

Strong problem solving and troubleshooting skills with experience exercising mature judgment.
Comfortable working with open-source technologies.

Strong communications skills, interpersonal skills, and a sense of humor.

Preferred:
Data Skills: RDBMS SQL and NOSQL, structured and unstructured data, BigQuery

Programming: Java; SQL; Python;, XML, JSON

Tools: Jupyter, C24, familiarity with ETL, CDC and workflow tools

Big data and cloud processing experience, e.g. HDFS, Spark

Experience working in a cloud based environment, such as GCP or the AWS

Experience with RESTful web services a plus

Additional Information

At Crux, diversity is valued and and treatment of employees and applicants are based on merit, talent and qualification. We believe the key to success is bringing together unique perspectives and we never discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. For employment qualified applicants with criminal histories, consideration will be consistent with the requirements of the San Francisco Fair Chance Ordinance.

All your information will be kept confidential according to EEO guidelines."
Software Engineer,Whiteboard Federal Technologies,"We are looking for a confident and capable full stack developer that enjoys being challenged and resolving critical path issues, to join Whiteboard's team on an R&D project at one of the National Labs!

Every member of our team codes across all tiers, and the desire to do the same is necessary. In a week, you might integrate with a classic RDBMS, NoSQL data store, implement REST services in Spring, integrate Spring security with OID, and implement UIs with jQuery/ExtJS and D3.js. At the research lab, we move fast and constantly challenge each other (in a good way). You should expect to be able to defend your designs and ideas, without being defensive, and do the same of everyone elses. The team grows from each others experience and realizes that everyone brings something unique to the team.

Required:
3+ years software development (focused on Service Layer or Web Tier development)

The list of tools/libraries/architectures that a candidate should be highly proficient in are: Java, JavaScript (OpenLayers, ExtJS 4.x, jQuery), CM Tools (JIRA, Confluence, FishEye, SVN, Subclipse/etc.), CI Tools (Hudson, Jenkins), Maven, ORM (Hibernate 3.6/4.0+), MVC (Spring, ExtJS 4.x), Annotations (e.g. as used in Hibernate/Spring), REST-based web services, HTML, CSS, Java Design Patterns & Best Practice

Desired:
Familiarity and/or experience with: Hadoop ecology (HDFS, ZooKeeper, writing MapReduce jobs)

Database and SQL knowledge

Unix/Linux Operating System familiarity

The list of tools/libraries/architectures that a candidate should be familiar with are: HTML5, Ajax, JAXB, Reverse Ajax implementations (DWR/Web Sockets), Grails, Spring Security, basic Linux command line navigation, SSL configuration, Web container configuration (setup of cluster, load-balancing, etc. in containers like JBoss/Tomcat)"
Software Engineer,Asimov,"Asimovs Biodesign Software Team builds our computer-aided design platform for programming biological systems.

Position: Were looking for a full-time Software Engineer to expand our genetic design automation platform, including our cell biophysics engine, DNA compiler, and -omics analysis pipeline. The ideal candidate is obsessed with the question of how to best engineer complex biological systems. This is a unique opportunity to work at a nimble, forward-thinking synthetic biology startup and help build the foundation for engineering biology.

As part of the Biodesign Software Team, you will:
Design and implement the backend for our cell CAD pipeline, including the genetic compiler and simulation engine using Java and Javascript/HTML
Innovate new biophysical models to guide molecular and cellular engineering
Develop machine learning approaches to analyze genomics, transcriptomics, and other molecular debugging data streams using TensorFlow
Collaborate frequently with the Synthetic Biology Team to incorporate biological design principles and experimental data into the software pipeline
Communicate methods and results with other scientists, industry executives, and academic researchers
Manage software project and write elegant code
Work effectively as part of a multifunctional team in support of a synthetic biology design platform
Qualifications:
M.S. or Ph.D. in Computational Biology, Computer Science, or a related field
Knowledge of cell and molecular biology a must
2+ years of experience of professional software development
Ability to work both independently and in a collaborative team environment
About us: Were fueled by a vision to transition synthetic biology to a fully-fledged engineering discipline. Should you join our team, you will grow with a constantly evolving organization, and push the frontiers of biological engineering. Culture is key to Asimov - we believe that our mission can only be achieved by a diverse team that brings a mixture of perspectives to creating a future powered by engineered biology."
Software Engineer,Aurora Flight Sciences,"Job Qualifications:
Aurora enables a high degree of execution authority and responsibility for its individual contributors. Virtually all Aurora engineers can point to one or more products and say ""My work is on that UAV system"". this position will design and develop flight or mission-critical onboard software for UAV systems. As part of a dynamic, multidisciplinary team, the candidate will participate hands-on in the full software life-cycle, from concept through implementation, integration all the way to flight test.
Specifically the candidate will:
Define, analyze and review software requirements to meet defined and anticipated customer needs and system quality and performance standards.
Collaborate with other enigneering disciplines in planning, design and development of systems to ensure software and hardware performance and compatibility
Design software architecture and interfaces and implement functionality using a model-based approach.
Write test plans and test cases, conduct unit-, integration-, and system-level testing.
Typical software functionality for a UAS includes guidance, navigation & control, mission sequencing, payload control, redundacy and contingency management, uplink and downlink packet encoding and decoding, converting between different serial protocols, hardware-in-the-loop simulation, ground based GUIs, and aircraft subsystem control (i.e., engine, electrical system, fuel system).
Minimum Requirements:
B.S. in Computer Science or applicable engineering or science field, Master's degree preferred.
0 to 5 years of experience.
Must be willing and able to be an active contributor (developer) in any of the software life-cycle phases.
Must have at least one year of hands-on professional experience in at least one, ideally a combination, of the following areas:
Model-based design and/or test using the Simulink/Stateflow tool chain.
Software development in a relevant subject matter area: UAS or other robotic system guidance, navigation and control; ground- or airborne mission systems; general aerospace flight control or cockpit avionics systems; ground stations; payload control.
Development/operations of hardware-in-the-loop simulators, conducting testing and troubleshooting of HW/SW interfaces.
Service Oriented Architectures (SOA) applied to aerospace or military systems (C2, payload data, etc.)
Demonstrated capability to have personally developed (i.e. modeled or written) production software within the last 12 months.
Must be a US Person, minimum Green Card Holder.
Must be willing to work under, and help refine, a defined software development process.
Prior experience in any of the following areas is a plus.
Work experience or internship with developing production software in an autonomous vehicle or robotics environment.
Operating in a regulated development environment, like CMMI (level 3 or higher) or DO-178B.
Aurora Flight Sciences is an Equal Opportunity Employer"
Software Engineer,MediaMath,"MediaMaths strength is in numbers. Our technology analyzes 200 billion customer opportunities daily more volume than the top 10 stock exchanges in the world, combined.
Over 700 Mathletes in 16 global offices are trusted by two-thirds of the Fortune 500 and partner with thousands of marketers to ensure brands connect with right audiences, in the right place, in the right time.
We believe consumers want to have meaningful conversations with their favorite and yet-to-be-discovered brands across all digital touchpoints. Our omnichannel, integrated programmatic platform unites digital media and big data to maximize the return on every marketing dollar spent by making advertising relevant, personalized, measurable and controllable. From inventing the DSP category in 2007 to being named a DMP Forrester Challenger (our first year participating in the DMP Wave!) in 2017, we continue to deliver results for marketers more quickly and accurately than any other solution.

Technology is changing the way brands interact with consumers. MediaMath is powering that change. Come be a part of it!
Responsibilities
Develop software applications for major data pipelines feeding into our data management platform
Support, maintain, and document software functionality
Participate in software design meetings and analyze user needs to determine technical requirements
Provide support for day-to-day operational work, as needed
Follow secure coding best practices
This is not an exhaustive list of responsibilities. As part of our global team, you may be required to be work off-hours or be on-call on a rotating basis. Other duties may be assigned, as needed. MediaMath retains the right to change job duties at any time.
Qualifications
Experience Requirements
BA/BS Degree in computer science, mathematics, or related experience
2+ years of programming experience in Clojure, Python, Java, Scala or Golang
Preferred Skills
Strong analytical and problem-solving skills
Familiarity with the agile development process
Familiarity with JVM languages such as Java, Scala or Clojure
Detail-oriented with excellent self-motivation, planning, documentation and communication skills
MediaMath is privately held, employee owned, and headquartered in New York. Mathletes enjoy: Company equity. Performance Bonus. Comprehensive Insurance. Global Internal Mobility. Open Paid Time Off, Philanthropy and Holidays. 401(k) match. Paid Parental Leave. Cell Phone Reimbursement. Modern office space. Onsite Fitness & Wellness. MediaMath.org.

If there might be a match, you'll be scheduled for a first round interview; a 30-minute phone call with our recruiting team so we may get a better understanding of why you are interested in MediaMath and why you think it's a fit. We do our best to respond to everyone, however due to the volume of applications received, only those selected for interviews will be contacted. If you really think weve missed the mark, please follow up with globaltalentacquisition@mediamath.com and let us know why youre the perfect fit!"
Software Engineer,Milliman,"The Boston Predictive Analytics practice of Milliman, Inc. located in Cambridge, Massachusetts is looking for motivated, critical, creative thinkers to contribute to our team as Software Engineer. You will use what you have learned in class and be able to apply it alongside a team of engineers representing the best schools in the area. You are comfortable thinking outside the box across projects and use-cases. You are able to hit the ground running, independent, fast learner, comfortable within a fast-paced, unstructured environment.

In this role, you will contribute towards building new functionality for our platform by applying what you know, learning new technologies and tools, and expanding your competence in multiple areas; you will contribute towards building and improving our current platform to help us fundamentally change the way an entire industry works.

Responsibilities

Youll contribute to any and all parts of our flow for developing and deploying high-volume predictive models: from data infrastructure to modeling engines and algorithms, to high-performance cloud computing interfaces, to client-facing BI/analytics. You will:
Assist Integration of predictive models into database-backed environments, real-time query APIs, and existing modeling pipelines, considering computational and other constraints.
Contribute to significant strategic projects focused on enhancements to core tooling, automation, scalability, and broad statistical machine learning capabilities.
Collaborate with other team members, including machine learning researchers, database engineers and project managers, to design, generalize, optimize and implement predictive models and research.

Desired Skills

The nature of your work calls for a unique mixture of skills. What is most important is that you are exceptional in what you do know, and have the desire and ability to learn new things quickly and build on your current set of skills.

Required basic qualifications:
Currently pursuing a BS level or higher is CS or have an applicable background.
Fluency in modern scripting languages, ideally Python.
Experience with predictive analytics, algorithms, or machine learning, and at least one supporting technology stack (e.g., Spark, TensorFlow, sklearn).
Ability to pay close attention to detail.

Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities

The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractors legal duty to furnish information."
"Research Scientist, Advanced Analytics","Hitachi America, Ltd.","Company: Hitachi America, Ltd.

Division: R&D/Big Data Lab

Location: Santa Clara, CA

Status: Regular, Full-Time

Summary

Hitachi America, Ltd. ( http://www.hitachi-america.us/ ) has an opening for a Research Scientist, Advanced Analytics in the Big Data Laboratory located in Silicon Valley. The mission of this laboratory is to help create new and innovative solutions in big data and advanced analytics. The laboratory focuses on research, technology innovation, proof-of-concept activities, technical marketing and business development for novel big data solutions, leveraging Hitachis technologies, reinforcing Hitachis messaging around big data, and establishing Hitachi as a leader in big data.

We are seeking machine learning scientists to create innovate algorithms and solutions for smart industry, social infrastructure and IoT. The successful candidate will have an opportunity to solve challenging real world problems, work on massive industrial data and to create solutions that will benefit society. This position is a perfect match for a visionary scientist who is passionate about the unlimited potential of machine learning/AI/data mining/statistics for solving real problems and empowering society, industry and the things that matter.

More information about the research done at the Big Data Laboratory can be found at:

http://www.hitachi-america.us/rd/about_us/bdl/

More information about Hitachi IoT technology in Smart Industry can be found at:

https://www.hitachiinsightgroup.com/en-us/smart-industry.html

Responsibilities:  Research and develop innovative data analytics algorithms and solutions for smart industry, social innovation and the Internet of Things
 Prepare and execute proof-of-concepts of the solutions with customers
 Work with business units to commercialize the successful proof-of-concepts
 Protect Hitachis IP through patents and other activities, and publish technical results in leading venues to establish Hitachi as a leader in big data and advanced analytics.
Qualifications
 Ph. D degree in Computer Science or related field
 Strong research experience in machine learning algorithms such as classification, regression, and clustering - Knowledge of time series analysis techniques is a plus.
 Good programming skills and knowledge of data science tools and libraries in R, Python or MATLAB - Knowledge of big data platforms such as Hadoop and Spark is a plus.
 Self-motivated and passionate about solving real-world problems and building innovative systems with big data analytics.
 Demonstrated ability to effectively create and deliver technical communications, including patent applications, presentations, papers, and reports.
 Ability to collaborate successfully with R&D co-workers, Hitachi group company customers, partners, engineers, and business units in Hitachi with the goal of commercializing the proposed solution.
 Ability to travel within the US and internationally as needed.
EOE-Females/Minorities/Protected Veterans/Individuals with Disabilities

If you need a reasonable accommodation to apply for a job at Hitachi, please send the nature of request and contact information to accommodation@hal.hitachi.com. Queries other than accommodation requests will not be responded to."
Software Engineer,JP Morgan Chase,"As a member of our Software Engineering Group you will dive head-first into creating innovative solutions that advance businesses and careers. Youll join an inspiring and curious team of technologists dedicated to improving the design, analytics, development, coding, testing and application programming that goes into creating high quality software and new products. Youll be tasked with keeping the team and other key stakeholders up to speed on the progress of whats being developed. Coming in with an understanding of the importance of end-to-end software development-such as Agile frameworks-is key. And best of all, youll be working with and sharing ideas, information and innovation with our global team of technologists from all over the world.
This role requires a wide variety of strengths and capabilities, including:

Advanced knowledge of application, data and infrastructure architecture disciplines

Understanding of architecture and design across all systems

Working proficiency in developmental toolsets

Ability to collaborate with high-performing teams and individuals throughout the firm to accomplish common goals

Proficiency in one or more general purpose programming languages: HTML, Java, JavaScript, J2EE, Python, SQL

Understanding of software skills such as business analysis, development, maintenance and software improvement

Proficiency in programming in java, with a strong base in object-oriented design and development

Knowledge of professional software engineering practices & best practices for the full software development life cycle, including coding standards, code reviews, source control management, build processes, testing, and operations

Experience building scalable, fault tolerant infrastructure software or distributed systems, preferably in a Unix or Linux platform

Experience in CI/CD automation tooling

Understanding of Agile and Scrum development concepts

Knowledgeable with source/build systems such as GIT, GITflow, Gerrit, and Jenkins

Strong understanding of the web services industry, highly scalable web servers and caching servers, REST and SOAP, etc...

Our Global Technology Infrastructure Group is filled with innovators who love technology as much as you do. Together, youll use a disciplined, innovative and cost-effective approach to deliver a wide variety of high-quality products and services. Youll work in a stable, resilient and secure operating environment where youand the products you deliverwill thrive.

When you work at JPMorgan Chase & Company, you're not just working at a global financial institution. You're an integral part of one of the world's biggest tech companies. In 14 technology hubs worldwide, our team of 40,000 technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.

At JPMorgan Chase & Company we value the unique skills of every employee, and we're building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you're looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies around the world, we want to meet you."
Data Standards Engineer,Jacobs,"Jacobs Technology is the advanced technology arm of Jacobs Engineering (NYSE: JEC), one of the largest engineering and technical service companies in the United States. Jacobs has partnered with NASA to support space flight programs for more than 40 years and held the predecessor Engineering and Science Contract (ESC) since 2005. We look forward to continuing that work as the prime contractor for the JSC Engineering, Technology, and Science (JETS) contract along with our eleven teammate companies to provide engineering, scientific and technical contract services at NASA's Johnson Space Center (Houston, TX). JETS provides products and technical services related to human operations in space through development and integration of a broad spectrum of engineering requirements. This includes human spacecraft flight and flight development products, human exploration mission planning for NASA, institution support services, and new technology development. At Jacobs, we believe that people are our greatest asset, and that is why we offer a partnership in which you can grow personally and professionally with the advantages of strong leadership, competitive compensation and rewarding career paths.

Our long-term client relationship with NASA has led to a need for a Data Standards Engineer.

This position is a part-time position, not to exceed 20 hours per week.

The Data Standards Engineer will
Co-lead Johnson Space Center's (JSC's) Data Standards development.
Perform as a representative at the Agency level as well as International working groups, including the Spacecraft Onboard Interface Systems (SOIS) Wireless Working Group (WWG) within the Consultative Committee for Space Data Systems (CCSDS).
Evaluate technology gaps for wireless communications and sensors for human exploration.
Develop roadmaps and technology maturation plans to advance the technology.
Collaborate with industry and universities to advance wireless communication technology needed for human exploration.
Participate in wireless working group meetings, agency and international workshops and meetings on protocols and standards for wireless and space communications systems.
Prepare and present briefings and technical papers to NASA Division, Center and other forums as needed.
Perform other duties as required.

Qualifications
Required Education/Experience/Skills:
BS degree in engineering from an accredited engineering school and a minimum of fourteen (14) years of direct engineering experience, or a MS degree from an accredited engineering school and twelve (12) years of direct engineering experience, or Ph.D. degrees and nine (9) years direct engineering experience.
Must have experience with Data Standards of Wireless Space Communication Systems.
Why Work for JETS?
Opportunities for growth and advancement
Much, much more!
Dont miss out on this great opportunity; for immediate consideration, apply now! ( www.wehavespaceforyou.com ).
Must be a U.S. Citizen and successfully complete a U.S. government background investigation.
Management has the prerogative to select at any level for which this position has been advertised.
Essential Functions
Work Environment
Generally an office environment, but can involve inside or outside work depending on task.

Physical Requirements
Work may involve sitting or standing for extended periods (90% of time). May require lifting and carrying up to 10 lbs (5% of time).

Equipment and Machines
Standard office equipment (PC, telephone, printer, etc.).

Attendance
Regular attendance in accordance with established work schedule is critical. Ability to work outside normal schedule and adjust schedule to meet peak periods and surge requirements when required.

Other Essential Functions
Must be able to work in a team atmosphere. Must put forward a professional behavior that enhances productivity and promotes teamwork and cooperation. Grooming and dress must be appropriate for the position and must not impose a safety risk/hazard to the employee or others.
Jacobs is an Equal Opportunity Employer and employment selection decisions are based on merit, qualifications, and abilities. Jacobs does not discriminate in employment opportunities or practices on the basis of: race, color, religion, gender, national origin, age, sexual orientation, gender identity, disability, veteran status, or any other characteristic protected by country, regional, or local law."
IT Product Master Data Engineer,Hewlett Packard Enterprise,"HPEs Global IT Master data management (MDM) practice is essential to the effective running of our business across Sales, Services, Supply Chain, Marketing and other functions. Master data is a single source of business data used across multiple systems, applications, and processes. When master data is accurate, on time and complete it saves time and money and allows in better-informed business decisions. High-quality master data is an asset to our business.

As part of the MDM team, this role is responsible for ensuring the highest level of data integrity, governance, and service to internal customers. The role will support developing and improving master data management to increase operational efficiency, strengthen data governance, and increase data value through deployment of user-friendly processes, system tools, and data architecture simplification. Processes master data requests in line with service level agreements and respond to queries in a timely fashion. Drives internal control, regulatory, and process compliance through active understanding, engagement, and vigilance.

Provides problem-solving support, advice and guidance to internal customers which is professional, responsive, and assumes responsibility for conclusion. Completes daily, weekly and monthly processes and tasks to the required standard. Maintains a full audit trail of data changes. Provides timely and complete resolution of ad-hoc internal and external data queries Determines priority of work and addresses issues in a timely manner based on understanding of critical factors.

Performs timely accurate peer review of completed requests which enhance data quality, integrity, and team learning. Manages own time by estimating time to complete, setting deadlines, and communicating in advance when not expected meeting deadlines. Understands the impact of master data within the organization and systems for applicable data domains. Identifies patterns within the data to leverage information to create value Supports documentation of data dictionary, process, and business rules; providing document updates in timely manager.

Proposes process efficiency improvements in execution of work, automation opportunity, rationalization of processes, and training/education. Executes knowledge transfer to new or less experienced team members. Skills/Qualifications: Education and Experience: At least 7 years of business/technical/analytical experience Bachelor's Degree in relevant field of study Exposure to the business functions of a high tech enterprise from an operations, finance, supply chain, sales, channels, marketing or business analysis perspective. General: Excellent attention to detail and accuracy; strong critical self-review skills.

Demonstrates ability to deliver communication which is clear, concise, and relevant to audience through appropriate methods and tools. Appreciates what constitutes good customer service and displays consistent commitment to delivering. Exhibits a high degree of professionalism. Organized methodical application of established data governance standards.

Proactive approach to role and problem solving; solution rather than problem focused. Able to work collaboratively and communicate effectively with key stakeholders both within and outside of the MDM team to get the job done. Comfortable pushing back with customers and stakeholders to protect the integrity of the data management process. Inquisitive and thorough in approach.

Displays a passion for working in master data management. Self-motivated, flexible, with the ability to deal with high levels of complexity, change and evolving processes, often at short notice. Technical: Experience in managing master data eco system across MDM, SAP, SFDC with integration to D&B. Comprehensive understanding of MDM Data Models Understanding of concepts around entities, hierarchies, cleansing, standardization and de-duping across company and contact records.

Demonstrated mid-level or above proficiency with MS Office Excel, analyzing and manipulating large datasets through formulas and macros. Experience participating in technology implementation projects. Proficiency in Microsoft Visio, Access, and Project. Strong analytical abilities with experience extracting data and developing reports.

Strong interest in systems with demonstrable systems mindset and exposure to a large MDM system. Understanding of SDLC and project management methodology. Experience defining, writing and implementing business processes and data standards in one or more data domains. CPA, Six Sigma, PMP, or similar."
Software Engineer,Appian,"Are you equally as talented at as you are passionate about writing sophisticated code? Are you ready to dive deep into the intricacies of one of the worlds most cutting-edge software platforms in the industry?
As a Software Engineer working on the Appian platform, your mission will be to ensure that Appian is always fast, scalable, and up to whatever tasks our customers configure it to do. You will be solving problems of scale and flexibility that most engineers never see, building a product capable of serving our customers in ways you never imagined. Your scope extends from the high-performance data layer to the cross-platform user interface. This position requires the mental agility to jump from the deep code-level view of Appian, to the designers view, to the end users view, and back again. Learn how to write software that runs software, and enhance our declarative programming language.
You will design and develop systems that allow our customers to build amazing enterprise apps for mobile and the web without writing a line of code. The product features you write will impact millions of Appian users worldwide, and will expose you to an incredible range of frameworks, patterns and methods as you hone your comp sci kung fu.

About the Job :
Master the advanced technologies we use to revolutionize the way software is built.
Utilize your knowledge of our software to design new features and implement improvements to existing features.
Design features of increasing sophistication in small, tightly integrated, and highly-skilled teams.
Develop new user-oriented features; engineer complex systems.
Manage availability, latency, scalability, and efficiency of Appian by engineering reliability into software and systems.
Respond to and resolve emergent problems; write software and build automation to prevent problem recurrence.
About Us :
Appian delivers an enterprise platform for Business Process Management (BPM) and Case Management solutions. Appian unites users with all their data, processes, and collaborations in one environment, on any mobile device, through a simple social interface. On-premise and in the cloud, Appian is the fastest way to solve complex issues with modern apps. For more information, visit www.appian.com .

About You :
B.S. or MS in Computer Science or Computer Engineering preferred, other related technical disciplines considered
Superior academic achievement with a GPA of 3.7 or higher (preferred)
Fluency in Java, C++, and familiarity with git and Shell
Experiences building web-based J2EE apps preferred - Previous software development internship(s) preferred"
Software Engineer,Equifax,"As a Software Engineer, you will be process billions of requests a day and helping us building elegant solutions to this rather unique technical problem. The Software Engineer is essential to exploring new ideas and finding ways to improve software engineering practices across the entire team. The Software Engineer position will be using efficient data structures and algorithms to enable data processing at scale.

As part of the Identity Fraud Solutions team, you will be working on a high priority project developing technology for end consumers. This team is working with a wide variety of ever changing tools, so we are seeking candidates who are willing to learn and flexible. In order to succeed in this role you will need to be highly motivated, creative, self-directed, and thrive in small project teams.

Additional responsibilities of the Software Engineer include:
Performance and performance monitoring
Play a key role in helping shape all aspects of our products from cutting edge usability experiences to large scale data manipulation of terabytes of data

Required Skills:
2+ years of Development experience with .Net, C#, and Java
Able to create elegant, efficient and testable code
Currently valid US Security Clearance (or the ability to obtain clearance)

Preferred Skills:
Experience with distributed computing and performance analysis
Experience with Linux, JavaScript, Web Services, HTML5, CSS, SQL & NOSQL

Education :

Bachelor's Degree

Primary Location:
USA-Reston

Function:
Function - IT Development and Client Services

Schedule:
Full time"
Software Engineer,Bank of America,"Job Description:
TCA Software Engineer

The TCA Software Engineer will be responsible for designing, delivering & support of the next generation T+0 and T+1 TCA (Transaction Cost Analysis) applications within Bank of America Global Markets Equities Data and Analytics group. The candidate would be a hands-on developer in a fast paced agile environment. (S)he is required to work independently with minimum supervision. Candidate should have strong communication skill (both verbal and written) as (S)he would be working in a geographically distributed team. Candidate is supposed to demonstrate sense of ownership and take the responsibility for end to end delivery of the assigned tasks
Responsibilities
Working directly with users and analysts to translate business use cases into precise functional and technical requirements
Design and develop distributed, high volume, high velocity multi-threaded real-time processing systems using core java technology
Utilizing the Hadoop framework to process historical OMS and Market Data
Produce T+0 and T+1 TCA reports to present the performance analysis of our execution services across assets and order flows.
Provide support and tools to allow end users to identify the underlying data issues or performance outliers
Requirements
10+ years of experience as a professional Software Engineer.
Strong Core Java Skills including but not limited to Multithreading, Collection, File IO, Distributed programming etc.
Strong knowledge and experience with complex SQL queries and data analysis
Understand order lifecycle of the trading systems and microstructure of the market data
Strong Scripting skills in Unix/Linux (Shell/Perl/Python/)
Desire to explore the underlying details and provide the root cause analysis on inquiries
Good Team player with excellent interpersonal skills and integrity
Knowledge of Continuous integration tools like Jenkins, Teamcity.
Exposure to source code management tools - svn, git.
Good experience working with IDE like Eclipse, Intellij.
Desired skills.
Experience with Hadoop framework like Scala, Spark, Hbase, Hive, Solr is a plus
Experience with Stream processing like Storm, Kafka is a plus
Experience with BI reporting tools like Cognos, Tableau is a plus
Experience working on MPP databases.
Exposure to Agile development model.
Exposure to build automation tools like Maven, Gradle, ANT, sbt.

Shift: 1st shift (United States of America)

Hours Per Week: 40"
"Architect, Data Engineering",Intellipro Group Inc.,"DUTIES AND RESPONSIBILITIES: Essential Functions

Archive and manage uploaded usage data from customers
Analyze collected usage data
Evaluate and deploy machine learning/deep learning framework that used for data analytics
Design and deploy data ETL flow
Design and implement of usage data visualization
Design and implement collectible usage data that meet privacy regulation
Evaluate and deploy machine learning/deep learning framework that used for data analytics
JOB SPECIFICATIONS (Education, Knowledge, Skills, and Abilities); (refers to job, not incumbent)

Education:
Minimum: Bachelor's Degree

Preferred: Master's Degree

Major/Discipline: Computer Science/ Math/ Statistics or related

Relevant Work Experience

Minimum: 2+ years of professional data engineer experience

Preferred: 5+ years of professional data engineer experience

Skills & Abilities: (Technical or General)

Minimum Requirements:
Understanding of modern smartphone systems, including hardware architecture, Android and Linux kernel
Understanding of power management practice on mobile devices
Proven track of utilizing scripting language, e.g. shell and python
Experience of using SQL, or SQL-like database, and design and deployment of ETL
Experience of data visualization design and performance metrics design
Proven track of applying machine learning/deep learning/AI to solve real-world problems
Preferred:
Excellent written and oral communication skills
Willing to go extra miles for excellency
Detailed-oriented working attitude
Willing to travel up-to 25% of time"
Data Integration Engineer (NY),C3 IoT,"C3 IoT has an opening for a Data Integration Engineer . You will be required to create advanced application integration solutions and configure, deploy and enhance enterprise cloud applications. C3 IoT product suite is entirely data-driven, so a great candidate will have a passion for acquiring, analyzing, and transforming data to generate insight. This role is very hands-on and requires a structured mindset and solid implementation skills.
Qualified candidates will have a solid knowledge of integration and data manipulation technologies.

Key responsibilities:
Engage directly with customers to participate in design and development of data integration/transformation solution according to functional requirements
Perform debugging, troubleshooting, modifications and unit testing of integration solutions
Support, monitor, execute production application jobs and processes
Participate in the development of documentation, technical procedures and user support guides

Required Skills & Experience:
2+ years of experience with system/data integration, development or implementation of enterprise and/or cloud software
Proficiency in data integration/EAI technologies, such as Tibco
Demonstrated proficiency with JavaScript
Experience with Unix-based operating systems
Familiarity with version control/SCM is a must (experience with git is a plus)
Experience with relational databases (any vendor)
Solid understanding of concepts of cloud computing
Engineering degree in Computer Science or Electrical Engineering
Working knowledge of Agile Software development methodology
Strong organizational and troubleshooting skills with attention to detail
Strong analytical ability, judgment and problem analysis techniques
Interpersonal skills with the ability to work effectively in a cross functional team

C3 IoT provides a competitive compensation package:
Competitive salary
Generous stock options
401K
Medical, dental, and vision benefits
Fully stocked kitchen with catered lunch
Table tennis and pool in office
Free membership for on-site gym
Fun team filled with some great people
Friday evening social hours with food, drink and music"
Python Software Engineer,"BigBear, Inc.","BigBear, Inc. is a leading provider of big data computing and analytic solutions. We help people make sense of their data using our cloud-based platform and big data processing algorithms. Each day, we crunch massive volumes of structured and unstructured data into usable and actionable information for our customers.

We currently have an immediate position for a talented and passionate Software Engineer to build high-performing, scalable, enterprise-grade applications. You will be responsible for server side application development while providing expertise in the full software development lifecycle, from concept and design to testing. You will collaborate directly with customers to ensure our products meet and exceed operational requirements. The successful candidate will be a self-starter that demonstrates excellent communication and problem solving skills with a focus on customer service.

Requirements:
BS/MS degree in Computer Science, Engineering or a related subject
3+ years experience with the Python programming language
Experience developing and scripting in Linux based environments.
Experience with Unix Shell
Experience with database technologies
Strong communication and leadership skills
Strong desire to learn and master new technologies
Adaptive to different technology interfaces and frameworks.
Must be US Citizen
Preferred Skills:
Experience developing in large distributed environments
Experience integrating and maintaining applications within cloud environments
Experience with RESTful service architectures
Ability to document and demonstrate solutions with diagrams, charts, and clean code.
Ability to create and execute action plans for both deployment and maintenance.
Experience with performance tuning
Responsibilities:
Collaborate with Project Manager and engineering to implement innovative solutions
Design and develop high-volume, low-latency applications for mission-critical systems
Contribute in all phases of the development lifecycle
Write well designed, testable, efficient code and unit tests
Ensure designs are in compliance with specifications
Perform demonstrations and briefings to customers to ensure program success
Perform system monitoring, optimization, and high performance tuning.
Perform requirements analysis, system design, and draft technical documentation
Participate in research and development for system engineering and product advancement
Provide off-hours support in rare circumstances
BigBear is an Equal Opportunity Employer"
Software Engineer,Posh Technologies,"Software Engineer

Location: Redmond, WA

Duration: Long Term, 12-18 Months

Description

The job is to develop Windows applications to be used internally by the group.

Skills Required
Bachelor of Science degree in Computer Science
Knowledge building Windows applications (classic desktop, or store applications), ideally with C#
Knowledge of database systems (preferable Microsoft SQL Server), to design database systems, and also to build Windows applications that work with SQL databases (to store and retrieve data).
Fluent coding skills in C/C++/C#, preferable C#
Self-driven, with the ability to work independently as much as possible given general directions"
IS Data Analytics Engineer - Splunk,Partners HealthCare(PHS),"As a not-for-profit organization, Partners HealthCare is committed to supporting patient care, research, teaching, and service to the community by leading innovation across our system. Founded by Brigham and Womens Hospital and Massachusetts General Hospital, Partners HealthCare supports a complete continuum of care including community and specialty hospitals, a managed care organization, a physician network, community health centers, home care and other health-related entities. Several of our hospitals are teaching affiliates of Harvard Medical School, and our system is a national leader in biomedical research.

Were focused on a people-first culture for our systems patients and our professional family. Thats why we provide our employees with more ways to achieve their potential. Partners HealthCare is committed to aligning our employees personal aspirations with projects that match their capabilities and creating a culture that empowers our managers to become trusted mentors. We support each member of our team to own their personal developmentand we recognize success at every step.
Our employees use the Partners HealthCare values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.

General Summary/Overview

The IS Data Analytics Engineer will be part of the Application Performance Management (APM) team that is responsible for production and analysis of large data sets and their corresponding visual representation in Splunk. This role includes the acquisition of multiple disparate data sources, and the analysis of trends and anomalies. The IS Data Analytics Engineer will work with machine performance data, application metrics, and network metrics. S/he will also produce analysis and dashboards, according to the general standards in place.
Analysis of trends in regards to application metrics, demand metrics and capacity metrics, and their cross reference to related services and systems is required. Extensive collaboration with other teams for the implementation, maintenance, and management of performance monitoring tools throughout Partners is expected.

Principal Duties and Responsibilities

 Provide reporting capabilities, interactive dashboards and thoughtfully applied analytics.
 Triage performance issues.
 Coordinate with a diverse community to gather requirements and document applications, and communicate results.
 Objectively categorize performance, identify performance pain points, and provide recommendations to improve overall performance.
 Communicate with key stakeholders on use and interpretation of data as well as reporting methodologies and tools.
 Work with IS management, application and business owners, and application teams to understand and define their performance management needs.
 Use the Partners HealthCare values to govern decisions, actions and behaviors. These values guide how we get our work done: Patients, Affordability, Accountability & Service Commitment, Decisiveness, Innovation & Thoughtful Risk; and how we treat each other: Diversity & Inclusion, Integrity & Respect, Learning, Continuous Improvement & Personal Growth, Teamwork & Collaboration.

Qualifications
 Minimum of a Bachelors degree in Computer Science or a related discipline required. Masters degree preferred.
 Minimum of 5 years of technical experience in reporting and data analytics.
 Knowledge of SPL
 Knowledge of statistics and demonstrated experience using statistical packages for analyzing datasets.
 Windows PowerShell or related scripting experience.
 Experience with one or more of the following tools:

o Splunk
o Dynatrace DC RUM
o Excel / SQL query analyzer / BI tools

 Knowledge and demonstrated experience with Windows and Unix monitoring, and related key performance indicators.
 Knowledge of SAN monitoring and key performance indicators.
 Knowledge of VMware monitoring and key performance indicators.
 Experience with MS office suite of products.

Skills/Abilities/Competencies Required

 Strong analytical and documentation skills.
 Understand ethical behavior and business practices, and ensure that own behavior and the behavior of others is consistent with these standards and aligns with the values of the organization
 Establish and maintain positive working relationships with others, both internally and externally, to achieve the goals of the organization.
 Excellent interpersonal skills to effectively communicate with technical teams, cross-functional teams, and staff at all levels of the organization including both technical and non-technical personnel
 Develop new and unique ways to improve operations of the organization and to create new opportunities.
 Work cooperatively and effectively with others to set goals, resolve problems, and make decisions that enhance organizational effectiveness.
 Ability to successfully negotiate and collaborate with others of different skill sets, backgrounds and levels within and external to the organization
 Positively influence others to achieve results that are in the best interest of the organization.

Working Conditions

Works from a single office location.
Participates in 24x7 on-call support rotation once every 4 weeks.

Supervisory Responsibility

No direct reports

Fi scal Responsibility

Demonstrate fiscal responsibility by effectively using Partners resources.

EEO Statement Partners HealthCare is an Equal Opportunity Employer & by embracing diverse skills, perspectives and ideas, we choose to lead. All qualified applicants will receive consideration for employment without regard to race, color, religious creed, national origin, sex, age, gender identity, disability, sexual orientation, military service, genetic information, and/or other status protected under law.

Primary Location : MA-Somerville-Assembly Row - PHS

Work Locations : Assembly Row - PHS
399 Revolution Drive
Somerville 02145

Job : IT/Health IT/Informatics-Engineer
Organization : Partners HealthCare(PHS)
Schedule : Full-time
Standard Hours : 40
Shift : Day Job

Employee Status : Regular
Recruiting Department : PHS Information Systems
Job Posting : Nov 30, 2017"
Software Engineer,Cisco,"What You'll Do
At Insieme, you'll develop the Nexus 9000 line of data center and cloud switches that feature application awareness to make the network infrastructure flexible and agile for dynamic response to application needs and virtual machine workload mobility. Insieme also developed a 40G Ethernet transceiver designed to drastically lower the price of 40G optics to better compete with other competitor switches. Who You'll Work With
At Insieme Business Unit, you will be working with a team of dynamic professionals who are passionate in designing and developing world's fastest growing cloud development platform. The environment within the team here is challenging and will give people who are driven an opportunity to influence and impact the way Customers will use our products and transform their business. Who You Are
You Have:
* Strong Knowledge of C++ and Python
* Strong background in Linux internals
* Prior experience with large scale distributed systems * Prior experience designing scalable object management systems * Prior experience with stateless idempotent control systems * Understanding of code generation systems * Excellent communication skills and a proven ability to manage competing high priority tasks. Minimum Qualifications
BSCS/MSCS, with 8+ years experience Desired Skills
* In depth understanding of virtualization technologies including Docker, KVM, VMWare, Why Cisco
We connect everything: people, processes, data, and things. We innovate everywhere, taking bold risks to shape the technologies that give us smart cities, connected cars, and handheld hospitals. And we do it in style with unique personalities who aren't afraid to change the way the world works, lives, plays and learns. We are thought leaders, tech geeks, pop culture aficionados, and we even have a few purple haired rock stars. We celebrate the creativity and diversity that fuels our innovation. We are dreamers and we are doers. We Are Cisco. *LI-IS1"
Data Mining Engineer,Red Arch Solutions,"Job Description ***Active TS/SCI with Polygraph Required***

Red Arch Solutions is a proven and effective small business integrator and consultant, recognized as a leading provider of IT development to the Federal Government, and primarily focused within the Intelligence Community.

We are an official AWS Partner, and are pleased to now offer Cloud Solutions Architecture and Engineering using Amazon Web Services.

Red Arch is seeking a Data Mining Engineer to support NGA.

The scope of this effort relates to the NGA Source Content Management and Services providing data stewardship, data identification, data transfer, data dissemination, ensuring the posting of GEOINT products and data to multiple NGA domains.

Responsibilities/Job Function:
Provide on-demand Data Transfer Officer (DTO) support to NGA mission critical Focus Cells. Review and transfer multiple GEOINT products for Analysts supporting crisis efforts for the duration of an NGA approved crisis

Administer and update the Battlefield Information Collection Exploitation System (BICES) web server software, to include identifying server location for data and product transfer. Specific update tasks will include updating indexes and metadata

Ensure all required software, hardware, and DTO certification (training) are compliant with Intelligence Community Standard 500 to perform GEOINT product retrieval, review, and dissemination functions

Perform quality assurance of all items transferred and loaded including text, images, hyperlinks, metadata, and HTML meta-tags. Ensure all transferred items are properly presented/viewable in multiple web browsers

Perform final human review of items to be released to the target security/network domain. Complete reviews daily for each GEOINT transfer and ensure all GEOINT reports are posted within 4 hours of receipt. Complete special DTO transfers within 1 hour of receipt

Review, transfer, and post/load GEOINT reports between 10 unclassified/classified NGA networks (SBU, NIPRNet, SECNet, SIPRNet, BICES, InfoSharing, NGANet, JWICS, StoneGhost (2 networks)). Collect, maintain, and provide statistics related to quantities and timelines for products and data transferred to/from the 10 networks on a weekly basis
Minimum Qualifications
Mandatory Security Clearance Requirement: TS/SCI with CI Poly
Experience with Hypertext Markup language (HTML)

Familiarity with internet protocols

Use of Classification Management tools

Use of ESRIs ArcGIS tool

Use of Microsoft Office applications

Experience working as a member of a team

Strong communication and customer service skills

A Bachelors Degree from an accredited institute in an area applicable to this position (e.g. information systems, computer science, math, or engineering) and four (4) years of relevant technical experience
An additional four (4) years of relevant technical experience may be substituted for the Bachelors Degree
Must presently be 8570 compliant (IAT Level 2 preferred); must be 8570 compliant (IAT Level 2) by date of security indoctrination with any necessary continuing education (CE) for certification

Desired Qualifications Red Arch Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, national origin, age, marital status, disability, or protected veteran status. Red Arch Solutions takes affirmative action in support of its policy to advance in employment individuals who are minorities, women, protected veterans and individuals with disabilities."
Graduate Engineer A or B or Engineer A,City of Austin,"This position will be filled at one of the following levels depending upon qualifications:

Graduate Engineer A
Graduation from an accredited four-year college or university with major coursework in Engineering or in a field related to the job
Licenses and Certifications Required: None

Graduate Engineer B
Bachelors degree in an appropriate engineering discipline with at least two (2) years of engineering experience
Licenses or Certifications: State of Texas Engineer-In-Training Certification

Engineer A
Graduation from an accredited four-year college or university with major coursework in Engineering or in a field related to the job, plus four (4) years engineering experience acquired either before and/or after licensing as a professional engineer
Licenses or Certifications: State of Texas Professional Engineer License

Notes to Applicants
This position will be filled at one of the following levels depending upon qualifications:
Graduate Engineer A  $28.95 to $37.50 per hour
Graduate Engineer B  $30.65 to $39.74 per hour
Engineer A  $32.47 to $42.10 per hour

When completing the City of Austin employment application:
Please be sure to detail on the application all previous employment that you wish to be considered as part of your qualifications.
A detailed, complete employment application is required. It helps us to better evaluate your qualifications and will be used to determine salary if you are selected for this position. Be sure to provide job title and employment dates for all jobs you wish to be considered.
A rsum is required, but will not substitute for a complete employment application. Incomplete applications will not be considered. Please include contact information from previous employers.
A cover letter is also required. In the cover letter candidates should describe, in concise detail, their interest in this position and highlight any prior experience that is relevant to this position.
If you are selected to interview:
Military/Veterans must provide a copy of their DD214 at the time of initial interview to receive military/veteran interview preference.

If you are selected for hire:
If you are identified as a top candidate, verification of your education (which may include high school graduation or GED , undergraduate and/or graduate degrees) is required. You must provide proof of education from an accredited organization or institution. You must also provide proof of your professional licenses or certifications.
A valid Texas Class C Drivers License is preferred.

Effective October 1, 2014, all Austin Water worksite locations are tobacco-free. Use of tobacco products and/or e-cigarettes is not permitted on any AW worksite  including construction sites, parking lot, garage or in any personal vehicle located on the premises.

Pay Range
Commensurate

Hours
8:00 to 5:00 Monday through Friday or other hours as required

Job Close Date

01/15/2018

Type of Posting
External

Department
Austin Water Utility

Regular/Temporary
Regular

Grant Funded or Pooled Position
Not Applicable

Category
Professional

Location
Waller Creek Center, 625 E. 10th. St., Austin, TX

Preferred Qualifications
This position is part of the Collections Systems Engineering Renew Austin Program
B.A. degree or higher in Civil Engineering
Experience diagnosing and analyzing the operation of water distribution systems
Experience in analytical studies related to the condition of the existing water mains to define new water main CIP projects
Underground utility design and construction experience
Experience in project prioritizing and scoping, preparation of cost analysis and negotiating proposals for implementation of water capital improvement projects ( CIP )
Experience in project management and sponsorship of water and wastewater lines
Experience in ArcGIS
Experience producing special graphic layouts, combining various data sources to produce plots using GIS software, Arc Map
Experience using Asset Management software such as the Citys Hansen database to research and interpret source records and documents
Experience in developing, preparing and presenting information contained in a variety of technical reports, maps, correspondence, manuals, procedures and other written materials
Valid Texas Class C Drivers License

Duties, Functions and Responsibilities
For the Duties, Functions and Responsibilities of this position, please click the appropriate job description below:

Graduate Engineer A Job Description

Graduate Engineer B Job Description

Engineer A Job Description

Knowledge, Skills and Abilities
For the Knowledge, Skills and Abilities of this position, please click the appropriate job description below:

Graduate Engineer A Job Description

Graduate Engineer B Job Description

Engineer A Job Description

Does this job require a criminal background investigation?
This position does not require a criminal background investigation

EEO/ADA
City of Austin is committed to compliance with the American Disabilities Act. If you require reasonable accommodation during the application process or have a question regarding an essential job function, please call (512) 974-3210 or TTY (512) 974-2445.

Information For City Employees: If you are an employee within
the department, are in good standing and meet both the minimum and preferred qualifications, then you will receive an initial interview."
Software Engineer,Referentia Systems Incorporated,"Join an innovative Hawaii-based software company in developing the next generation of high performance, data management and analysis tools to manage the increasingly complex systems that are critical to society.

The electric grid, aircraft fleets, computer networks, and other critical complex systems generate diverse types of increasingly huge volumes of sensor data. Operators of these complex systems need to analyze sensor data in order to improve decision making and efficiency, reduce downtime, and plan for the introduction of new technologies (e.g. renewable power generation technologies). However, the solutions currently available are overwhelmed by increasing data volumes and the need for new types of analysis.

The successful candidate will become part of the in2lyitcs engineering team that is developing a foundational product that will significantly improve industrys ability to manage todays complex systems.

Duties:
Participate in the entire software development life cycle from design and development to testing and debugging
Able to balance fixing customer issues and project work to insure all deadlines are met
Understand customer needs, perform analysis, and develop innovative data collection, management, analysis, and/or visualization solutions
Implement, test, document and release solutions to customers
Help provide instruction and technical support to customers on an as-needed basis
Adheres to all development standards while implementing features
Performs quality control testing
Additional Duties for Senior Level Software Engineer
Help take high level requirements to engineering requirements. Ensure requirements are accurately implemented (coding level) and verified.
Designs, develops, and tests software solutions. Uses software design methodology to implement real-time software systems
Defines business problems through interaction with customers.
Provides insights into project standards and practices
Provides technical guidance for other team members
Qualifications:
Bachelor of Science or Master of Science in computer science, engineering, or related field
Solid understanding of the principles, design, and implementation of databases, software algorithms, computer operating systems, and computer networks
4+ years of software development experience (6+ years for Senior level)
Proficient in an object oriented programming language
Demonstrated ability to estimate feature development times and deliver quality features while meeting established deadlines
Ability to work independently in a dynamic team environment
Excellent written and oral communication skill
Ability to implement, monitor and maintain quality control standards
Must be able to obtain security clearance.
Preferred Additional Qualifications
Experience developing high performance parallel and/or distributed software components
Graphical user interface (browser-based or desktop application) development experience
Scientific/technical computing experience
Java, Python, .NET, and/or MATLAB experience"
Software Engineer,Capital One,"1750 Tysons (12023), United States of America, McLean, Virginia

At Capital One, were building a leading information-based technology company. Still founder-led by Chairman and Chief Executive Officer Richard Fairbank, Capital One is on a mission to help our customers succeed by bringing ingenuity, simplicity, and humanity to banking. We measure our efforts by the success our customers enjoy and the advocacy they exhibit. We are succeeding because they are succeeding.

Guided by our shared values, we thrive in an environment where collaboration and openness are valued. We believe that innovation is powered by perspective and that teamwork and respect for each other lead to superior results. We elevate each other and obsess about doing the right thing. Our associates serve with humility and a deep respect for their responsibility in helping our customers achieve their goals and realize their dreams. Together, we are on a quest to change banking for good.

Software Engineer

Were Capital One. Yep, the Whats in your wallet? people. But we are oh so much more than that. Did you know were a full service bank? Checking accounts, savings accounts, mortgages, and car loans. We could tell you more, but youre going to check out our web site anyway, right?

Here are some of our favorites:
https://youtu.be/YWHZOx0YTc0
https://developer.capitalone.com

Capital One has a history of disrupting, and we are doing it again. Were experimenting, innovating, and delivering really cool stuff to 65 million customers. We love to be curious, to dream, to ask, What if? Oh, and we love to write code.

Your commitment to learn new things is every bit as important to us as what you have already done. Maybe even more so because we dont want to be doing the same thing tomorrow that were doing today. You accept change, want to grow, and evolve into a better member of the team.

What will you work on as a Capital One Software Engineer? Buzz buzz buzz. Customer-facing web and mobile applications, to highly-available, highly-scalable micro-services, to back-end systems with sophisticated data pipelines. All on the cloud!

You will drive design, implementation, testing, release (buzz buzz DevOps) in an Agile environment, using modern methodologies, and open source tools. Whether a new feature or a bug fix, you will lead your work and deliver the most elegant and scalable solutions, all while learning and growing your skills. Most importantly, youll work and collaborate with a nimble, autonomous, cross-functional team of makers, breakers, doers, and disruptors who, love to solve real problems and meet real customer needs.

The person we're looking for:
got a wee bit excited about what they read above
has a sense of intellectual curiosity and an addiction to learning
is self-driven, actively looks for ways to contribute, and knows how to get things done
is deliriously customer-focused
values data and truth over ego
has a strong sense of engineering craftsmanship, takes pride in the code they write
believes that good software development includes good testing, documentation, and collaboration
has good communication and reasoning skills, including the ability to make a strong case for technology choices

What the search engines are looking for to help you find this job, as well as the type of environment, technologies, and languages that we work in:
Agile, DevOps, AWS (Amazon Web Services), Docker, micro services, Go, Java, Scala, Clojure, C#

Basic Qualifications:
At least 3 years of experience in software development

At this time, Capital One will not sponsor a new applicant for employment authorization for this position."
Software Engineer,General Dynamics Information Technology,"General Dynamics Information Technology is hosting a virtual event for this position on December 13th . Please schedule a time that works best for you to speak to the recruiter for this position  Jon Weisenborn. Interviews can be done from the comfort of your home, car or on a work break. I am excited to meet you virtually!!
CLICK HERE TO REGISTER
General Dynamics Information Technology, a market leader and technology innovator, is seeking talented professionals to deliver cutting edge solutions to our customers.

General Dynamics Information Technology has an immediate opening for a Java and Javascript Developer to join our team. The position provides an opportunity to develop the cutting-edge technology that supports some of our nation's fundamental defense services. General Dynamics Information Technology employees work closely with esteemed clients to develop solutions that allow them to carry out high-stakes national security missions.

The selected Java and Javascript Developer candidate will work with collaborative teams to build and further advance General Dynamics Information Technology software systems and applications. In addition to receiving a competitive salary and generous health and personal benefits, the Java and Javascript Developer will enhance their skill-set by working among a talented and technically accomplished group of colleagues. The employee will also enjoy a flexible work environment where contributions are recognized and rewarded.

Key Responsibilities:
A Java and Javascript Developer participates in the research, design and development of systems software, software applications and/or tools for new programs and subprograms. Responsibilities also include enhancements, modifications, and corrections to existing software. In addition, these employees are responsible for coding, testing, integrating, deploying, and documenting software solutions.

Additional Responsibilities:
* Exercise creative thinking to advance our business performance
* Deliver innovative, flexible, integrated solutions to meet customers' changing business needs
* Support and engage in programs, projects and practices behind the General Dynamics Information Technology's culture and strategy, and comply with all policies and procedures
* Follow industry and department trends and developments to ensure General Dynamics Information Technology's services are consistent with, and/or superior to, industry best practices

Job Description:
Candidates must show a passion for innovation, a good understanding of software systems and applications, and the ability to effectively create software solutions. As member of a devops agile team (managed via Scrum), the developer will need to effectively integrate into the team to deliver quality software in a timely fashion.

The Java and Javascript Developer position requires an Associate's degree in Software, Computer, or Electrical Engineering, Computer Science, or a specialized area or equivalent field and five years of related experience. A Bachelor's Degree is preferred.

No security clearance is required at time of hire. However, applicants selected will be subject to a U.S. Government Public Trust background investigation. Ability to obtain DHS Suitability is required. Applicants must be granted Government Suitability in order to meet eligibility requirements to work on the program and to receive access to U.S. Government information. Due to the nature of work performed within our facilities, U.S. citizenship is required.
#CJPOST
#DICE
Education
Bachelors Degree in Computer Science, Engineering or a related technical discipline, or the equivalent combination of education, technical training, or work/military experience.
Qualifications
2-5 Years of related experience required.
Required Key Skills:
* Java and Javascript development experience
* Development experience with Spring Framework
* Development experience with Angular
* Understanding of, and experience, with Agile Development in a DevOps environment
* A proven record of software delivery and deployment

Qualifications Preferred:
* In-depth understanding of object-oriented methodology, algorithms, data structures, and design patterns
* Development experience with Oracle
* Development experience with OpenShift
* Experience writing software to be deployed to a containerized environment (Docker)
* Experience with Docker, Ansible, and Kubernetes
* Experience with testing tools such as Cucumber and Selenium
* Familiarity with DEVOPS tools and platforms such as JIRA, Jenkins, GitHub, Amazon Web Services
As a trusted systems integrator for more than 50 years, General Dynamics Information Technology provides information technology (IT), systems engineering, professional services and simulation and training to customers in the defense, federal civilian government, health, homeland security, intelligence, state and local government and commercial sectors.With approximately 32,000 professionals worldwide, the company delivers IT enterprise solutions, manages large-scale, mission-critical IT programs and provides mission support services.GDIT is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, or veteran status, or any other protected class."
Software Engineer,Vencore,"Overview
Vencore is a proven provider of information solutions, engineering and analytics for the U.S. Government. With more than 40 years of experience working in the defense, civilian and intelligence communities, Vencore designs, develops and delivers high impact, mission-critical services and solutions to overcome its customers most complex problems. Headquartered in Chantilly, Virginia, Vencore employs 3,800 engineers, analysts, IT specialists and other professionals who strive to be the best at everything they do. Vencore is an AA/EEO Employer - Minorities/Women/Veterans/Disabled

Responsibilities Software engineers will perform a variety of activities, including working to meet tactical and strategic requirements for frameworks and systems, engineering multi-threaded web-based data applications, and working on the development of complex software products

Qualifications

Bachelors degree in computer science, IT, or equivalent technical discipline, or approximately eight or more years work experience in relevant focus areas.

Approximately eight or more years current experience developing complex software products applying different methodologies.

Demonstrated experience prioritizing and meeting tactical and strategic requirements for frameworks and systems to deliver end user tools/capabilities.

Demonstrated experience engineering scalable multi-threaded web-based data applications to meet high-availability requirement.

Subject matter expert in software engineering using current web, application and database technologies.

Demonstrated experience delivering technical solutions using COTS and GOTS relational and non-relational database technologies and big-data solutions.

Demonstrated expertise in building solutions using web services, APIs x.509 and LDAP controls to meet mission requirements with unique datasets and user requirements while simultaneously adhering to complex security and compliance requirements.

Demonstrated experience building and managing the code and processes to establish common code repositories, standards for re-use and efficiencies across team(s) and projects. \

Demonstrated experience developing unit test/validation frameworks and processes.

Demonstrated experience managing software bug mitigation and evolving software to address end user requirements.

Demonstrated experience transitioning legacy software solutions to modern COTS, GOTS, and Open-Source solutions.

Demonstrated experience delivering software solutions using Cloud technologies.

DESIRED QUALIFICATIONS:

 Demonstrated experience transitioning legacy software solutions to modern COTS, GOTS and Open-Source solutions

 Demonstrated experience delivering software solutions using Cloud technologies

 Prior experience delivering software solutions that adhere to IC clients data handling, compliance and security requirements

 Experience integrating Cloud technologies in the IC environment.

 Experience with data analysis; target needs assessment; systems engineering and integration; data acquisition/distribution/ management/ enhancement; data prioritization; Sigint/Humint Targeting and Analysis."
Machine Learning Engineer,Doximity,"Why work at Doximity?

Doximity is rewiring healthcare and is the 6th fastest growing technology company in North America. ( http://www.prnewswire.com/news-releases/doximity-is-fastest-growing-company-in-bay-area-per-deloittes-2016-technology-fast-500-300367390.html ) Here's how clinicians use our products ( https://res.cloudinary.com/dhttas9u5/image/upload/WhyDocsUseDox-Infographic_20160930_adpvj6.pdf ). For us, transparency is key, so ensuring your goals and values align with ours is also an important step. Take a look at how we Work at Doximity. ( https://workat.doximity.com/ )

Skills & Requirements:
3+ years of industry experience; M.S. in Computer Science or other relevant technical field preferred.
3+ years experience collaborating with data science and data engineering teams to build and productionize machine learning pipelines.
Fluent in SQL and Python; experience using Spark (pyspark) and working with both relational and non-relational databases.
Demonstrated industry success in building and deploying machine learning pipelines, as well as feature engineering from semi-structured data.
Solid understanding of the foundational concepts of machine learning and artificial intelligence.
A desire to grow as an engineer through collaboration with a diverse team, code reviews, and learning new languages/technologies.
2+ years of experience using version control, especially Git.
Familiarity with Linux, AWS, Redshift.
Deep learning experience preferred.
Work experience with REST APIs, deploying microservices, and Docker is a plus.
What you can expect:
Employ appropriate methods to develop performant machine learning models at scale, owning them from inception to business impact.
Plan, engineer, and deploy both batch-processed and real-time data science solutions to increase user engagement with Doximitys products.
Collaborate cross-functionally with data engineers and software engineers to architect and implement infrastructure in support of Doximitys data science platform.
Improve the accuracy, runtime, scalability and reliability of machine intelligence systems
Think creatively and outside of the box. The ability to formulate, implement, and test your ideas quickly is crucial.
Technical Stack

We historically favor Python and MySQL (SQLAlchemy), but leverage other tools when appropriate for the job at hand.
Machine learning (linear/logistic regression, ensemble models, boosted models, deep learning models, clustering, NLP, text categorization, user modeling, collaborative filtering, topic modeling, etc) via industry-standard packages (sklearn, Keras, NLTK, Spark ML/MLlib, GraphX/GraphFrames, NetworkX, gensim).
A dedicated cluster is maintained to run Apache Spark for computationally intensive tasks.
Storage solutions: Percona, Redshift, S3, HDFS, Hive, Neo4j, and Elasticsearch.
Computational resources: EC2, Spark.
Workflow management: Airflow.
Fun facts about the Data Science team

We have access to one of the richest healthcare datasets in the world, with deep information on hundreds of thousands of healthcare professionals and their connections.
We build code that addresses user needs, solves business problems, and streamlines internal processes.
The members of our team bring a diverse set of technical and cultural backgrounds.
Business decisions at Doximity are driven by our data, analyses, and insights.
Hundreds of thousands of healthcare professionals will utilize the products you build.
A couple times a year we run a co-op where you can pick a few people you'd like to work with and drive a specific company goal.
We like to have fun - company outings, team lunches, and happy hours!
HQ Benefits & Perks

Comprehensive benefits including: medical, vision, dental, Life/ADD, 401k, flex spending accounts, and commuter benefits
Pre-IPO stock incentives
Work from home Wednesdays
3+ weeks of PTO
12 company holidays, including company shutdown last week of December
Fully ergonomic setup (chairs and stand-up desks)
Free lunch, snacks and beverages
Rooftop game room and deck with a beautiful view of SF skyline
Team trips to fun places like Lake Tahoe, Sonoma, Seattle, and Park City
Sabbatical after 5 years
Remote Benefits & Perks

Comprehensive benefits including: medical, vision, dental, Life/ADD, 401k, and flex spending accounts
Pre-IPO stock incentives
3+ weeks of PTO
12 company holidays, including company shutdown last week of December
Team trips to fun places like Lake Tahoe, Sonoma, Seattle, and Park City
Sabbatical after 5 years
About Doximity

Doximity is the leading online medical network with over 70% of U.S. doctors as members. We have strong revenues, profits, real market traction, and were putting a dent in the inefficiencies of our $2.5 trillion U.S. healthcare system. After the iPhone, Doximity is the fastest adopted product by doctors of all time. Launched by Jeff Tangney in 2011; Jeff previously founded healthcare pioneer Epocrates (NASDAQ: EPOC). Our beautiful offices are located in SoMa San Francisco.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
Software Engineer,University of Texas at Austin,"Purpose Build complex, interactive, database driven web applications. Design, deploy, and maintain the web presence and content management system in service to the instructional, administrative, and research needs of the Department of Computer Science.

Essential Functions Develop and maintain large, often database driven, web applications to be used by faculty, staff, and students. Develop and maintain web integrations to existing applications and data sources. Configure, deploy, and maintain a web content management system; ensure that all departmentally-provided content is kept up-to-date. Maintain data presentation compliance in accordance with campus policy. Improve the web application design and development process, ensuring quality and improving efficiencies. Act as consultant to technical staff on modern, agile software development and deployment techniques. Design and maintain the department and affiliate web sites, ensuring a modern, consistent, and coherent design. Provide general design, layout, and theme support for other content developers in the department. Serve on a team with departmental technical staff to bring database-driven web application solutions to the development and operations of instructional, administrative, and research support in the department.

Marginal/Incidental functions Other related functions as assigned.

Required qualifications Possess a broad knowledge of software development and operating system principles. A demonstrated understanding of security best-practices. Strong PHP skills. Strong Drupal 7 skills and experience with module and theme development. Strong MySQL Experience. Experience with css preprocessors like sass. Experience with javascript/jquery. Experience with git. Knowledge of accessibility best practices in web development. Experience with php frameworks like codeigniter. Basic linux experience. Ability to handle multiple tasks and projects simultaneously. Strong written and verbal communication skills with both non-technical and technical users. Excellent interpersonal skills and ability to work as part of a team and independently. One year of software development or system administration experience. Production experience with at least one computer programming language. Equivalent combination of relevant education and experience may be substituted as appropriate.

Preferred Qualifications A Bachelor's degree in Computer Science or related field. A minimum of two years of experience with web design and content management systems. At least three years of experience with web application development. At least three years of database programming. A willingness to learn new technologies. Strong Python skills. Strong Django skills. Experience with the Perl programming language. Experience with production software development and deployment with git. A strong understanding of modern software development best-practices. Graphic design experience. Experience with PyPE. Experience with PostgreSQL. Experience administering an apache web server. Experience administering MySQL server. Experience with usability testing.

Working conditions May work around standard office conditions Repetitive use of a keyboard at a workstation Use of manual dexterity Climbing of stairs Occasional climbing of ladders. Occasional lifting and moving. Occasional working with standard cleaning chemicals.

A criminal history background check will be required for finalist(s) under consideration for this position.

The retirement plan for this position is Teacher Retirement System of Texas ( TRS ), subject to the position being at least 20 hours per week and at least 135 days in length.

The University of Texas at Austin, as an equal opportunity/affirmative action employer, complies with all applicable federal and state laws regarding nondiscrimination and affirmative action. The University is committed to a policy of equal opportunity for all persons and does not discriminate on the basis of race, color, national origin, age, marital status, sex, sexual orientation, gender identity, gender expression, disability, religion, or veteran status in employment, educational programs and activities, and admissions.

If hired, you will be required to complete the federal Employment Eligibility Verification form, I-9. You will be required to present acceptable, original documents to prove your identity and authorization to work in the United States. Information from the documents will be submitted to the federal E-Verify system for verification. Documents must be presented no later than the third day of employment. Failure to do so will result in dismissal.

UT Austin is a Tobacco-free Campus"
Data Communication Engineer,Penske,"A Data Communications Engineers main responsibility is to provide day-to-day support of the Companys wide area and local area networks. They support, monitor and maintain network hardware and WAN connectivity to ensure maximum operability. The Data Communications Engineer is responsible to install and maintain PTL locations networks. They manage and administrate the Companys DNS and Cisco devices to ensure proper connectivity. The incumbent monitors network stability and performance; bring non performing

sites to the attention of management. They participate in DR testing and maintain detailed documentation of the primary and redundant network infrastructure. A Data Communications Engineer will participate in multiple projects that have department or division-wide implications and impact. Such efforts will require cross-function involvement with IT groups. They are responsible for the creation of project plans, timelines, milestones, goals and ownership. The position requires the ability to participate in after hours tasks such as on-call responsibilities and organizational outage windows.

Major Responsibilities:

Technology:
Management, maintenance, updating, and support of Companys WAN and LAN

Identification and correction of location network connectivity issues

Understanding and execution of TCP/IP skills and IP address subnetting

Manage traffic in and out of network hardware (routers) to ensure maximum utilization

Project Management:
Participate in the development and planning of projects.

Identify and recommend solution to meet customer needs, and implement chosen solution.

Manage projects from identification of need to implementation and ongoing monitoring of system/solution performance.

Analytics:
Involved in analyzing system/software performance and adjust network specifications based on any identified problems

Analyze network connectivity to ensure that hardware is utilized at its highest capacity

Perform network analysis and disaster recovery planning and testing for the Companys WAN environment.

Customer Experience:
Communicate with client groups on a regular basis to determine needs

Work closely with customers leadership and project team during scope development and option analysis phase.

Manage the projects and/or solutions to client group expectations

Qualifications:
Associates degree required, advanced degrees preferred

CCNA certification preferred or equivalent experience.

1-2 years of functional experience

1-2 years of project management (including mid-to-large scale projects, and managing multiple projects at one time)

Experience and full fluency in the following hardware/software (depending on role):

-Cisco tools (routers, switches, wireless access points, and WLSE)

-Frame Relay and Site-to-Site VPN implementations

-DNS

-IP Addressing schemes.

-Bandwidth / Circuit monitoring tools.

-Microsoft office applications (including Visio)

-HTML / PHP / MySQL Skills

-Windows Server Administration
UNIX Server Administration

Regular, predictable, full attendance is an essential function of the job.

Willingness to work the required schedule, work at the specific location required, travel as necessary, complete Penske employment application, submit to a background investigation (to include past employment, education, and criminal history) and drug screening are required.

Physical Requirements:

-The physical and mental demands described here are representative of those that must be met by an associate to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.

-The associate will be required to: read; communicate verbally and/or in written form; remember and analyze certain information; and remember and understand certain instructions or guidelines.

-While performing the duties of this job, the associate may be required to stand, walk, and sit. The associate is frequently required to use hands to touch, handle, and feel, and to reach with hands and arms. The associate must be able to occasionally lift and/or move up to 25lbs/12kg.

-Specific vision abilities required by this job include close vision, distance vision, peripheral vision, depth perception and the ability to adjust focus.

Penske is an Equal Opportunity Employer, including individuals with disabilities and protected veterans.

About Penske Truck Leasing

Penske Truck Leasing Co., L.P., headquartered in Reading, Pennsylvania, is a partnership of Penske Corporation, Penske Automotive Group, and Mitsui & Co., Ltd. A leading global transportation services provider, Penske operates more than 260,000 vehicles, employs more than 29,000 people, and serves customers from more than 1,000 locations in North America, South America, Europe, Australia, and Asia. Product lines include full-service truck leasing, contract maintenance, commercial and consumer truck rentals, used truck sales, transportation and warehousing management and supply chain management solutions. Visit http://www.GoPenske.com to learn more.

Work Locations: 100 Gundy Drive Reading, PA

Primary Location: United States-Pennsylvania-Reading

Job: Information Technology

Req ID: 1713689"
Data Engineer - Enterprise Data Science,Micron,"Req. ID: 100843
Were looking for people who are excellent at data warehousing and want to be part of a team who is predicting the future with data science.

As a Data Engineer in Microns Enterprise Data Science team, you will work shoulder-to-shoulder with data scientists and business experts to craft datasets that are rich for mining new data insights. You will orchestrate data feeds from many sources, mash them together, engineer new predictive features, and persist datasets fit for training predictive models. Qualified applicants will be well seasoned in a variety of database technologies and have extensive practice modeling data, querying, and plumbing complex datasets using ETL tools.

Micron Technology operates in a highly competitive industry where innovation depends on talented minds extracting fresh insights from an ever-expanding data universe. We work in a diverse, collaborative environment where problem solving is a team sport and creative solutions are recognized and rewarded. Does this sound like the right team for you? Good news - Were hiring!

We're looking for applicants who have:
Interest in learning data science topics including predictive modeling and machine learning
3+ years of experience data modeling for Data Warehouses
3+ years using ETL tools (Microsoft SSIS, NiFi, Informatica, WhereScape)
3+ years developing with relational databases (MS SQL, Oracle, MySQL)
Experience with data visualization tools (Tableau, Business Objects, SSRS)
Experience with MPP data platforms (Hadoop, Teradata) a plus
Experience with NoSQL databases (HBase, Cassandra) a plus
Excellent teamwork, communication, and problem solving skills

Education Requirements :
A minimum of a B.S. degree (or equivalent experience) in Statistics, Computer Science, Business Analytics, or related field

We recruit, hire, train, promote, discipline and provide other conditions of employment without regard to a person's race, color, religion, sex, age, national origin, disability, sexual orientation, gender identity and expression, pregnancy, veterans status, or other classifications protected under law. This includes providing reasonable accommodation for team members' disabilities or religious beliefs and practices.

Each manager, supervisor and team member is responsible for carrying out this policy. The EEO Administrator in Human Resources is responsible for administration of this policy. The administrator will monitor compliance and is available to answer any questions on EEO matters.

To request assistance with the application process, please contact Microns Human Resources Department at 1-800-336-8918 (or 208-368-4748).

Keywords: Boise || Idaho (US-ID) || United States (US) || SGA || Experienced || Regular || Information Systems and Technology || #LI-JF1 ||"
Data Scientist,JP Morgan Chase,"As an experienced Infrastructure Development professional, your love of technology will have a direct impact on the future of the business. As a senior member of a high-performance team, youll be immersed in all the elements of Software Development Lifecycles- design, development, integration, operation, support and testing of infrastructure services. Youll ensure team goals are met and best practices, architectural design standards, data, risk and security management policies are adhered to. Youll be instrumental in designing, developing and testing code, solving more difficult technical issues, developing integration elements, building data models, APIs, and open 3rd-party SDKs. Youll see your ideas come to life as part of a small, success-driven team. And as part of JPMorgan Chase & Co.s global technology community, youll also have the ability to collaborate with peers around the world to tackle big challenges.

This role requires a wide variety of strengths and capabilities, including:
Significant experience in both Systems Engineering and Software development
Proficiency in at least one of these disciplines:
o Internals of distributed Operating System (Unix/Linux, Windows, Z/OS)
o Systems programming
o Network programming
Experience using large scale software development in Python
Experience using system and software security and entitlements such as SSO, windows, Kerberos, LDAP, Windows AD
Experience with new and emerging technologies such as cloud and virtualization
We are looking for someone who understands the end to end stack of what it takes to use data to drive decisions and priorities including:

Big Data and NoSQL technologies

Extract Transform and Load (ETL) technologies

Distributed streaming platforms

Traditional RDMS technologies

Data Visualizations platforms
As part of the Configuration Management and Automation Services (CMAS) team you will help deliver the next generation of automation frameworks by driving the priorities and decisions through data that will be adopted by all technology teams that transforms how JP Morgan Chase approaches resiliency, operational efficiencies, and adoption of modern DevOps practices.

In the role of Data Scientist, you will be responsible for:
Working with Cassandra, Spark, Kibana, ElasticSearch, Oracle, and Pentaho Data Integration

Working with internal technical operations and risk teams to define problems, data collection, synthesize relevant data, build analytical models and forecasts

Communicate recommendations and drive improvements

Capture requirements, translate customer problems into predictive analytics, clearly communicate results in both written and oral presentations, demonstrate predictive analytics capabilities and solutions to prospective clients

Gather, analyze, and normalize relevant information related to business processes, functions, and operations to evaluate data credibility and determine relevance and meaning

Exhibits expertise in core areas: process knowledge, systems thinking, software development, analytical, collaboration, and technology platform management

Plays a dual role of a data scientist and application developer

Collaborate cross functionality with infrastructure developers, support teams, and other software engineering teams

Working with a team of software engineers and infrastructure developers to deliver automation frameworks that will be adopted by all technologists

Provide recommendations for continuous improvements

2+ years of work experience involving with data and data analysis

Expert in the following technologies

Apache Spark

Cassandra

Kibana

ElasticSearch

Oracle

Pentaho Data Integration or Informatica

Hands-on experience with strong understanding of infrastructure (operating systems, web, middleware, and networking)

Background in an engineering or support role to manage infrastructure

Strong knowledge of Continuous Integration and Continuous Delivery

Experience with Scrum/Agile development methodologies

Capable of delivering on multiple competing priorities with little supervision

Excellent verbal and written communication skills

Computer Science or similar degree with experience in the following software/tools:

Infrastructure automation technologies: Ansible, Puppet, Chef, etc.

Declarative Programming languages: YML, Ruby, etc.

Scripting languages like Python, Perl, Shell, etc.

Build automation technologies: Maven, Jenkins, etc.

Monitoring technologies: Nagios, Tivoli, SCOM, etc.

Use of APIs and services using REST, SOAP, etc.

Our Corporate Technology team relies on smart, driven people like you to develop applications and provide tech support for all our corporate functions across our network. Your efforts will touch lives all over the financial spectrum and across all our divisions: Global Finance, Corporate Treasury, Risk Management, Human Resources, Compliance, Legal, and within the Corporate Administrative Office. Youll be part of a team specifically built to meet and exceed our evolving technology needs, as well as our technology controls agenda.

When you work at JPMorgan Chase & Company, you're not just working at a global financial institution. You're an integral part of one of the world's biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.

At JPMorgan Chase & Company, we value the unique skills of every employee, and we're building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If you're looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
"Data Engineer, Materials Design and Discovery",Toyota Research Institute,"The Toyota Research Institute (TRI) is an R&D enterprise designed to bridge the gap between fundamental research and product development. It has been launched with mandates to (1) enhance the safety of automobiles, with the ultimate goal of creating a car that is incapable of causing a crash; (2) increase access to cars to those who otherwise cannot drive, including those with disabilities and the elderly; (3) help translate outdoor mobility technology into products for indoor mobility; and (4) accelerate scientific discovery by applying techniques from artificial intelligence and machine learning. Toyota believes artificial intelligence has significant potential to improve the quality of life for all people, bringing ease, comfort and safety to all aspects of life.

The long-term vision of the TRI Accelerated Materials Design and Discovery (AMDD) program is to aid in the development of truly emissions free and carbon neutral vehicles for the Toyota 2050 Environmental Challenge. Realizing the vision will require the development of new materials for batteries, fuel cells, catalysts and more. The timescale for new materials development is traditionally measured in decades. Our aim at TRI is to merge advanced computational materials modeling, new sources of experimental data, machine learning, and artificial intelligence to dramatically accelerate the materials design and discovery process. In addition to TRIs growing internal research team, TRI is supporting $35 million in external research contracts with a dozen US universities.

For this effort we need we need a data engineer with a scientific background to design and implement the data models, data warehouse architecture and ETL pipelines for the AMDD program, that integrates the scientific results from different projects and prepare them for machine learning applications. You will join a team of materials scientists with programming skills that works closely with data engineers and data scientists from the TRI core teams. You will collaborate with world class research groups from top universities who work at the front edge of materials research for future energy solutions. You will pioneer the field of cloud materials informatics by applying data models and principles from event sourcing and business intelligence to materials science using cloud technologies.

Responsibilities
Lead the data integration effort within the materials program
Design and implement data pipelines for AMDD in the cloud
Work with our university collaborators to get their data into the operational data store in the cloud
Collaborate with the team to create dimensional models for the presentation area
Create and manage the ETL pipeline that populates the dimensional models from the operational store
Collaborate with the team to maintain a RESTful API for the data warehouse
Explore possibilities for integration with existing online materials databases
Qualifications
Bachelor's degree in Computer Science or the physical sciences coupled with programming experience
5 years of experience.
Strong communication skills. Team player.
Strong ability to write well-factored unit testable code.
Strong interest in materials discovery, AI and ML.
Experience integrating with Cloud APIs especially AWS.
Experience with big data pipeline, and pipeline orchestration frameworks such as Spark, Airflow, Kafka.
Experience with big data stores and related technologies for ingesting, indexing and analyzing large amounts data: S3, Parquet, Alluxio, big data filesystems, Cloudera stack etc."
Data Analyst,Blend Labs,"At Blend, were dedicated to improving lending. Were an enterprise technology company, but our product affects the most important purchase most people will make in their lifetimetheir home. For homebuyers, our product means a clear, guided path to a new home. For lenders, it means modern, easy-to-use tools that let employees spend their time helping customers, rather than on repetitive, manual tasks. By aligning and modernizing this archaic industry, we believe everybody wins.

We need someone whos driven to solve hard problemsthe harder the better. Were motivated by the fact that our product wont just affect the lives of a few people in the Bay Areait affects people all over America, not to mention a foundational part of the U.S. economy. Founded in 2012 by former Palantir leaders, were currently backed by Greylock, Founders Fund, Andreessen Horowitz and other prominent investors.

The Data Analyst will support data needs across our growing company. As an early member of our analytics team, you will help to define how we work with data and influence decision making throughout Blend with your insightful analysis and reporting. You will work closely with our Data Engineering teams, gaining expertise in data analysis, visualization, and infrastructure.

Our ideal Data Analyst is passionate about data, insights, and data-driven decision making. She or he has experience working in SQL and Python, has solid foundations in statistics, and is very effective in communication. You may be a great fit if you work well both independently and collaboratively, take ownership of your work, and can explain complicated ideas to just about anyone.

Blend is an equal opportunity employer that values diversity, inclusion and belonging. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status.

Responsibilties
Create and maintain dashboards to enable data-driven decision making.
Answer ad-hoc analytics questions about our products and operations.
Collaborate with Data Engineers to develop and implement an ETL system for Blends rapidly growing datasets.
Conduct exploratory analysis to discover valuable insights for our organization.
Requirements
Extensive experience working with SQL.
Familiarity with Python is very helpful.
Understanding of fundamental probability and statistical concepts, such as hypothesis testing, maximum likelihood, and basic regression.
Demonstrated the ability to structure complex problems, derive insights from data, and communicate and inspire actions.
BS in a quantitative discipline."
Software Engineer,ALK Technologies,"Were looking for passionate software engineers at all levels of experience, from college graduate to senior-level professional. ALK is an established leader, providing software solutions for the transportation and logistics needs of our customers. Our engineers directly shape our future and have the opportunity to influence products from day one.

This role is important because we are building the next generation of products across all major mobile and desktop platforms while also maintaining our existing revenue-generating products already in the field. We need more talented and versatile developers to help solve a range of interesting problems.

Job Description

Software Engineers at ALK are responsible for developing the structure of a complex, mature software application and be able to locate and repair problems in the code as well as make enhancements within the framework of the existing product. This position will ensure that all code changes meet our quality standards and the needs of our customers, applying knowledge of object oriented software development and experience in C/C++, C#, and .NET. This position will also design and code software, utilizing experience in working with a large established software base to locate problems and develop new features. The Software Engineer in this position will analyze the basic code/project structure as well as basic data structures used throughout our code base.

Qualifications
BS in Computer Science, Computer Engineering, or similar degree field
Excellent coding, debugging, and problem solving skills
Professional or academic experience coding in C++
Experience using object oriented concepts in any programming language
Experience using complex data structures and algorithms to solve problems
Understanding of relational databases and SQL
Desire to automate testing in the interest of development efficiency and product quality
Strong verbal and written communication skills.
ALK is located on Route 1 in Princeton NJ in a new office beautifully designed to support our recent and expected future growth. ALK offers great benefits such as paid healthcare, 401K, stock options, free on-site gym, and fun perks like summer BBQs.

ALK is proud to be an Equal Opportunity and Affirmative Action Employer and considers qualified applicants for employment without regard to race, gender, age, color, religion, national origin, marital status, disability, sexual orientation, status as a covered veteran in accordance with applicable federal, state and local laws, or any other protected factor. EOE/M/F/V/D"
Software Engineer,Teradyne,"Design, implement, test, and provide customer support for software to drive Testers. Design and implement new features, using C#, C++, and COM/ATL. Perform unit testing and verification of code, as well as application distribution and installations. Interact with other cross-functional organizations. Directly support customers as they migrate to newer versions of De-modulation software, and troubleshoot online integration issues in UltraFlex. Interface directly with De-modulation supplier, and provide reproducible test cases. Guide Product owner and Scrum Master and other engineers temporarily brought onto project.

Requirements include a Bachelors degree or equivalent in Computer Science, Computer Engineering, or related field and five years of work experience in the job offered or related field of electronic test and measurement equipment; or, in the alternative, a Masters degree or equivalent and three years of work experience.

If qualifying by bachelors degree, must have at least three years experience working in an agile software development environment. Must have at least four years experience with developing software applications, Windows internals, application distribution, and distributed computing. Must have at least four years experience with Object-Oriented Design and software data structures.

If qualifying by masters degree, must have at least two years experience working in an agile software development environment. Must have at least three years experience with developing software applications, Windows internals, application distribution, and distributed computing. Must have at least three years experience with Object-Oriented Design and software data structures.

Applicants must have unrestricted authorization to work in the United States.

Work Location: North Reading, MA"
VOICE/DATA COMMUNICATION ENGINEER (ENTRY-LEVEL),TWD,"SUMMARY:
The Entry Level Voice/ Data Communications Engineer will Monitors and responds to complex hardware and software problems. Interfaces with contractor support service groups to ensure proper escalation during outages or periods of degraded system performance. Maintains various systems including Data VOIP, wireless and VTC.

ESSENTIAL DUTIES AND RESPONSIBILITIES:
 Be responsible for analyzing and developing wireless telecommunications capabilities for various operations. Design and build custom applications.
 Monitor and analyze statistical and drive test data to improve system performance indicators such as dropped calls, blocked calls, origination failures, handoff features.
 Provide infrastructure support to ensure high availability, security and usability of the data and voice network.
 Support and maintain existing Voice and data network.
 Produce documentation to a high standard.

EDUCATION and/or EXPERIENCE:
Required:
 Bachelors Degree in Computer Science, Information Systems or other related field, or an equivalent combination of experience, education, and / or training.
 5 years of IT experience including 3 years in a voice discipline
 Voice or Data Network Operations or Engineering experience.
 Excellent problem solving skills required.
 Must have strong writing abilities, presentation skills, and interpersonal skills

Federal Background Investigation REQUIRED please read.
The selected candidates for this position must be able to successfully obtain and maintain a favorably adjudicated government background investigation. This background investigation requires U.S. citizenship and will include an investigation of your financial, education, employment, medical, residence, and personal records. The cost of this investigation will be paid by TWD & Associates."
"Data Scientist, Manufacturing Analytics",Bosch Group,"Job Description

Robert Bosch is a world-class manufacturer with over 200 plants and thousands of assembly lines world-wide. Our products impact hundreds of millions of people every day, many in safety critical systems. We rely on data for every aspect of our manufacturing operations and we collect a lot of it.

Our team is responsible for streaming Bosch manufacturing data to centralized analytics platforms and building data-based services for a wide variety of Bosch engineering teams.

We are looking for a talented engineer who is passionate about building fault-tolerant data services and analytics tools. Your work will be used by hundreds of Bosch engineers and have global impact by improving the quality and value of Bosch products.

Primary Responsibilities:
Collaborate with other data scientist and data engineers to design, prioritize and implement new features for our AI platform to solve real customer problems
Write clear, maintainable and tested code
Work with manufacturing engineers to drive adoption of our data services
Up to 10% travel may be required. Qualifications
MS or PhD in computer science, or related quantitative technical field
3+ years experience with Python (Scikit-learn, Pandas, Numpy)
3+ years applying probabilistic, statistical and/or machine learning methods to solve real-world problems
1+ year experience developing and operating customer-facing data services
1+ year experience using distributed systems to process large volumes of data (e.g. Apache Spark / Hadoop)
experienced practitioner of software development best practices
proficient with Linux & Bash scripting
entrepreneurial and customer-oriented mindset
excellent communication skills
Additional Information

BOSCH is a proud supporter of STEM (Science, Technology, Engineering & Mathematics) Initiatives
FIRST Robotics (For Inspiration and Recognition of Science and Technology)
AWIM (A World In Motion)
By choice, we are committed to a diverse workforce - EOE/Protected Veteran/Disabled."
"Senior Data Engineer, Emerging Technologies Elasticsearch",Discover Financial Services,"As Discovers Business Technology organization continues to innovate with new open source technologies, a new opportunity has been opened on the Emerging Technologies team in the Enterprise Architecture group.

*** The position will initially be based in our Riverwoods, Illinois Headquarter. However it is possible that the position will be re-located to our downtown Chicago office. As it stands, the team is working 3 to 4 days per week in the downtown office. ***

This position will join a team focused on innovative data engineering solutions. The initial focus for this role will be to deliver a centralized logging data lake solution, working closely with our partner teams to ensure they are successful in on-boarding. As a Sr. Associate, you will be responsible for implementation of solution as well as ongoing operational support.

There will be opportunities to promote Discover and its innovative solutions (i.e., Developer API) that this team will be involved in driving forward, so we expect this individual to be comfortable in these outreach responsibilities.

Other Responsibilities:

 Support of the Centralized Logging Service (CLS) and assistance in the design and implementation of the solution, based on Elasticsearch, Logstash, Filebeats, Kafka and Nifi services.
 Maintain a high level of competency in big data solutions and aligned technologies. i.e. ElasticSearch, Kibana, Logstash, and the Hadoop ecosystem.
 Develop self-service capabilities by which applications will board the service.
 Take an agile pro-active approach that delivers improvements to the organization in a timely basis.
 Identifying opportunities to innovate on top of the CLS data, which promote skills in Data Analytics.
 Delivering new capabilities with new technologies (i.e., machine learning) that deliver additional value.
 Develop data science expertise.
 Provide technical leadership to application developers and other internal stakeholders regarding all aspects of Centralized Logging Solution.
 Perform strategic planning, upgrades, maintenance and support.
 Actively contribute to and drive creation of artifacts for CLS Architectures, Design Considerations and Best Practices.
 Maintain working knowledge of alternative Log Analytic solutions (i.e. Splunk, Elastic Filebeats/Logstash and Apache Metron), as well as cloud-based offerings.
 Promote a risk-aware culture, ensure efficient and effective risk and compliance management practices by adhering to required standards and processes.

Skills

 Bachelors Degree
 2+ years of hands on experience with ElasticSearch and Kibana.
 1+ years of hands on experience with Kafka.
 Experience supporting a product or service provided to a development community.
 Experience in a support based operations capacity, middleware or web is preferred.

 BIG Data experience preferred. Chef experience beneficial.
 Exposure to: Apache NiFi, Hadoop (Hortonworks preferred), Kafka, Java (experience supporting, with tuning and stability experience preferred).
 Experience with Amazon Web Services (AWS) would be beneficial.

About Us
Since Discover launched in 1986, we've made a business of redefining the credit card industry for the better. We were first to have no annual fee, first to offer cash rewards, first to provide 24/7 customer service, and we rank #1 in customer loyalty. As a Fortune 500 company, we strive to provide our employees with an environment where they can contribute in a way that's different from other large institutions. If you're looking for a place to make a difference and let your voice shine through, we'd love to meet you.

Apply today at discover.com/careers.
Discover Financial Services is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, age, protected veteran status, among other things, or as a qualified individual with a disability.

#LI-VF1

We are an Equal Opportunity Employer and do not discriminate against any employee or applicant for employment because of race, color, sex, age, national origin, religion, sexual orientation, gender identity, status as a veteran, and basis of disability or any other federal, state or local protected class."
Sr. Data Engineer,"Pandora Media, Inc.","At Pandora, we're a unique collection of engineers, musicians, designers, marketers, and world-class sellers with a common goal: to enrich lives by delivering effortless personalized music enjoyment and discovery. Peoplethe listeners, the artists, and our employeesare at the center of our mission and everything we do. Actually, employees at Pandora are a lot like the service itself: bright, eclectic, and innovative. Collaboration is the foundation of our workforce, and were looking for smart individuals who are self-motivated and passionate to join us. Be a part of the engine that creates the soundtrack to life.

Discover your future at Pandora.

Job Details:
Data engineers at Pandora are responsible for the services and infrastructure capable of processing and making available the extensive volume of data produced on its platform each day. Data Engineers build the infrastructure needed to enable analysts and scientists to query and author data products that operate against our largest collections (billions of events per day). At Pandora the data team supports a variety of business functions including our science, marketing, product, finance and sales teams. You should have a solid understanding of Java software development, and take personal responsibility for testing the code you write. You should have strong academic credentials and a degree in Computer Science or a related field. You should be enthusiastic about learning new technologies and skills. You must be capable of managing your time well and working collaboratively. Excellent communication skills, both written and verbal, are required.

Requirements:
5+ years development experience of which 2-3 years are focused on data or analytics engineering working with big data technologies (Hadoop: i.e. MapReduce, HDFS, Tez, Hive, Spark)
Experience with one of the following distributed databases: MySQL, Postgres, Redis, NoSQL or NewSQL
Experience developing in one of the following: Java, Scala, C/C++, or Python
Experience developing for Linux-based deployment platforms, developing scalable, multithreaded server side software for deployment
Experience developing SQL applications of significant complexity
Experience developing service oriented architectures/orchestration including the support of data science
Experience with API design/development (i.e. RPC, REST, JSON, XML, SOAP)
Significant experience unit testing with frameworks i.e. JUnit

We're considering candidates for multiple positions and levels. Successful candidates will be placed at appropriate level depending on their qualifications or experience.

Pluses:
Experience collaborating with data scientists, exposure to machine learning algorithms and/or statistical modeling methods.
Experience with recommender, or search systems.
Experience with Apache Spark and Kafka.
Experience working across the full technology stack
BA/BS or above in Computer Science or a related field
Pandora is committed to diversity in its workforce. Pandora is an equal employment opportunity employer and considers qualified applicants without regard to gender, sexual orientation, gender identity, race, veteran or disability status. Women and people of color are encouraged to apply.

Pandora is also a VEVRAA federal contractor. Pandora requests priority referrals of protected veterans from each ESDS, as required by regulation.

If you believe you need a reasonable accommodation in order to search for a job opening or to apply for a position, please contact us by sending an email to disability@pandora.com

In your email, please include the following:
The specific accommodation requested to complete the employment application.
The location or office to which you would like to apply
The subject of the email should read ""Request for Reasonable Accommodation""."
Data Engineer - Machine Learning,Susquehanna International Group,"Overview

In this role, you'll be working as a data engineer with a team of quantitative researchers. This group will utilize machine learning techniques to capitalize on trading opportunities for our equities, futures, and options products and will be responsible for spearheading the application of deep learning to our daily trading activity.

Additionally, you will collaborate with other researchers, developers, and traders to improve existing proprietary strategies and develop new trading algorithms that analyze and optimize our performance in capital markets. You will use your software development and data mining skills to build data sets, data quality metrics, and automation tools to enhance our research and system development.
What we're looking for

Students pursuing a degree in Computer Science, Statistics, Machine Learning, Applied Mathematics, or a similar major
Strong programming skills - Python or C++ is preferred in this role
Interest in applying machine learning/deep learning theories in a professional research environment
Visa sponsorship is available for this position.

We dont post salary ranges externally so any salary estimate you see listed on a third party website was not provided by SIG and may not be accurate.

SIG is not accepting unsolicited resumes from search firms. All resumes submitted by search firms to any employee at SIG via-email, the Internet or directly without a valid written search agreement will be deemed the sole property of SIG, and no fee will be paid in the event the candidate is hired by SIG."
Software Engineer,Philips,"In this role, you have the opportunity to

Be a part of Philips Wellcentive, a leader in population health that helps customers provide care management for more than 30 million patients and achieve more than $500 million annually in value-based revenue.

You are responsible for

Responsible for making Java enhancements, bug fixes, and code support to a complex Software as a Service (SaaS) platform, while also utilizing your scripting skills on a healthcare data interface - specifically, integrating disparate data sources into a normalized cloud based patient-centric repository so that the quality of care can be measured across large populations.

As you learn the business and become more proficient with your development, you will be fast-tracked into more challenging architecture and new development projects.

In this position, you start off by diving into a wide array of file formats searching for and correcting data quality issues. You will be working closely with external client customers, so excellent communication skills are critical .

You are a part of

A team that will be able to help accelerate the technologies used at Philips Wellcentive and provide valuable insight from the position. We are looking for sharp, passionate, and hard working individuals who not only want to make an impact, but also grow their technical and leadership skills.

To succeed in this role, you should have the following skills and experience:

Knowledge of Agile development methodology
Experience working in an Agile/Scrum development team i.e. ceremonies
Coding ability in Java/J2EE/JavaScript
Exposure to RDBMS (MySQL, Oracle or SQL Server)
Knowledge of Unix/Linux
Experience with REST

Preferred:
Healthcare experience
File formats (HL7, CCD, CCDA)
Interfacing
Scrum Master experience
Certified Scrum Master
Why should you join Philips?

Working at Philips is more than a job. Its a calling to create a healthier society through meaningful work, focused on improving 3 billion lives a year by delivering innovative solutions across the health continuum . Our people experience a variety of unexpected moments when their lives and careers come together in meaningful ways.

To find out more about what its like working for Philips at a personal level, visit the Working at Philips page on our career website, where you can read stories from our employee blog . Once there, you can also learn about our recruitment process , or find answers to some of the frequently asked questions .

Philips is an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, age, sex (including pregnancy), sexual orientation, gender identity, national origin, genetic information, creed, citizenship, disability, protected veteran or marital status.

As an equal opportunity employer, Philips is committed to a diverse workforce. In order to ensure reasonable accommodation for individuals protected by Section 503 of the Rehabilitation Act of 1973, the Vietnam Veterans' Readjustment Act of 1974, and Title I of the Americans with Disabilities Act of 1990, applicants that require accommodation in the job application process may contact 888-367-7223 , option 5, for assistance.

#LI-PH1

#DICE

Contact

If you forgot your password, you can click the Forgot Password button on the Sign In screen to reset it. In case of technical difficulties, please send an email to careersite@philips.com .
(Note: To ensure fairness and legal compliance in our recruitment processes, only technical issues will be monitored through the above inbox. Please do not submit resumes or applications to this email, as they will not be reviewed. Only applications received through the online application process will be considered.)"
Software Engineer,Belden Inc.,"The software engineer will work with other engineers, both development and QA, as well as other departments as part of an Agile team to design, code, build, test, and deliver either prototype or production security software for Tripwires Vulnerability Management solution. This position will have the opportunity to work with front-end, back-end, and database code as well as cross team integrations.

Key Responsibilities & Duties:
Development will be primarily in Python and C++, with some C. You must be very strong in one of these languages.
You must be comfortable targeting a broad range of operating systems. The primary development platform will be on Linux.
Ideally, you will be proficient with SQL, database design and designing data structures that perform well under scale and load.
Must be self-directed and capable of working effectively in a highly collaborative and fast paced environment.
Must be effective working in less structured, smaller team environments, taking responsibility for helping define requirements and rapidly prototyping ideas.
Support the full range of day-to-day priorities of the team, primarily focused on software development and implementing new security research ideas.
Contribute to testing, documenting, supporting build systems, interacting with stakeholders, etc. as needed on a small team.
Must have the legal and ongoing right to work in the USA without sponsorship
Qualifications:
BA/BS CS, EE, IT or equivalent industry experience
3-10 years product development experience using Python, C, and C++
Javascript, Angular and UI development knowledge helpful
Use of source control systems  SVN, GIT  very useful
Hands-on knowledge and configuration of multiple operating systems (i.e. Unix, Linux, Windows, and Mac) required
Knowledge of network protocols and hands-on configuration of network equipment extremely helpful
Knowledge and experience in the field of security
Agile process based on the SCRUM framework
You will demonstrate clear thought processes and communicate well with technical and non-technical audiences.
You should thrive in a collaborative environment.
You should be a creative problem-solver, providing thoughtful approaches to challenging technical problems that solve real business needs.

City:
Alpharetta
Location:
Alpharetta, GA"
Software Engineer,InComm,"Overview
Leveraging deep integrations into retailers point-of-sale systems, InComm provides connectivity to a variety of service providers that allow consumers to conduct everyday business at more than 450,000 points of retail distribution worldwide. Whether those consumers are activating prepaid products, paying bills, enjoying real-time discounts through a membership card, purchasing digital goods in-store or adding funds to an online account, InComm is there to provide unique gift-gifting opportunities, cater to on-the-go shoppers, deliver added value through loyalty programs and serve cash-based consumers. With 186 global patents, InComm is headquartered in Atlanta with a presence in over 30 countries in North and South America, Europe and the Asia-Pacific region. Learn more at www.incomm.com or connect with us onwww.twitter.com/incomm, www.facebook.com/incomm, www.linkedin.com/company/incomm or www.incomm.com/blog.

About This Opportunity
The Senior Software Engineer will lead, plan, conduct, and coordinate software development activity. Designs, develops, documents, debugs, and deploys software releases, as well as providing mentoring to junior members of the team. This particular position shall focus primarily on development of fulfillment applications using a Java technology stack, but experience within a Mule/Apache CXF technology stack is also desirable.
Responsibilities
Lead Development and Support of Java based applications
Ensure product requirements adhere to overall development best practices in their format, documentation, communication and implementation as directed by the manager.
Work together with the team to add new features and optimize our code and database to improve scalability
Technical design and analysis of business use cases in preparation for development.
Document in code and contribute to team and project wiki spaces for delivery to platform owners and deployment personnel.
Provide unit testing to ensure quality builds in a continuous integration environment.
Support QA in automation and integration testing.
Ensure the best technologies are implemented for the highest impact of success of the product's vision.
Provide guidance to junior members of the teams
Qualifications
5-10 years of progressive Java development experience including JSE, JEE in enterprise level scalable, distributed and fault tolerant environments, JMS
Bachelors Degree in Computer Science, Engineering or equivalent work experience
Experience participating in the full lifecycle of projects, including effective use of version control, build management, unit testing, and issue tracking software (ex. SVN, Maven, JIRA, Jenkins)Experience in generating estimates & deliverables (E&D) based on requirements
Experience with Java frameworks such as Spring and Apache CXF
Experience in developing Web Services (ex. RESTful or SOAP) and strong experience utilizing XML
Experience with high volume transaction processing design and development
Experience developing within a Service Oriented Architecture environment
Experience with data modeling methodologies and master data management
Experience working within PCI and SAS70 environments
Experience with application frameworks like JSF, Spring MVC, Hibernate
Experience developing using Agile practices (Specifically XP, Scrum and TDD)
Experience using Queuing technologies like RabbitMQ, ActiveMQ etc
A proven track record in task delivery
Candidate must be a highly motivated individual and willing to bring energy and enthusiasm to the position
NICE TO HAVE
Previous experience with transaction or financial services (Pre-paid card, bank card, credit card).
Experienced with the following technologies: MS SQL Server, MySQL, Eclipse, Maven, JUnit, JMeter, Mule, Apache CXF and Subversion.

InComm is an equal employment opportunity employer. We adhere to a policy of making employment decisions without regard to race; color; religion; national origin; sex; sexual orientation; pregnancy, childbirth, or related medical conditions; age; disability or handicap; citizenship status; service member status; or any other category protected by federal, state, or local law.

This position is eligible for the Employee Referral Bonus Program - Tier 4"
Software Engineer,Cox Automotive,"Description
Looking for a rock star developer! Do you love manipulating large data sets? Are you an expert programmer in Java with experience transforming metrics using Hive, Python, Spark, and other tools? Do you have the ability and desire to learn new technologies and build prototypes in days?

If your answers are Yes! we want to talk to you!

Description:
We are looking for a Software Engineer with Business Intelligence experience to join our Media Analytics team at Cox Automotive. This Software Engineer will work with their associated delivery teams to solution projects using Java, Python, SQL, AWS, and internal tools on Hadoop and Netezza. As a fully functioning scrum team member, you will be responsible for a variety of tasks needed to get the work completed including analysis, design, programming, and testing. In addition our scrum teams own their code so you will be responsible for product support as well as defect handling.

Specific Responsibilities include, but are not limited to:
Gather and process raw, structured, semi-structured, and unstructured data at scale, including writing scripts, developing programmatic interfaces against web APIs, scraping web pages, processing twitter feeds, etc.
Design, review, implement and optimize data transformation processes in the Hadoop (primary) utilizing Java, Python, Scala, and Hive
Design, review, implement and optimize data transformation processes in Oracle and Netezza utilizing ETL/Informatica
Analyze data needs and recommend data models or changes to data models while supporting reporting
Ability to turn data into metrics and reporting through visualization tools
Create solutions for data movement or transformation utilizing Java, Python, Scala, and/or Hive
Test and build working prototypes for new data integration tools
Active participant in the working of all assigned tasks/projects with the ability to multi-task between projects and assignments easily
Practice active ownership in their assigned pieces of functionality and ensure successful on time quality completion and maintenance
Respond quickly and effectively to production issues and taking responsibility for seeing those issues through to resolution
Proactively communicate project status to program team and delivery team
Mentor less experienced team members
Ability to collaborate with other development teams
Expected to present project replays and relevant external learnings to local team in the form of Knowledge Transfer sessions
Stay current with changes in the technical area of expertise by being actively engaged in both internal and external learning opportunities to provide better solutioning Qualifications

Bachelors degree in Computer Science, Information Services, Mathematics, Statistics, or other applicable area from a four-year university or equivalent industry experience

4+ years maintaining and developing software in a large scale data or web application environment like Oracle and Netezza

3+ years in business intelligence including big data and a more advanced understanding of EDW concepts of warehousing, data movement, and data transformation

1-2 years developing for Hadoop environment including HDFS, Hive, MapReduce, Spark, and Distributed Computing

2+ years in Agile methodology

2+ years experience in data ingestions including streams and batch processing

Expert knowledge of object oriented analysis, design, and programming in Java or Python. (Your interview will include coding scenarios designed to test your analytical skills as well as your coding capabilities.)

Solid understanding of software engineering basics including data and architecture in areas like site activity

High performer with the ability to demonstrate their higher-level technical experience on a daily basis Preferred Qualifications

Cloud: AWS, Goggle

ETL: Informatica, Mulesoft

Services: Rest API development, API frameworks

Messaging: Kafka

Others: Dockers, CI/CD

About Cox Automotive

Cox Automotive is transforming the way the world buys, sells and owns cars with industry-leading digital marketing, retail, financial and wholesale solutions for consumers, dealers, manufacturers and the overall automotive ecosystem worldwide. The Cox Automotive family includes Autotrader, Dealer.com, Dealertrack, Kelley Blue Book, Manheim, NextGear Capital, vAuto, VinSolutions, Xtime, incadea and a host of other brands. With a complete and connected view of the automotive ecosystem, Cox Automotive is a global company connecting people, cars and capabilities with services that link the automotive value chain. Cox Automotive is a subsidiary of Cox Enterprises. For more information about Cox Automotive, visit www.coxautoinc.com .

Organization: Cox Automotive

Primary Location: US-GA-Atlanta-3003 Summit Blvd

Employee Status: Regular

Job Level: Individual Contributor

Shift: Day Job

Travel: No

Schedule: Full-time

Unposting Date: Ongoing"
Software Engineer,Bridgevine Inc.,"Join our dynamic Technology Team and make an impact at Bridgevine Inc., the leading provider of customer acquisition solutions for iconic home service brands at the Alpharetta, GA office. We offer an invaluable opportunity to gain exposure to all facets of our business  we work hard and we play hard. And well expect you to do both.

Position Overview

We are seeking a .NET engineer to contribute to our Agile based development team. This position requires an individual comfortable with working in a large code base with a natural curiosity to dive in and understand the components theyre modifying. This role will provide a good mix of feature development, bug fixing, refactoring, investigation and deployment. The engineering team will slowly be introducing newer SOA based architecture leveraging .NET Core and AWS. This is a relatively small engineering team (4 to 5 engineers) so the opportunity to make an impact is large. Experience working with SaaS solutions and integrating with APIs will be helpful for this role. We believe in Agile/CI/CD practices and tools to help facilitate that methodology. We use TeamCity and Octopus for CI/CD and Github for source control.

Responsibilities
Be an active collaborator in our Agile based team.
Assist in new feature development and bug fixes.
Contribute to peer design reviews, leveraging experience to propose solutions for upcoming projects.
Propose refactoring solutions to encourage extensibility and maintainability.
Create unit tests to confirm functionality changes.
Assist business analysts in behavior discovery.
Code against external APIs and perform calculations and business logic on responses.

Requirements
Bachelors Degree in Computer Science or combination of equivalent experience.
.NET, C# 3.5 and 4.0
Understanding of API development and SOA
Proficient in XML/SOAP
Proficient in Lambdas and LINQ
Proficient in data modeling, stored procs with MS SQL Server.
Proficiency with the .NET inheritance model.
Experience with NoSQL solutions such as MongoDB is a plus.
Experience in distributed systems, cloud based architectures is a plus.
Experience with JavaScript is a plus

Company Overview

Bridgevine powers customer acquisition solutions through its next generation technology platform. We thrive in a digital market where product and service supply from leading service brands intersects with residential and SMB customer demand. Through a blend of online direct marketing and proprietary distribution channels, over 8 million qualifications each month transact in Bridgevines vast marketplace. Participating partners in the telecommunications, cable, satellite, home security, and energy sectors delight in acquiring new customers, building brand loyalty and unlocking new revenue streams. Our technology platform has generated over $1.9 billion in annual recurring revenue for business partners.

Culture

Bridgevine seeks energetic thought leaders and knowledge workers who thrive on exciting assignments and working alongside smart, humorous and dedicated co-workers. Our roots are firmly planted in our core values, which inspire us every day to be Collaborative, Driven and Inquisitive.

Bridgevine, Inc. is an Equal Opportunity Employer. This position is based out of Alpharetta, GA. No relocation assistance provided."
Software Engineer,MModal,"WE ARE
M*Modal, a fast-moving technology company in healthcare industry. Our conversational speech recognition and natural language understanding technologies are widely recognized as the most advanced in the industry. We are the leading innovator in the field of conversational documentation services (CDS) where speech recognition and natural language understanding are combined in a unique setup targeted to truly understand conversational speech and turn it into actionable and meaningful data. Our proprietary speech understanding technology  operating on M*Modals computed grid hosted in our national data centers  is redefining the way clinical information is captured in healthcare.

We are looking for a talented, highly motivated and technically excellent full time Software Engineer to join a team of engineers working on advanced products in our Coding Solutions Organization. We are seeking an Engineer who enjoys learning new technologies and creating innovative solutions to solve a broad spectrum of problems.

YOU ARE
 Creative, curious, friendly and flexible
 Have excellent communication skills, both verbal and written
 Able and willing to learn new technologies in short time and discuss them with competence
 Self-motivated and able to work effectively with minimal instruction and supervision
 Able to work on multiple products at the same time and quickly switch contexts
 Interested in healthcare IT
 Known for your critical thinking, reasoning and problem solving

YOU WILL
 Work hand-in-hand with internal and external customers or stakeholders
 Architect, design, develop, test and deploy software solutions
 Create and maintain design documentation
 Present your work during technical workshops
 Participate and contribute towards the improvement of the software development process

YOU SHOULD HAVE...
 Good understanding and practice with data-structures, algorithms, paradigms and design patterns
 Experience developing software in an object-oriented programming language (C# or Java preferred)
 Experience with large-scale web development using JavaScript, jQuery, HTML5, CSS, AJAX, ASP.NET, and SQL
 Experience creating components using a modern JavaScript framework or library such as Angular or React
 Good understanding of networking and systems design
 Experience developing software utilizing modern software engineering practices
 B.S. or M.S. degree in Computer Science or related engineering field
 Strong communication and presentation skills
 NOTE: These qualifications are only a guideline. We encourage you to apply even if you dont match all the requirements if you feel this is the right job for you!

IT WOULD BE AWESOME IF YOU ALSO...
 Have experience using React with MobX or Redux
 Have experience with TypeScript, ES6, webpack, single page applications (SPAs), node, and npm
 Have experience in healthcare IT and/or medical coding
 Have familiarity with HL7 standards and interfaces
 Have experience in machine learning or big data systems

A LITTLE MORE ABOUT US...
 Our office is located on the north side of Atlanta.
 We have a relaxed dress code, except when customers/visitors are onsite.
 We ask employees to report to the office during normal work hours, though we do offer flexible hours and the ability to work from home after establishing sufficient product knowledge.
 We offer a competitive salary based on experience."
"Software Engineer-(ETL""Ab Initio"",Talend & Big Data )",JP Morgan Chase,"The Software Engineer is responsible for working on an Agile development team to determine the changes needed to the data model , databases along with the necessary ETL coding and implementation for regulatory reporting. Responsible for design and development of various ETL processing using Oracle PL/SQL, Ab Initio and Unix Shell scripting. Responsible for writing complex SQLs for data analysis and data profiling. Knowledge of database architecture/practices is essential. Experience with Business Intelligence (BI) tools such as Tableau and Bigdata querying & ETL tools is preferred. This is an individual contributor that may have team lead responsibilities for 1-4 people.

Specific responsibilities/skills:
Advanced experience & demonstrated proficiency in all aspects of Ab Initio tool set to create graphs, using performance improvement components, parallel processing, graph tuning, single and multi-file processing, metadata processing using EME etc.

Significant experience & proficiency in all aspects of Oracle PL/SQL programming and data analysis, including Cursors, Ref-cursors, Oracle procedures/packages, Collections, Partitioned Tables, Triggers, Table Indexing. Extensive experience in Complex SQL Queries, Dynamic SQL, Analytical function, various Oracle Objects, etc.

Strong experience on database design, best architecture practices, normalization and dimensional modeling etc.

Experience working on Agile methodologies as developer.

Preferred experience with Bigdata technologies and tools such as Apache Hadoop/Cloudera/MapR/HortonWorks, HDFS, MapReduce, Spark, Big Data querying tools (such as Pig, Hive, Impala), NoSQL databases (such as HBase, Cassandra, MongoDB)

Preferred experience with Business Intelligence tools such as Tableau

Fundamentals and Solid facilitation skills to gather the necessary (functional & technical) information to document and/or develop a strategic/tactical plan. Able to properly document (functional & technical) source to target mappings, process flows, data flows, technical specification and unit test document

Work independently in the design, development, testing, implementation and maintenance of systems of moderate size/complexity with a fast turn-around

Perform extensive business process analysis including data analysis and GAP analysis

Apply innovation, creativity, and improvement to the development process

Problem-solving, keep abreast of new technologies

Serve as a process expert for a defined functional area, which includes:

Managing key data sources & inputs

Planning and ensuring implementation of end state automation activities

Providing guidance for the design, development, and implementation of automated processes

Required Qualifications:
Bachelors degree in Computer Science, Engineering, Information Technology or related area

5+ years experience in Distributed Multi-tier Application Development, Database Design, Data processing, Data Warehouse and Support mainly with Oracle technology on different platforms

Candidates must have advanced experience with Oracle SQL, PL/SQL, Data Analysis and Performance Tuning. Hands on experience of Tools like TOAD, SQL Developer, and SQL Loader.

Advanced hands on experience at Ab Initio ETL design and development including EME and BRE

Experience with BigData fundamentals, querying and ETL technologies

Working experience as Agile developer and good understanding of SDLC methodologies/guidelines

Experience at developing complex UNIX shell scripts

Experience with Subversion or similar source code versioning tools and coding standards

Experience with scheduling tool such as Control-M or similar tools

Preferred Qualifications:
2-3 years of Mortgage/Banking/finance analytical support experience preferred, but not mandatory.

Experience with Talend ETL tool

Experience with Tableau or Cognos BI tools"
Software Engineer,IMC Financial Markets,"Trading nowadays happens in a highly competitive technological landscape; the best trading idea alone doesnt cut it anymore. Instead, only the best trading ideas that are enabled via robust, scalable and fast technology win.
Do you enjoy the process of problem solving, a process where you recognize areas of improvement and iterate and innovate to improve? Does your curiosity and desire to learn drive you?
SOFTWARE ENGINEERING AT IMC:
We trade in the most competitive markets across the globe. Our software makes millions of trading decisions daily. We win by making better and faster decisions than our competition. As a software developer youll be working alongside traders to implement strategies which will challenge you to be creative with your implementation. Youll need to produce solutions which compete in the extremely latency sensitive environment, whilst managing complexity.
Youll be working in a highly motivated team, controlling end-to-end product development. We work in an environment where slight oversights can be extremely costly, while new features can be extremely profitable. As a team we aim to balance the race to production with software quality and testability. Youll be placed in an environment consisting primarily of C++ and Java. We have a build chain which has been designed around the concept of daily production releases. A variety of systems and technologies keeps life interesting.
Upon joining you will be expected to manage your own time and tasks. We rely on our developers to drive change by taking feedback from traders, and shaping that into an implementation. Your ownership of the task continues until a working production release. Our most successful developers comfortably work in any part of our stack
WHAT MAKES IT FUN?
One of the most fun aspects of the job is that we have a very quick feedback loop. We release early and often to get incremental benefits as they are developed
We operate at the bleeding edge of technology. If something new can potentially bring an advantage we will actively invest in developing and utilizing the solution
We really believe in sharing knowledge and technology between the different offices. Much of our technology stack is shared between all 3 regions, and we provide opportunities to travel between the regions both for personal growth and to assist where it has the biggest impact.
To get the best results we encourage everybody to gain understanding of both the trading strategy and how exchanges work on a technological level.
Working at IMC is also an opportunity to learn about financial markets. We know from experience that a lot of people really enjoy learning about a field beyond their immediate area of expertise, its one of the things that makes this job more interesting than others.
We employ a broad range of people with varying backgrounds. What they have in common is their superior technical expertise, their extraordinary smarts and their collaborative approach. We will not accept less.
WHO YOU ARE:
Have at least 2 years of professional work experience in software development post graduation
Excellent Java or C++ programming skills
Development experience in a Linux/UNIX environment
Extensive experience with OOP/OOD
Strong knowledge of algorithms, data structures, and threading
Proficiency in design patterns
Scripting knowledge
Non-relational, distributed database experience is a plus (not a requirement)
Strong analytical skills and desire to solve complicated problems programmatically
Must be self-directed, and able to work productively under minimal supervision
Willingness to work closely with traders in a constantly changing environment
Interest in the Financial Markets; previous knowledge is NOT required
Anyone graduating in 2017 or 2018 should apply through our graduate application process found on our website; please do not fill out an application through this posting if you're graduating in 2017 or 2018
OUR CULTURE:
We are at the core a trading firm, however we value trading and technology equally and we believe that cooperation between traders and technologists is one of our great strengths. This is also reflected in our organizational and remuneration policies. We believe in fostering a truly flat environment in which great ideas can be recognized as well as put into practice from anybody within our organization
WHO WE ARE:
IMC Financial Markets is among the worlds leading proprietary trading firms, and a market maker in securities listed on exchanges across the globe. Our cutting-edge technology drives everything we do. High performance algorithms, smart strategies and collaborative teams are the core of our business.
Today, IMC Financial Markets is 500+ people working together to build software and trade financial products in our offices in Amsterdam, Chicago and Sydney. What does this mean for you? The chance to join a multi-national, multi-cultural team of exceptional individuals, focused on making IMC the worlds best trading firm."
Data Engineer - Big Data Analytics & Data Warehouse,Penske,"The Data Engineer should be an expert familiar with all of the data warehousing technical components (e.g. ETL, reporting, and data Model) infrastructure (e.g. hardware and software) and their integration. The ideal candidate will be responsible for developing overall architecture and high level design. The candidate must have extensive experience with star schemas and dimensional models in traditional Data Warehouses (DW) as well as in Big Data/Advanced Analytics domains. He/she will be implementing data summarizations models for analytics on Greenplum/big data environment. Using this model, he/she will recommend best practices for data loading, data integration and analytics implementation on Greenplum/ (Postgres) Big data environment.
Major Responsibilities:

- Model, summarize and integrate data in Big data environment.

- Create and implement efficient DW/summary for analytics usage .This should have efficient data integration, transfer and summary strategy on GP database.

- Lead/support Big Data Platform initiatives for the enterprise specifically related to data analytics on GP platform.

- Make recommendations and give expert advice to management on topics related to: GP Data Platform Architecture, Tools, Technologies and Infrastructure, analytics

- Manage expectations of internal and/or external customers. Handle complaints and offer meaningful short and long term resolutions.

- Support all the groups with various needs of data analytics.

- Create strategy to ingest and presentation of data of Green plum or other big data environment.

Qualifications
 B.A. /B.S. Computer Science, Mathematics, Engineering, or equivalent work experience required
 Minimum 5 years work experience as an Green Plum or MPP databases, and
Big Data
 Must have comprehensive knowledge of database technologies
 Background in relational databases like Oracle/Teradata is helpful.
 Experience with Linux scripting is required (Perl and/or Korn shell/Bourne Shell/ Python)
 Understanding of basic SQL commands necessary to perform administrative functions.
 PostgreSQL is a plus
 Knowledge in Greenplum/Postgress architecture, configuring Greenplum system
defining and managing the database objects and data in Greenplum is a plus.
 Managing the Schema, Tablespace and Partitioning in Greenplum or any other MPP databases is a plus.
 Knowledge in the following is very beneficial:
Environments: Amazon Web Services, Cloud Foundry and vCloud Air
Big Data Technologies  Hadoop, Kafka, Zookeeper, Hbase, Hive , Amazon Redshift
 Ability to work independently and with other database administrators, application administrators, product developers, and product management as required
 Ability to plan work to meet project deadlines, accommodate demands by users, set priorities and escalate issues appropriately
 Superior communication skills (verbal, written and presentation), customer service-oriented
 Strong problem solving skill set, and the ability to understand new technologies quickly are essential
 Telecommute is an option.
Regular, predictable, full attendance is an essential function of the job
 Willingness to travel as necessary, work the required schedule, work at the specific location required, complete Penske employment application, submit to a background investigation (to include past employment, education, and criminal history) and drug screening are required.

Physical Requirements:
-The physical and mental demands described here are representative of those that must be met by an associate to successfully perform the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions.
-The associate will be required to: read; communicate verbally and/or in written form; remember and analyze certain information; and remember and understand certain instructions or guidelines.
-While performing the duties of this job, the associate may be required to stand, walk, and sit. The associate is frequently required to use hands to touch, handle, and feel, and to reach with hands and arms. The associate must be able to occasionally lift and/or move up to 25lbs/12kg.
-Specific vision abilities required by this job include close vision, distance vision, peripheral vision, depth perception and the ability to adjust focus.

Penske is an Equal Opportunity Employer, including individuals with disabilities and protected veterans.

About Penske Truck Leasing
Penske Truck Leasing Co., L.P., headquartered in Reading, Pennsylvania, is a partnership of Penske Corporation, Penske Automotive Group, and Mitsui & Co., Ltd. A leading global transportation services provider, Penske operates more than 260,000 vehicles, employs more than 29,000 people, and serves customers from more than 1,000 locations in North America, South America, Europe, Australia, and Asia. Product lines include full-service truck leasing, contract maintenance, commercial and consumer truck rentals, used truck sales, transportation and warehousing management and supply chain management solutions. Visit http://www.GoPenske.com to learn more.

Work Locations : 100 Gundy Drive

Reading, PA 19607

Primary Location : United States-Pennsylvania-Reading
Job : Information Technology
Penske (Oracle) Job Name: PTL.Data Engineer"
Maps Search - Engineer/Data Scientist,Apple,"Apple Maps are being used by millions of users every single day and powers thousands of applications. As a fundamental tool for human activity, Maps technology is evolving and new techniques are emerging. As a part of Apple Maps search team, you will play a big part in the next revolution in maps to enable users to find things in maps by working in a high-performing and collaborative team. You will have plenty of opportunities to create groundbreaking technology for large scale machine learning (including deep learning), cutting-edge search ranking and relevance, natural language processing and more. If you are excited with new technology and have significant background in machine learning and/or search technology and/or natural language processing, please consider joining the Maps Search team at Apple.
Key Qualifications
Experience in machine learning or natural language processing, such as learning to rank, feature engineering, named entity recognition, query parsing, deep learning, machine learning platform.
Desire to solve challenging problems at the intersection of user experience and computational algorithms
Strong programming experience in one or more of the following: Java, C++, Python or equivalent
Hard working, self-starter with proven ability to manage multiple projects to strict deadlines. Excellent written and oral communication skills and ability to interact with all levels of researchers and engineers
Description
The goal of Maps Search team is to take Apples Maps to the next level of intelligence and accuracy using advanced machine learning and artificial intelligence techniques. Engineers and scientists on this team work on a wide spectrum of approaches in improving search experience on Apple Maps. You would work on applied natural language processing algorithms to understand user query and intent. You would work on applied machine learning algorithms and models that help improve search relevance and experience. You would also work on developing scalable tools and platform foundations that enable other engineers and scientists to develop, evaluate and release algorithms, models and features quickly. We impact millions of users by developing a phenomenal product with a prime focus on accuracy, quality and performance of the product.

This position involves a wide variety of skills and innovation, and is a unique opportunity that sits at the cutting-edge of machine learning and natural language understanding. Ultimately, your work would have a huge impact on millions of users across the globe.

Education
Bachelor/Masters in Computer Science, with a specialization in machine learning or natural language processing, or related fields. PhD and/or equivalent experience is preferred.
Additional Requirements
Experience/knowledge of large scale machine learning applied to very large and diverse datasets (preferred)
Experience shipping products, especially ones incorporating machine learning (preferred)
Experience/knowledge of search technology and related fields (preferred)
Experience/knowledge of maps technology and related fields (preferred)"
Lead Big-Data Engineer for AWS,Intellisoft Technologies,"Thanks Ragini!

Large company based in Grapevine, TX is seeking an AWS Developer for a contract-to-hire opportunity on their Big Data team. The manager will interview for this role immediately and can start this AWS Developer before the end of the year. The pay rate for this role is 80 - 90 per hour based upon experience.

Responsibilities:
In the role as AWS Developer, you will serve as a Lead Big-Data Engineer for AWS / Hadoop Integration and serve as an integral part of the technical team responsible for the company's consumer connectivity, customization, and commerce efforts as part of the data management organization. This role contributes to emerging technologies in data analysis, architecture and development.

This AWS Developer / Big Data Engineer will be part of the design of the company's next generation consumer platform which will power future personalized consumer experiences across all touch-points. This role is generally characterized by working levels of scope and independent decision making, with a moderate level of technical complexity.

Additionally, you will:
Guide and mentor local, on-shore and off-shore delivery teams in all aspects of Data Integration including ETL, data movement, data replication, data distribution and cloud technologies.
Develop and produce project documentation to ensure consistent results across all projects.
Provide technical leadership for solution development and review with regard to data integration architecture design and implementation for initiatives within the Enterprise.
Develop and drive the review / approval of data integration specific Solution Definitions through the enterprise architecture review process.
Collaborate with the data modelers, business intelligence architects, business analysts, and data subject matter experts to define and enforce integration standards and best practices that meet the needs of the business.

Required Skills:
At least 7 years of related organizationally-based experience required, including.
Working on large volume databases is required.
Hadoop experience is required.
Shell scripting experience is required.
Terradata, Data Integration, and cloud technologies experience is a preferred.
Working in complex projects with multi-tier architecture required.
Expert knowledge in Big Data ETL Hadoop Stack.
Minimum 5 years development experience on Hadoop platform including PIG, Hive, Sqoop, Hbase, Flume, Spark and related tools.
Minimum 1 year development experience on AWS EMR, Lambda, Dynamo DB, S3 services.
Minimum 7 Years Data Integration Development Experience.
Expert knowledge of relational and dimensional data concepts.
Expert knowledge of some ETL tools such as SSIS ,InformaticaPowerCenter, or SnapLogic.
Working knowledge of SQL including DDL and DML.
Demonstrated ability to influence critical business outcomes in a global environment.
Working knowledge of Waterfall or Lean / Agile delivery environment.
Ability to communicate effectively, with others using spoken and written English, to audiences with varied levels of responsibility and technical expertise."
Software Engineer,Structural Integrity Associates,"The Fossil Plant Services division of Structural Integrity Associates, Inc. (SI) is currently looking for a Software Engineer to join our team. SI is an engineering consulting firm dedicated to the analysis, control, and prevention of structural failures for the power industry and related applications. In addition to consulting services, SI offers a software product portfolio that provides online asset monitoring solutions. With that, we help our clients in the digitalization of their assets, for better decision making and safe operation. Big Data and IIoT is a reality for us. This position offers candidates the chance to work in a dynamic environment, with the most recognized experts in the power industry.

About this position:

As a member of the Data & Analytics team, the software engineer will be instrumental in the further improvement and development of SIs cloud-based software platform. They will take ownership of the product life cycle, from concept development to maintenance of implemented applications. The successful candidate will have experience in working with and leading software development teams, and is familiar with common methodologies deployed (Agile, Scram, etc.). Excellent interpersonal skills are a must, as interaction with our corporate clients will be part of this role.

The software engineer will work closely with our analysis and project engineers, and external partners SI is cooperating with. This ensures an interesting work environment and creates opportunities for professional and personal growth.

Required Skills:
Software/web application development, including database structures

Design, build, test, and deploy scalable data pipelines and APIs.

Experience in product architecture development

Product Life Cycle Management (from cradle to grave)

Application of state-of-the-art software development methodologies (Agile, Scrum, etc.)

Leadership to a team of software developers

Excellent interpersonal and communication skills

Desired:
Web development experience on .net Framework, C#/C++, XAML (Silverlight, WPF), SQL, Java

Experience in Computational Engineering (Big Data)

Understanding of Data Historians (e.g. OSIsoft PI, Emerson Ovation)

3D graphics integration into web applications

Why Consider SI?
Employee owned company
Incomparable industry reputation
Robust compensation and benefits programs
Entrepreneurial spirit
Relocation assistance provided
Employee stock ownership (ESOP) participation"
Junior Engineer 1,exp,"Job Description:
At EXP, we know that great experiences start with the
right people. We believe that work should be
challenging, and the challenge should be fun. We also
believe that exponential possibilities are more likely
to occur with a respectful, satisfying and empowering
company. We value and respect our employees - their
experience and expertise as well as their energy,
passion and diversity - and their innovative approach to
work and to life.
Your Challenge
EXPs Houston office is seeking a Junior Engineer I to
join our team for a fixed term of 1 year with the
potential of an extension. The successful candidate will
be joining a fast-paced and rapidly expanding group.
Your Responsibilities
Responsible for assisting Intermediate Engineers, Senior
Engineers and/or Project Managers in managing the
development and execution of a broad range of natural
gas or oil facility projects, including but not limited
to high pressure transmission pipelines, compression,
storage, gathering, treating and processing. May be
required to travel.
Defines and prepares job scopes and cost estimates
for assigned projects.
Interacts with various personnel, clients and state
and/or federal agencies regarding assigned projects.
Creates and maintains project specific documentation
during and following completion of a project to
ensure compliance with state or federal laws,
corporate policies and procedures or other governing
bodies.
Prepares and delivers presentations that communicate
project proposals, descriptions and status to senior
management, state or federal agencies, and/or the
public.
Communicates the status of developmental activities
to business sponsors.
Develops and provides monthly project cash flow
forecasts.
Assists with the management of projects, ensuring
safety, reliability, environmental and human impacts
are mitigated while meeting project goals including,
but not limited to schedule, budget, safety and
overall quality.
Analyzes, monitors and provides recommendations
related to process improvement.

Job Requirements:
The Skills, Knowledge and Abilities We Seek
Bachelors degree in Mechanical, Civil or Chemical
Engineering from an accredited educational
institution.
0 to 4 years of professional experience.
Ability to communicate effectively verbally and in
writing.
Ability to maintain and establish effective working
relationships.
Ability to review project work, quantities, costs,
accuracy of specifications, completeness, and
constructability.
Ability to meet project deadlines.
Ability to perform assigned responsibilities with
minimal to moderate supervision.
Ability to prioritize, organize and perform multiple
work assignments simultaneously.
Ability to compile, review and reconcile data for
accuracy, completeness and compliance.
Ability to perform work accurately in a detail
oriented environment.
Ability to prepare clear and concise reports,
procedures, correspondences, and other written
materials. Ability to interpret
architectural/construction drawings, specifications,
codes, standards and regulatory requirements.
Ability to perform on-site design services and field
engineering work.
Knowledge of and ability to use standard computer
applications including, but not limited to Microsoft
Word, Excel, PowerPoint and Outlook.
What EXP Can Offer
With a mission to understand, innovate, partner and
deliver, EXP provides engineering, architecture, design
and consulting services to the worlds built and natural
environments.
Our heritage dates back to 1906, when the earliest of
EXPs predecessor companies started its engineering
infrastructure practice. Today, over 3,000 creative EXP
professionals across North America provide the passion
and expertise needed to deliver successful projects
around the world.
Our promise is to offer you a challenging career in a
positive and dynamic work environment, and it's a
promise we take seriously.
Join a dynamic team at EXP that provides you with
innovative projects, the capacity to develop your
career, a full range of benefits, flexible working
hours, and much more! When you explore what EXP has to
offer, youll find exponential possibilities.
For more information, visit www.exp.com.
EXP U.S. Services is an equal opportunity employer
committed to diversity in the workplace.
Minority/Female/Disabled/Protected Veteran."
Oracle PL/SQL - Software Engineer,JP Morgan Chase,"As a member of our Software Engineering Group you will dive head-first into creating innovative solutions that advance businesses and careers. Youll join an inspiring and curious team of technologists dedicated to improving the design, analytics, development, coding, testing and application programming that goes into creating high quality software and new products. Youll be tasked with keeping the team and other key stakeholders up to speed on the progress of whats being developed. Coming in with an understanding of the importance of end-to-end software development-such as Agile frameworks-is key. And best of all, youll be working with and sharing ideas, information and innovation with our global team of technologists from all over the world.
This Oracle PL/SQL Engineer role requires a wide variety of strengths and capabilities, including:

BA/BS degree and/or equivalent work experience

Advanced knowledge of application, data and infrastructure architecture disciplines

Understanding of architecture and design across all systems

Working proficiency in developmental toolsets

Ability to collaborate with high-performing teams and individuals throughout the firm to accomplish common goals

Minimum 5+ years experience with developing/maintaining triggers, stored procedures, packages using Oracle PL/SQL.

Should be able to enhance, troubleshoot and performance tune SQL queries.

Experience with Reporting Tools preferably Actuate eReport Designer (BIRT) and/or JEE technology is a huge plus.

Agile experience is a plus.

Understanding of software skills such as business analysis, development, maintenance and software improvement

Our Consumer & Community Banking Group depends on innovators like you to serve nearly 66 million consumers and over 4 million small businesses, municipalities and non-profits. Youll support the delivery of award winning tools and services that cover everything from personal and small business banking as well as lending, mortgages, credit cards, payments, auto finance and investment advice. This group is also focused on developing and delivering cutting edged mobile applications, digital experiences and next generation banking technology solutions to better serve our clients and customers.

When you work at JPMorgan Chase & Co., youre not just working at a global financial institution. Youre an integral part of one of the worlds biggest tech companies. In 14 technology hubs worldwide, our team of 40,000+ technologists design, build and deploy everything from enterprise technology initiatives to big data and mobile solutions, as well as innovations in electronic payments, cybersecurity, machine learning, and cloud development. Our $9.5B+ annual investment in technology enables us to hire people to create innovative solutions that will not only transform the financial services industry, but also change the world.

At JPMorgan Chase & Co. we value the unique skills of every employee, and were building a technology organization that thrives on diversity. We encourage professional growth and career development, and offer competitive benefits and compensation. If youre looking to build your career as part of a global technology team tackling big challenges that impact the lives of people and companies all around the world, we want to meet you."
SQL Database Programmer/Engineer,Kaiser Permanente,"This individual contributor is primarily responsible for translating business requirements and functional specifications into software solutions, for developing, configuring or modifying integrated business and/or enterprise application solutions, and for facilitating the implementation and maintenance of software solutions.

Essential Responsibilities:
Completes work assignments by applying up-to-date knowledge in subject area to meet deadlines; following procedures and policies, and applying data and resources to support projects or initiatives; collaborating with others, often cross-functionally, to solve business problems; supporting the completion of priorities, deadlines, and expectations; communicating progress and information; identifying and recommending ways to address improvement opportunities when possible; and escalating issues or risks as appropriate.

Pursues self-development and effective relationships with others by sharing resources, information, and knowledge with coworkers and customers; listening, responding to, and seeking performance feedback; acknowledging strengths and weaknesses; assessing and responding to the needs of others; and adapting to and learning from change, difficulties, and feedback.

As part of the IT Engineering job family, this position is responsible for leveraging DEVOPS, and both Waterfall and Agile practices, to design, develop, and deliver resilient, secure, multi-channel, high-volume, high-transaction, on/off-premise, cloud-based solutions.

Provides insight into recommendations for technical solutions that meet design and functional needs.

Provides systems' incident support and troubleshooting for basic to moderately complex issues.

Assists in identification of specific interfaces, methods, parameters, procedures, and functions, as required, to support technical solutions.

Supports collaboration between team members, architects, and/or software consultants to ensure functional specifications are converted into flexible, scalable, and maintainable solution designs.

Assists in translating business requirements and functional specifications into code modules and software solutions, with guidance from senior colleagues, by providing insight into recommendations for technical solutions that meet design and functional needs.

Assists in the implementation and post-implementation triage and support of business software solutions, with guidance from senior colleagues, by programming and/or configuring enhancements to new or packaged-based systems and applications.

Develops and executes unit testing to identify application errors and ensure software solutions meet functional specifications.

Supports component integration testing (CIT) and user acceptance testing (UAT) for application initiatives by providing triage, attending test team meetings, keeping the QC up-to-date, performing fixes and unit testing, providing insight to testing teams in order to ensure the appropriate depth of test coverage, and supporting the development of proper documentation.

Assists in the development, configuration, or modification of integrated business and/or enterprise application solutions within various computing environments by designing and coding component-based applications using programming languages.

Writes technical specifications and documentation.

Assists with efforts to ensure new and existing software solutions are developed with insight into industry best practices, strategies, and architectures.

Assists in building partnerships with IT teams and vendors to ensure written code adheres to company architectural standards, design patterns, and technical specifications.

Participates in some aspects of software development lifecycle phases by applying an understanding of company methodology, policies, standards, and internal and external controls.

Works with vendors (e.g., offshore, application, service).

Minimum Qualifications:
Bachelor's degree in Computer Science, CIS, or related field and Minimum three (3) years experience in software development or a related field. Additional equivalent work experience may be substituted for the degree requirement.

Preferred Qualifications:
Four (4) years of development experience with Microsoft Dynamics xRM
Seven (7) years of programming experience with SQL Server
Four (4) years of development experience with Tableau
Two (2) years experience in systems analysis, including defining technical requirements and performing high level design for complex solutions.
Two (2) years experience working in a large matrixed organization.
Four (4) year experience working with IT vendors.
One (1) year experience in data modeling and analytics.
Four (4) year of work experience in a role requiring interaction with senior leadership (e.g., Director level and above)"
Data Analyst,Xsolis,"XSOLIS is seeking to fill a full-time Data Analyst position on the Analytics/Data Science team. We are looking for a highly-motivated, skilled individual with a solid background in statistics, data analytics, and computer technologies related to these areas. This position is in the Nashville area and does not offer relocation. If you are applying from out of state, please let us know when you are relocating and start date. This is an immediate need.

ESSENTIAL DUTIES AND RESPONSIBILITIES

 Identify, pull, and prepare datasets for model consumption

 Prepare exploratory statistical analysis and data visualization

 Research tools, products, and data architectures to improve workflow

 Run model retraining and update events

 Collaborate with and support data scientists, database engineers, and executive staff

Requirements

 An understanding of relational databases and experience developing queries

 Ability to read and write functional Python code

 Experience in data wrangling/munging and statistical analysis

 Strong written and verbal communication skills

 Working experience with MS SQL/T-SQL is a plus

 Experience with big data architectures is a plus

 Healthcare data experience is a plus

 Ability to work independently is a plus

XSOLIS provides a full range of standard employee benefits. Compensation is commensurate with the candidates background and experience.

Visit our website at www.xsolis.com for more information on our company."
Field Engineer - Entry/Junior,MWH,"MWH Constructors, now part of the Stantec Construction Group, is a global leader in heavy civil construction of water and wastewater facilities. MWHC is seeking an entry level/junior Field Engineer to support project work outside of Boston, MA.

Selected individual will provide technical engineering information to company supervision and contractors to insure construction work complies with all contract documents and engineering standards.

ESSENTIAL FUNCTIONS
Perform ongoing Quality Inspections to ensure the Subcontractor is using approved material and installing per design.
Interpret design/drawings for contractors that are installing the work.
Research and recommend resolutions to drawing interpretation problems, conflicts, interferences, and errors.
Review contractor submittals, RFIs and pay applications for conformance and accuracy.
Manage the submittal and RFI process electronically,
Verify all completed work complies with applicable codes, drawings, and specifications.
Prepare and disseminate all required documentation records such as status reports, punch lists describing work items to be done, sketches of work already completed, material requirement calculations, etc., to supervisor.
May provide technical support and direction of project material control functions including implementation and operation of maintenance program.
Prepare drawings and sketches to support construction work, change orders, estimates, etc.
Represent company, project and/or department during A/E, client and project management meetings.
May provide necessary building control lines and elevations for accurate measurement and correct installation of materials.
Work in a manner to ensure your personal safety and that of fellow employees by following company health and safety guidelines and policies.
Perform additional assignments per supervisor's direction.
Possess a 4-year Engineering or Construction Management Degree
Working knowledge of construction equipment and techniques, drawings and specifications, building materials and required standards applicable to discipline.
Ability to perform trigonometric calculations, either manually or with calculator.
Ability to assume responsibility, and interfaces and communicates effectively with others.
2 years of related FIELD experience.
Familiarity with Project Management data base systems such as Primavera Contract Manager, Expedition, Share Point, etc.
MWH Constructors, now part of the Stantec Construction Group, is a global project delivery company with a focus on water and energy. With the ultimate goal of delivering maximum value to our clients and their local communities, we provide single-source, integrated construction services and the full range of project delivery methods. Incorporating industry-leading preconstruction services and safety practices, our multi-disciplined team of construction professionals delivers a wide range of projects  including new facilities, infrastructure improvement and expansion, waste-to-energy construction, and capital construction services.

MWH Constructors offers integrated program management, engineering and construction expertise and proficiencies derived from our global engineering experience and our self-perform capabilities as a leading global contractor.

MWH Constructors leads the Stantec Construction Group comprising MWH Constructors and Slayden in the U.S. and Stantec Treatment in the U.K.
MWH is an EEO/AA/M/F/V/D - MWH considers applicants for all positions without regard to race, ethnicity, religion, creed, color, sex, gender, gender identity or expression, national origin, age, disability, veteran status, medical condition, marital status, sexual orientation, citizenship or other basis in accordance with federal, state or local laws or regulations."
IT Big Data Engineer,Hitachi Vantara,"The Company
Ignite your career with Hitachi Vantara! We have a proven track record of creating the future for more than 100 years. Thousands of the most mission critical systems in the worlds largest enterprises use our solutions today. Were going to change the way the world works and were going to make it a better place. Not by helping our customers and partners innovate but rather, by helping them intelligently innovate so they can deliver outcomes that truly matter for business and society.

Hitachi Vantara, a wholly owned subsidiary of Hitachi, Ltd., helps data-driven leaders find and use the value in their data to innovate intelligently and reach outcomes that matter for business and society. We combine technology, intellectual property and industry knowledge to deliver data-managing solutions that help enterprises improve their customers experiences, develop new revenue streams, and lower the costs of business. Only Hitachi Vantara elevates your innovation advantage by combining IT, operational technology (OT) and domain expertise. We work with organizations everywhere to drive data to meaningful outcomes.

Great careers start with innovation and here at Hitachi Vantara, our promise is to deliver insights that power smarter businesses and inspire social innovation solutions for a healthier, safer future. The key to our innovation is our people -- our culture values respect, diversity, and collaboration. Join our Hitachi family and together, lets lead the way to extraordinary!

The Role
We are looking for an outstanding Big Data Engineer / Developer who is passionate about technology & architecture, loves hands-on development and wants to build scalable, high performance state of the art systems that will and help drive the Hitachi Digital Transformation.

Responsibilities
The right candidate for this position has a proven track record as a principal consultant with superior technical skills, demonstrates an ability to lead small technical teams and can manage business/customer relationships and will:

 Work directly with business users to translate requirements into designs and ultimately develop and deploy cutting-edge solutions that leverage cloud based Hadoop distribution

 Be an evangelist for big data frameworks within the enterprise

 Handle all aspects of solution development - architecture, design, development, build, deployment, monitoring and operations, accounting for near-term product deliverables and technical debt

 Lead small development teams both local and remote

 Own overall solution architecture including direction, principles, various levels of abstraction, and current state

 Work closely with the extended analytics team to create a shared architectural vision for all analytics components

 Coordinate technical kick-off sessions by providing detailed product architecture overviews and determining the customer's design, integration and development plans

Qualifications
7+ years of Hands-on experience working with the Hadoop framework (e.g. HDFS, MapReduce, Pig, Hive, Hbase, Spark, Impala, Flume, Kafka, etc.)
Knowledge of cloud computing infrastructure (Amazon Web Services EC2, Elastic MapReduce, Redshift etc.) and considerations for scalable, distributed systems is an advantage
10+ years of IT Software Development Lifecycle experience with C++, Java, .NET, Python or Ruby or other languages
Experience with Data Virtualization technologies is desirable
Experience with data analytical tools, languages or libraries (SAS, SPSS, R, Mahout, etc.) is an advantage
Familiar with architectural frameworks/methodologies such as TOGAF
Highly motivated, goal driven, innovative, curious and open minded, Fun to work with, team player
Physical Requirements
Ability to spend extended periods of time on computer equipment and/or in meeting/classroom environments.
Must be able to speak and listen clearly on telephone.
Ability to maintain regular, punctual attendance consistent with the ADA, FMLA and other federal, state and local standards.
All qualified applicants will receive consideration for employment without regard to race, color, religion, place of origin, ethnic origin, national origin, ancestry, age, sex, sexual orientation, gender identity, transgender status, genetic information, mental or physical disability, marital status, pregnancy, veteran status, or any other characteristic protected by applicable national, state, or local law."
"Senior Data Engineer (Insights, Intelligence, & Audience)",Publicis Media,"Job Description

Data Engineers within Publicis Media will assist in the development, implementation, and ongoing innovation of world class leading engineering products, business analytics, and next generation technologies. The Engineer will be involved with designing a new workflow for data science and analytics that are a big part of the roadmap for 2017, and implementing data science algorithms that scale for data in the billions. Candidates will be considered based on their ability to design large distributed technical solutions.

Your day to day will include:
Design and manage data pipe-line using luigi
Integrate products from data projects into portal built in Ruby/Rails
To enjoy being challenged and solve complex problems on a daily basis
Excellent oral and written communication skills
Proficient in designing efficient and robust ETL workflows
Manage real time streaming application and data flow
Investigate, procure and ramp up to new technologies
Ability to work in teams and collaborate with others to clarify requirements
Design, create, maintain and optimize software applications for business intelligence solutions using a variety of languages, tools and technologies
Think quickly, communicate clearly and work collaboratively with engineering, QA and operations teams
Embrace solid, agile development practices, such as unit-testing, code reviews and design documentation Qualifications

You have:
Bachelor's Degree in Mathematics, Computer Science/Engineering, Statistics, or related field; advanced degree preferred
1-3 years of data engineering, data science, or analytics experience
Programming experience working with Python or Scala
Knowledge of Big Data Architectures (Hive/Hadoop, Redis and/or Dynamo DB)
Strong experience with SQL
Strong interest in emerging technologies: MapReduce, NoSQL, MPP databases, etc.
Comfortable with agile development practices
Outstanding communication skills (oral, written and presentation) and strong interpersonal skills
High energy level, strong team player and good work ethic Nice to Have:
Experience with Storm or Spark
Experience with NoSQL datastores (Dynamo, BigTable, Aerospike)
Experience with Java
Familiarity with Amazon Web Services
Digital Marketing and Advertising Domain Knowledge

Additional Information

All your information will be kept confidential according to EEO guidelines."
Senior Backend Engineer (Data Analytics),The Climate Corporation,"Position Overview:
The Climate Corporation is revolutionizing the agriculture industry with a platform and products that are helping the worlds farmers sustainably increase productivity with digital tools. We are leveraging Machine Learning and Big Data to build meaningful products that allow farmers to produce enough food to help feed a growing population. The Data and Analytics team is focused on creating competitive advantage for Climate and our customers through novel data infrastructure, metrics, insights and data services. We are a small but rapidly growing analysis and engineering team that builds and leverages state-of-the-art analytics systems. Our work informs decisions and direction for our business, while also impacting our products. We are looking for a seasoned software engineer to not only build data pipelines to efficiently and reliably move data across systems, but also to build the next generation of data tools to enable us to take full advantage of this data. In this role, your work will broadly influence the company's products, data consumers and analysts.

What You Will Do:
Expand our business by providing strategic data to partner groups and product groups via platform level real-time REST based API services for targeting, personalization and recommendations
Develop and maintain the infrastructure and architecture that enable the data pipeline for analytics.
Develop infrastructure to enable scientists to rapidly train ML models
Develop infrastructure that enables product management to garner key insights into customer behavior and refine product roadmap to meet customer needs
Develop architecture and infrastructure to enable dealers and growers to manage their operations more effectively
Evaluate new technologies and products.

Basic Qualifications:
BS, MS or PhD in Computer Science or related technical discipline (or equivalent)
5+ years programming experience in Java or another language.

Preferred Qualifications:
Experience working with web applications and REST based API services at large to massive scale.
A solid foundation in computer science, with strong competencies in data structures, algorithms and software design.
Experience designing and building a Hadoop/Spark high performance Cluster
Experience working with relational databases and especially MPP systems like Redshift/Vertica/Teradata and map/reduce systems like Hadoop or Spark is an added plus.
Familiarity with NOSQL tools like Hadoop (hive, pig, spark, hbase, membase) and Spark is an added plus.
Experience working with AWS

What We Offer:
Our teams are composed of industry experts, top scientists, and talented engineers. The environment is extremely engaging and fast-paced, with dozens of specialties coming together to provide the best possible products and experiences for our customers.
We provide competitive salaries and some of the best perks in the industry, including:
Superb medical, dental, vision, life, disability benefits, and a 401k matching program
A stocked kitchen with a large assortment of snacks & drinks to get you through the day
Encouragement to get out of the office and into the field with agents and farmers to see first-hand how our products are being used
We take part and offer various workshops, conferences, meet-up groups, tech-talks, and hackathons to encourage participation and growth in both community involvement and career development

We also hinge our cultural DNA on these five values:
Inspire one another
Innovate in all we do
Leave a mark on the world
Find the possible in the impossible
Be direct and transparent

Learn more about our team and our mission: The Climate Corporation - The Technology Behind Making A Difference https://youtu.be/c5TgbpE9UBI #LI-EF1"
Summer Research Program Intern-Data Scientist,MIT Lincoln Laboratory,"The Advanced Concepts and Technologies Group develops radar, communications, and systems technologies for use in future air defense systems. Of particular interest is the development of highly digitized phased-array radars and advanced signal processing techniques to enable the next generation of shipboard and airborne surveillance sensors. Major activities within the group include system concept development, modeling and simulation, signal processing algorithm design, prototype system design and development, and experimental field testing and data analysis.

Advanced Concepts and Technologies group seeks a summer intern to work alongside signal processing engineers, systems analysts, and hardware developers on the development of statistical modeling, optimization algorithms, graph modeling, graph structure learning, graph parameter learning for machine intelligent, electronic warfare applications. The successful candidate will work to understand the challenges of the application area and the methodology and limitations of existing algorithms. Within a larger project framework, the candidate will then be free to select and focus on a specific area of algorithm development of interest to him/her. The primary task will be development and implementation of a computationally efficient algorithm that enhances performance in one of the aforementioned areas.

Education Requirements:

The successful candidate will be working toward a B.S., M.S or Ph.D. in electrical engineering or a related field.

Job Requirements:

The ideal candidate will have a strong foundation in statistical signal processing, machine intelligence, and programing to implement, analyze and demonstrate the same. Additionally, familiarity with system modeling is a plus. Proficiency in MATLAB and/or C++, excellent analytical abilities, demonstrated strong oral and written communication skills, and demonstrated ability to work both independently and on small teams are required.

MIT Lincoln Laboratory is an Equal Employment Opportunity (EEO) employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, gender identity, national origin, age, veteran status, disability status, or genetic information; U.S. citizenship is required."
Database Engineer,Red Arch Solutions,"Job Description ***Active TS/SCI with Polygraph Required***

Red Arch Solutions is a proven and effective small business integrator and consultant, recognized as a leading provider of IT development to the Federal Government, and primarily focused within the Intelligence Community.

We are an official Amazon Web Services (AWS) Partner, and are pleased to now offer Cloud Solutions Architecture and Engineering using AWS. We promote the advancement of our staff in achieving AWS certifications.

Red Arch is seeking a Database Engineer to support NGA.

The scope of this effort relates to the development and maintenance of several databases for NGA to include Print-on-Demand (PoD), Intent-to-Publish (NIP), Source Content Management and Services, and Forward Based Geospatial Intelligence (GEOINT)  Geospatial Services/Computer to Plate (FBG-GS/CTP) Production System.

Responsibilities/Job Function:
Create data scripting automation to replace manual data entry procedures

Conduct data mining of internal NGA and external NSG databases for the purpose of data comparisons and synchronization. The databases comprise of: Print on Demand Database, Forward Based GEOINT Database, DART, GEOSQRL, and DLA Catalog

Prepare Mapping, Charting and Geodesy (MC&G) chart files for hardcopy, print, and online softcopy dissemination

Enhance existing databases that enable the tracking of NGA maps, charts and digital products that are produced across multiple NGA Source offices

Maintain databases on multiple domains (NGANet and SECNet), adhering to NGA guidelines and policies NI 8955.5R5 and NI 8010.2R7

Attend the Production Management Working Group (PMWG) weekly meetings

Update and maintain databases such as: Oracle, MySQL, Sybase, Visual Basic, Visual Basic.Net, and SQL Server

Perform database scripting using Unix, Linux, SQL, Java, Oracle, Database Management Tools, HTML, XML, multiple Operating Systems to ensure daily loading of GEOINT data on multiple NGA networks

Use COTS and GOTS applications (Unix, Linux, SQL, Java, Oracle, Database Management Tools, HTML, XML, multiple Operating Systems, C Shell, JSON Cloud Administration) to load Geospatial data into Geospatial databases and into disk storage and extract Geospatial data from relational Geospatial databases

Monitor data dissemination and performance activities on the multiple security domains produced by NGA Production Systems

Track and provide metrics on data holdings and any quality issues

Minimum Qualifications
Mandatory Security Clearance Requirement: TS/SCI with CI Poly

Demonstrated experience in Unix, Redhat Linux, and Windows operating environments

Demonstrated experience in the planning, design, development, and maintenance of relational database management systems such as: Oracle, Oracle Spatial, MySQL, Sybase, Visual Basic, Visual Basic.Net, and SQL Server

Demonstrated experience in SQL and CGI scripting

Demonstrated experience using style guides (organization-wide)

Demonstrated experience using COTS and GOTS tools to load data into databases and into disk storage and extract data from databases

Demonstrated understanding of Controlled Access Program Coordination Office (CAPCO) Markings and Implementation

A Bachelors Degree from an accredited institute in an area applicable to this position (e.g. information systems, computer science, math, or engineering) and eight (8) years of relevant technical experience

An additional four (4) years of relevant technical experience may be substituted for the Bachelors Degree
Must presently be 8570 compliant (IAT Level 2 preferred); must be 8570 compliant (IAT Level 2) by date of security indoctrination with any necessary continuing education (CE) for certification
Desired Qualifications
Experience transitioning and maintaining databases in the AWS C2S cloud

Red Arch Solutions is an Equal Opportunity Employer. All qualified applicants will receive consideration for employment and will not be discriminated against on the basis of race, color, religion, sex, sexual orientation, national origin, age, marital status, disability, or protected veteran status. Red Arch Solutions takes affirmative action in support of its policy to advance in employment individuals who are minorities, women, protected veterans and individuals with disabilities."
"Full-Stack Software Engineer, Data Visualization",Houzz,"About Houzz
Join Houzz in revolutionizing the home and remodeling industry and have an impact on over 40 million homeowners and 1.5 million professionals around the world. Frequently ranked as one of the top 10 most disruptive companies in the world and backed by marquee venture capitalists, Houzz will allow you to drive the future of an industry worth $1.2 trillion in the US and Europe alone.

Our entrepreneurial culture and warm family atmosphere will surround you with a fun and dynamic work environment you will only find at Houzz.

Our engineers play a direct role in the direction of our company and are able to work across multiple groups to implement fresh ideas that allow Houzz to be the industry leader. If you are interested in applying your passion to create products that will transform the lives of millions of users remodeling and decorating their homes, then welcome to Houzz.

Position Overview
We are looking for a world-class software engineer to join our family and be part of our development team. You will take high level ideas and transform them to live products, used by millions of homeowners and professionals from around the world. You will be working on all parts of our web application, its libraries, widgets and custom features.

We are looking for someone to work with our data engineers and analysts to provide meaningful ways to present information internally and externally. This work will be critical for understanding day to day operations of the business and also provide insight to our end users, our community of professionals, and our advertisers. The perfect person for this role will be a full stack engineer, is as comfortable in CSS as in writing SQL and has a passion for portraying information effectively and beautifully.

Contributions
Developing internal dashboards to understand historical trends, usage behaviors, and report metrics
Creating newer more powerful views into home remodeling preferences for our professionals and user community
Working with data team to understand how data is collected and what information it might convey
Working with the rest of development to produce useful A/B test dashboards and reports on new features
Required Skills and Experience
Familiarity with web-based data visualization frameworks such as D3
Expertise with statistical concepts such as deviation, confidence limit, and significance
Understands web development patterns such as MVC, backbone.js, and Django and both service-based and RESTful architecture
Can verbally create clear pictures for others; can articulate reasoning for applying different information techniques to data sets
Degree in CS, Human-Computer Interaction, Information Science, Information Graphics, or related field
Desired Skills and Experience
Expertise with HTML/CSS/JavaScript, Python, and relational databases (e.g. MySQL); previous experience with Hadoop and Hive
Benefits
Competitive Salary, Flexible Paid Time Off, Commuter Benefits, Medical, Dental, Vision, & Pet Insurance, Employee Assistance Program, 401k Retirement Savings, Flexible Spending Accounts, Gym Partnership Discounts, Catered Meals, Fully-Stocked Kitchens, and much more!
Together, we dont just accept differences -- we celebrate, support, and thrive on inclusivity for the betterment of our products and our community."
JUNIOR ENGINEER,University of Pennsylvania,"The High Energy Physics group at Penn is responsible for projects that involve the design, development, installation and testing of front end electronics to read out large arrays of sensors ranging from hundreds to millions of channels of individual readouts. Information from these sensor systems often needs to be correlated by the readout electronics and sent on to large data acquisition systems or other similar subsystems that participate in data readout decisions. We are seeking a technically skilled person to help with several projects currently underway: The development of an analog readout of an Avalanche Photo Detector with a target of 10 pico-second timing precision, the completion of a Positron Emission Tomography detector readout for medical imaging focusing on debugging multiple channels of high speed data links and with the development of a high throughput detector triggering board for the upgrade of the ATLAS detector at CERN (www. ATLAS .ch). Documentation skills are important for participation in these projects including the use of elogs to understand current status and inform other persons in the group of the work performed on each project.

Qualifications
High School degree and 5-7 years of related experience, or equivalent combination of education and experience, is required. Applicants should have previous work experience with Analog circuit design for sub-nanosecond rise time small signal amplifiers high speed data links and digital FPGA programming. Candidates should also have experience with printed circuit board layout, assembly and testing. Must possess a high level of curiosity about functional applications of circuits. Skilled in documentation and presentation of work.

Affirmative Action Statement
Penn adheres to a policy that prohibits discrimination on the basis of race, color, sex, sexual orientation, gender identity, religion, creed, national or ethnic origin, citizenship status, age, disability, veteran status, or any other legally protected class."
Data Engineer,Economic Modeling Specialists Intl.,"JOB DESCRIPTION

Overview

At Emsi we process job postings, resumes, and professional profiles to better answer questions about what skills are available in the workforce, what colleges should be training students for, and where opportunities lie for job-seekers. This job is all about coming to grips with the messy world of user-submitted data at scale (hundreds of millions of documents) and extracting useful answers from it. We're looking for people who have the technical skills and an interest in solving problems like: are biotech graduates from university A more likely to get jobs at Fortune 500 companies than graduates from university B?

Our focus is the student-to-employment journey. We use labor market data to connect and inform people, education, and business. For better than 15 years, weve served as labor market advisors to leaders in higher education, business, and community development. Emsi partners with colleges to help them leverage their economic impact, offer industry-aligned academic programs, and drive student success. Emsi helps economic and workforce development professionals help their communities prosper by bringing in new businesses, helping existing companies grow, and finding good jobs for their residents. Emsi helps businesses use data to drive talent strategy and align recruiting efforts  empowering them to hire the talent they need as quickly and affordably as possible.

This role will focus on processing loosely structured data such as job postings, resumes, and other documents, with the goal of providing analytics and creating meaning from them.

Major Responsibilities and Activities

Be familiar with our data clients, both in-house and external, and determine how best to get our data into their tools.
Build and maintain data processing pipelines for several major data sources.
Solve big data problems with smart algorithmic solutions.
Create and preserve knowledge with well-written documentation.
JOB REQUIREMENTS

Minimum Skills and Qualifications

Strong software engineering, programming, and QA skills.
Proficiency with Linux (or other POSIX).
Proficiency with a systems programming language.

Preferred Skills, Experience, and Technologies

Experience with lean/agile development, kanban, and test-driven development practices.
Git, AWS ecosystem.
D language experience.
Experience with out-of-core algorithms and data structures."
Senior Hadoop Big Data Engineer,Vantiv,"The Company

Vantiv is making payments smarter, faster and easier for all our partners, as well as the consumers they serve. From the largest retailers in the U.S. to the coffee shop down the street, we are leading the transformation in payments through chip-enabled cards, mobile wallets, eCommerce, and payment solutions for businesses of all sizes. We are the fastest growing payments company in the U.S., first in PIN Debit transactions and the second largest merchant acquirer. Visit Vantiv.com for more.

The Opportunity

Vantiv's growth is due in large part to our highly-sophisticated products and services. By offering some of the most advanced and user-friendly payments processing options available, we can continue our growth by beating our competitors. Enter the need for skilled Engineers who can build the tools we need to continually improve upon our product offerings. From software to technical architecture and more, investing in our team of Engineers is investing in our future.

Vantiv is sitting on a mountain of valuable data. The Data and Analytics organization focuses on harnessing this valuable data to drive better outcomes for our customers.

The Senior Big Data Engineer works on sourcing, ingesting, modeling and visualizing this data. A vigilant developer, he/she will work with business, understand the requirements and own developing the solution. This will be a combination of modeling, development and maintenance. He/she will also work with cross-functional teams to build software solutions for current issues or to incorporate advancements in technology. Always searching to continuously improve Vantiv's catalog of products and services, we will create and maintain stable, reliable systems for our clients and partners.

Rewarding Impact. Fulfilling Careers.

Making an impact isn't something reserved for people in corner offices. At Vantiv, it comes from people in every corner of the office. People with ambition, optimism and courage. We provide growth and opportunity and give employees flexibility in how they get the job done. You might not expect that from a big payments company, but we're smart enough to know how to hire the best and when to step aside and let them lead. Our goal is that you never stop learning and you never cease to amaze--especially yourself. If this fits your career goal, we can't wait to welcome you on board.

The Day-to-Day

Responsibilities:
Plans, designs, develops and tests data solutions
Works cross-functionally to address issues and emerging needs in software systems.
Rigorously tests software in preparation for deployment.
Works on projects from enhancements, new features and bug fixes to new and replacement products.
Works on problems where analysis of situations or data requires a review of a variety of factors.
As a seasoned, experienced professional with a full understanding of area of specialization, he/she resolves a wide range of issues in creative ways.
Works on problems of diverse scope where analysis of data requires evaluation of identifiable factors.
Demonstrates good judgment in selecting methods and techniques for obtaining solutions.
Networks with senior internal and external personnel in own area of expertise.
Coordinates release of work product into production environment
Qualifications:
Typically requires a minimum of 5 years of related experience with a Bachelor's degree; or 3 years and a Master's degree; or a PhD without experience; or equivalent work experience.
More than 5 years experience in developing enterprise class Data/Application/API solutions
More than 2 years experience in Java or Python
Skilled in working on Unix systems
More than 2 years experience working on databases
Some exposure to modeling and more than 2 experience in one of the major SQL dialects
Knowledge of Big Data landscape as demonstrated through experience or certification. Eg., Hadoop, Spark, Hive, Pig, Impala
Exposure to Agile development methodologies
All the above duties and responsibilities are essential job functions for which reasonable accommodation will be made. All job requirements listed indicate the minimum level of knowledge, skills and/or ability deemed necessary to perform the job proficiently. This position description is not to be construed as an exhaustive statement of duties, responsibilities or requirements. Employees may be required to perform any other job-related instructions as requested by their leader, subject to reasonable accommodation.

Equal Opportunity Employer Minorities/Women/Protected Veterans/Disabled"
Senior Big Data Engineer,NinthDecimal,"NinthDecimal is the leading mobile audience intelligence company empowering marketers with the most comprehensive understanding of people by connecting their digital and physical lives. Fortune 500 companies and marquee brands, including Microsoft, Target, Kraft, Comcast, American Express, Starbucks and Toyota, use NinthDecimal to achieve high performing campaigns by engaging the most precise and relevant audiences across all media channels on a single platform. By bridging online and offline data sources, NinthDecimal offers the most precise audience solution in the market to create a true understanding of mobile consumers and how to engage them throughout their path-to-purchase. Its Location Conversion Index (LCI) is the mobile industrys most accurate ROI metric for measuring the incremental lift in store visits directly attributed to mobile ad campaigns; and its data partnerships with leading companies like Datalogix and Neustar provide a comprehensive closed-loop solution for marketers.

Position Summary

We are looking for a strong engineer with experience in building and scaling big data pipelines. We are dealing with terabytes of new data coming every day and the demand and use case on this data are ever increasing. The candidate for this position will play in important role in scaling the companys data capabilities  which is a core asset at NinthDecimal.

Responsibilities:
As a member of the Core Data Services team, participate in development and optimization of data pipelines
Stay on top of evolving technology (streaming, etc.) to suggest, prototype and implement improvements to the data architecture
Assure data quality and consistency of produced data shapes.
Qualifications Required:
Hands-on experience in building scalable data pipelines at multi-terabyte scale using Spark with scala/python, Kafka or other streaming technologies, sql and nsql based systems
Expert in performance tuning of processes in hadoop based ecosystem
Expert in designing and implementing efficient data pipelines
Expert in scala/python/sql. Java and C++ are a plus
Experience with streaming and queuing technologies
Commitment to best software engineering practices (unit testing, code reviews etc.) and agile process
Passion for data quality
Great communication skills  this position will be in a middle of discussions with product owner, other engineers, QA and devops, therefore ability to understand requirements and articulate solutions is super critical
At least 3 years of working with Big Data
Nice to have: Experience in media/advertising business. Experience with statistical analysis, data mining, machine learning
What's in it for you?
100% mobile revenue growth year over year - the company is a great acquisition target
This role is crucial for the companys future
Be the lead of the most disruptive revolution in the mobile advertising industry
Work with wicked smart people in the hottest area of modern business  mobile/local/social targeting
Excellent benefits and stock option plan
NinthDecimal Culture:

NinthDecimal, a name as interesting as its people. We are passionate about everything we do, whether its working, telling the NinthDecimal story at industry events, playing our highly competitive ping-pong games, or participating in Beer Fridays. We come from a background of companies like, YP, TiVo, CNET, AOL, Walmart, Coca Cola and many we just picked up out of college. Together we have created a company that is awesome, inspiring, innovative, fast-passed, nimble, and forward thinking.

Sound like a company you want to work for? We are looking for smart, hungry, humble people who are truly passionate about building a great company! If you are looking for a place to put your handprint and make an impression  NinthDecimal wants you! For immediate consideration, please submit your resume to http://NinthDecimal.com/jobs.

NinthDecimal is an AA/EEO employer that actively pursues and hires a diverse workforce."
Software Engineer - Go / Python,DISH Network,"Sling TV L.L.C. provides an over-the-top television experience on TVs, tablets, gaming consoles, computers, smartphones and other streaming devices. Distributed across a variety of strategic device partners, including Google, Amazon, Apple TV, Microsoft, T-Mobile, Sprint, Roku, Samsung, Netflix, and many others, Sling TV offers two primary domestic streaming services that collectively include more than 100 channels of top content. Featured programmers include Disney/ESPN, Fox, ABC, NBC, HBO, AMC, A&E, EPIX, Cinemax, Starz, NFL Network, NFL Networks, NBA TV, NHL Networks, Pac-12 Networks, Hallmark, Viacom, Univision, and more. For spanish-speaker customers, Sling Latino offers a suite of standalone and extra Spanish-programming packages tailored to the U S. Hispanic market. And for those seeking International content, Sling International currently provides more than 300 channels in 20 languages (available across multiple devices) to U.S. households.

Sling TV is the #1 Live TV Streaming Service
(Based on the number of OTT households as reported by comScore as of April 2017)

Sling TV is a next-generation service that meets the entertainment needs of todays contemporary viewers. Visit www.Sling.com .

Qualifications:A successful Software Engineer will have the following:
Bachelors Degree in Computer Science or related field from four-year college or university; and four years of related experience and/or training; or equivalent combination of related experience and education.
4+ years experience programming in platforms including Linux.
4+ years experience with at least one object oriented scripting language (Python & Go highly preferred).
3 or more years experience with a combination of Postgresql, Couchbase, ElasticSearch, Cassandra, and Solr.
2 or more years experience operating database technologies in a Linux environment and an understanding of structured and unstructured data.
Experience with contributing to a project using Git.
Cloud native & microservice patterns with hands on experience.
At Sling TV, we think services based on big bills, bundles, and useless channels are a thing of the past. Sling TV is uniting cord-cutters and smarter consumers everywhere with simply priced mobile access to the top live TV networks  join our team of spirited innovators in changing the way we watch TV. Our office is light, lively, and buzzing with dynamic people. We love what we do, we love who we work with, and we love working to change TV. Brainstorm with us, build with us, banter with us, and even backpack with us  start your next adventure with Sling TV.

Primary responsibilities fall into the following categories:
An intimate understanding of Object Oriented design principles and patterns.
Participate in code reviews from other developers to insure quality, stability, and overall architecture of our code base.
Building highly scalable, highly available web services to handle millions of transactions per day.
Working on an agile team using CI / CD best practices to deliver the highest quality software possible quickly.
You live to experiment, test, fail fast, and learn as you go, we are not looking for a cookie cutter solution to the complex problems we solve.
Working collaboratively in a SAFe or other enterprise agile methodologies"
Software Engineer,AMain Sports & Hobbies,"Job Title: Software Engineer

Reports to: CEO

Summary:
Responsible for the design, development, and maintenance of an e-commerce website.

Duties and Responsibilities:
Development on both back end and front-end ecommerce web applications and desktop applications written in the C# and PHP programming languages using Windows Forms and ASP.NET MVC.

Maintain shipping modules, and integration with internal shipping systems.

Maintain payment modules, and integration with internal accounting systems.

Debug and fix issues reported by users and/or customers.

Optimize and re-factor SQL queries.

Design and prototype new features for the applications.

Help develop new streamlined business operations throughout all departments by developing new software solutions. Skills and Education/Experience:
Language Skills: Ability to read, analyze, and interpret common scientific, technical and programming journals. Ability to write speeches and articles for publication that conform to prescribed style and format. Ability to effectively present information to top management and public groups.
Mathematical & Reasoning Ability: Ability to define problems, collect data, establish facts and draw valid conclusions. Ability to interpret an extensive variety of technical instructions in mathematical or diagram form and deal with several abstract and concrete variables.
General Computer Skills: Word processing / Spreadsheet / Email: Working knowledge of general office application software and email communication.
Software Development Skills: Experience developing web applications and desktop applications in the Microsoft C# programming language as well as Javascript, XML, and CSS. Experience with the Microsoft .Net platform, specifically Windows Forms, Windows Communication Foundation (WCF), and ASP.NET MVC. Experience developing web applications using the PHP programming language is useful, but not critical. Extensive knowledge of batch files, scripting, and programming languages. Use of developer tools including Visual Studio 2008 through 2012 .
Source Code Control. Strong understanding and knowledge of source code control systems, such as Perforce for managing software solutions.
Database Software: Experience implementing, managing, and using relational databases, especially MySQL. Experience developing with NoSQL systems such as Couchbase is useful, but not critical.
Education: BS/BA Degree with 7-10 years of prior experience. Specialized or technical training in: Windows, C#, Microsoft .NET, MySQL, PHP, Javascript, XML, CSS, HTML, scripting.

Work Environment:
The work environment characteristics described here are representative of those an employee encounters while performing the essential functions of this job. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions. The noise level in the work environment is usually moderate.

Physical Demands:
While performing the duties of this job, the employee is regularly required to sit and use hands. The employee is frequently required to talk or hear. The employee is occasionally required to stand, walk, stoop, kneel, crouch or crawl. The employee must occasionally lift and/or move up to 2 pounds. Requires close vision at 20 inches or less. Reasonable accommodations may be made to enable individuals with disabilities to perform the essential functions"
Sr. Data Engineer,Blizzard Entertainment,"Blizzard Entertainment games dont just begin with game ideas, or end once those games are released. A lot more goes into the creation of a Blizzard product than the work of developersand we support our games for years after theyre in the hands of gamers world-wide.
Blizzard Entertainment is looking for a Senior Data Engineer to join our Global Insights team and shape the next generation of the global data infrastructure
In this role, you will be tasked with the evolving design, development, maintenance, and overall integrity of a reliable, scalable data platform that empowers analytics at scale, is a trusted source of truth for analytical data, and provides the security of governance and control without impeding innovation or agility. You will be an integral part of a cross-functional team dedicated to ensuring both strategic cohesion and operational integration between all components of the Global Insights tech stack; proactively identifying and responding to evolving user trends and needs; maximizing system and service stability, reliability, visibility, and overall output; and establishing best practices, providing ongoing consulting, training, troubleshooting, and optimization.
Ultimately, you will be responsible for providing accurate, timely, reliable, and ultimately useful data to the analytical data ecosystem. This mandate involves everything from the discovery, access, and integration of source data into the ecosystem to guaranteeing its health and robustness within that ecosystem to modeling and presenting that data in a manner that allows stakeholders within the ecosystem to perform their duties effectively. You will work closely with other members of the Core Data Team and the Tools and Technology team and will partner fully with the franchise Data Engineers to create a full understanding of the use cases and needs of stakeholders; you will use this deep understanding to develop standardized models and approaches that can then be leveraged by all other parties. You will ultimately be responsible for designing, developing, implementing, and evolving pipeline components  processes, procedures, technology, et. al. - that together promote the maximum visibility and trust in the analytical data ecosystem overall.
Our ideal candidate is the personification of the Khala: a common thread binding all users together; a keyline into a reservoir of power, giving each individual the ability to tap into something both greater than their own island and shared between many; a give-and-take between the individual and the group by which individuals can take from the group what they need and the group can see what those needs are and make adjustments; something that exists above and beyond, held to a higher standard with a perfect clarity of purpose, but which ultimately exists to serve.
Responsibilities
Design, develop, implement, and evolve data pipelines powering core data sets and key business and performance metrics for Blizzard
Identify, troubleshoot, and resolve any performance, system or data related issues, and work to ensure data consistency and integrity
Act as the hub team in a hub-and-spokes model, providing best practices and support to the franchise data engineer spokes
Work with the franchise data engineers as primary technical resources for system escalation
Plan, coordinate, and execute on complex technical projects affecting core data sets and key technologies
Collaborate with the franchise data engineers to exchange ideas and elevate once franchise-specific implementations into the core data infrastructure when needed
Ensure the quality, accuracy, and timeliness of analytical data
Act as a primary technical liaison between Global Insights and the greater tech organization at Blizzard
Requirements
5+ years working in a large analytical data ecosystem
Very strong technical understanding of data modeling, design, architecture principles, and techniques to take business requirements from concept to implementation
Very strong knowledge of relational databases, MPPs, SQL, with an emphasis on Teradata fundamentals
Very strong experience with the Hadoop ecosystem including HDFS, MapReduce, Spark, Hive/Pig, et. al
Strong experience and interest in system performance, optimization, stability, and reliability
Strong verbal and written communication skills across both technical and non-technical audiences
Knowledge of Python, Java, Linux architecture and scripting
Extensive background extracting and transforming complex data sets (ETL process design and administration)
Experience with database design and star schema data warehouse theory
Pluses
Passionate video gamer with in-depth knowledge of Blizzard Entertainment games, products, and services
Experience in technical project management"
Machine Learning Engineer,Hired,"Were on a mission to help get everyone a job they love. Find the job you love here at Hired!

ABOUT US:
At Hired we believe we are entering a new era of work, where people are not looking for jobs  they are seeking opportunities. Focusing on in-demand technology roles, we intelligently match outstanding people to full-time and freelance opportunities at the world's most innovative companies. By taking the pain out of the job search, we help people build purpose-driven careers and businesses find the talent to fulfill their missions. Ultimately, we want to empower everyone to find and do their best work, from one opportunity to the next.

Hired is headquartered in San Francisco, with offices in cities across North America, Europe, Asia and Australia, and plans to expand into new geographies, industries and job categories. For more information, news and tips for candidates and employers, visit Hireds blog.

ABOUT THE ROLE:
Hired is looking for a Machine Learning Engineer to join our Data Science and Relevance team to build new products using machine learning algorithms.

WHAT WE NEED YOUR HELP WITH:
Define key metrics for data science team that have a direct impact on high level business metrics
Use statistics tools to analyze data to understand and model the relationships between different metrics and identify key insights
Generate innovative new features using external and internal data sources to enhance the recommendation engine and other data science products
Strong understanding of all the data in the company and champion data collection from all sources
Collaborate with Product Managers, Business teams to identify opportunities to build data science products
Consult with other engineers to help them run and analyze A/B tests effectively
Have strong understanding of product, business, key KPIs across product and business
Work with leadership to identify key new product initiatives to drive growth
WHAT WE LOOK FOR:
5+ years of experience building products using data science and machine learning algorithms
Expert level knowledge in SQL and at least one of the advanced stats tools
Strong experience in programming languages like Python, Java or similar languages
Strong experience in statistics, machine learning models and mathematical theory
Passion for learning new technologies fast
MS or PhD in Computer Science or related field
COMP, BENEFITS, & PERKS:
Competitive compensation & equity options
Open vacation & sick time
Flexible work hours
Employer paid health insurance, vision, and dental
401(k)
Pre-tax commuter benefit
Gym reimbursement
Weekly educational classes & catered lunch
Monthly team events
Stocked kitchen & beverages
Cell phone reimbursement
We believe that when we can bring our whole selves to work on a day-to-day basis we become happier, more comfortable, more confident and more excited to do great things for our company, each other, our product and our users. Hired aims to both build an internal team as well as help our clients build their teams with talent from all different backgrounds and lifestyles.

Hired is committed to building a diverse, equitable and inclusive workforce. Hired is an equal opportunity employer; we welcome and consider qualified applicants regardless of gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, pregnancy status, veteran status, or any other differences. Members of communities historically underrepresented in tech are encouraged to apply.

For San Francisco applicants: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records for employment."
Video Data Communication VTC Engineer,"Technical Management Resources, Inc.","Responsibilities
Setup, operation, troubleshoot issues & maintain video teleconferencing (VTC) equipment.
Coordinate, schedule and maintain calendar for VTC sessions.
Provide A/V equipment support for internal promotion ceremonies.
Strong ability to multitask & work well under pressure.
Exceptional written, verbal and interpersonal skills.
Extensive use of business computer systems including Microsoft Office applications such as MS Word, Excel and Outlook.

Qualifications Working experience with videoconferencing equipment such as:
Codecs from Polycom and Cisco
Familiarity with AMX & Crestron systems  MCU and control systems from Polycom and Cisco etc.
323 Endpoints, Gateways, MCUs, and Gatekeepers

Education Requirements:
Associates Degree and 5 years experience; or
Bachelors Degree and 3 years experience; or
Masters Degree and 1 year experience; or
5 years experience and 2 years specialized training; or
7 years experience

Additional Requirements:
At OTC, the Voice/Data Communications Engineer (Journeyman) will be a working leader and will need to meet the aforementioned qualifications and will need 2 years experience leading 1 or more people.
Must possess DISA DVS Level II Certification or equivalent certification.
Relevant experience includes operating a VTC suite, VTC gateway and teleconference bridges. Ability to obtain/maintain Top Secret clearance is required.

MANDATORY REQUIREMENT: Candidate must be a US Citizen in order to obtain and maintain a Top Secret security clearance."
Data Warehouse Engineer,Veterans United Home Loans,"Veterans United Home Loans is seeking a Data Warehouse Engineer to join the Software Services department in the Technology division. As a Data Warehouse Engineer, you will perform data modeling, business requirements gathering, and assist in the design and development of the companys Enterprise Data Warehouse. This position will not be originating loans.

Responsibilities:
Thoroughly and quickly analyze business requirements for reporting and analysis, and translate them into a good technical data design.
Partner with Product Owners to refine the business requirements that must be met for building and maintaining the Enterprise Data Warehouse.
Provide expertise in data profiling.
Translate business requirements into conceptual, logical, and physical data models.
Develop and maintain acceptance tests per requirements.
Support, develop and maintain technical specification documentation.
Constantly strive to improve data accuracy and organization.
Focus on reducing redundancy of data within the Enterprise Data Warehouse.

Necessary Skills:
Ability to manage time and multitask.
Strong interpersonal communication skills (written and verbal).
Proven ability to learn new concepts, tools, languages and techniques quickly.
Excel in fast-paced, results-oriented environment.
Thrive in a highly collaborative work setting.

Preferred Experience:
Strong background with data warehousing and reporting architectures, dimensional modeling (Kimball).
Familiarity with Agile methodologies.
Experience collaborating and gathering requirements from both technical and business-facing teams.
Proficiency in at least one ETL toolset (ideally, SSIS), with experience creating and maintaining data in an Enterprise Data Warehouse environment.
Experience using Microsoft SQL Server DBMS.
Familiarity and experience with ERWIN or similar data modeling software.
Working knowledge of common data visualization/reporting tools (Tableau, Power Pivot, tabular cubes, etc.).
Experience with Team Foundation Services or other change management software.

Required Experience:
Proficient in developing complex SQL queries.
Proven professional experience in data analysis, reporting, and data warehousing.
Proficient in basic computer technologies, appropriate for an internet-based company.

Veterans United is proud to be an EOE/ADA and Military Friendly employer"
Software Engineer,"NTT DATA Consulting, Inc.","At NTT DATA Services, we know that with the right people on board, anything is possible. The quality, integrity, and commitment of our employees have been key factors in our companys growth and market presence. By hiring the best people and helping them grow both professionally and personally, we ensure a bright future for NTT DATA and for the people who work here. Our Consulting practice helps solve problems and drive change by providing the expertise and specialized counsel to clients with industry-centric solutions.

Basic Qualifications:
3 + years experience in Software Development and/or Quality Assurance
Desired Skills:
Agile/Scrum
JavaScript/Protractor or Angular
Create Automation Frameworks (data driven, hybrid etc)
CI/CD - Jenkins
Cucumber
Test-driven development, continuous integration, and automated testing
Designing, developing and executing software
Works closely with Developers, Architects, and Operations to ensure application code Quality thru use of code reviews, UNIT test automation and will also be critical in standing up continuous integration and continuous delivery capabilities
Experience with Automation Tools & Technologies: (similar)Selenium Web Driver/Coded UI/ Protractor/SAHI Pro/KARMA, Parasoft SOAtest, Parasoft Service Virtualize
Ability to drive Strategy & Change across organizations, build Continuous Integration/Delivery automation solutions for each product/project
Effectively communicate, collaborate, and deliver in a cross-team environment
Strong problem solving skills"
Database Engineer,CalAmp,"Overview Based in beautiful Irvine, CA, we are CalAmp, the pure-play pioneering leader of the Connected Car, Connected Truck and broader Internet of Things (IoT) marketplace. Currently, we are seeking a Sr. Database Administrator (DBA) with at least 5 years of data management experience. This is an exciting opportunity for those who wish to work for a stable, well-established company that builds transformational technologies within the revolutionary domain of IoT.

We believe that people are our greatest asset and we are committed to being an employer of choice in our industry. CalAmp offers an engaging and diverse work environment that permits our people to take pride in their contributions and share in the companys success. Our employees can expect the space to showcase their talent, sharpen their skills, develop new capabilities, and be a part of a global team that develops revolutionary technologies. We proudly offer the stability and security of a large publicly-traded tech company without the rigidity and red tape.

In particular, we offer:
Meaningful work with the potential to disrupt an entire industry
Visionary leadership
Excellent compensation packages
An extensive suite of medical and retirement benefits
Flexible time off policy and accommodating work schedules
Education Assistance Program (Tuition Reimbursement)
Access to cutting-edge tools and technologies
Innovative, intelligent, collaborative teammates

Responsibilities
Database administration and maintenance of all SaaS databases (Oracle, MySQL, PostgreSQL, Mongo)
Develop backup procedures for daily execution by IT/DevOps
Conduct ""Health check"" of the database regularly
Troubleshoot and support existing database processes; deliver fixes and optimizations where appropriate
Analyze, tune and document schema and queries
Monitor for database exceptions
Manage any database schema evolutions required
Develop arching scripts to prune live data
Develop ETL environment for data mining
Architect, design, configure and deploy next generation of the database system

Qualifications Must Haves
5 years of Database Administration and/or Engineering experience
PostgreSQL
Oracle
Excellent SQL skills
Experience with Cloud based in environments
Experience with database performance tuning
ETL
Database server, storage & network systems lifecycle administration
Database clustering architecture & administration
Scripting alerts and controls for DB system monitoring agents
Familiar and experience with SDLC - requirements analysis, software development and installation, integration, evaluation, maintenance, testing, and problem diagnosis and resolution.
Ability to provide high confidence estimate for design change or solution implementation
Excellent written and oral communication skills
Bachelors in Information Systems, Computer Science, Computer Engineering degree or similar
Strong Pluses
MongoDB
MySQL
Experience with AWS RDS implementation and support
scripting with Python/Perl, etc.)
Linux systems implementation (RedHat, CentOS, Ubuntu, SUSE), including strong SELinux knowledge of security contexts, iptables, access control, and logging
Experience with Distributed and High-availability (HA) DB systems, networking concepts & protocols
Experience in a business operation requiring high-volume, high-availability information processing
AWS Certified, Mongo Certified, PostgreSQL Certified.
Experience with 3rd party reporting tools
Mission critical database performance enhancement and tuning

If you are a talented Sr. Database Administrator with at least 5 years of data management experience and an interest in the Internet of Things domain, we want to speak with you. Interviews are occurring this week and next so apply now if interested.

#L1-RB1"
Senior Data Engineer,Okta,"Position Description:
We are looking for an exceptional Senior Data Engineer who is responsible for designing and developing robust, scalable solutions for large-scale data infrastructure in a fast-paced agile environment. You will be encouraged to initiate new projects to collect, store, and analyze data and to partner with analysts, engineers, business stakeholders to push Okta forward. The ideal candidate will have a strong engineering background with the ability to tie engineering initiatives and business impact. You will participate in detailed technical design, development and implementation of applications using cutting-edge technology stacks.

Job Duties and Responsibilities:
Design highly-scalable and reliable data pipelines to consume, integrate and analyze large volumes of complex data from different sources (both batch and near-real-time) to support the fast-growing needs of our business
Build data access layer to provide data service to internal and external business stake holders
Constantly evolve data pipeline and data models to balance scalability and performance
Strong understanding of analytics needs and proactively come up with architectural patterns to improve the efficiency
Interface with Data Analysts and Data Scientists to gather requirements and build functionality
Support and enhance existing data infrastructure
Build data expertise and own data quality for allocated areas of ownership
Build and experiment with different tools and technology, and share learnings with the team
Contribute to evaluation of new technologies
Required Skills:
BS Computer Science, Engineering or another quantitative field of study
5-7 years in a data engineering role
Strong knowledge in data warehouse and data lake space
Experience with building distributed system
Expertise in a programming language (Preferably Python or JAVA)
SQL, SQL, SQL!
Strong Experience with ETL tools such as Pentaho, Talend, or Informatica
Strong experience with relational database and columnar MPP database Redshift or Teradata, and Vertica
Strong experience with database and application performance tuning
Experience with different operating system (Unix/Linux/Windows)
Experience with Continuous Integration and Continuous Deployment tools and procedures such as Jenkins, Git, Chef, and Ansible
Experience with cloud infrastructure/platform(AWS, Azure, Google Cloud Platform)
Hands on and deep experience with schema design and data modeling
Familiar with Jira, Confluence and Agile methodology
Experience with real-time or near real-time data streaming using Storm or Spark
Experience with Kinesis or Kafka, Storm, Elastic, S3, Splunk, Spectrum, Redis, MongoDB, Spark, Zookeeper
Experience with building API using Java, python, php or other programming languages
Familiar with Tableau, Looker, Qlikview, Microstrategy, or other data visualization tools
Experience with CRM systems such as Salesforce
Preferred Skills:
Distributed data visualization web application development experience
Machine learning
Big Data concept and experience
Soft Skills:
Team player
Detail oriented
Self-driven personality
Innovative and ability to execute
Excellent oral and written communication skills. Must be able to interact cross-functionally with both technical and non-technical people
Okta is an Equal Opportunity Employer.

#LI-SS1"
Software Engineer,"Resolvit, LLC","RESOLVIT
Bringing Solutions That Make Business Better

We are seeking a Software Engineer to be part of a creative, forward-thinking team. Our success at deploying skilled, highly knowledgeable experts has landed us on the Inc. 5000 list of Americas fastest-growing companies four times  and were just getting started.

As the Software Engineer , you will develop and maintain software solutions by building high-quality, innovative, and fully performing software in compliance with coding standards and technical design. You will create and manage code that runs a suite of software product lines while overseeing multiple, dissimilar development projects simultaneously. You will also be responsible for ensuring that deliverables are completed on time. Additionally, you will:
Design, modify, develop, write, and implement software programming applications
Participate in the testing process through test review and analysis, test witnessing, and certification of software
Meet with stakeholders to define project scope
Oversee projects completely, from requirements document to delivery, by reviewing user stories, creating acceptance criteria, coordinating with project manager to maintain project visibility and completion estimates, and managing change control
Code according to designated standards and guidelines
Assign and manage project tasks, ensuring tasks are completed by assigned developers on time and in accordance with standards and guidelines
Train both colleagues and end users
Assist in preliminary support of projects during the early stages of the release cycle
Be capable of discussing almost all aspects of the business units system when required, such as during conferences
Rely on experience and judgment to plan and accomplish goals
Exhibit a wide degree of creativity and latitude
What Youll Need to be Successful:
2-4 years of relevant experience in the software development field or a related area
Knowledge of programming languages: HTML, ASP.net, VB.NET, C#, Java, JavaScript, and SQL
Experience with Microsoft Visual Studio and using MySQL and MS SQL database engines
Experience with UI development (using WinForms and WPF)
Strong computer skills as well as excellent verbal and written communication skills
Organized, detail-oriented, self-motivated, and willing to develop professionally
A strong team player with the ability to build positive and collaborative relationships within the organization
A strong understanding of the business units system, code base, and processes
Ability to work on multiple priorities and/or projects simultaneously with sufficient efficiency and quality
Ability to lead and manage the progress of a project and meet deadlines
Ability to understand complex algorithms, work under general supervision, and manage multiple, dissimilar deliverables while training others on deliverables
Great Additional Skills:
Bachelors degree in a Computer Science-related field is preferred
We currently have more than 100 open career opportunities across the country, so be sure to mention the appropriate Job Code with any correspondence!

About Resolvit:

Resolvit is an international technology consulting firm with industry-leading customers in the financial services, high tech, manufacturing, retail, life sciences, and government sectors. Through its partnerships, Resolvit delivers highly impactful, innovative solutions across five core areas: Infrastructure Modernization, Application Development Services, Enterprise Data Management & Analytics, Knowledge & Content Management, and Strategic Staffing."
Senior Backend Engineer - Data,Cruise,"We're the driverless car company. Were building the worlds best autonomous vehicles ( https://www.driverless.id/news/video-breakdown-gms-unicorn-cruise-shows-off-level-4-skills-sf-passing-uber-maybe-waymo-0176031/ ) to safely connect people to the places, things, and experiences they care about.

Our vehicles are on the road in California, Arizona, and Michigan navigating some of the most challenging and unpredictable driving environments. Were hiring people who want to solve some of todays most complex engineering challenges and make a positive impact.

We're looking for a Backend Engineer to join our Data team!

Responsibilities:
Design and implement backend systems to manage our growing fleet of autonomous vehicles
Work closely with vehicle engineering to enable real time vehicle communication and feedback
Build highly scalable data pipelines to handle ingestion and processing of vehicle data
Design, build, and improve data stores for our fleet of self-driving cars
Requirements:
At least 3-5 years of experience building scalable backend APIs
Experience with SOA or microservice-based architectures
Experience working with relational and NoSQL databases
Experience programming with Go, Node.js, C++, or similar
BS, MS or PhD in CS, or equivalent real-world experience
Passionate about self-driving technology and its potential impact on the world
Attention to detail and a passion for correctness
Bonus Points:
Experience working with Docker development and deployment workflows
Experience installing, configuring, and managing AWS or on-premises server infrastructure
Experience with Kafka, Hadoop, Spark, or other data processing tools
Perks:
Solve difficult problems that have immediate and valuable real-world applications
Competitive salary and benefits including matched 401k, medical / dental / vision, AD+D and Life
Paid parental leave
Flexible vacation and 10 paid company holidays
State of the art equipment for your work station
Lunch, snacks, and dinner
Free rides in self-driving cars!
GM Cruise LLC provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity or expression, veteran status, or genetics. In addition to federal law requirements, GM Cruise LLC complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Privacy Statement ( https://getcruise.com/privacy )"
Senior Data Engineer,Cruise,"We're the driverless car company. Were building the worlds best autonomous vehicles ( https://www.driverless.id/news/video-breakdown-gms-unicorn-cruise-shows-off-level-4-skills-sf-passing-uber-maybe-waymo-0176031/ ) to safely connect people to the places, things, and experiences they care about.

Our vehicles are on the road in California, Arizona, and Michigan navigating some of the most challenging and unpredictable driving environments. Were hiring people who want to solve some of todays most complex engineering challenges and make a positive impact.

We're looking for a Data Engineer to join our team!

Responsibilities:
Design and implement backend systems to manage our growing fleet of autonomous vehicles
Work closely with vehicle engineering to enable real time vehicle communication and feedback
Build highly scalable data pipelines to handle ingestion and processing of vehicle data
Design, build, and improve data stores for our fleet of self-driving cars
Requirements:
5+ years working with big data
Experience building scalable data pipelines
Experience working with relational and NoSQL databases
Experience with Kafka, Hadoop, Spark, or other data processing tools
BS, MS or PhD in CS, or equivalent real-world experience
Passionate about self-driving technology and its potential impact on the world
Attention to detail and a passion for correctness
Bonus Points:
Experience programming with Go, Node, C++, or similar
Experience working with Docker development and deployment workflows
Experience installing, configuring, and managing AWS or on-premises server infrastructure
Perks:
Solve difficult problems that have immediate and valuable real-world applications
Competitive salary and benefits including matched 401k, medical / dental / vision, AD+D and Life
Paid parental leave
Flexible vacation and 10 paid company holidays
State of the art equipment for your work station
Lunch, snacks, and dinner
Free rides in self-driving cars!
GM Cruise LLC provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity or expression, veteran status, or genetics. In addition to federal law requirements, GM Cruise LLC complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Privacy Statement ( https://getcruise.com/privacy )"
Senior Data Engineer,BigCommerce,"Summary

BigCommerce named a Best and Brightest place to work in San Francisco is looking for a Senior Data Engineer. This is an exciting opportunity to build a real time big data pipeline and SaaS analytics platform using latest cloud technologies for the #1 e-commerce SaaS platform. Our perfect candidate will have a background in Computer Science or a related technical field and possess extensive experience in building distributed scalable data pipelines and working with large data stores. You will be working with cross-functional teams in highly visible role. You are scrappy, focused on results, a self-starter and have demonstrated success in using engineering and analytics to drive the understanding, growth and success of a product.

What youll do:
Build scalable and reliable near real time data pipeline on cloud (AWS and GCP) that collects, transforms, loads and curates data from various internal and external data sources
Build a scalable distributed data store that will be central source of truth
Own data quality for the pipelines you build and make them auditable
Build self service tools that helps our data consumers to extract, analyze and visualize data faster
Evaluate new technologies and build prototypes for continuous improvements in Data Engineering
Partner with Infrastructure and Engineering teams to ensure instrumentation, logging and monitoring is in place
Who you are:
Extensive experience in using big data technologies such as Kafka, Spark, HBase or their equivalents
Experience with AWS or GCP cloud data platform
3+ years of experience with Java/Scala and Python
2+ years of experience in building and monitoring near real time scalable ETL pipelines
Experience with shell scripting
Excellent written and verbal communication skills
BS or MS in Computer Science or related technical field
Nice to have:
Experience with Redshift
Experience with Google BigQuery, BigTable, Cloud Dataflow and PubSub will be plus
Experience with Go and PHP will be a plus
Experience with tools such as AirFlow for scheduling and monitoring of data workflow will be a plus
Experience working with a team distributed over time zones
BigCommerce is a robust, flexible commerce platform that provides established and emerging brands with everything they need to launch, promote, manage and scale a successful online store. BigCommerce merchants earn more than retailers on competitive commerce solutions thanks to advanced features, better site performance and streamlined business processes. Founded in 2009, the company has received more than $100 million in funding from SoftBank Capital, American Express, Telstra Ventures, General Catalyst, Revolution Growth and FLOODGATE. BigCommerce supports thousands of retailers around the world from its offices in San Francisco, Austin and Sydney. Were a team of smart, passionate and talented people on a mission to democratize commerce. If youre looking to work with industry leaders, we want to hear from you."
Senior Data Engineer,Hired,"Were on a mission to help get everyone a job they love. Find the job you love here at Hired!

ABOUT US

At Hired we believe we are entering a new era of work, where people are not looking for jobs  they are seeking opportunities. Focusing on in-demand technology roles, we intelligently match outstanding people to full-time and freelance opportunities at the world's most innovative companies. By taking the pain out of the job search, we help people build purpose-driven careers and businesses find the talent to fulfill their missions. Ultimately, we want to empower everyone to find and do their best work, from one opportunity to the next.

Hired is headquartered in San Francisco, with offices in cities across North America, Europe, Asia and Australia, and plans to expand into new geographies, industries and job categories. For more information, news and tips for candidates and employers, visit Hireds blog.

ABOUT THE ROLE

Hired is looking for a data infrastructure engineer to help build the next generation of data platform and products. The person will contribute to the vision of data infrastructure, building a scalable data platform to store, process and query terabytes of data. Youll work on some of the most interesting big data technologies (Kinesis, Spark, Redshift) in the world.

The pace and quality of our decision-making is dependent upon data availability and reliability. As we are starting to build a data infrastructure team, we are looking for a senior data infrastructure engineer to lead the effort to design and build a scalable platform.

If youre passionate about big data and data-driven engineering, curious about any problem where data can be applied, and love scaling datasets, wed love to hear from you!

WHAT WE NEED YOUR HELP WITH

4+ years of experience in an engineering environment
Design and build a scalable data platform to store, process and query terabytes of data
Diagnose and fix complex distributed systems problems
You will be driving the technologies and policies needed to ingest, store, manage and interact with Hireds data
You will be coordinating with Search, Data Science, Sales, Marketing and Insights teams to help understand and support their needs
WHAT WE LOOK FOR

Excellent problem solver and communicator, and can document your work
Experience in working with one or more distributed systems like Hadoop, Spark, Presto or Redshift
Experience programming in Python or Java
You should have designed and developed production quality data pipelines in a distributed systems environment using data pipeline frameworks such as Luigi, Airflow or Pinball
THESE WOULD ALSO BE NICE

Experience with Amazon AWS
Experience programming with Ruby
COMP, BENEFITS, & PERKS

Competitive compensation & equity options
Open vacation & sick time
Flexible work hours
Employer paid health insurance, vision, and dental
401(k)
Pre-tax commuter benefit
Gym reimbursement
Cell phone reimbursement
Weekly educational classes & catered lunch
We believe that when we can bring our whole selves to work on a day-to-day basis we become happier, more comfortable, more confident and more excited to do great things for our company, each other, our product and our users. Hired aims to both build an internal team as well as help our clients build their teams with talent from all different backgrounds and lifestyles.

Hired is committed to building a diverse, equitable and inclusive workforce. Hired is an equal opportunity employer; we welcome and consider qualified applicants regardless of gender identity or expression, sexual orientation, race, religion, age, national origin, citizenship, pregnancy status, veteran status, or any other differences. Members of communities historically underrepresented in tech are encouraged to apply.

For San Francisco applicants: Pursuant to the San Francisco Fair Chance Ordinance, we will consider qualified applicants with arrest and conviction records for employment."
Software Engineer,"Synthetic Genomics, Inc.","At Synthetic Genomics, Inc., we are dedicated to developing and commercializing genomic-driven solutions to address global challenges. We are currently seeking a Software Engineer to join our fast-growing, dynamic and collaborative team in La Jolla, CA.

The successful candidate will plan, design, and write software modules and web applications in support of laboratory process tracking and order intake and management.

This position interacts with all levels of internal customers and external vendors. That interaction requires excellent communication and analytical/troubleshooting skills in identifying problems and implementing efficient solutions.

Effectiveness in this role requires an in-depth working knowledge of software engineering and manufacturing processes, equipment and systems. The incumbent works on problems of moderate to high scope and complexity requiring analysis of various factors, exercises judgment within broadly defined procedures. The incumbent normally receives no instruction on routine work and general instruction on new assignments.

Required Skills:
Microsoft .NET application programming using C#, ASP.NET MVC, ADO.NET, Entity Framework, Javascript, AJAX, and jQuery.
Design, develop, and support advance data-driven applications/reports with Microsoft SQL Server, SQL Server Integration Services (SSIS), SQL Server Reporting Services (SSRS), DTS, SQL, and stored procedures.
Design and develop web services using WCF, ASP.NET Web API, SOAP, REST, XML, and JSON.
Plan, design, write and debug production software to automate and track production and inventory.
Responsible for generation of software validation protocols and test programs as well as software requirements and specification documents
Review product and/or application information including manuals and brochures for technical accuracy
Able to communicate non-technically to laboratory clients related to production issues and gathering new functional requirements.
Collaborate with Engineering staff and professionals in other departments in the design, development, construction, installation and maintenance of software applications.
Contribute to the Software Engineering effort on a high-level in cross-functional project teams, identify project specifications and equipment needs and carry out the necessary effort. Will interface with external contractors or OEM developers and coordinate with hardware/firmware development.
Troubleshoot the most challenging problems and recommend and implement solutions.
May perform other related duties as required and/or assigned.
Required Experience:
Bachelors or Masters degree in Computer Science or Software Engineering.
Requires 5+ years relevant engineering experience or equivalent.
Requires 3+ years design and development experience using Microsoft .NET Framework, Microsoft SQL Server, C#, ASP.NET, and web services.
Requires experience working in a laboratory or manufacturing environment.
Familiarity with Agile software development process.
Familiarity with development of software under FDA/GMP regulatory or ISO standards preferred.
Experience with testing frameworks a plus.
Experience with database administration a plus.
Experience with any LIMS a plus
Experience with Linux a plus.
Experience with Apache a plus.
Experience with IIS a plus.
Experience with ActiveDirectory a plus
Experience with Continuous Integration servers (Jenkins or similar) a plus
Experience with Docker a plus"
Senior Data Infrastructure Engineer- Spark,Cruise,"We're the driverless car company. Were buildingthe worlds best autonomous vehicles ( https://www.driverless.id/news/video-breakdown-gms-unicorn-cruise-shows-off-level-4-skills-sf-passing-uber-maybe-waymo-0176031/ ) to safely connect people to the places, things, and experiences they care about.

Our vehicles are on the road in California, Arizona, and Michigan navigating some of the most challenging and unpredictable driving environments. Were hiring people who want to solve some of todays most complex engineering challenges and make a positive impact.

Our Infrastructure team is building a highly resilient, performant, and secure internal Platform as a Service which hosts our core backend services to control our fleet of vehicles, runs driving simulations at scale, and executes machine learning training jobs for our Autonomous Vehicle engineering team. Were currently using AWS, Docker, Kubernetes, Vault, and Spinnaker, but were also interested in your experience and suggestions.

Responsibilities:
Build highly scalable data pipelines to handle ingestion and processing of vehicle data
Design systems to manage our petabyte-scale data clusters
Manage the operations of our data analysis clusters (Apache Spark) to ensure proper operations and capacity planning
Design, build, and improve data stores for our fleet of self-driving cars
Requirements:
5+ years working with big data
Experience building scalable data pipelines
Experience working with relational and NoSQL databases
Experience with Kafka, Hadoop, Spark, or other data processing tools
BS, MS or PhD in CS, or equivalent real-world experience
Passionate about self-driving technology and its potential impact on the world
Attention to detail and a passion for correctness
Experience working with Docker development and deployment workflows
Bonus Points:
Experience programming with Python, Go, C++, or similar
Experience installing, configuring, and managing AWS or on-premises server infrastructure
Perks:
Solve difficult problems that have immediate and valuable real-world applications
Competitive salary and benefits including matched 401k, medical / dental / vision, AD+D and Life
Paid parental leave
Flexible vacation and 10 paid company holidays
State of the art equipment for your work station
Lunch, snacks, and dinner
Free rides in self-driving cars!
GM Cruise LLC provides equal employment opportunities (EEO) to all employees and applicants for employment without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, gender identity or expression, veteran status, or genetics. In addition to federal law requirements, GM Cruise LLC complies with applicable state and local laws governing nondiscrimination in employment in every location in which the company has facilities. This policy applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training. Applies to all terms and conditions of employment, including recruiting, hiring, placement, promotion, termination, layoff, recall, transfer, leaves of absence, compensation and training.

Privacy Statement ( https://getcruise.com/privacy )"
"Senior Data Science Engineer - Reston, VA","Davidson Technologies, Inc.","As a Senior Data Science Engineer, the successful candidate will be responsible for utilizing statistical techniques to analyze and extract relevant information from large amounts of data, automate & optimize data extract, cleaning, processing & analyzing functions, develop, validate, & implement data models to solve problems/answer questions, develop & implement novel statistical approaches to address complex issues, and perform data science/engineering with little guidance and independently write reports detailing methods, results, & impacts to assigned issues.

Qualifications

Requirements:
Doctorate in Mathematics, Statistics, Operations Research, Engineering, Science or equivalent experience with a minimum of ten (10) years' experience performing data science/engineering work and writing reports based on the resulting work/accomplishments
Significant current experience, within the last two (2) years, in data science/engineering work
Published data science reports and/or authored new procedures/methods to create innovative statistical approaches to solve complex issues
Proficiency in statistics packages such as SPSS, SAS, S-PLUS, and R
Programming skills sufficient to extract, transform, and clean multi-terabyte databases

Clearance:
Must possess a top secret clearance with the ability to obtain a top secret/sensitive compartmented information (TS/SCI)"
"Sr. Software Engineer(Data Engineering), Programmatic TV",Adobe,"TubeMogul, now part of the Adobe family, is a leader in video advertising enabling brands and agencies to plan and buy video advertising across desktops, mobile, streaming devices and TVs. TubeMogul officially joined Adobe in December 2016. The combination of Adobe and TubeMogul will create the first end-to-end independent advertising and data management solution that spans traditional TV and digital formats, simplifying what has been a complex and fragmented process for the worlds biggest brands. The planned integration with Adobe Audience Manager will allow marketers to unify audience planning and media buying for video advertising.
About the role:
This role will be part of a team within the PTV engineering group that designs, builds and maintains core PTV data services. TV audience and viewership data are fundamental to the systems and processes in Programatic TV planning, buying and reporting. The goals of PTV data services include ensuring that this data is accurate, consistent and is updated and available to application consumers according to established SLAs.
What youll do:
Design, build and maintain highly scalable data pipelines to ingest and process TV audience and viewership data on a daily basis from several sources.
Leverage various data storage facilities to support data access patterns (e.g. relational DBs, in-memory caches, elasticsearch).
Design, develop and maintain APIs to expose core data to PTV planning, ordering and reporting systems.
Develop tools and processes to guarantee data integrity and accuracy from data source to API.
Design and plan out core infrastructure enhancements to meet performance and scalability requirements.
Use and promote software engineering best practices across the team.
What youll need to be successful:
Expert level Java development experience (3+ years experience)
Experience with big data pipeline tools and techniques
Strong experience with SQL and NoSQL databases and data modeling
Solid Linux/Unix skills
Excellent problem solving skills and ability to work independently
Strong sense of ownership and track record of quality work
Excellent communication and collaboration skills
At Adobe, you will be immersed in an exceptional work environment that is recognized throughout the world on Best Companies lists . You will also be surrounded by colleagues who are committed to helping each other grow through our unique Check-In approach where ongoing feedback flows freely.
If youre looking to make an impact, Adobe's the place for you. Discover what our employees are saying about their career experiences on the Adobe Life blog and explore the meaningful benefits we offer.
Adobe is an equal opportunity employer. We welcome and encourage diversity in the workplace regardless of race, gender, religion, age , sexual orientation, gender identity, disability or veteran status."
Business Intelligence Engineer,Amazon.com,"Amazon Lab126 is an inventive research and development company that designs and engineers high-profile consumer electronics. Lab126 began in 2004 as a subsidiary of Amazon.com, Inc., originally creating the best-selling Kindle family of products. Since then, we have produced groundbreaking devices like Fire tablets, Fire TV and Amazon Echo. What will you help us create?
The successful candidate will have strong technical acumen and experience in developing reporting infrastructures and flexible front-end tools. The candidate will also have strong communication skills and the ability to work effectively with cross-functional teams in a fast-paced and ever-changing environment.

While we are looking for a business intelligence engineer, we will also consider business intelligence analyst candidates with the right skill sets.

Responsibilities
Conduct deep dive analyses of business growth opportunities and formulate conclusions and recommendations to be presented to senior leadership.
Produce written recommendations and insights for key stakeholders that will help shape effective strategies to better meet our customers needs.
Build solutions to have maximum scale and self-service ability by stakeholders.
Improve back-end data sources for increased accuracy and simplicity.
Recognize and adopt best practices in reporting and analysis: data integrity, test design, analysis, validation, and documentation.
Support business with time-critical tactical data analyses.
Understand a broad range of Amazons data resources and know how, when, and which to use and which not to use.

Basic Qualifications
Bachelors degree in math, finance, engineering, statistics, or related discipline is required
5+ years professional experience in web analytics, business analysis, or comparable analytics position.
Experience in designing and implementing custom reporting systems by managing or guiding technical teams with tools such as Tableau or Cognos
Experience and expertise with MS Excel, Access and SQL/SAS
Solid written and verbal communication skills
Demonstrated ability to manage and prioritize workload and roadmaps

Preferred Qualifications
Familiarity with AWS solutions such as EC2, DynamoDB, S3, and Redshift.
Masters degree in Computer Science, Engineering, Math, Finance, Statistics or a related discipline.
Advanced knowledge and expertise with SQL, Linux, Oracle and OLAP and data warehousing concepts.
Knowledge and direct experience using business intelligence reporting tools. (OBIEE, Business Objects, Cognos, Tableau, MicroStrategy, SSAS Cubes, etc.).
Knowledge of scripting for automation (e.g. Python, Perl, Ruby)

Lab126 is part of the Amazon.com, Inc. group of companies and is an Equal Opportunity-Affirmative Action Employer  Minority/Female/Disability/Veteran/Gender Identity/Sexual Orientation."
Digital Big Data -Software Engineer,JP Morgan Chase,"JPMorgan Chase & Co. (NYSE: JPM) is a leading global financial services firm with assets of $2.6 trillion and operations worldwide. The firm is a leader in investment banking, financial services for consumers and small business, commercial banking, financial transaction processing, and asset management. A component of the Dow Jones Industrial Average, JPMorgan Chase & Co. serves millions of consumers in the United States and many of the worlds most prominent corporate, institutional and government clients under its J.P. Morgan and Chase brands. Information about JPMorgan Chase & Co. is available at http://www.jpmorganchase.com/ .

As an Application Developer, you will be responsible to provide high quality technology solutions that address business needs by developing applications within mature technology environments. You will utilize mature (3rd or 4th Generation) programming methodologies and languages and adhere to coding standards, procedures and techniques while contributing to the technical code documentation.

You will participate in project planning sessions with project managers, business analysts and team members to analyze business requirements and outline the proposed technical solution. Primarily, you will participate in sprint planning, backlog grooming, and daily stand up discussions as part of an application development scrum team. You will participate in design reviews and provide input to the design recommendations, incorporate security requirements into design, and provide input to information/data flow, and understand and comply with Project Life Cycle Methodology in all planning steps. You will adhere to IT Control Policies throughout design, development, and testing and incorporate Corporate Architectural Standards into application design specifications. Enforcement of development and testing standards will be achieved through regular code reviews of peers. Additionally, you will work with your team to ensure proper documentation of detailed application specifications, translate technical requirements into programmed application modules, and develop and enhance software application modules. You will participate in code reviews and ensure that all solutions are aligned to pre-defined architectural specifications, identify and troubleshoot application code-related issues, and review and provide feedback to the final user documentation. You will be expected to encourage collaboration within your team as well as across development teams.

The position requires excellence with Java/J2EE Enterprise web development and developing all tiers of an application (middleware, integration, and database). It requires knowledge of the Spring framework and Object Oriented methodology of design and development. It requires knowledge of JDBC including transaction management, caching, and performance tuning and web services Service Oriented Architecture (SOA).

Minimum five years of Application Development work experience

Solid application design, coding, testing, maintenance and debugging skills

Minimum of five year of work experience in a client server environment

Advanced experience in Java, J2EE, Spring, multi-threaded applications, and web service programming and design techniques

Solid understanding of Test Driven Development, JUnit, Mockito, MQ, Maven, Apache Tomcat, Repository Management (Git)

Basic knowledge of NoSQL and SQL

Knowledge of version and revision control practices and procedures"
"Data Engineer, Infrastructure",Doximity,"Why work at Doximity?

Doximity is rewiring healthcare and is the 6th fastest-growing technology company in North America ( http://www.prnewswire.com/news-releases/doximity-is-fastest-growing-company-in-bay-area-per-deloittes-2016-technology-fast-500-300367390.html ). Here's how clinicians use our products. ( https://res.cloudinary.com/dhttas9u5/image/upload/WhyDocsUseDox-Infographic_20160930_adpvj6.pdf ) For us, transparency is key. Ensuring your goals and values align with ours is also an important step. Take a look at how we Work at Doximity. ( https://workat.doximity.com/ )

You will join a small team of data infrastructure engineers (4) to build and maintain all aspects of our data pipelines, ETL processes, data warehousing, ingestion and overall data infrastructure. We have one of the richest healthcare datasets in the world, and we're not afraid to invest in all things data to enhance our ability to extract insight.

Job Summary

Help establish robust solutions for consolidating data from a variety of data sources.
Establish data architecture processes and practices that can be scheduled, automated, replicated and serve as standards for other teams to leverage.
Collaborate extensively with the DevOps team to establish best practices around server provisioning, deployment, maintenance, and instrumentation.
Build and maintain efficient data integration, matching, and ingestion pipelines.
Build instrumentation, alerting and error-recovery system for the entire data infrastructure.
Spearhead, plan and carry out the implementation of solutions while self-managing.
Collaborate with product managers and data scientists to architect pipelines to support delivery of recommendations and insights from machine learning models.
Required Experience & Skills

Fluency in Python, SQL mastery.
Ability to write efficient, resilient, and evolvable ETL pipelines.
Experience with data modeling, entity-relationship modeling, normalization, and dimensional modeling.
Experience building data pipelines with Spark and Kafka.
Comprehensive experience with Unix, Git, and AWS tooling.
Astute ability to self-manage, prioritize, and deliver functional solutions.
Preferred Experience & Skills

Experience with MySQL replication, binary logs, and log shipping.
Experience with additional technologies such as Hive, EMR, Presto or similar technologies.
Experience with MPP databases such as Redshift and working with both normalized and denormalized data models.
Knowledge of data design principles and experience using ETL frameworks such as Sqoop or equivalent.
Experience designing, implementing and scheduling data pipelines on workflow tools like Airflow, or equivalent.
Experience working with Docker, PyCharm, Neo4j, Elasticsearch, or equivalent.
Our Data Stack

Python, Kafka, Spark, MySQL, Redshift, Presto, Airflow, Neo4j, Elasticsearch
Fun Facts About the Team

We have access to one of the richest healthcare datasets in the world, with deep information on hundreds of thousands of healthcare professionals and their connections.
Business decisions at Doximity are driven by our data, analyses, and insights.
Hundreds of thousands of healthcare professionals will utilize the products you build.
Our R&D team makes up about half the company, and the product is led by the R&D team.
Our Data Science team is comprised of about 20 people.
HQ Benefits & Perks

Comprehensive benefits including medical, vision, dental, Life/ADD, 401k, flex spending accounts, and commuter benefits
Pre-IPO stock incentives
Work from home Wednesdays
3+ weeks of PTO
12 company holidays, including company shutdown last week of December
Fully ergonomic setup (chairs and stand-up desks)
Free lunch, snacks, and beverages
Rooftop game room and deck with a beautiful view of SF skyline
Team trips to fun places like Lake Tahoe, Sonoma, Seattle, and Park City
Sabbatical after 5 years
Remote Benefits & Perks

Comprehensive benefits including medical, vision, dental, Life/ADD, 401k, and flex spending accounts
Pre-IPO stock incentives
3+ weeks of PTO
12 company holidays, including company shutdown last week of December
Team trips to fun places like Lake Tahoe, Sonoma, Seattle, and Park City
Sabbatical after 5 years
About Doximity

Doximity is the leading social network for healthcare professionals with over 75% of U.S. doctors as members. We have strong revenues, profits, real market traction, and were putting a dent in the inefficiencies of our $2.5 trillion U.S. healthcare system. After the iPhone, Doximity is the fastest adopted product by doctors of all time. Our founder, Jeff Tangney, is the founder & former President and COO of Epocrates (IPO in 2010), and Nate Gross is the founder of digital health accelerator RockHealth. Our investors include top venture capital firms who've invested in Box, Salesforce, Skype, SpaceX, Tesla Motors, Twitter, Tumblr, Mulesoft, and Yammer. Our beautiful offices are located in SoMa San Francisco.

We are an equal opportunity employer and value diversity at our company. We do not discriminate by race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status. Pursuant to the San Francisco Fair Chance Ordinance, we will consider for employment qualified applicants with arrest and conviction records."
Database Engineer,eBay Inc.,"Looking for a company that inspires passion, courage and imagination, where you can be part of the team shaping the future of global commerce? Want to shape how millions of people buy, sell, connect, and share around the world? If youre interested in joining a purpose driven community that is dedicated to creating an ambitious and inclusive workplace, join eBay  a company you can be proud to be a part of. StubHubs purpose is to connect people through inspiring event experiences. We connect fans with their favorite teams, shows and artists and introduce them to the ones they'll love next. As the largest ticket marketplace in the world, we enable fans to buy and sell tickets to tens of thousands of events, whenever they want, through our desktop and mobile experiences, including our StubHub app for iPhone, iPad, Apple Watch and Android. Offering a superior fan experience at its core, StubHub reinvented the ticket resale market in 2000 and continues to lead it through innovation. Our industry firsts include the introduction of the first ticketing application, the first interactive seat mapping tool and the first live entertainment rewards program, Fan Rewards. Our business partners include more than 130 properties in MLB, NBA, NHL, MLS and NCAA, plus AEG, AXS and Spectra Ticketing & Fan Engagement. With the acquisition of Ticketbis in August 2016, throughout the world, StubHub provides the total end-to-end event going experience. StubHub is an eBay company (NASDAQ: EBAY). For more information on StubHub, visitStubHub.com or follow @StubHub on Twitter, Facebook and Instagram or YouTube.com/StubHub. We're actively looking for a mid to senior level Oracle DBA with 5+ years experience that will be part of the production DBA team providing services to Stubhub supporting this exciting platform. Qualifications : Bachelor's degree in Computer Science/Engineering or equivalent work experience Strong communications skills 5+ years experience in administering Oracle 11g/12c databases on RAC and non-RAC  24 x7 environments 3-5 years experience with Golden Gate and/or Streams or any other real time replication technology Execute database creation, configuration, upgrades, patches, and refreshes Excellent knowledge and experience with Oracle database architecture in a RAC and Data Guard configuration Excellent experience and understanding of patching, cloning, Enterprise Manager Cloud /Grid Control 12c, security, RMAN backup and recovery procedures Production Support, Dataguard, Goldengate, DevOPS , Couchbase, MongoDB,Delphix Experience in managing highly transactional oracle databases. Experience with Solaris operating system Excellent troubleshooting skills and ability to resolve issues quickly and effective Strong experience in Unix Shell scripting Be part of the on-call rotation Define and execute database performance and tuning scripts. Ability to work well in teams with good oral, written, and interpersonal communication skills Experience with Oracle Golden Gate is highly desirable Install new databases and apply maintenance fixes/patches Requirements : Bachelor's degree in Information Systems or a related field and 7-10 years related experience 5+ years experience with Oracle 11g and/or Oracle 12c RDBMS required Experience with Oracle RMAN and Oracle Enterprise Manager highly preferred Experience with Oracle RAC required Experience with Microsoft SQL-Server desired StubHub is a Subsidiary of eBay. This website uses cookies to enhance your experience. By continuing to browse the site, you agree to our use of cookies View our privacy policy View our accessibility info
R0020973"
"Data Engineer - Senior Consultant - Washington, DC",Clarity Insights,"Do you have a passion for data? Clarity Insights is a leading professional services firm focused exclusively on data and analytics. We own our solutions, providing business and technology landscape review, gap analysis, and go-forward strategy for our clients, in addition to implementing the future-state vision.

We are...

 The Industry-recognized data and analytics leaders
 Passionate problem solvers across a broad spectrum of technologies and industries
 Value seekers for measurable business outcomes
 Continuous learners through training and education
 Focused on a work-life balance with an unlimited paid time off policy

Data engineers are challenged with building the next generation of data solutions for many of the most high-profile and technologically-advanced organizations nationally. Our engagements typically target a variety of use cases across data engineering, data science, data governance, and visualization.

Data engineers deliver value through...
Hands-on, self-directed design and development of highly-scalable, reliable, and performant pipelines to consume, integrate and analyze large volumes of complex data using a variety of best-in-class proprietary and open-source platforms and tools
Demonstration of technical, team, and solution leadership through strong communication skills to recommend actionable, data-driven insights
Collaboration with team members, business stakeholders and data SMEs to elicit requirements and to develop business metrics and analytical insights
Internal contribution and influence over the growth of their consultancy with direct lines of communication from team member to CEO
A data engineers skills include, but are not limited to...
Bachelors Degree and 5+ years of work experience
5+ years of professional IT work experience
SQL, SQL, SQL!
Programming / Scripting (Python, Java, C/C++, Scala, Bash, Korn Shell)
Linux / Windows (Command line)
Big Data (Hadoop, Flume, HBase, Hive, Map-Reduce, Oozie, Sqoop, Spark)
Cloud Platforms (AWS, Azure, Google Cloud Platform)
Data Concepts (ETL, near-/real-time streaming, data structures, metadata and workflow management)
Data Integration Tools (Ab Initio, DataStage, Informatica, SSIS, Talend)
Databases (DB2, HANA, Netezza, Oracle, Redshift, Teradata, Vertica)
Markup Languages (JSON, XML, YAML)
Code Management Tools (Git/GitHub, SVN, TFS)
DevOps Tools (Chef, Docker, Puppet, Bamboo, Jenkins)
Testing / Data Quality (TDD, unit, regression, automation)
Solving complex data and technology problems
Leading technical teams of 2+ consultants
Ability to design components of a larger implementation
Excellent communication to narrate data driven insights and technical approach


If this sounds like you, lets talk!

Candidates must be comfortable with a national travel model to client locations weekly (M-TH is typical).

Clarity Insights is an Equal Employment Opportunity Employer. We believe in treating each employee and applicant for employment fairly and with dignity.

#LI-BB1
GLDR"
Software Engineer,Magento,"Magento Business Intelligence builds software that aggregates, stores, transforms and visualizes data to deliver actionable insights to business users. Our hosted end-to-end solution allows marketers, product managers, analysts and executives to be data-driven without leaning on engineers or other specialists to provide data or run analyses.

WHAT YOU'LL DO

Our product and engineering team practices iterative development and continuous deployment. We work in small teams, deploy dozens of times a day, and keep our projects short and focused. Engineers rotate between projects and areas of the product to learn and take on new challenges. As a member of the Magento Business Intelligence engineering team, you can expect to.
Build and maintain features in our platform ranging from distributed data processing systems to in-browser data visualization tools.
Work with teammates to identify challenging product and engineering problems and then dream up elegant solutions.
Recognize weaknesses in our technology and organization and frequently tell us what we are doing wrong and why.
WHAT YOU'LL NEED

(We Use The Following In Various Capacities And Expect That You've Worked With Some, But Probably Not All Of These)
Clojure, PHP, ruby, java, python
MySQL, Postgres, redis, mongoDB, apache, node.js, hadoop
AngularJS, javascript, coffeescript
aws, Git, jenkins, virtualbox, chef
To Be Eligible For Our Software Engineer Opening, You Should...
Have at least 1-3 years of experience, in addition to a bachelor's degree or equivalent experience in computer science
Have a track record of solving technical challenges and owning critical systems
Recognize weaknesses in our technology and organization, and frequently tell us what we are doing wrong and why
Be knowledgeable of fundamental computer science concepts.
Be passionate about building technology and products.

WHO WE ARE

At Magento, we inspire and empower data-driven people with powerful hosted software. Hundreds of high-growth e-commerce companies use our analytics platform to collaborate and make smarter decisions using data."
"Senior / Lead Data Engineer, ML / Deep Learning",Salesforce,"Senior/ Lead Data Engineer - Deep Learning / Machine Learning / Einstein

Roles available in Palo Alto and San Francisco

The role:

Salesforce is looking for both Senior and Lead Data / DL/ ML Engineers with Java, Python, Scala and/or Spark experience, to help us take on one of the worlds most extensive data sets and transform it into amazing products that feel like magic. You will work on cutting-edge AI applications and products. Brainstorming data product ideas with data scientists and engineers to build data products used by hundreds of millions people every day.

A typical day for you might include the following:
Developing data infrastructure that ingest and transforms data from different sources and customers at scale.
Creating machine/deep learning infrastructure that generalizes across hundreds of thousands of Salesforce customers, but is expressive enough to generate high lift.
Partner end-to-end with Product Managers and Data Scientists to understand customer requirements and design prototypes and bring ideas to production
Working with internal product teams to ingest their data and sprinkle machine/deep learning fairy dust on their products.
Participating in meal conversations with your team members about really important topics, such as: Should the cuteness of panda bears be a factor in their survivability? Is love a decision tree or a regression model? How far ahead would society be today if we had 12 fingers instead of 10?
What we care about:
We develop real products. You need to be an expert in coding. We use Scala and functional programming principles, great if you do too or are willing to learn.
We have massive scale. You need to have experience in distributed, scalable systems. Consistency / availability tradeoffs are made here. Youve tinkered with modern data storage, messaging, and processing tools (Kafka, Spark, Hadoop, Cassandra, etc.)
Were a growing, diverse team and we work together on projects. We love to collaborate and help each other, and we want someone to share that ideology.
You have to be a very quick learner - we face new challenges every day, anything that ranges between the operating model of a financial services companies, conversation model for chatbots, tinkering with convolutional and recurrent networks, to how to make Spark work with the S3 file system. No school could prepare you for all of these, so you need to be very quick on your feet.
Self-starter who can see the big picture, and prioritize their work to make the largest impact on the business and customers vision and requirements
Professional industry experience is preferred; especially with Scala and/or Spark. Advanced degrees alone do not replace real world experience.
Preferred Skills:
We run on AWS. We dockerize applications. You should have some notion of how to build, test, and deploy code to run on cloud infrastructure.
Experience with open source tools for information retrieval (e.g. Solr)
Search, Data Scoring/ Ranking expertise
Data visualization
Experience with Deep Learning for NLP
Salesforce, the Customer Success Platform and world's #1 CRM, empowers companies to connect with their customers in a whole new way. The company was founded on three disruptive ideas: a new technology model in cloud computing, a pay-as-you-go business model, and a new integrated corporate philanthropy model. These founding principles have taken our company to great heights, including being named one of Forbess Worlds Most Innovative Company six years in a row and one of Fortunes 100 Best Companies to Work For nine years in a row. We are the fastest growing of the top 10 enterprise software companies, and this level of growth equals incredible opportunities to grow a career at Salesforce. Together, with our whole Ohana (Hawaiian for ""family"") made up of our employees, customers, partners and communities, we are working to improve the state of the world."
Machine Learning/Data Engineer Intern,Zillow,"About the role
Zillow is looking for an intern software engineer with interest in web-scale distributed systems, data engineering and machine learning algorithms to join a growing extraordinary data science and engineering group dedicated to excellence. In this role, you will:
Design and code highly scalable, data engineering and or machine learning applications processing large volumes of data.
Work closely with business stake holders for data engineering and research scientists on prediction models.
Follow agile processes with a focus on delivering production-ready testable code in small iterations.
Participate in the entire development life cycle, from concept to release.
Participate in all phases of quality assurance and defect resolution.
About the team
The Analytics organization at Zillow is responsible for cutting-edge products such as the Zestimate, Rent Zestimate, personalization, and housing indices on Zillow's unparalleled living database of all homes.
We utilize modern open-source Big Data technologies such as Apache Hadoop and Spark as the foundation for our data engineering and ML platform.
We build consumer-facing analytic products powering Zillows industry-leading online marketplace puts the team and this role at the core of Zillows mission to empower the consumer.
Get to know us
Zillow Group is the leading real estate and rental marketplace dedicated to empowering consumers with data, inspiration and knowledge around the place they call home, and connecting them with the best local professionals who can help.
We are a community at Zillow Group - we care about one another and encourage each other to be successful. We collaborate in small teams to tackle big ideas, which gives everyone a chance to have an impact on what we do and how we do it.
Zillow Group is an equal opportunity employer committed to fostering aninclusive, innovative environment with the best employees. Therefore, weprovide employment opportunities without regard to age, race, ethnicity,national origin, religion, disability, sex, gender identity or sexual orientation,or any other protected status in accordance with applicable law."
Software Engineer,Stanford University,"We are looking for a full stack software engineer to join the Learning Systems and Services team in the Office of Vice Provost for Teaching and Learning at Stanford. Stanford has many of the best and the biggest online courses on the web. This team builds and operates the open-source platform behind many of them. Students on-campus and worldwide learn via our course materials and the online communities they create.
Technology is disrupting higher education. We believe universities can benefit from this if it's done right. Supporting research is core to what we do. We also believe that universities should be in control of how this space evolves, and that working with an open-source platform is the best way to do that.

As a member of our full-stack engineering team, youll configure and deploy Open edX-based learning systems, work with the project team to build new features that meet the needs of our customers, and contribute feature back to the open source community. We accomplish that with a lightweight, mostly-Agile development process: quick daily standups, group planning, and stakeholder demos. We work on the Stanford Campus.

CORE DUTIES:
Design, develop and test new features for Open edX platform, configure, customize, and deploy to staging and production servers, and help troubleshoot when CourseOps report production issues.
Work closely with CourseOps, User Experience team, and Instructional Designers to understand requirements and specifications for new feature requests, communicate platform updates to support team/IDs, and seek feedback.
Document code clearly and thoroughly, so others can maintain and support the code. Document new configuration or processes, and keep all documentation up to date.
Serve as a technical resource for applications.
Follow team software development methodology.
Mentor RAs and Curis Summer Interns as needed.
ADDITIONAL INFORMATION:

For more information check these links out:
Take a class (our production site): https://lagunita.stanford.edu/
Learn more about Stanford's approach at Stanford Online: https://online.stanford.edu/
Learn more about Open edX: https://open.edx.org/
See our code: https://github.com/Stanford-Online and http://github.com/edx

Qualifications
PREFERRED QUALIFICATIONS:
At least three years development experience (or equivalent) using modern web technologies. Shipped production-quality code.
Proficient in Python or other OO programming languages.
Comfortable with full-stack development, ranging from HTML, CSS/SASS, Javascript/jQuery, to Django, MySQL, etc.
DevOps experience working with AWS, using automation tools such as Ansible, Jenkins.
Experience with relational and NoSQL database systems. Able to efficiently get data into and out of a store using the right tool.
Knowledge of systems and networking. For example, you should be comfortable identifying the bottleneck resource on a busy system or debugging a distributed systems issue.
Self-motivated. You work well when empowered to make decisions about what needs to be done, and when and how to do it.
You're a hacker. You value shipping real software to solve real problems. You know that done is better than perfect. You aren't afraid of someone else's code. You're a tenacious debugger.
Assisting other developers by doing good code and design reviews.
Comfortable with DVCS using git and Github. You know your way around ""git rebase -i"".
You've got a good sense for how to figure out what's hard and easy, and can communicate that to others.
Bonus: Open Source development experience - extra points for sharing your GitHub/Bitbucket etc.
MINIMUM REQUIREMENTS:

Education & Experience:

Bachelor's degree and five years of relevant experience, or a combination of education and relevant experience.

Knowledge, Skills and Abilities:
Expertise in designing, developing, testing, and deploying applications.
Proficiency with application design and data modeling.
Ability to define and solve logical problems for highly technical applications.
Strong communication skills with both technical and non-technical clients.
Ability to lead activities on structured team development projects.
Ability to select, adapt, and effectively use a variety of programming methods.
Knowledge of application domain.
Certifications and Licenses:

None

PHYSICAL REQUIREMENTS*:
Constantly perform desk-based computer tasks.
Frequently sit, grasp lightly/fine manipulation.
Occasionally stand/walk, writing by hand.
Rarely use a telephone, lift/carry/push/pull objects that weigh up to 10 pounds.
* - Consistent with its obligations under the law, the University will provide reasonable accommodation to any employee with a disability who requires accommodation to perform the essential functions of the job.

WORKING CONDITIONS:
May work extended hours, evening and weekends.
WORK STANDARDS:
Interpersonal Skills: Demonstrates the ability to work well with Stanford colleagues and clients and with external organizations.
Promote Culture of Safety: Demonstrates commitment to personal responsibility and value for safety; communicates safety concerns; uses and promotes safe behaviors based on training and lessons learned.
Subject to and expected to comply with all applicable University policies and procedures, including but not limited to the personnel policies and other policies found in the University's Administrative Guide, http://adminguide.stanford.edu.
Stanford is an equal opportunity employer and all qualified applicants will receive consideration without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status, or any other characteristic protected by law.

Job : Information Technology Services
Location : Vice Provost for Teaching and Learning

Schedule

: Full-time Grade: J Job Code: 4822"
Software Engineer,Bloomberg BNA,"Bloomberg BNA seeks a Software Engineer to design, build, implement, and maintain software applications and programs to satisfy a variety of publishing and technical requirements of BBNAs internal and external customer and business partners. Develop system designs and specifications for multi-vendor and open source application environment. Oversee the work of and provides technical guidance to less experienced programmers, as assigned.
REQUIREMENTS :
Under general supervision,
Design, build, implement, and maintain software applications and programs to satisfy a variety of publishing and technical requirements of BBNAs internal and external customer and business partners
Analyze system and application requirements.
Perform integration of software components from internal and/or external developers.
Test and debug programs. Resolve application problems.
Document application procedures and specifications.
Write program code based on system requirements as necessary.
Develop system designs and specifications for multi-vendor and open source application environment with an emphasis on creating reusable components.
Develop systems designs and specifications as requested by systems analyst or project manager.
Work with systems analysts and project manager to develop project schedules and estimates. Monitor progress of projects.
Prepare technical system and application documentation.
Oversee the work of and provide technical guidance to less experienced programmers.
Ensure that BBNAs software engineering standards are followed in own work and the work he/she oversees.
Maintain knowledge of programming languages, principles, and procedures.
Assist in coordinating programming activities with users and other information systems personnel.
Collaborate with QA staff to ensure production of defect-free software. Provide guidance for quality assurance activities.
Participate in special projects and perform other duties as assigned.
REQUIREMENTS :
3+ years software development experience, including experience working with software in a multi-vendor environment and experience in programming languages currently in use in BBNA; ability to comprehend and solve the most complex programming and analysis problems with emphasis on object-oriented design and programming; demonstrated knowledge and applied use of database and data communication programming languages, scripting languages, and other database applications; ability to troubleshoot problems in a multi-vendor environment on the latest platforms; ability to write, organize, and edit technical documentation; analytical and logic skills, accuracy, and attention to detail; ability to work effectively, independently and in a team environment, in an atmosphere of multiple projects, shifting priorities, and deadline pressure; and excellent verbal and written communication skills. Bachelors with coursework in computer science, information science, and software engineering is required. Technical training in programming languages and computer systems is required. Technical writing experience/coursework is a plus.
Bloomberg BNA IS AN EQUAL OPPORTUNITY EMPLOYER and fully subscribes to the principles of Equal Employment Opportunity. Bloomberg BNA has adopted an Affirmative Action Program to ensure that all applicants and employees are considered for hire, promotion, and job status without regard to race, color, religion, sex, national origin, age, disability, sexual orientation, marital or familial status, genetic information, disabled veteran, veteran, veteran of the Vietnam Era, or any other classification protected by law."
"Sr. Software Engineer, Data Engineering",Apple,"At Apple, we work every day to create products that enrich peoples lives. Our Advertising Platforms group makes it possible for people around the world to easily access informative and imaginative content on their devices while helping publishers and developers promote and monetize their work.

Our technology and services power advertising in Apple News and Search Ads in App Store. Our platforms are highly-performant, deployed at scale, and setting new standards for enabling effective advertising while protecting user privacy.

We are looking for a self-motivated individual who can thrive in an Agile environment. You will be designing high-performing, scalable software solutions that will handle billions of transactions every day.
Key Qualifications
Minimum of 5 years experience in code and test major features, as well as work jointly with other team members to deliver complex changes
Experience with high throughput and scaleable applications.
5+ years of experience in Java Programming (design & architecture, algorithms).
Designing and implementing systems to process Terabytes to Petabytes of data using Hadoop.
Experience with MapReduce, HDFS, Hive, HBase Or Cassandra.
Experience with Oracle 10g, 11g databases.
Python and Bash Scripting Experience is a plus.
Experience with Analytical Tools is a plus.
Experience with Flume and Oozie is a plus.
Experience working closely with operational teams on deployment, monitoring, management concerns
Ability to communicate effectively, both written and verbal, with technical and non-technical cross-functional teams.
Prior experience in advertising industry is a huge plus
Description
We are looking for a skilled Software Engineer to be part of a dynamic team building high performance and scalable applications.

Candidate will be responsible for the following:
Implement scalable Hadoop based data processing infrastructure.
Build modular components in the large volume data movement and management.
Work with architects in implementing the data pipeline.
Education
BS in computer science or equivalent field with 3+ years relevant experience. Advanced degree preferred."
Software Engineer,Electronic Arts,"Through the use of modern datacenters, cloud technologies, and containers EA Tech Ops is bringing unrivaled reliability and performance to millions of players around the world. Design and build distributed software systems to manage large scale multi-region infrastructure.

Tasks:
Build and maintain high-performance, fault-tolerant, scalable distributed software systems
Solve problems at scale ( &
gt;20k servers) Contribute code on a daily basis
Work with cutting edge technologies such as Docker and Mesos
Work with dedicated servers, private clouds technologies, and public clouds
Be an active contributor to open-source projects

Required Skills:
Bachelor's degree from an accredited education institution in Computer engineering, computer science, information systems or equivalent
Keen interest in DevOps methodologies and technologies (Docker, Chef, Puppet, etc..)
Analytical and problem solving skills aligned with enthusiasm, aptitude, attitude and motivation to learn
Working knowledge of at least one of the following languages: Go, Python, or Ruby
Working knowledge of distributed source control such as GIT
Exposure to various front-end languages such as as HTML, JavaScript, and CSS
Experience with general system administration and shell scripting
Experience with various server operating systems such as Red Hat/CentOS, Ubuntu/Debian, and/or Windows Server
Working knowledge of database concepts, database structures, relational data bases and related applications
Understanding of document and/or key/value store such as Cassandra, Memcached, Reddis, RethinkDB, or MongoDB
Excellent communication and presentation skills"
Software Engineer,Emerson,"Are you a person that loves solving complex problems? Are you looking for a career that allows you to gain experience and expand your skills in many different software technologies? Emerson is looking for an experienced software engineer to work with our customer support teams to improve the reliability and robustness of our product by solving their most challenging software problems.
You will have the opportunity to work in many different areas of the DeltaV process control system, in both C++ and C#, including embedded firmware, SQL and object-oriented databases, HTML5, WCF, TCP and UDP networking, and large-scale real-time software architectures.
A strong candidate for this position will have excellent investigation skills, outstanding problem solving abilities, and be able to quickly learn new software domains and technologies.

Responsibilities
Design and implement customer-requested product enhancements
Investigate and propose technology-driven product improvements
Investigate and resolve customer issues with released products
Perform code reviews and unit and integration testing
Identify and report suggestions for improvements to product development teams

Requirements
BS Computer Science, Computer Engineering, Electrical Engineering or related field + 4 years related experience; OR MS Computer Science, Computer Engineering, Electrical Engineering or related field + 3 years related experience, including developing software as part of complex systems
Proficient in troubleshooting and debugging commercial software
Proficient with data structures and algorithm analysis
2+ years of experience with C++ or C# as well as Object Oriented development
Excellent written and verbal communication skills
Preferred Qualifications
Experience with:
Computer Networking
Embedded Firmware Development
Real-time Operating Systems
.NET Technologies (WCF, WPF, C#)
HTML5
Team Foundation Server
POSIX
SQL
Work Authorization

No calls or agencies please. Emerson will only employ those who are legally authorized to work in the United States. This is not a position for which sponsorship will be provided. Individuals with temporary visas such as E, F-1, H-1, H-2, L, B, J, or TN or who need sponsorship for work authorization now or in the future, are not eligible for hire.

Equal Opportunity Employer
Emerson is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to sex, race, color, religion, national origin, age, marital status, political affiliation, sexual orientation, gender identity, genetic information, disability or protected veteran status. We are committed to providing a workplace free of any discrimination or harassment.

If you have a disability and are having difficulty accessing or using this website to apply for a position, you can request help by calling 1-314-553-2544 (V/TTY/TDD) or by sending an email to idisability.administrator@emerson.com. About Emerson
Imagine being surrounded by intelligent, driven, and passionate innovators all working toward the same goalto create groundbreaking solutions that leave our world in a better place than we found it. Emerson is a global technology and engineering company providing innovative solutions for customers in industrial, commercial, and residential markets.

Our Emerson Automation Solutions business helps process, hybrid, and discrete manufacturers maximize production, protect personnel and the environment while optimizing their energy and operating costs. Our Emerson Commercial and Residential Solutions business helps ensure human comfort and health, protect food quality and safety, advance energy efficiency, and create sustainable infrastructure.

A dynamic environment is what youll discover at Emerson, a Fortune 500 company with $14.5 billion in sales and 155 manufacturing locations worldwide. Together, were changing the world, and we have all the resources to help you achieve your professional goals.

Whether youre an established professional looking for a career change, an undergraduate student exploring options, or recently received your MBA degree, youll find a variety of opportunities at Emerson. Join our team and start your journey today."
Software Engineer,Kaplan,"Kaplan is looking for a Software Engineer with expertise in integration and SOA technologies. In this role, you will work in our fast-paced Technology Team that is responsible for building an API platform and prides itself in being extremely well organized and self-driven. You will lead our efforts in architecture, design, and governance of services and integrations built using various EAI tools.
To be successful in this position you need to have
+5 years of experience working as a Developer with Java.
+1 years of experience developing integration platforms using an ESB like Webmethods or Mulesoft.
Proficient in building REST and SOAP based web services.
Experience building high volume data and service integrations using EAI tools.
Expert SQL knowledge and good understanding of database internal concepts.
Basic knowledge of XML, HTML and Javascript.
Hands on experience in Ant, Maven or any other building tool.
Experience working in Unix environments (basic shell programming and command line)
Knowledge on test driven development and metric driven decision making.
Experience working in a fast paced scrum based agile environment.
Expertise in the entire software lifecyclerequirements capture, analysis, design, implementation, quality assurance, deployment, and maintenance.
Demonstrated ability to effectively work in team environment with minimal supervision.
Excellent problem solving and communication skills.
Education background:
BS or MS in Computer Science or related technical field required. Strong relevant work experience considered."
Software Engineer,BGC Partners,"Create and provide technical leadership to manage complex client and internal challenges with the additional skills to translate business requirements into conceptual, logical and physical data models for our clients commercial real estate portfolio needs. Work with a broad range of subject matter experts to develop applications which will be deployed and used by clients and out internal teams. Responsible for scoping, designing, building, and deploying applications with a customer focused mindset. Ongoing development of the online platform that is used across all areas of real estate portfolios. Develop custom client applications. Responsible for internal training of custom applications. Quality Assurance (QA) and management of the Development Life Cycle.

Day to day activities may include:
COTS software design and configuration
Testing of high-integrity software systems
Exposure to marquee name clients
First level technical support

As part of our structured apprentice program, candidates: experience the proper techniques to configure an IWMS (Integrated Workplace Management System) application; understand the full software configuration release cycle from design through development to deployment; contribute to each aspect of the lifecycle of a project; execute within an agile, test driven development environment; build, execute, and debug functional and technical tests for company supported applications; be exposed to a variety of functional domains, high-profile clients, as well as teams and departments within the company. Successful individuals will become proficient in configuring IWMS applications that lead to a career path in software engineering at NGKF and have immediate opportunities for career growth.
Top individuals will be given an opportunity to advance into one of several positions at NGKF to support TRIRIGA systems.

Required Skills and Experience:
Working knowledge of Object Oriented programming/configuration and concepts
Minimum 2 years software development experience either in job offered or as Developer.
Foundation knowledge in HTML and XML
Experience building active/production web applications and/or websites
Familiarity with at least one of the following programming languages: C/C++, .Net, Java, Ajax
Understand the Software Development Life Cycle (SDLC) processes: Requirements Gathering, Design, Development, Testing and Deployment.
Experience with RDBMS - Primary Keys, Foreign Keys
Basic data modeling skills
Must have 2 years experience with the following: working with Visual Studio ASP.NET MVC, HTML5, CSS3, Bootstrap, JavaScript, jQuery, Knockout JS, C Sharp, SQL Server 2014, Azure SQL DB, Active Domain Web Application Integration, RESTful API Web Services and Web applications with SQL

Education Requirements:

Masters degree in Computer Science, Engineering or related

Desired Skills and Experience:

Knowledge of agile and test driven development processes is desirable
Metadata-driven definitional development experience is a plus
Understand Database Management Systems (DBMS) like Oracle and SQL Server
Knowledge on how to write SQL statements
IWMS (Integrated Workplace Management System) experience
CAFM (Computer-Aided Facility Management) experience
CAD (Computer-Aided Design) experience (i.e. AutoCAD, Microstation).

Required Soft Skills:

Have long-sustaining positive can do attitude
Highly organized and detail-oriented
Excellent documentation, communication and interpersonal skills
Excellent analytical and problem-solving abilities
Have self-motivated confidence without being ego-centric
Enjoy working with and supporting a team
Excellent in both verbal and written communication
Innovate solutions to technical problems
Flexible and adaptive to the role and job expectation
Consistent on-time delivery of assignments
Consistent at setting expectations for when assignments are to be met
Willingness to learn new concepts beyond what is expected
Ability to plan and organize effectively

Working Conditions: Normal working conditions with the absence of disagreeable elements"
Entry Level Software QA Engineer,Agdata,"Who we are:
For over 30 years, we have been a technology partner that delivers data solutions and insights for companies in the Agricultural and Animal Health industries. From manufacturers to distributors and retailers, AGDATA has unrivaled experience in turning data into solutions to power businesses. Quite simply: ""We enable the people who feed the world"".

Are you ready to cultivate your career?

What we are looking for:
Are you looking for an incredible opportunity to start your professional software and technology career? At AGDATA, we will introduce you to Software QA Testing methodologies, where you can learn from the best as you develop and sharpen your skills!

Upon completion of training, you will join an Agile software engineering team at AGDATA and continue honing your skills, where you will be responsible for functional testing including writing automated test scripts using a test automation tool and a scripting language such as Selenium and C#.

What you will do:
Be passionate about Quality and Technology

Use critical thinking to solve problems

Attention for detail and a knack at finding edge cases, bugs, flaws, gaps etc.

Demonstrate strong end user empathy

Contribute to improve the end user experience and usability of the software

Effectively communicate with your agile team

Understand the data, not just the software

Focus on continual learning and improvement

Experience:
BS in Computer Science, Mathematics or a related field

What we offer:
Competitive pay and benefits

Paid Time Off and 10 paid holidays

Health, dental, and vision insurance

Professional development to help you grow your career with AGDATA

Ability to make an immediate impact"
"Fullstack Engineer (Python, Go, React.JS)",MediaMath,"MediaMaths strength is in numbers. Our technology analyzes 200 billion customer opportunities daily more volume than the top 10 stock exchanges in the world, combined.
Over 700 Mathletes in 16 global offices are trusted by two-thirds of the Fortune 500 and partner with thousands of marketers to ensure brands connect with right audiences, in the right place, in the right time.
We believe consumers want to have meaningful conversations with their favorite and yet-to-be-discovered brands across all digital touchpoints. Our omnichannel, integrated programmatic platform unites digital media and big data to maximize the return on every marketing dollar spent by making advertising relevant, personalized, measurable and controllable. From inventing the DSP category in 2007 to being named a DMP Forrester Challenger (our first year participating in the DMP Wave!) in 2017, we continue to deliver results for marketers more quickly and accurately than any other solution.

Technology is changing the way brands interact with consumers. MediaMath is powering that change. Come be a part of it!
Responsibilities
We're currently seeking a fullstack software engineer in the area of API development. Key responsibilities for this role include but are not limited to:
Work in an agile team on Golang/Python/Perl API and microservices architecture to build a market module in MediaMath's TerminalOne platform.
Implement, test, deploy and monitor scalable API services and UI front-end for complex workflows.
Create elegant, testable, and maintainable software using TDD.
Collaborate with product managers and team members to gather requirements, design, and develop solutions.
Participate in software design meetings and analyze user needs to determine technical requirements.
Provide support for day-to-day operational work as needed.
Qualifications
Master's Degree in computer science, related degree, or equivalent relevant experience
2+ years of professional software development experience.
Experience developing modern enterprise or REST applications using languages like Go/Python/Scala etc.
Knowledge of RDBMs like PostgreSQL/MySQL, data modeling, SQL tuning and optimization.
Familiarity with NoSQL data stores like DynamoDB, Elasticsearch, Redis, Memcache.
Knowledge in build automation, TDD/BDD, CI/CD
A self-starter who brings energy, passion and creativity to work every day.
GitHub (or similar) account for us to know you better.
MediaMath is privately held, employee owned, and headquartered in New York. Mathletes enjoy: Company equity. Performance Bonus. Comprehensive Insurance. Global Internal Mobility. Open Paid Time Off, Philanthropy and Holidays. 401(k) match. Paid Parental Leave. Cell Phone Reimbursement. Modern office space. Onsite Fitness & Wellness. MediaMath.org."
Software Engineer,NT Concepts,"Overview

NT Concepts is currently hiring Developmental to SME level Software Engineers for immediate openings for positions located in the Washington DC Metro and Northern Virginia area. Additionally, we are seeking candidates that are interested in being sponsored for a clearance. To be considered for a clearance, all candidates must be U.S. citizens and have a current Clearance or have the ability to obtain a clearance.
Responsibilities

Seeking a highly motivated individual to assist with the development, design and integration of a high-profile enterprise application. The candidate will be a strong developer with a rounded understanding of a multi-disciplinary candidate that in an expert in a variety of IT concepts and is comfortable as a team leader in design, innovation and a full mastery of the COTS product. A candidate will work in an integrated test-driven development environment that is highly reliant on DevOPS, AWS and related emerging technologies. The candidate will be expected to provide independent working ability as well as utilizing paired programming techniques to construct resilient and creative solutions to answer the challenges of this system. Team members are expected to develop specifications and make recommendations on existing systems using new and emerging technologies to promote system stability for the Sponsor's partners. Also, the candidate will be responsible with assisting with the daily operation and maintenance, which includes assisting with the extension, enhancement, refactoring, and architecture of the existing software and responding to a variety of ad hoc requests. This position will join an integrated project team and new team members will be expected to work closely with a variety of skill sets, such as Project Managers, System Engineers and Architects
Qualifications

Mandatory Skills

Demonstrated experience writing/transforming/extending code using Java 8 enterprise.
Demonstrated experience working with a DEVOPs orchestration tool such as Jenkins, Rundeck and Artifactory.
Demonstrated experience working with the enterprise Github including Maven and/or Gradle.
Demonstrated experience in code coverage, unit testing, mocking, and integration testing.
Demonstrated experience with HAL conformant JSON and XML using complaint APIs, OpenSSL, PKI and OAuth protections.
Demonstrated experience with a knowledge of Linux variants (CentOS, Redhat etc.).
Demonstrated experience with Amazon Web Services (AWS).

Desired Skills

Demonstrated experience in Spring Frameworks/Spring IO such as Spring MVC, Spring Boot, IOC, AOP, Spring Data, Spring Test.
Demonstrated experience performing software and application design.
Demonstrated experience performing architectural consulting from a development and system performance perspective.
Demonstrated experience conducting unit testing and test processes/frameworks.
Demonstrated experience with Test Driven Development and Continuous deployment with Continuous Integration tools.
Founded in 1998, NT Concepts is a woman-owned technology solutions company focused on data analytics, software engineering, investigative services, and geospatial information systems. We provide a broad range of Federal Civilian, DoD, and Intelligence Community customers with solutions for enterprise, cloud-based, and mobile environments.

NT Concepts is committed to hiring and retaining a diverse workforce. We are proud to be an Equal Opportunity/Affirmative Action Employer (M/F/D/V), making decisions without regard to race, color, religion, creed, sex, sexual orientation, gender identity, marital status, national origin, age, veteran status, disability, or any other protected class.

All resumes are held in confidence. No recruiters or agencies without a previously signed contract. No faxes please. Only candidates whose profiles closely match requirements will be contacted during this search.

NT Concepts participates in E-Verify.

If you need a reasonable accommodation for any part of the employment process, please send an e-mail to jobs@ntconcepts.com and let us know the nature of your request and your contact information. Reasonable accommodations are considered on a case-by-case basis."
Senior Data Scientist-Machine Learning,Cigna,"Are you looking to have meaningful and major impact of your data science work? Are you tired of working on toy machine learning problems involving flower petal lengths? Cigna is ramping up its data science team to help improve the lives of its customers by leveraging terabytes of data. This is an unprecedented opportunity to get in on the early stage of that growth and make a real difference in the lives of people.
Cigna is not only committed to quality service for its customers, but is committed to building a world-class environment to develop, test, and deploy data science solutions. The current data science team is diverse in scope, background, and applicationright now, the application areas are virtually unlimited: anything that can have a positive impact on customers by leveraging data. Furthermore, the ideal candidate can take advantage of remote working possibilities to focus on producing results. More than geographic location, we are looking for top data scientists who can deliver insightful outcomes to our stakeholders using state-of-the art prototyping environments.

Cigna has recognized that data scientists are not only in demand, but are part of a rapidly changing field. As a result, Cigna has created a Data Scientist Personal Growth Program to create flexibility and granularity within a data scientists personal growth and changing roles. These roles and associated development opportunities facilitate personal development in a way that benefits both Cigna and the data scientist.

Summary:

We are looking for a Data Scientist to apply and develop new machine learning methods that can help unlock the knowledge hidden in our data, combine it with other data, to help make better decisions for our customers. The scope of the projects can vary from automating a manual process to improving an existing automated process with data: recommendation systems, automated scoring, predicting of clinical outcomes, anomaly detection, organizing information for presentation. The primary focus is on delivering high-speed, high-quality data science systems that can be integrated into current or new products.

Responsibilities:

 Obtain data from internal sources using state of the art big-data tools such as Teradata and Hive.

 Combine and Augment the Data with data from external sources.

 Scrub, clean, prepare, and browse the data: fill in missing data values, determine outliers, regularize and normalize the data (e.g., names), transform it into a more useful form (e.g., time series).

 Explore and visualize the data using tools such as Python modules including pandas, matplotlib and Seaborn, Tableau, and Looker. Select appropriate features that optimize performance.

 Build models using state of the art machine learning methods such as Clouderas Data Science Workbench and Data Robot.

 Evaluate the results using performance metrics.

 Interpret, visualize, and present the results to stakeholders.

 Work with data engineers to deploy the models.

 Develop processes and tools to monitor the models performance after deployment.

Qualifications

 BS/MS/PhD in Computer Science, Math, Statistics, or in any technical field that provides a solid basis for analytics highly desired.

 Excellent understanding/experience with machine learning methods such as neural networks, deep learning, KNN, Naive Bayes, SVM, Lasso Regression, Elastic Net Regression, Boosting and Bagging Random Forest, Association Rules, Cross Validation Method, and Variable Clustering.

 Experience with Pythons data science toolkits such as Scikit-learn, pandas, Numpy and Seaborn through Jupyter or iPython notebooks.

 Experience with data visualization tools such as Python modules including matplotlib, Seaborn, bokeh, Tableau, and Looker.

 Experience with Hive and database access languages such as SQL. Experience with Hadoop and Spark is a plus.

 An understanding of probability distributions and basic statistics.

 Minimum 2-3 years of experience programming in Python to clean and transform data

 Intellectual curiosity and internal motivation (e.g., projects youve done outside of being paid or in school).

 A data-driven personality (as opposed to a theory-driven personality).

Qualified applicants will be considered for employment without regard to age, race, color, religion, national origin, sex, sexual orientation, gender identity, disability, veteran status. Need an accommodation? Email: SeeYourself@cigna.com"
Mid Software Engineer (Java Generalist & Big Data),Next Century,"Next Century Corporation is seeking an innovative Mid Level Software Engineer to join a stellar team working at our customer site in Reston, VA! This position provides the opportunity to play a key role in supporting a system that aggregates collected raw data which allows analysts to convert it into intelligence products. This system supports a number of mission-critical legacy applications, as well as the architecture of the future for intelligence analysis of HUMINT- enabled computer operations!

To be successful, you will need:
3-5 years' of hands on experience in software engineering utilizing Java frameworks
Solid knowledge of back-end development in ANT, Apache Tomcat, JDBC, Hadoop, Oracle 11g, and Weblogic. Knowledge of AWS is a huge plus!
An active TS/SCI with polygraph clearance

Your day to day duties and responsibilities include (but are not limited to):
Maintaining and developing in Java.
Using JavaScript for parsing, processing, and visualizing raw data.
Analyzing, designing, coordinating, and supervising expertise for the development of software systems to form a basis for the solution of information processing problems.
Analyzing system specifications and translating system requirements to task specifications for junior programmers.
Analyzing of current programs including performance, diagnosis, and troubleshooting of problem programs, and designing solutions to problematic programming.
Developing new programs and proofing the program to develop needed changes to assure production of a quality product.
Developing new programs, analyzing current programs and processes, and making recommendations which yield a more cost effective product.
Writing, editing, and debugging new computer programs for assigned projects, including necessary records and desired output.
Testing new programs to ensure that logic and syntax are correct, and that program results are accurate: assisting lower-level programmers with programming assignments.
Documenting code consistently throughout the development process by listing a description of the program, special instructions, and any changes made in database tables on procedural, modular, and

About Next Century

We were founded as a direct result of the 9/11 attacks and provide solutions that integrate situational awareness, at a glance analysis, decision support, collaboration, and other core capabilities utilizing our expertise in data visualization, user interface design, GIS, image exploitation, and mobile computing.

Who We Are Seeking

At Next Century, we are committed to growing our team of high performers to accomplish our mission of saving lives and protecting our country. We are not a body shop; we avoid staff augmentation, and we are totally committed to excellence in all that we do.

Joining our team is not just a position, but a journey with a team of world class software engineers who share a passion for using their skills and experience to make a difference.

We are seeking those that hold these traits and beliefs:
Passionate about protecting our country and saving lives
Aware of the concept of a high-performance organization and is committed to achieving them
Engineers that desire to work closely with end users to identify the most critical information
Demonstrated passion for learning and curiosity of the world
In compliance with federal law, all persons hired will be required to verify identity and eligibility to work in the United States and to complete the required employment eligibility verification form upon hire."
Geospatial Data Engineer,Planet,"Welcome to Planet. We believe in using space to help life on Earth.

Planets mission is to image the whole world every day and make global change visible, accessible, and actionable. We have a people-centric approach toward culture and community and we are iterating in a way that puts our team members first and prepares our company for growth. Be a part of our mission and help build a company that is changing the world.

Our powerful cloud-based platform passes daily imagery from Planet's satellites through a fully-automated processing pipeline. The platform allows our customers to download, search, manipulate, and extract information from a dataset with global coverage. With more daily geospatial imagery data than ever in history (the platform processes upwards of 5 terabytes of imagery per day), Planets Engineering team is now dedicating significant effort in the development of core platform capabilities towards delivering analytics and insights from geospatial data.

Imagery Analytics is a high impact team building foundational and customer facing geospatial analytics capabilities. To achieve this were working on software that utilizes the latest techniques in data science, computer vision and machine learning (including deep learning). Were looking for talented engineers that have applied product experience to join our growing team.

Working as a bridge between customers and engineering teams, Geospatial Data Engineers use customer input and sales information to analyze multimodal geospatial data, design, develop and test complex software programs and applications.

The Role:
Works on customer facing components and products using foundational capabilities developed by the imagery analytics team
Learning and engaging with prospective customers to develop product requirements in collaboration with the Product Management team
Maintain a strong understanding of the geospatial imagery from Planet and other satellite constellations
Support the Planet Machine Learning and imagery analytics practice in the delivery of client facing analytics packages and products
The Must Haves:
BS or MS in GIS, Computer Science or related fields; 3+ years of software development experience.
Expertise in one or more programming languages such as Python, Scala and Java
Experience with multiple GIS software applications and packages ArcGIS, QGIS, GDAL
Experience with GIS data from satellite constellations like Landsat and Sentinel
Strong knowledge of cloud platforms and experience of developing applications on the cloud platforms using various cloud services
Strong analytical and quantitative problem solving ability.
Excellent communication, relationship skills and a strong team player
The Nice to Haves:
5+ years experience building software
Experience in owning, designing, and developing enterprise-scale SaaS applications
Experience with one or more general purpose programming languages including but not limited to: C/C++, Objective C, Python, Ruby, JavaScript, SQL or Go
Knowledge and experience with statistical modeling, machine learning, or data-mining techniques
Some press about us:
Our CEO, Will Marshall's Dreamforce Talk ( https://www.salesforce.com/video/183624/ )

""Tiny Satellites ushering in the New Space Revolution"" ( https://www.bloomberg.com/news/features/2017-06-29/the-tiny-satellites-ushering-in-the-new-space-revolution )Bloomberg Businessweek

""The All-Seeing Eye in the Sky video"" ( https://www.bloomberg.com/news/videos/2017-06-28/the-all-seeing-eye-in-the-sky-video ) Bloomberg Businessweek video"
Big Data Engineer II,zulily,"zulily is a retailer obsessed with bringing our customers something special every day. Unique products from up-and-coming brands are featured alongside favorites from top brands, including clothing, home decor, accessories, toys and gifts, giving customers something new to discover each morning at an incredible price. We are fast-paced, innovative and metric-driven, with a team passionate for delighting our customers. Whether you want to work with incredible product selection, develop smart marketing plans, help us in our pursuit of faster fulfillment, or do something else awesome in between, we have a place for you. With locations in Seattle, Reno, Columbus and Bethlehem, our team is excited to be changing the way people shop every day - and we are just getting started.

As a Bid Data Engineer II at zulily, you are chartered with building the technology that that powers of the fastest growing online retailers in history. Our incredible growth means we face new challenges on a consistent basis. In this position, you must explore new technologies and collaborate with other engineers to design and build world-class technical services. Your team deploys production code rapidly to keep pace with zulilys changing business, you are mindful of avoiding technical debt, and you take ownership of your team's road map and everything you do.

QUALIFICATIONS
2+ years experience in each of the following: database development and reporting; MPP systems such as Vertica, Netezza, Mongo, or others; BI reporting framework (Tableau preferred)
Experience in Python and big-data technologies such as Hadoop, Hive, Impala
Work history in a fast paced, evolving, growing and dynamic environment
Ability to work with and influence others
Proven facility for thinking system wide and translate business requirements into code
Strong analytical skills with a data driven approach and ability to measure impact of project
Demonstrated advance proficiency writing complex SQL queries and stored procedures
zulily is an Equal Opportunity Employer

zulily participates in E-Verify. zulily will provide the Social Security Administration (SSA) and, if necessary, the Department of Homeland Security (DHS) with information from each new employee's I-9 to confirm work authorization.

Federal law requires employers to provide reasonable accommodation to qualified individuals with disabilities. Please tell us if you require a reasonable accommodation to apply for a job. Examples of reasonable accommodation include making a change to the application process, providing documents in an alternate format, using a sign language interpreter, or using specialized equipment. You may reach us at (206) 388-0920."
Software Engineer,Jacobs,"Jacobs Technology is the advanced technology arm of Jacobs Engineering (NYSE: JEC), one of the largest engineering and technical service companies in the United States. Jacobs has partnered with NASA to support space flight programs for more than 40 years and held the predecessor Engineering and Science Contract (ESC) since 2005. We look forward to continuing that work as the prime contractor for the JSC Engineering, Technology, and Science (JETS) contract along with our eleven teammate companies to provide engineering, scientific and technical contract services at NASA's Johnson Space Center (Houston, TX). JETS provides products and technical services related to human operations in space through development and integration of a broad spectrum of engineering requirements. This includes human spacecraft flight and flight development products, human exploration mission planning for NASA, institution support services, and new technology development. At Jacobs, we believe that people are our greatest asset, and that is why we offer a partnership in which you can grow personally and professionally with the advantages of strong leadership, competitive compensation and rewarding career paths.

Our long-term client relationship with NASA has led to a need for a Software Engineer with UTC, a JETS teammate company.

** We are actively searching for qualified candidates for this potential opening, however we do not have assurance at this time if and when the position will be finalized and opened. Because we have strong reason to believe the position will be approved soon, we are proceeding with identifying qualified candidates and conducting interviews.**

The Software Engineer will:
Develop an autonomous robotic flight system that can be used to explore areas on the surface of planets and moons in advance of human exploration
The system will be capable of translating to a specified site, landing, collecting data and sending it to a remote location, recharging and repeating the process
Develop C++ software
Perform software testing and verification
Develop software documentation
Participate as software Subject Matter Expert in project meetings.
Perform other duties as assigned

Qualifications
Required Education/Experience/Skills:
BS degree in engineering from an accredited engineering school and five (5) years of direct software engineering experience, or a MS degree from an accredited engineering school and four (4) years of direct software engineering experience, or Ph.D. and zero (0) years of direct software engineering experience
Strong C++ embedded programming skills and experience within the last 5 years
Experience with Linux Kernel, device drivers and embedded Real-time Operating System (RTOS)
Preferences:
Experience with robotic systems
ROS (Robot Operating System) or a similar platform in a significant way
Experience writing software for automation/process control, including hardware interfaces (such as Ethernet, serial ports, etc.)
Experience with the Git version control system)
Linux experience running on ARM processors using the TI SDK
Experience with USB video, video for linux, HD video and image processing (gstreamer framework, h.264, MPEG, JPEG, etc)
Why Work for JETS?
Opportunities for growth and advancement
Comprehensive Medical Coverage (medical, dental, vision)
401(k)
Benefits Tuition Reimbursement
Much, much more!
Dont miss out on this great opportunity; for immediate consideration, apply now! ( www.wehavespaceforyou.com ).
Must be a U.S. Citizen and successfully complete a U.S. government background investigation.
Management has the prerogative to select at any level for which this position has been advertised.
Essential Functions
Work Environment
Generally an office environment, but can involve inside or outside work depending on task.

Physical Requirements
Work may involve sitting or standing for extended periods (90% of time). May require lifting and carrying up to 10 lbs (5% of time).

Equipment and Machines
Standard office equipment (PC, telephone, printer, etc.).

Attendance
Regular attendance in accordance with established work schedule is critical. Ability to work outside normal schedule and adjust schedule to meet peak periods and surge requirements when required.

Other Essential Functions
Must be able to work in a team atmosphere. Must put forward a professional behavior that enhances productivity and promotes teamwork and cooperation. Grooming and dress must be appropriate for the position and must not impose a safety risk/hazard to the employee or others."
Software Engineer,DRS Technologies,"As a software engineer for the enterprise software engineering team you will be responsible for developing, testing, and maintaining a range of applications and tools that support our test engineering and factory automation processes. Your work will require teamwork alongside members of the engineering and production operations organizations whose backgrounds span disciplines such as embedded systems development, product engineering, design engineering, wafer fabrication and manufacturing.

Basic Qualifications and Required Skills
B.S. in Computer Science, Computer Engineering, Software Engineering or a related discipline from an accredited institution
0-5 years of experience in software development
Experience developing applications with the .NET framework, the Base Class Library (BCL) and.NET languages (C#, VB.NET, etc)
Experience developing web services and web applications
Excellent written and verbal communication skills
Expertise in object oriented programming concepts
College level mathematics (calculus, statistics, linear algebra, etc.)
Strong problem solving and troubleshooting skills
An ability to quickly learn and apply new software engineering technologies and methodologies
Ability to work under minimal supervision and within the framework of a multi-discipline team
U.S. Citizenship

Desired
Experience developing applications with Visual Basic 6.0 or Visual Basic for Applications (VBA)
Experience developing applications with C/C++.
Experience designing and implementing databases and stored procedures using SQL
Experience with MATLAB or other numerical computing environments
Experience with automation of test and measurement devices such as oscilloscopes, multimeters, power supplies, data acquisition devices, frame grabbers, etc.

Duties and Responsibilities
Responsible for software development tasks including requirements gathering, application/library design, implementation, verification, validation and maintenance.
Responsible for assisting engineers from other disciplines with tasks including engineering tool development and test set debugging/troubleshooting.
Responsible for ensuring conformance to DRS software engineering processes
General database tasks such as maintenance and the development of new tables and stored procedures to support existing and/or new software services and/or clients.
Other duties as assigned

Only candidates that meet the qualifications as outlined above will be contacted for further information.
Applicants selected for this position will be subject to a government security investigation and must meet eligibility requirements for access to classified information. Only US citizens are eligible for a Security Clearance.

DRS Technologies, Inc. is an equal opportunity/affirmative action employer. We consider applicants without regard to race, color, religion, creed, gender, national origin, age, disability, genetic information, marital or veteran status, or any other category protected by federal, state or local law. #C4ISR"
Junior Rail Engineer,Jacobs,"In this role, the Junior Rail Engineer will assist Engineers by performing the routine aspects of civil engineering assignments, subsequently providing familiarity with the programs, staff, and operating procedures of the company. The Civil EIT will work on a variety of projects pertaining to transportation projects - roadway, transit and rail.
 Prepare plans and reports related to a variety of transportation design efforts in the areas of roadway, transit and rail.
 Assist with the development of transportation alternatives in consideration of design and environmental issues.
 Assist our Project Engineers and Managers on a variety of tasks, including preparation of contract plans, specifications and construction cost estimates on transportation projects.
 Perform field investigations to collect data including field measurements, photographs, and sketches.
 Candidates will work in a diverse and challenging environment and must be: 1) capable of working on and coordinating simultaneous tasks, 2) have ability to coordinate technical matters with public and private entities, and 3) ability to adapt to various project sizes and differing clients

Qualifications
Bachelor of Science in Civil Engineering is required
2-5 years of civil engineering experience
Experience in Microstation, InRoads, OpenRoads.
EIT certification preferred

At Jacobs, we help prepare people for new opportunities and challenges. With positions at every level, openings in multiple disciplines, expertise in a range of markets and offices around the globe, we create an environment where you can learn, grow, and thrive. From our competitive benefits program to our Health and Safety initiative of Beyond Zero workplace injuries, we believe that you'll find a flourishing career here at Jacobs.

Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law. Jacobs is a background screening, drug-free workplace."
Software Engineer,General Electric,"About Us:
GE is the world's Digital Industrial Company, transforming industry with software-defined machines and solutions that are connected, responsive and predictive. Through our people, leadership development, services, technology and scale, GE delivers better outcomes for global customers by speaking the language of industry.
At GE Digital, we are creating technology and solutions to enable social, mobile, analytical and cloud capabilities for the Industrial Internet. The Industrial Internet is an open, global network that connects people, data and machines. Its about making infrastructure more intelligent and advancing the industries critical to the world we live in. At GE, we believe its about the future of industryenergy, healthcare, transportation, manufacturing. Its about making the world work better. GE is transforming itself to become the world's premier digital industrial company, executing critical outcomes for our customers. Explore how you can drive greater asset reliability, lower operating costs, reduce risk and accelerate operational performance with our Predix platform and software solutions.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer . Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

Role Summary:
GE Digital is looking for an experienced Software Engineer to work on a number of exciting projects that will have a very positive impact on GEs business.

Essential Responsibilities:
In this role you will:
Act as a member of the Software Center of Excellence, representing the face of high quality design and development practices.

Develop high performing, scaling and innovative end-to-end web applications.

Collaborate with system engineers, frontend developers and software developers to implement solutions that are aligned with and extend shared platforms and solutions.

Apply principles of SDLC and methodologies like Lean/Agile/XP, CI, Software and Product Security, Scalability, Documentation Practices, refactoring and Testing Techniques.

Writes codes that meets standards and delivers desired functionality using the technology selected for the project.

Build features such as web services and Queries on existing tables.

Understand performance parameters and assess application performance.

Work on core data structures and algorithms and implement them using language of choice.

Qualifications/Requirements:
Basic Qualifications:
Bachelors degree in STEM, a similar technical field of study or equivalent practical experience.

A minimum of 2 years of professional experience in IT.

GE Leadership Program Graduates will get credit towards relevant work experience, commensurate to the program they have completed.

Eligibility Requirements:
Legal authorization to work in the U.S. is required. GE will not agree to sponsor an individual for an employment visa now or in the future if there is a shortage of individuals with particular skills.

Must be willing to travel(0-5%).

Must be willing to work out of an office located in New Orleans, Louisiana.

Desired Characteristics:
Technical Expertise:
Experience developing web applications (Spring, RESTful, Jersey, RestEasy,JAX-WS, Web Services)

Experience with front-end technologies such as Javascript, HTML5, CSS, AngularJS, BackboneJS, EmberJS, KnockoutJS and/or experience with back-end technologies such as Java, C#

Experience with database tools (JPA, Hibernate,JDBC, Spring Data)

Experience with modern tooling (Gradle, Maven, Git, SVN)

Business Acumen:
Have a solid understanding of the underlying infrastructure needed to run a large distributed web application

Has the ability to break down problems and estimate time for development tasks

Has the ability to make basic technology choices based on experience

Demonstrates awareness about competitors and industry

Leadership:
Has the ability to take ownership of small tasks and deliver without supervision while using their discretion to seek help when necessary

Voices opinions and presents clear rationale. Uses data or factual evidence to influence

Recognizes collaborative behavior and participates in collaborative activities

Learns organization vision statement and decision making framework. Able to understand how team and personal goals/objectives contribute to the organization vision

Personal Attributes:
Demonstrate the ability to help team members.

Seeks to understand problems thoroughly before implementing solutions. Asks questions to clarify requirements when ambiguities are present.

Identifies opportunities for innovation and offers new ideas. Takes the initiative to experiment with new software frameworks.

Adapts to new environments and changing requirements. Pivots quickly as needed. When coached, responds to need & seeks info from other sources.

#DTR

Locations: United States; Louisiana; New Orleans

GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer . Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditional upon the successful completion of a background investigation and drug screen."
Data Integration Engineer (TS/SCI),Blue Canopy,"Founded in 2001, Blue Canopy is an award-winning provider of business solutions in the public and commercial sectors. Were focused on delivering outcomes that matter by combining the best use of technology, process improvement, and the system of governance. Our Mission is to serve our clients and help them achieve success by combining governance, industry experience, and information technology excellence.

At Blue Canopy, innovation, collaboration, and teamwork are the driving force behind our success. We are committed to hiring, retaining, and developing best-in-class professionals because we recognize that our team is what differentiates us in the marketplace as an industry-leader. We are driven by excellence, committed to integrity, and inspired to achieve limitless possibilities.

Blue Canopy is currently seeking a seeking a mid-level Data Integration Engineer to support a new program providing IT support to Department of Homeland Security. This position is located in the Washington D.C. area.

The Role:
Employ techniques and methodologies for solving tough analytical challenges in support of one of nation's premier law enforcement entities;
Use data science and development skills to automate manual analytical processes;
Utilize technical developer skills including, but not limited to: Python, R, Java, JavaScript, d3, Leaflet, Pig, Elasticsearch, PHP, Oracle, Angular JS, XML, .NET, Hadoop and other related technologies;
Develop prototypes and bring structure to large quantities of data to make automated analysis possible;
Provide technical expertise to support migration of applications and data sets to the cloud.
What You Need to Bring to the Table:
Bachelors Degree and six years experience in data integration and/or data sciences - additional experience may be substituted for degree;
Keen interest in using the latest data science techniques and methods to solve real world problem sets vital to national security;
Understanding of the Intelligence Community Information Technology Environment (IC ITE) data architectures, tools and services;
Understanding of cloud concepts and enabling tools and technologies (AWS/C2S, MS Azure, Informatica, VMWare etc.);
Familiarity with containers, data layers, SQL/NoSQL databases, and microservices;
Experience performing functional data modeling based on user requirements;
Current TS/SCI.
Preferred Qualifications:
Direct Experience with DHS and Intelligence Community (IC) component's data architectures and environments;
Expertise in various COTS, GOTS, and open source tools which support data integration;
Experience with DevOps environments;
One or more of the following certifications: DoD 8570 IAT Level II, ITIL, AWS Associate or Professional Solution Architect.
Clearance: Active TS/SCI clearance

As a full-time employee of Blue Canopy, you are eligible for an attractive benefits package, which includes medical, dental, life insurance, and short-term and long-term disability insurance. Our benefits also include paid holidays, Paid-Time-Off (PTO), a company-sponsored 401(k) plan, tuition reimbursement plan, and flexible spending accounts to allow you to pay for Health, Dependent care and Commuter costs with pre-tax income.

Blue Canopy Group, LLC is EOE/AA/M/F/Vet/Disability

#cj #cb"
Software Engineer,Aegis Software,"Aegis Software is seeking talented software developers to join our team for the continued development of the companys commercial software products. This includes enhancing, customizing and providing maintenance support for the companies entire product suite.

Candidates should have a passion for software development using the latest and greatest technologies. Our customers and market demand continual improvement through proper code structure, regular code reviews, and thorough testing on everything we produce. This ensures Aegis offers the best functionality to our customers while maintaining performance in high transaction manufacturing environments.

Responsibilities
Strong Development Skills in Object Oriented Programming Language (C# preferred)
Coordinating with Aegis Global Service Team to Diagnose and Correct Software Defects reported from the field
Ability to work independently from a specification through Aegis Custom Engineering Process
Enhancing the existing Aegis product suite with a strong focus on the end user scenarios and requirements
Requirements
Bachelors Degree in Computer Science or Related Degree
Past employment with multi-tier software solution provider
Working knowledge of full-stack .NET development with ability to work across User Interface, Business Logic, and Data Access Layers
Experience and/or working knowledge of one or more of the following technologies: WPF (Windows Presentation Foundation), WCF (Windows Communication Foundation), SQL Syntax, Entity Framework, LINQ, Lambdas/delegates, .NET Remoting, Visual Studio 2015, TFS (Team Foundation Server), DevExpress, C++, WinForms, Unit Testing, Agile/Scrum development processes
Ideal candidate would also have experience in a manufacturing sector, or with a software provider involved in the manufacturing vertical"
Software Engineer,RCS Corporation,"RCS has a direct hire opportunity for a Software Engineer within the Fossil Plant Services division of Structural Integrity Associates, Inc. (SI) in their Huntersville (Charlotte), NC office. SI is an engineering consulting firm dedicated to the analysis, control, and prevention of structural failures for the power industry and related applications. This position offers candidates the chance to work in a dynamic environment, with the most recognized experts in the power industry.

Who We Are
RCS Corporation is a certified Woman Business Enterprise specializing in the recruitment of professionals throughout the United States. With two corporate offices, over 20 years of experience, and a presence in 30+ states, we are well-positioned to service employers, employees, and candidates across every discipline. Our emphasis is on excellent work ethic, accountability, transparency, effective communication and safety.

Our Client
Structural Integrity Associates, Inc. (SI), founded in 1983, is an engineering consulting firm of approximately 300 highly qualified professionals and support personnel. With over three decades of stable and strong growth, SI is known for high quality service and innovative products and we strive for excellence in all we do. Work alongside industry experts in a familyoriented culture where employees respect one another.

Description
The Fossil Plant Services division of Structural Integrity Associates, Inc. (SI) is currently looking for a Software Engineer to join our team. SI is an engineering consulting firm dedicated to the analysis, control, and prevention of structural failures for the power industry and related applications. In addition to consulting services, SI offers a software product portfolio that provides online asset monitoring solutions. With that, we help our clients in the digitalization of their assets, for better decision making and safe operation. Big Data and IIoT is a reality for us. This position offers candidates the chance to work in a dynamic environment, with the most recognized experts in the power industry.

Summary
As a member of the Data & Analytics team, the software engineer will be instrumental in the further improvement and development of SIs cloud-based software platform. They will take ownership of the product life cycle, from concept development to maintenance of implemented applications. The successful candidate will have experience in working with and leading software development teams, and is familiar with common methodologies deployed (Agile, Scrum, etc.). Excellent interpersonal skills are a must, as interaction with our corporate clients will be part of this role. The software engineer will work closely with our analysis and project engineers, and external partners SI is cooperating with. This ensures an interesting work environment and creates opportunities for professional and personal growth.

Requirements
Software/web application development, including database structures.
Design, build, test, and deploy scalable data pipelines and APIs.
Experience in product architecture development.
Product Life Cycle Management (from cradle to grave).
Application of state-of-the-art software development methodologies (Agile, Scrum, etc.).
Leadership to a team of software developers.
Excellent interpersonal and communication skills.
Preferred
Web development experience on .net Framework, C#/C++, XAML (Silverlight, WPF), SQL, Java.
Experience in Computational Engineering (Big Data)
Understanding of Data Historians (e.g. OSIsoft PI, Emerson Ovation).
3D graphics integration into web applications.
Why Consider SI?
Employee owned company
Incomparable industry reputation
Robust compensation and benefits programs
Entrepreneurial spirit
Employee stock ownership (ESOP) participation
Open, flexible approach to work environment and job responsibilities.
Opportunities with RCS change daily. For a complete listing of our current openings, please visit our website at www.rcscorporation.com.

RCS is an Equal Opportunity Employer. RCS provides equal employment opportunities without regard to race, color, religion, gender, sex (including gender identity and sexual orientation), national origin, age, disability, veteran or marital status)."
"Junior Water Engineer - Baton Rouge, LA",CH2M,"Work as a junior water engineer in Baton Rouge and for other municipal clients while having a hand in transforming tomorrow.
We are looking for a highly motivated junior engineer to work on a variety of water, wastewater, and coastal engineering tasks. This position will primarily support senior level engineers to complete planning/study documents, preliminary design, detailed design, plan review, operational support, and periodic on-site observation/management of construction activities. Each day, you will be an important team member as you participate in the design, coordination, and execution of projects for our clients. Our senior and mid-level engineers are committed to strong mentorship and providing opportunities to work independently and with peers. You should be a self-motivated professional with a desire to grow personally as well as professionally with each project in our diverse and fast-paced office. Candidates should be self-motivated, detailed oriented, quality driven, team-focused.

Our junior water engineers:
Work under the supervision of senior and mid-level level engineers.

Perform a wide variety of routine assignments which are clearly defined and require the application of standard design engineering techniques, procedures and criteria.

Prepare models, engineering specifications, data sheets, sketches, drawings and perform calculations in support of engineering activities.

Make field trips and conduct field audits.

Independently evaluate the selection and/or modification of standard techniques, procedures, criteria and systems to efficiently meet project goals.

Prepare planning documents, reports, studies, and calculations.

Perform data analysis using Microsoft Excel and Access

Support in schedule development and maintenance using Microsoft Project and/or Primavera P6

Possess an outgoing and positive attitude, are dependable and meet all assigned project deadlines.

The minimum qualifications for this position are:
Bachelors Degree in Civil Engineering, Environmental Engineering, or a related engineering discipline

Engineer Intern (EI) License or the ability to obtain within 3 months

0 to 2 years competency in working with municipal, state and federal clients in engineering roles
The ideal candidate will possess the following skills and experience: Masters degree in Civil Engineering, Environmental Engineering or a related engineering discipline
Experience in serving various clients involving a wide variety of fields including water, wastewater, and coastal for planning, consulting, design, operational support, and construction services
Advanced Experience with Microsoft Excel, Access, and Project
At CH2M, the greatest challenges provide the biggest rewards. Each day, your drive and creative ideas will be providing solutions that help build a better tomorrow. Whether it is the pride that comes with accomplishment, personal growth or making a difference in the world, you will discover true success in a career that brings out the best in you at CH2M. Developing People through Challenging Projects.

CH2M is an Equal Opportunity Employer - M/F/Veteran/Disability. Learn more about your rights under Federal EEO laws and supplemental language ."
Software Engineer,iBasis,"Software Developer

We are looking for a highly-motivated individual to join our team responsible for building internal and external web-based applications.

The ideal candidate has solid skills and practical experience developing web-based applications and services.

The candidate must be an energetic self-starter, driven to learn and enhance his or her skills, and to share and apply that knowledge in a cooperative team environment.

This position requires good design, data modeling and web application experience, and makes use of technologies such as Angular, NodeJS, JQuery, CSS3, HTML5, Spring, Struts, Linux/Solaris.

Responsibilities
Support, maintain, and improve existing applications
Collaborate with business, to define functional specifications using Agile practices
Rapidly prototype, implement, deploy, and document new applications and application components
Write unit tests
Participate in and contribute to peer code reviews

Requirements
1-2 years of experience developing complex enterprise and/or commercial applications
Ability to work effectively in a team as well as working proactively as an individual contributor
Excellent written and verbal communication.
CS degree or equivalent

The ideal candidate will have some experience in the following technologies:
J2EE experience, including EJB, JPA, Struts, Spring, JSP and JSTL
Experience with JavaScript/ES6/TypeScript, particularly Angular, NodeJS, NPM, Gulp, etc.
Familiar with writing native SQL queries
Web server administration experience (Apache HTTP & Tomcat configuration)
Experience using Git for source control management in a team environment. Branching, merging and rebasing should be familiar concepts.
Comfortable using modern IDEs, but not afraid to break out vi from time to time.
Comfortable on the command line in a Linux-based environment, including package installation and shell scripting.
Practical experience developing telecommunications, e-commerce, or data visualization solutions is a plus"
Intern-Data Science,Palo Alto Networks,"Palo Alto Networks is the fastest-growing security company in history. We offer the chance to be part of an important mission: ending breaches and protecting our way of digital life. If you are a motivated, intelligent, creative, and hardworking individual, then this job is for you!

The Summer Internship Program includes:
1:1 mentorship
Fun and engaging events that inspire your intellectual curiosity
Expand your knowledge and work on challenging projects
Connect with other interns, recent grads, and employees across the company

The Big Data and Analytics works very closely with business stakeholders across various functions like Sales, Marketing, GCS, Infosec, Operations, and Finance. We leverage latest technologies from Big Data ecosystem to improve business outcomes. We are seeking intern for data science team who will work closely with other Prinicipal Data Scientists.

Key Job Functions Include:
Identify/develop appropriate machine learning/data mining/text mining techniques to enable better business outcomes.
Understand and analyze data sources including sampling biases, accuracy, and coverage.
Ask questions and break apart problems scientifically. Form hypotheses and validate.
Use analytical rigor and statistical methods, machine learning, programming, data modeling, simulation and advanced mathematics to analyze large amounts of data, recognizing patterns, identifying opportunities, posing business questions and making valuable discoveries.
Research new ways for modeling and predictive behavior for large scale projects.
Generate and test hypotheses, designing experiments to answer targeted questions of advanced complexity.
Collaborate with data engineers to identify data preparation/cleansing/ETL pipelines
Define data needs, evaluate data quality, and extract/manipulate data in a ""Big Data"" environment.
Documents projects including business objective, data gathering and processes, leading approaches, final algorithm, detailed set of results and analytical metrics.
Interprets and communicates insights and findings.
Validate score performance.
Conduct ROI and benefit analysis.
Document and present model process and performance.

Qualifications
Advanced degree in Machine Learning, Computer Science, Electrical Engineering, Physics, Statistics, Applied Math or other quantitative fields from a reputed university (Ph.D. a plus).
0-3 years of working experience in analytics, data mining, and/or predictive modeling.
Excellent knowledge of machine learning/data science approaches.
Comfortable interacting with business peers to understand and identify use cases. Be able to articulate solutions & present them to business.
Familiarity with data visualization tools like Tableau.
Exceptional coding skills in Python, R, PySpark.
Ability to adapt to scrum & agile methodology, comfortable with development tools like Jira/Confluence.
Experienced in applying data science to business problems (sales, IT, etc.) preferred.
Experience with Hadoop and NoSQL related technologies.
Knowledge of NLP/Text mining techniques and related open source tools."
Software Engineer,Vistronix,"Overview ASRC Federal - Vistronix is a national security solutions provider specializing in transforming big and complex data sets into mission critical intelligence. Ingesting, processing, and exploiting Big Data is at the core of everything we do: Cyber & SIGINT Operations, C4ISR & Multi-INT Processing, and Enterprise & Open Source Analysis. As a national security middleweight, we have a passion for our customers' mission and value ingenuity, agility, speed, and the ability to think and deliver at scale. For more information, visit www.vistronix.com.

ASRC Federal - Vistronix is currently seeking an experienced Software Engineer for a role on one of our subcontracts. This full time position affords the successful candidate the opportunity to work in an engaging, high-tech development environment, working with an excellent team and customer base.

Responsibilities The Customer is looking for an energetic, creative resource with a proven track record of successful technical project implementation. The Candidate must excel working within a team, showing a willingness to learn new technologies and demonstrating initiative. High performing candidates will be positively challenged as they interact with highly talented technical peers and a forward leaning leadership team. The software engineer will work within an Agile/Scrum development team to design, develop, integrate, test, and deploy COTS and custom components for a new, enterprise, Externalized Authorization Management (EAM) system.

This is an exciting phase for Identity and Access Management (IAM) as the future of Externalized Authorization will lead to a new set of paradigms for access control. Recent investments in IAM now leverage state of the art solutions using real, industry standards. Digital policies will be authored in a distributed manner, allowing policy changes to be implemented in near real time instead of weeks or months. Since not all customers live and work on one network, there is a recognition that enterprise IAM solutions should be made available for use on other networks. Cloud hosting will allow IAM services to leverage the full capabilities that make availability demands and self-healing services a reality. The software engineer will be a part of turning this vision into reality and will have the opportunity to learn new skills along the way. The software engineer will assist in the installation, deployment, and maintenance of the COTS product, making configuration changes as needed. Responsibilities will include interfacing with the infrastructure teams during upgrades or troubleshooting. The software engineer may support the development of new customer digital policies and integrating them with the enterprise service. The software engineer may interface with customers during digital policy creation, policy ingestion, policy integration, and testing. The software engineer will support the implementation of an automated unit and integration testing capability for access control policies and other service components using JUnit. While the ideal candidate has strong experience deploying solutions in a cloud environment, candidates without this experience will learn this process with the rest of the team. The software engineer will also assist in the creation of technical documentation and other routine programmatic and security accreditation artifacts. This is full lifecycle software management. While deployment of a COTS tool is the tactical project objective, development of Java components which call the COTS software APIs will be critical to achieving strategic objectives like delivering a highly scalable service, meeting uptime requirements through automation, and automating the distributed policy authoring process. The successful candidate may support end user training on the creation and management of access control policies. Interested candidates must have more than intermittent exposure to mandatory skills. Strong candidates will have a commanding knowledge of mandatory skills and be able to articulate depth and breadth of each skill.

The work location is in Northern Virginia. Local travel in the DC Metro area, and pager duty is required.

Qualifications
Demonstrated experience writing code in an object oriented language.
Demonstrated experience testing new functionality prior to deployment.
Demonstrated experience troubleshooting issues encountered by users.
Demonstrated experience writing LINUX shell scripts.
Optional Skills:
Experience working on an agile development team, writing and working off user stories.
Experience developing applications in a cloud environment.
Demonstrated, professional experience with deployments in the Sponsors environment.
Experience developing JUnit test cases or other automated testing experience.
This position requires an active Security Clearance.

Positions require a Top Secret security clearance, based on current background investigation (SBI), as well as the favorable completion of polygraph. Clearance and polygraph processing will be completed by the U.S. Government.

ASRC Federal and its Subsidiaries are Equal Opportunity / Affirmative Action employers. All qualified applicants will receive consideration for employment without regard to race, gender, color, age, sexual orientation, gender identification, national origin, religion, marital status, ancestry, citizenship, disability, protected veteran status, or any other factor prohibited by applicable law."
Software Engineer,Sony Music Entertainment,"Overview The Orchard is an independent music and film distribution company distributing music from artists such as Sleigh Bells, Charles Bradley, and Slayer as well as films like Academy Award-nominated Cartel Land, Golden Globe-nominated Neruda, and Hunt For The Wilderpeople. With our industry-leading technology and operations, we partner with companies of all sizes to make their music, films and videos available across hundreds of digital outlets around the world, and physical retailers too. At The Orchard, the focus is to provide a comfortable, social and engaging environment to encourage productivity and creativity.

The Orchard is looking for a Software Engineer to join our NYC (East Village) development team.

Responsibilities
Build amazingly crafted software and systems to provide our clients with a cutting-edge digital media management, delivery, and reporting/analytics suite.
Participate in ongoing Sony platforms integration efforts to deliver next generation, full service distribution products.
Collaborate with a talented team of engineers and designers.
Work with new technology and tools in a highly-collaborative, Agile environment.
Use industry standards and adopt new technologies to manage big data transformations and analysis.
Advocate for best schema designs to manage Orchards master data to ensure scalability and flexibility.

Qualifications
Familiarity with design patterns. Experience with microservices a plus.
Agile & results-focused. Ability to work effectively on small, cross-functional teams following Agile development methodology (Kanban, Scrum).
Experience designing and implementing scalable ETLs.
Experience designing and consuming RESTful APIs.
Familiarity with relational databases and NoSQL databases.
Ability to implement XHTML/HTML5, CSS, and Javascript quickly and effectively.
Familiarity with AWS is a huge plus (DynamoDB, Redshift, SWF, Lambda).
Excellent verbal and written communication skills.
Love for music and film a plus.
Full stack proficiency. We are polyglots, currently using Python, PHP and Ruby but open to the best tool for the job. Check out http://stackshare.io/the-orchard for a better idea of what were using.

Additional Orch Information:
Tech Blog - Check out The Orchards tech blog: https://medium.com/orchard-technology.
Project Work - At The Orchard, were happy to open-source software that helps the community. For instance, a few months ago, we built Garcon, a framework to help write distributed workflows that run on Amazon SWF: https://pypi.python.org/pypi/Garcon.

About The Orchard

The Orchard is an independent music and video distribution company that provides an innovative and comprehensive sales and marketing platform for content owners. With our industry-leading technology and operations, we partner with companies of all sizes to make their music and videos available across hundreds of digital and mobile outlets around the world, and physical retailers too. We work with our clients to maximize their revenue, and provide them with the latest and greatest digital marketing tools and sophisticated sales analytics to help them run their businesses efficiently and effectively.

The Orchard was founded in 1997 in New York City to foster independence and creativity in the music industry. Our headquarters are in New York and London, with operations in more than 25 markets around the world, including France, Spain, Germany, Mexico, Argentina, India, Australia and more. At The Orchard, the focus is to provide a comfortable, social and engaging environment to encourage productivity, creativity and personal/business relationships. From monthly potlucks and game nights to company kickball teams and other staff perks, our employees like what they do and where they do it.

The Orchard is committed to providing equal employment opportunity for all persons regardless of age, disability, national origin, race, color, religion, sex, sexual orientation, gender identity or expression, pregnancy, veteran or military status, genetic information or any other status protected by applicable federal, state, or local law."
Software Engineer,Capgemini,"About Capgemini

With more than 193,000 people in over 44 countries, Capgemini is one of the world's foremost providers of consulting, technology and outsourcing services. The Group reported 2016 global revenues of EUR 12.539 billion. Together with its clients, Capgemini creates and delivers business and technology solutions that fit their needs and drive the results they want. A deeply multicultural organization, Capgemini has developed its own way of working, the Collaborative Business ExperienceTM, and draws on Rightshore, its worldwide delivery model Learn more about us at www.capgemini.com.
Rightshore is a trademark belonging to Capgemini
Capgemini America Inc is an Equal Opportunity Employer encouraging diversity in the workplace. All qualified applicants will receive consideration for employment without regard to race, national origin, gender identity/expression, age, religion, disability, sexual orientation, genetics, veteran status, marital status or any other characteristic protected by law.
This is a general description of the Duties, Responsibilities and Qualifications required for this position. Physical, mental, sensory or environmental demands may be referenced in an attempt to communicate the manner in which this position traditionally is performed. Whenever necessary to provide individuals with disabilities an equal employment opportunity, Capgemini will consider reasonable accommodations that might involve varying job requirements and/or changing the way this job is performed, provided that such accommodations do not pose an undue hardship.

Job Responsibilities

Job Title: SharePoint Administrator
Location: Charlotte, NC / Monterey Park, CA
The SharePoint Administrator is responsible for providing support and maintenance of the Microsoft SharePoint platform technology and methodologies.
This includes but is not limited to, installing, configuring, and upgrading SharePoint systems.
Manage system operations and services, testing and documentation.

Job Responsibilities:
Administer and maintain all SharePoint environments.
Deploy appropriate updates/patches to SharePoint in a timely manner.
Create and migrate SharePoint sites and information as necessary.
Provide and maintain SharePoint environments support and configuration documents.
Configure Service Applications and troubleshoot any issues.
Assist with implementation of new software products, tools or systems.
Assist with applications development project planning, scoping, scheduling, and implementation.
Assist with Nintex workflow platform.
Monitor performance and reliability to improve the environment.
Maintain established disaster recovery, best practices and governance policies/procedures for all business critical data and publishing content.
Work with the development team to assist in deployment and testing custom built solutions.
Participate in day to day support and on-call rotation schedule.
Maintain SharePoint Cross-Farm (Federated) Services Architecture.

Desired Skills:
Installation and configuration of SharePoint Server 2007, 2010, 2013, and 2016 enterprise environments.
Understanding of SharePoint administration including management of service applications, web applications, site collection administration, solution deployment, and backup/restore via Central Administration and PowerShell.
Knowledge of Nintex workflow and forms.
Knowledge how to troubleshoot network connection and authentication problems.
Knowledge of IIS architecture, Server Manager and its functionality.
Understanding of Active Directory (including LDAP queries).
Familiar with DNS, SMTP, Network Load Balancing (NLB).
Knowledge how to use Event Viewer and Performance Monitor.
Must be able to work independently at location.
Collaborate with remote support teams.
Positive teamwork and communication skills.
Excellent time and task management skills.
Ability and dedication to resolve problems effectively and efficiently."
Big Data Engineer,Booz Allen Hamilton,"Booz Allen Hamilton has been at the forefront of strategy and technology for more than 100 years Today, the firm provides management and technology consulting and engineering services to leading Fortune 500 corporations, governments, and not-for-profits across the globe. Booz Allen partners with public and private sector clients to solve their most difficult challenges through a combination of consulting, analytics, mission operations, technology, systems delivery, cybersecurity, engineering and innovation expertise.
Big Data Engineer
Key Role:
Design, model, document, and guide the logical and conceptual relationship of data and database changes for complex applications. Analyze the needs and requirements of existing and proposed systems, develop technical, structural, and organizational specifications, and create standards and models to monitor and enhance capacity and performance. Work on unusually complex technical problems and provide solutions which are highly innovative and ingenious. Act as an advisor to management and clients on advanced technical research studies and applications and serve as an advanced consultant with capability knowledge, exceling at client delivery with expertise developed in a specific area. Take a leadership role in generating intellectual capital to help the firm grow its business, leverage relationships developed with clients, and interact directly with senior-level clients. Develop competency in project and financial management and become more involved in managing teams and formulating business strategy. Guide the progress of projects and mentor junior staff in firm and technical competencies.
Basic Qualifications:
-3+ years of experience with Python required
-Experience with NiFi, Storm, and other ingestion technologies
-Experience with Hadoop and Accumulo
-Experience with REST layers, MongoDB, and ElasticSearch
-Ability to learn new programming languages and architectures quickly
-Active Secret clearance required
-BA or BS degree
Additional Qualifications:
-Experience in a rapid prototyping environment
-Possession of excellent oral and written communication skills
Clearance:
Applicants selected will be subject to a security investigation and may need to meet eligibility requirements for access to classified information; Secret clearance is required.
Integrating a full range of consulting capabilities, Booz Allen is the one firm that helps clients solve their toughest problems by their side to help them achieve their missions. Booz Allen is committed to delivering results that endure.
We are proud of our diverse environment, EOE, M/F/Disability/Vet.
AFH26, DIG100, SIG2017, TMJ16"
Sr. Data Engineer,Ethos Lending,"Our mission:
Ethos Lending is a Khosla Ventures backed series B startup, with a mission to automate the trillion dollar mortgage industry. Founded in 2013, Ethos Lending (Ethos) is a next-generation mortgage software company and direct lender that originates high-quality loans at amongst the lowest price points in the country. Leveraging technology, Ethos has re-engineered the mortgage supply chain, creating a highly intuitive, transparent and efficient experience for the consumer and mortgage broker. Through data-driven decisions, our loan data engine automates workflows and processes resulting in high throughput and reduced risk, while accelerating the loan process.
We need talented, passionate and out of box thinkers who challenge the status quo, working together to make this happen. We are looking to grow our teams with people who share our energy and enthusiasm for creating the best experience for our users.

Responsibilities:
As a Data Engineer, youll be responsible for the curation and stewardship of our data. You will be designing and maintaining processes around large volume data ingestion, clean-up, and processing that meet the needs of millions of agents, brokers, and home buyers. Youll work with a variety of teams to understand their data pipeline needs and come up with innovative solutions. Youll work with a team of engineers and collaborate with product managers and designers to investigate new data sources, monitor data quality issues, and set policies for future data collection.
Required Qualifications
BS in Computer Science, Engineering, or related technical discipline or equivalent combination of training and experience
5 + years of Data Warehouse and ETL design and implementation
Exposure to R, Tableau
Advanced SQL skills and solid Python and shell scripting skills
Experience building and maintaining a data dictionary
Knowledge master data management best practices
Knowledge of data quality and cleansing techniques
Excellent analytical/troubleshooting skills
Excellent communication skills including the ability to communicate data technologies and architectures
Nice to haves:
Experience with Java, Scala, and Spark
Current experience developing and deploying applications to public cloud (Azure or AWS)"
Software Engineer,"Two Sigma Investments, LLC.","Since 2001, our software engineers have helped tackle the complex and interesting challenge of discovering value hidden in the worlds data. We are pushing the technical envelope to solve these problems, and in the process are helping to redefine investment management and other fields. As the world of data continues to grow exponentially, the issues they address will only increase in difficulty, scale and excitement.

In order to meet these challenges, weve built a data accumulation platform that allows us to ingest over 10 terabytes of data per day, and a custom distributed storage solution to store the over 50 petabytes of information weve accumulated since our inception. Weve also built an entire suite of analysis tools that enable our quantitative researchers to utilize this data to produce predictive models that help us automatically invest each and every day.

This model-driven, technology-fueled approach to investing not only gives us a long-term advantage over old-school investors  it has also created a whole set of technical problems to solve that dont always have an obvious solution. This gives you the ongoing opportunity to build proprietary solutions and/or bring to bear the best open source options the market. We have created a robust infrastructure that empowers all of our engineers, and you will work within a collaborative work culture that ensures that a great idea can come from anywhere.

Problems at our level of complexity require you to possess a passion for learning as well as deep understanding across a wide array of technical competencies. Weve attracted technologists who possess special capabilities in a wide variety of domains including data transformation and visualization, performance optimization, cloud computing, and distributed systems. While we face large-scale problems, we hire only the best to take them on. This enables us to keep your team size small and your individual impact significant.

While there is no set recipe for success at Two Sigma, we find that you will tend to have the following qualifications:
You have at least a bachelors degree in a technical or quantitative field.
You have an understanding of data structures and algorithms.
You have experience with Java (or other JVM languages), Python, C or C++.
You have extraordinary programming skills.
You have demonstrated experience in large-scale systems.

We are an equal opportunity employer and value diversity at our company. We do not discriminate on the basis of race, religion, color, national origin, gender, sexual orientation, age, marital status, veteran status, or disability status."
Software Engineer,Princeton Consulting Group LLC,"5+ years of relevant professional experience
BA/BS Degree
Working understanding of generally accepted software
development methodologies, tools, and techniques

Minimum 2 years of recent development experiences using
Visual Studio, TFS, and MS SQL Server

Must have experience in the following: C#, SQL server,
T-SQL, writing queries, data modeling.

Experience with the following technologies a plus:
Microservice Oriented Architecture, ASP MVC, Angular, Typescript, Razor, Unity
IoC, Entity Framework

Recent experience in the financial services industry
working with financial data

Working knowledge of fixed income, especially structured
credit products like CLO and CMBS is a big plus.

Strong Quantitative analytical skills"
Data Scientist,General Electric,"About Us:
GE is the world's Digital Industrial Company, transforming industry with software-defined machines and solutions that are connected, responsive and predictive. Through our people, leadership development, services, technology and scale, GE delivers better outcomes for global customers by speaking the language of industry.
GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer . Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

Role Summary:
The Data Scientist will work in teams addressing statistical, machine learning and data understanding problems in a commercial technology and consultancy development environment. In this role, they will contribute to the development and deployment of modern machine learning, operational research, semantic analysis, and statistical methods for finding structure in large data sets.

Essential Responsibilities:
The Data Scientist will be part of a data science or cross-disciplinary team on commercially-facing development projects, typically involving large, complex data sets. These teams typically include statisticians, computer scientists, software developers, engineers, product managers, and end users, working in concert with partners in GE business units. Potential application areas include remote monitoring and diagnostics across infrastructure and industrial sectors, financial portfolio risk assessment, and operations optimization.

Develop analytics within well-defined projects to address customer needs and opportunities. Work alongside software developers and software engineers to translate algorithms into commercially viable products and services.

Work in technical teams in development, deployment, and application of applied analytics, predictive analytics, and prescriptive analytics.

Perform exploratory and targeted data analyses using descriptive statistics and other methods.

Work with data engineers on data quality assessment, data cleansing and data analytics.

Generate reports, annotated code, and other projects artifacts to document, archive, and communicate The work and outcomes.

Share and discuss findings with team members.

Qualifications/Requirements:
Basic Qualifications:
Bachelors Degree in a STEM major (Science, Technology, Engineering, Mathematics)

Eligibility Requirements:
Legal authorization to work in the U.S. is required. We will not sponsor individuals for employment visas, now or in the future, for this job.

Desired Characteristics:
Demonstrated skill in the use of one or more analytic software tools or languages (e.g., SAS, SPSS, R, Python)

Demonstrated skill at data cleansing, data quality assessment, and using analytics for data assessment

Demonstrated skill in the use of applied analytics, descriptive statistics, and predictive analytics on industrial datasets

Demonstrated awareness of feature extraction and real-time analytics methods

Demonstrated awareness of analytic prototyping, analytic scaleup, analytic scaling, and solutions integration

Works well with others in a collaborative environment

Self starter
#DTR

Locations: United States; Ohio; Evendale

GE offers a great work environment, professional development, challenging careers, and competitive compensation. GE is an Equal Opportunity Employer . Employment decisions are made without regard to race, color, religion, national or ethnic origin, sex, sexual orientation, gender identity or expression, age, disability, protected veteran status or other characteristics protected by law.

GE will only employ those who are legally authorized to work in the United States for this opening. Any offer of employment is conditional upon the successful completion of a background investigation and drug screen."
"Software Engineer, Site Reliability (SRE)",DiDi Labs,"Software Engineer, Site Reliability Engineer

DiDi Chuxing is the worlds leading mobile transportation platform. We offer a diverse range of mobile tech-based mobility options for more than four hundred million users across more than four hundred cities. Our services include premier taxi hailing, private car-hailing, Hitch (social ride-sharing), DiDi Chauffeur, DiDi Bus, DiDi Minibus, DiDi Test Drive, DiDi Car Rental, and DiDi Enterprise Solutions, all delivered through a single mobile app. More than 20 million daily rides were completed on DiDis platform as of October 2016 and our numbers continue to grow. Our mission is to become the global leader in smart transportation and automotive technology, the worlds largest operator of vehicle networks and the global leader in smart transportation systems.

DiDi Chuxing has now officially launched DiDi Labs in Mountain View, California. DiDi Labs will be at the forefront of our most cutting-edge research and development in AI-based security and intelligent driving technologies. Didi Labs offers the unique combination of the visibility and flexibility of a startup with the stability of a mature company. We aim to attract top engineers and researchers with the skills and passion to drive revolutionary transformation of the global transportation industry.

As a young team, our desire is to build and maintain a working environment that engenders free flow of ideas and sets our employees up for strong personal growth and career development.

Site Reliability Engineering (SRE) is an engineering discipline that combines software and systems engineering to build and run large-scale, massively distributed, fault-tolerant systems. SRE ensures that DiDi's services have reliability and uptime appropriate to users' needs and a fast rate of improvement while keeping an ever-watchful eye on capacity and performance.

Responsibilities

Engage in and improve the whole lifecycle of servicesfrom inception and design, through deployment, operation, and refinement.
Support services through activities such as developing software platforms and frameworks, system design consulting and capacity planning.
Maintain services by measuring and monitoring availability, latency and overall system health.
Scale systems sustainably through mechanisms like automation, and evolve systems by pushing for changes that improve reliability and velocity.
Practice sustainable incident response and blameless postmortems.
Qualifications

B.S. in computer science or related technical field.
Experience with algorithms, data structures, complexity analysis and software design.
Experience with Unix/Linux operating systems internals and administration.
Experience with one or more of the following: C, C++, Java, Go, Python, Perl or Shell.
Preferred Qualifications

Interest in designing, analyzing and troubleshooting large-scale distributed systems.
A systematic problem-solving approach, coupled with strong communication skills and a sense of ownership and drive.
Ability to debug and optimize code and automate routine tasks.
Experience with networking (e.g., TCP/IP, routing, network topologies and hardware, SDN)
Experience with container/virtualization.
Fluent in spoken Mandarin Chinese"
Software Engineer,KEYW Corporation,"KeyW is a total solutions provider for the Intelligence Community, solving their toughest challenges. We support the collection, processing, analysis and dissemination of information across the full life cycle of the Intelligence Communitys mission. We employ and challenge the most talented professionals in the industry with solving such complex problems as preventing cyber threats, transforming geospatial imaging into intelligence, and combating global terrorism.

Responsibilities The Software Engineer develops, maintains, and enhances complex and diverse software systems (e.g., processing-intensive analytics, novel algorithm development, manipulation of extremely large data sets, real-time systems, and business management information systems) based upon documented requirements. Works individually or as part of a team. Reviews and tests software components for adherence to the design requirements and documents test results. Resolves software problem reports. Utilizes software development and software design methodologies appropriate to the development environment. Provides specific input to the software components of system design to include hardware/software trade-offs, software reuse, use of Commercial Off-the-shelf (COTS)/Government Off-the-shelf (GOTS) in place of new development, and requirements analysis and synthesis from system level to individual software components.

Qualifications Required Skills:
Analyze user requirements to derive software design and performance requirements
Design and code new software or modify existing software to add new features
Debug existing software and correct defects
Integrate existing software into new or modified systems or operating environments
Develop simple data queries for existing or proposed databases or data repositories
Provide recommendations for improving documentation and software development process standards
Develop or implement algorithms to meet or exceed system performance and functional standards
Assist with developing and executing test procedures for software components
Write or review software and system documentation
Develop software solutions by analyzing system performance standards, confer with users or system engineers; analyze systems flow, data usage and work processes; and investigate problem areas
Serve as team lead at the level appropriate to the software development process being used on any particular project
Modify existing software to correct errors, to adapt to new hardware, or to improve its performance
Design, develop and modify software systems, using scientific analysis and mathematical models to predict and measure outcome and consequences of design
Design or implement complex database or data repository interfaces/queries
Oversee one or more software development teams and ensure the work is completed in accordance with the constraints of the software development process being used on any particular project
Design or implement complex algorithms requiring adherence to strict timing, system resource, or interface constraints; Perform quality control on team products
Confer with system engineers and hardware engineers to derive software requirements and to obtain information on project limitations and capabilities, performance requirements and interfaces
Coordinate software system installation and monitor equipment functioning to ensure operational specifications are met
Implement recommendations for improving documentation and software development process standards
Fourteen (14) years experience as a SWE in programs and contracts of similar scope, type, and complexity is required.
Location: Annapolis Junction, MD

Degree: Bachelors degree in Computer Science or related discipline from an accredited college or university is required. Four (4) years of additional SWE experience on projects with similar software processes may be substituted for a bachelors degree.

Clearance: Positions require a Top Secret security clearance, based on current background investigation (SBI), as well as the favorable completion of polygraph. Clearance and polygraph processing will be completed by the U.S. Government.

KeyW is an EEO employer. We are committed to providing fair and equal employment consideration, regardless of race, color, religion, national origin, gender, sexual orientation, age, marital status or disability."
Software Engineer,WEX Inc.,"The WEX Health Software Engineer role will work with development code and data structures in an Agile-rich environment. This individual will develop, modify, and maintain the WEX Health Cloud solution. The position requires timely assessments and will need to be able to switch contexts quickly and hand over requests in a critical and timely manner.

Primary Responsibilities

Designs/codes applications following specifications using the appropriate tools.
Maintains and modifies existing applications on their own as well as under direction from senior staff members.
Performs maintenance programming for existing version.
Assumes responsibility for ongoing data architecture for product.
Knowledge, Skills and Abilities

Ability to effectively communicate technical concepts to other technical staff members.
Ability to interface well with Professional Service staff and other non-technical members of the organization.
Demonstrated aptitude for learning new technologies.
Knowledge relating to the area in which analysis and coding is performed.
Qualifications

Minimum Requirements and Qualifications
Bachelors degree in Computer Science or related area or equivalent education or work experience.
3+ years of experience working with software systems and development experience in ASP.Net and C#.
Experience in database analysis, specifically SQL, troubleshooting customer problems and responding to customer requests.
Experience developing in a .Net framework preferred.
Excellent skills in supporting customers through problem resolution.
Excellent communication skills are required
Equal Opportunity Employer Vets/Disabilities
Primary Location

: United States
Other Locations

: U.S.-MN-Minneapolis, U.S.-ND-Fargo
Schedule

: Full-time
Job

: WEX Health"
SOFTWARE ENGINEER,Orzota,"Proposed Job Duties:
Develop required analytics components for Orzota Solutions.
Integrate Big Data technologies such as Cassandra, H2O, Deep Learning, etc. into the Orzota Platform.
Build streaming solutions using Kafka, Storm, Spark Streaming for IOT applications.
Build fault tolerant, self-healing, adaptive and highly accurate data computational pipelines for the Orzota Customer Knowledge and other Solutions.
Work as a member of Orzotas global development team, taking part in regular staff meetings, daily standups for agile sprints and help the company build world class products.

Qualifications:
Bachelors or Masters Degree in in computer science or computer engineering
At least 1-4 years of experience
Programming experience in SQL, Java, Python, R, Scala etc.
Experience using open source big data components like: Hadoop, Spark, Kafka, Cassandra etc."
Software Engineer,BTI Solutions,"BTI Solutions is a global telecommunication engineering company that offers mobile and system engineering solutions for the telecommunication industry

Job Title: Software Engineer

Locations: SF_Bay (CA) / Washington/Oregon / Michigan / Cleveland, Pittsburgh / Chicago / Minnesota/ Colorado...

Description:
Provide support to multi-functional team of R&D, Program Management, Core Networking, RF Engineering, Transport and Technical/Training Support to fulfill multi-technology wireless network deployment across customer markets

Prime responsibility include providing assistance in Radio Network Optimization by evaluating routine work methods and creating scripts and tools for efficiency improvement
Determines operational feasibility by evaluating analysis, problem definition, requirements, solution development, and proposed solutions
Work with experienced RF Engineers and Systems Engineers
Understanding of RF parameter audit requirements, data-fills, analysis of RAN and Core Operation metrics, Device Logs
Code, test and troubleshoot programs utilizing appropriate Hardware, Software and Programming language
Refine data and format final product
Create Efficiency improvement tools by means of scripting
Maintain and modify scripts/programs, make approved changes by amending flow charts, algorithms and programming logic
Create and maintain methods of procedures and other related documentation
Document issues, lessons learned and best practices techniques for future reference
Receive feedback from users and gain understanding of needed and recommended changes
Evaluate new tools and applications and provide business justification on use and requirements
Support troubleshooting calls (may happen during off business hours)
Protects company and operations by keeping information confidential.
Requirements:
Bachelors Degree; Masters Degree a plus; 8+ years work experience in the wireless industry
Previous experience and/or basic understanding of 3G and 4G LTE architecture
Knowledge of RF Performance metrics, traffic and User data
Proficient in programming languages such as VB, Perl, SQL, Xshell, MS Access, and Excel, and Data Analytics
Demonstrated research and problem solving skills via prior work experience
Experience with wireless infrastructure provider and/or operator a plus
Must have a strong work ethic, integrity and work extremely well in a team environment
Must have an aptitude for creativity and innovation
Advanced working experience on Microsoft Excel, Power Point and Access Database
Strong interpersonal communication skills
Proven ability to manage multiple priorities in a fast paced environment"
Senior-Big Data Engineer,AT&T,"The Big Data Engineer will be responsible for interpreting the requirements of various Big Data Analytic Use Cases and Scenarios, and driving the design and implementation of specific data models to ultimately help drive better business decisions through insights from a combination of external and AT&T's data assets. This role is also accountable for developing the necessary enablers and data platform in the Big Data Lake Environment and has the responsibility of maintaining its integrity during the life cycle phases.

Key Roles and Responsibilities: This position will define data requirements, gather and mine large scale of structured and unstructured data, and validate data by running various data tools in the Big Data Environment. This function needs to support the standardization, customization and ad-hoc data analysis, and will develop the mechanisms to ingest, analyze, validate, normalize and clean data. Implements statistical data quality procedures on new data sources, and by applying rigorous iterative data analytics, supports Data Scientists in data sourcing and preparation to visualize data and synthesize insights of commercial value. Will work with Big Data Policy and Security teams and Legal to create data policy and develop interfaces and retention models which requires synthesizing or anonymizing data. Develop and maintain data engineering best practices and contribute to Insights on data analytics and visualization concepts, methods and techniques.

Job Contribution: Seasoned technical professional. Contributes through proven technical expertise. Has significant dept/functional impact.

KNOWLEDGE Subject Matter Expert (SME) within own discipline/specialty area; basic knowledge of other disciplines/specialty areas. Deep technical knowledge. Applies in depth knowledge of discipline/specialty area standards/processes. Integrates industry experience and deep professional/technical knowledge. Technical leader and recognized SME on select AT&T technologies/systems/procedures.

ANALYSIS/PROBLEM SOLVING Solves unique problems through evaluative judgment/precedent. Independently applies sophisticated analysis in evaluating issues. Develops new concepts/methods/techniques for cross functional initiatives. Recognizes/pursues alternative methods.

INDEPENDENCE Guided by department goals/objectives. Exercises latitude in determining objectives/approaches to projects. Leads multiple projects of small to medium size and technical breadth.

CONTRIBUTION TO AT&T TECHNOLOGY Key contributor on complex projects/initiatives. Impacts current and future business opportunities through application of specialized technical/ industry knowledge. Develops methods/techniques based on strategic project objectives.

COMMUNICATION Mentors and provides technical guidance and explains advanced concepts to others in work area. Coordinates across multiple departments. Promotes active listening and open communication. Provides leadership/guidance to others.

Education: Bachelors of Science in Computer Science, Math or Scientific Computing preferred.

Experience: Typically requires 5-8 years experience. Supervisory: No."
Software Engineer,Schneider Electric,"Candidates will be part of a full lifecycle software development group for customer specific applications, add-on modules and reporting for power and energy management systems. Candidates will focus on acquisition, processing and reporting metered data, alarms and events.

Schneider Electric creates connected technologies that reshape industries, transform cities and enrich lives. Our 160,000 employees thrive in more than 100 countries. From the simplest of switches to complex operational systems, our technology, software and services improve the way our customers manage and automate their operations.
Help us deliver solutions that ensure Life Is On everywhere, for everyone and at every moment: https://youtu.be/NlLJMv1Y7Hk .

Great people make Schneider Electric a great company.

What do you get to do in this position?
The role includes all parts of the software development lifecycle from requirements management through maintenance on both new and existing applications.
Typical software applications include
Acquiring data from networked and serially connected automation devices
Processing algorithms
Database storage, retrieval and reporting
User interfaces either thin or thick client depending on requirements
Positions for Software Engineers are available locally in Nashville but might be open to other locations throughout the United States.

Qualifications
This job might be for you if:
Bachelors degree in Computer Science, Computer Engineering, Software Engineering or equivalent
Strong interpersonal and leadership skills
Proactive and highly motivated
Have excellent written and oral communications skills
Have coursework using
Object Oriented Programming
Data Structures or Database Management Systems
Programming Languages
Desired Skills :
Experience in C#
Experience in writing web applications using JavaScript and AngularJS
Experience in Source Code Configuration Management
Knowledge in creating tables, indexes, jobs and stored procedures in SQL Server
Experience in Testing Software
We seek out and reward people for being straightforward, open, passionate, effective and challenging the status quo. We want our employees to reflect the diversity of the communities in which we operate. We welcome people as they are, creating an inclusive culture where all forms of diversity are seen as a real value for the company. Were looking for people with a passion for success  on the job and beyond. See what our people have to say about working for Schneider Electric: https://youtu.be/6D2Av1uUrzY .

Let us learn about you! Apply today.

You must submit an online application to be considered for any position with us. This position will be posted until filled.

It is the policy of Schneider Electric to provide equal employment and advancement opportunities in the areas of recruiting, hiring, training, transferring, and promoting all qualified individuals regardless of race, religion, color, gender, disability, national origin, ancestry, age, military status, sexual orientation, marital status, or any other legally protected characteristic or conduct.

Concerning agencies: Schneider Electric does not accept unsolicited resumes and will not be responsible for fees related to such.

Primary Location : US-Tennessee-Nashville
Other Locations : United States
Schedule : Full-time
Unposting Date : Ongoing"
Software Engineer,Rockwell Automation,"We are looking for a data analyst to work on our analytics offerings. This position requires an individual who is organized, self-motivated, and has demonstrated the ability to be a self-starter. The candidate must be able to work independently and react quickly to changing priorities with a consistent sense of urgency. EOE, M/F/Disable, Vet Required Skills:  Bachelor degree in statistics, mathematics or similar field  Some work experience in statistics  Experience with modeling using Python, R, etc. is a must  Strong data analytic skills  Advanced MS Excel skills  Expert problem-solving and data-analysis skills Desired Skills:  Ability to review, analyze, and evaluate multiple data elements and present findings in a clear and concise manner  Assist in modeling and/or forecasting multiple data points  Ability to consolidate and synthesize large amounts of data from various sources; must be able to translate a large group of raw numbers into succinct presentation points  Ability to learn independently and quickly  Demonstrate critical thinking and initiative  Statistical or data analysis; strong data analytic skills  Format and validate data  Attention to detail with the ability to check your own work  Manage day-to-day work assignments  A passion for the data analysis profession  Must be able to identify gaps in data and resolve issues  Develop and generate standard reporting Legal authorization to work in the US is required. We will not sponsor individuals for employment visas, now or in the future, for this job opening"
Software Engineer,KLA-Tencor,"You will be a part of our highly motivated, technically challenged inter disciplinary team which deals with high volume of data at high speeds. The data comes from many sensors connected to the hardware, through an image data pipeline and many other sources. You will be responsible for the managing and analyzing the data, creation of machine control, image computer and data storage software for next generation semiconductor wafer inspection machines.

You will be responsible for requirement analysis, design, implementing and testing system software solutions through full SDLC. You will be closely working with hardware and systems team, and solve some of the most advanced technical challenges facing the semiconductor industry.

You will fit right in if you have a sound understanding of the fundamentals of Computer Science, practical experience building machine control software and reading/analyzing data.

You should be somebody who enjoys working on complex system software, is very customer-centric, understands the big picture and feels strongly not only about building good software but about making that software achieve its goals in reality.

Preferred Qualifications
Experience with Object-Oriented Software Analysis and Design using UML modeling, Java programming
Strong software development skills in one or more programing language (Java, Python, C#)
Requires knowledge of multi-threaded/parallel processing applications and embedded software development
Experience developing software processing algorithms.
Experience with hardware/software integration and testing
Good problem solving skills are required
Good to have skills:
Semiconductor equipment industry experience and/or Image/data processing experience a plus
Knowledge of large scale storage technologies
Full Python stack to enable machine learning / deep learning

Minimum Qualifications

Doctorate (Academic) with at least 2 years of experience.ORMaster's Level Degree with at least 4 years of experience.ORBachelor's Level Degree with at least 5 years of experience.

Equal Employment Opportunity

KLA-Tencor is an Equal Opportunity Employer. Applicants will be considered for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, protected veteran status, disability, or any other characteristics protected by applicable law."
Software Engineer,Ginkgo Bioworks,"Ginkgo Bioworks is YCombinator's first biotech company ... we were personally recruited by Sam Altman to join YC: http://techcrunch.com/2014/08/19/sam-altman-on-his-inaugural-batch-of-companies-as-y-combinators-new-head/

Our mission is to make biology easier to engineer. We engineer organisms to address fundamental challenges the world is facing today. Ginkgo bioengineers make use of an in-house organism foundry to engineer new organisms for making natural products, pharmaceuticals, biofuels, and more. We're making the code base, compiler and debugger for life. We're looking for programmers interested in developing the organism design toolchain - the CAD, CAM and CAT tools that will define how bioengineers design organisms at scale.

Ginkgo's programming languages of choice are Ruby, Python, Javascript/HTML and DNA, but you must be someone who loves writing elegant code in any language. Most importantly, you should be passionate about making biology the next engineering discipline. The 20th century was all about bits and the awesome technology of computers. The 21st century is all about atoms and the awesome technology of biology. Ginkgo is at the forefront of this revolution.

Responsibilities
Designing and implementing tools and workflows for genetic design of genes, metabolic pathways and genome modifications
Programming robots to assemble DNA, modify genomes, culture cells and prepare assay samples
Developing verification tools for strain construction
Developing data analysis tools and workflows for processing and analyzing NGS and mass spec data
Developing the Laboratory Information Management System (LIMS) that enables bioengineers to use and reuse Ginkgo's growing repository of genetic parts and strains in their designs
Requirements
BS or MS in computer science or equivalent experience
Loves to write elegant code
Excited to work hand in hand with the foundry team to develop the next generation of foundry for engineering organisms
To learn more about Ginkgo, check out some recent press:

http://www.wired.com/2015/11/move-over-jony-ivebiologists-are-the-next-rock-star-designers/

http://fortune.com/2015/07/23/ginkgo-bioworks/

https://medium.com/the-os-fund/a-new-era-of-industrial-engineering-using-biology-dd4571aa6d27

We also feel its important to point out the obvious here  theres a serious lack of diversity in our industry and it needs to change. Our goal is to help drive that change. We hope to continue to build a company whose culture promotes inclusion and embraces how rewarding it is to work with engineers from all walks of life. Making biology easier to engineer is a tough nut to crack - we cant afford to leave any talent untapped.

It is the policy of Ginkgo Bioworks to provide equal employment opportunities to all employees and employment applicants."
Software Engineer,Circle Internet Financial,"At Circle, we're shaking up the global economy by making money simple. With us, you can send money like a text  instant, secure and free  whether your friend is across the table or across the ocean. How exactly are we doing this? Unlike existing systems that are closed and proprietary, we use open internet standards and protocols, including the blockchain (allowing us to offer this service for free). Our mission is to connect every person in the world by exchanging value across borders, between currencies and among friends.

As part of platform engineering, youll join small, productive teams in an agile environment and gain real-world experience building and deploying secure, production-scale services for consumer facing products. Youll work alongside smart engineers who care about the product and producing quality code.

As part of our team, youll own software throughout the entire development life-cycle. Youll contribute ideas to improve our product and processes, make decisions that have significant impact, and provide feedback so we can continue to get better. Youll get the opportunity to develop your skills, collaborate across teams and continue to learn.

Projects we are working on (and you may work on, too)
Build a secure full-stack web application that allows us to better visualize our liquidity in multiple currencies and bank accounts worldwide, as we focus on international expansion.
Productize machine learning models and build a world class automated evaluation system to manage risk exposure while delivering the best possible experience to our customers.
Be a hero for our finance team by developing an automated reporting framework that allows new reports to be easily scheduled and downloaded each day, month, quarter or year.
Continue investing in our core technology and shared libraries by making it easier for all teams to leverage multi-region storage and infrastructure to improve our resilience during outages and increase our uptime.
Build an open transaction protocol where users can send money and exchange value that leverages both new and traditional technologies to deliver value the fastest and cheapest way possible.
Create, maintain, and extend a diverse array of sustainable tools and systems that will support fast product development and delivery flow, enable team autonomy and foster continual learning and experimentation.
Provide new search capabilities across all of our financial partners so that our Customer Support team can more quickly resolve payment issues.
Predict fraud before it happens by facilitating data science model creation and evaluation, and developing a high-performance rules engine.
Leverage blockchain technology in a new transaction framework so that Financial Institutions an exchange value and associated metadata in a compliant manner.
What youll bring to Circle
Experience writing high quality code in Java, JavaScript, Node.js, Python or Angular.js (this is our tech stack, some of these youll have and some youll learn)
Experience working closely with internal teams to identify requirements and iterate on solutions
Evangelize best practices for engineering teams, including ideas around design and code reviews and architecture
Debug complex production issues across various levels of the tech stack
Interest in building large-scale, distributed systems
Interest in impacting the way money moves between people

Circle was founded in 2013 by internet entrepreneurs Jeremy Allaire and Sean Neville and we're backed by $140 million from investors including Jim Breyer (Facebook), Goldman Sachs, IDG Capital (Baidu, Tencent), General Catalyst (AirBnB, Snapchat) and Accel Partners.

Check us out at circle.com and download Circle Pay for iOS and Android today."
Structural Design Engineer (Graduate),Jacobs,"Exciting opportunity to join the Transportation Team as a Spring 2018 graduate with the Structural Design group in Tampa, FL.
Perform routine aspects of engineering requiring knowledge and application of basic engineering principles under close supervision.
May use computer-assisted engineering software and equipment to perform engineering tasks.
Collects and prepares data for evaluation.
Assisting in the coordination of projects from conception (feasibility) through final plan preparations.
Providing support to engineering staff members as needed.
Qualifications
Candidate must have BS in Structural Engineering and be currently enrolled in a Masters program is required with a graduation date of May 2018.
Working knowledge of Word and Excel is required
Previous design intern experience is preferred.
Knowledge of AutoCAD and Microstation is preferred
Strong analytical, conceptual, communication and organizational skills.
Ability to effectively interface with all levels of personnel and management.
Ability to work and thrive in a team environment.
Strong problem solving skills.
Ability to self-motivate and work productively without supervision.
Excellent oral and written communication skills.

Only local candidates will be considered.

At Jacobs, we help prepare people for new opportunities and challenges. With positions at every level, openings in multiple disciplines, expertise in a range of markets and offices around the globe, we create an environment where you can learn, grow, and thrive. From our competitive benefits program to our Health and Safety initiative of Beyond Zero workplace injuries, we believe that you'll find a flourishing career here at Jacobs.

Jacobs is an Equal Opportunity/Affirmative Action Employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, sexual orientation, gender identity, national origin, disability, veteran status or other characteristics protected by law.

Jacobs is a background screening, drug-free workplace."
Software Engineer,Massachusetts General Hospital(MGH),"The Massachusetts General Hospital Department of Pathology is looking for an exceptional software engineer to join the Center for Integrated Diagnostics (CID). The CID is a leading clinical laboratory performing tumor genotyping for cancer patients, with the aim of identifying genetic mutations that can guide personalized treatment. Our results are routinely used in cancer care at MGH, often being the last hope for people with Stage III and Stage IV tumors. We innovate across multiple disciplines, including bioinformatics, laboratory science, data science and software engineering. We develop our own tools when off-the-shelf solutions are not available  including a Lab Information Management System (LIMS), large-scale bioinformatics analysis (CIDer), variant storage & reporting (VarVetter), among others. We are also one of the first clinical laboratories developing and using Machine Learning models in everyday clinical practice.

We are seeking a highly-motivated engineer to join our Clinical Informatics team, the engineering core of the Center for Integrated Diagnostics. You will work closely with a cross-disciplinary team of clinicians, pathologists, biostatisticians, bioinformaticians, data scientists and software engineers to employ and develop software tools for our laboratory operations. You would be joining an agile team that combines the best of
industry and academia: we use git, do code reviews for all production software, practice unit testing and continuous integration, and use the latest open-source libraries and tools: python, scikit-learn, biopython, pandas, numpy, TensorFlow, keras, Apache Spark, Docker, Kubernetes, postgresql/GreenPlum, jupyter, Semantic Media Wiki, GATK, and MongoDB.

As part of the Clinical Informatics team, you will develop innovative software that directly impacts patient care at MGH, including the integration of novel Machine Learning and decision support systems into clinical workflows. If you are looking to build robust systems that make a real impact, we would love to have you with us.

Primary Duties include:
Develop CIDs clinical systems, including LIMS, variant storage & reporting, and bioinformatics analysis
Implement solutions to open-ended programming tasks in Python, mostly individually but also collaborating with CIDs other developers when needed
Participate in software architecture planning and design
Integrate Machine Learning models and decision support tools into clinical systems
Integrate new data sources into CIDs data lake
Communicate engineering concepts to doctors, lab technologist and other non-engineering personnel
Mentor junior staff

Qualifications
Bachelors degree required in mathematics, physical sciences, computer science, engineering or related field.

Candidates must have a strong programming ability in at least one programming language.

One year of professional experience in a software engineering or related position strongly preferred

This position will mentor junior staff members.

Competency in programming language (Python, Java, Scala, C++ or equivalent, with Python strongly preferred)

E xperience with Unix/Linux and shell scripting

Ability to work as part of a software team, including agile methodology, unit

testing/continuous integration, refactoring, code reviews, version control and release

engineering

Expertise in front end web development (HTML, CSS, JavaScript and other client

side technologies) a plus

Ability to work in a highly collaborative and intellectually challenging environment.

Excellent oral and written communication skills.

Demonstrated competency on production systems

Ability to architect robust and scalable systems, including the design of internal and external APIs

Familiarity with Next Generation Sequencing

Familiarity with databases (SQL, Postgres, MongoDB)

Familiarity with HTML/Javascript and popular libraries (jQuery, D3)

EEO Statement Massachusetts General Hospital is an Equal Opportunity Employer. By embracing diverse skills, perspectives and ideas, we choose to lead. Applications from protected veterans and individuals with disabilities are strongly encouraged.

Primary Location : MA-Boston-MGH Main Campus

Work Locations : MGH Main Campus
55 Fruit Street
Boston 02114

Job : IT/Health IT/Informatics-Engineer
Organization : Massachusetts General Hospital(MGH)
Schedule : Full-time
Standard Hours : 40
Shift : Day Job

Employee Status : Regular
Recruiting Department : MGH Pathology/Clinical Labs
Job Posting : Sep 27, 2017"
Software Engineer,Meridian Technology Group,"Meridian Technology Group is seeking a Software Engineer .

Apply Now

Description : Software Engineer Overview :
The Test Software team plays a critical role in the testing of all of the advanced, proven, and developmental propulsion systems. Utilizing top-notch programming capabilities and skills, the software engineers design customized LabVIEW-based applications that support testing operations. As the amount of hardware arriving in for testing increases, the test software engineering team will play a critical role. By working on efficiencies and optimization, they utilize these tools to drive for better usability and automation with testing processes and procedures. Responsibilities :
Upgrade existing data and control system application code to use the latest framework revision
Troubleshoot DAQ hardware related problems
Benchmark system performance and performance margins
Provide technical support for testing operations

Perform other work as assigned Basic Qualifications:
Bachelor's Degree in electrical engineering, computer engineering or computer science
Experience with National Instruments SCXI/PXI/C series data acquisition systems
Minimum 2 years of LabVIEW development experience
Certified LabVIEW Developer (CLD) or Certified LabVIEW Architect (CLA) certification or 5 years LabVIEW development experience Preferred Skills and Experience:
Experience with Object Oriented Programming (OOP)
Experience with National Instruments SCXI/PXI/C series data acquisition systems
Experience using test instrumentation and control components (pressure transducers, thermocouples, RTDs, strain gauges, accelerometers)
Understanding of basic computer networking (TCP/IP, UDP, firewalls)
Intermediate skill level using Microsoft Office (Excel, Word, Power Point, Outlook)
Expert skill level using Windows Operating Systems
Ability to create intuitive graphical user interfaces
Ability to implement and troubleshoot National Instruments hardware
Ability to marshal resources to get things done effectively and efficiently
Ability to solve problems calling for technical analysis
Ability to make independent judgments with limited information
Ability to work effectively in a team environment
Ability to rapidly change roles and responsibilities while working in a fast-paced, challenging work environment Additional Requirements:
Ability to sit for long periods of time (8+ hours per day)
Must be available to work extended hours and weekends ITAR Requirements:
To conform to U.S. Government space technology export regulations, applicant must be a U.S. citizen, lawful permanent resident of the U.S., protected individual as defined by 8 U.S.C. 1324b(a)(3), or eligible to obtain the required authorizations from the U.S. Department of State. Learn more about ITAR here.

Meridian Technology Group is committed to equal employment opportunity (EEO) and non-discrimination for all employees in all job classifications and for prospective employees without regard to race, color, religion, sex, age, sexual orientation, veteran status, physical or mental disability, national origin, or any other characteristic protected by applicable federal or state law. All hiring is contingent on eligibility to work in the United States. We are unable to sponsor applicants for work visas. No 3rd party companies/candidates.

Please email your resume now or

contact us for more details:
Meridian Technology Group Recruiting Team
(214)273-4497 (817)601-4651 in Texas(800)698-0853 outside Texas
www.meridiangroup.com
jobs.meridiangroup.com

Ready?
click to apply
online"
Machine Learning Intern (Summer 2018),Rocket Software,"The Emerging Technology Research team at Rocket is looking for a passionate Machine Learning Engineer to help us implement and deploy advanced analytic models that enable the intelligence for an enterprise scale real-time cognitive feedback engine. You will work on building, publishing and measuring machine learning models for enterprise IT to analyze, predict faults, recommend mitigation strategies based on trade-off analytics, and predict user behavior for a better more intuitive experience. You will be involved in a team that is solving some of the hardest problems for our Fortune 500 customers.

Join our team and take on this unique opportunity to reinvent IT operations as we know it. If you are a developer who is passionate about data science and analytics, we want to talk to you!

What You'll Be Doing?

Bring together a combination of mathematical rigor and innovative algorithm design to create recipes that extract relevant insights from millions of rows of data to meaningfully improve enterprise IT operations.

Learn, develop, and apply new techniques in the intersection of math, probability, and optimization.

Translate unstructured, complex business problems into an abstract mathematical framework, making intelligent approximations when needed to put your algorithm to work at scale.

Deploy, maintain and tweak scoring models based on feedback loops and real-time performance.

Work closely with various product development engineering teams to solve key prediction, mitigation, recommendation and discovery problems in the field of IT operations.

Explore, investigate and prototype innovative machine learning concepts.

Required Skills

What You'll Bring?

You are passionate about building software that matters.

You believe in reimagining and redefining the world through software that is elegant and simple.

You have an obsession about discovering and solving customer issues.

You can work independently and drive results.

You are open to new challenges, and willing to learn.

You have a strong background in analyzing and discovering patterns in seemingly disparate data.

You are a strong Java or Scala programmer who loves Apache Spark.

The intersection of mathematical models and algorithm design excites you.

Bonus

You are familiar with the IBM z Systems platform.

You are familiar with the operational aspects of an enterprise relational database like IBM DB2 for z/OS.

You have experience with R or other such data science tools.

Why Join Us?

The internship experience will provide an insider view of how a fast moving, innovative, and entrepreneurial company operates through an exciting and challenging work experience. Our interns will operate as a team to design and build, from the ground up, brand new and innovative applications related to statistical analysis, and supervised machine learning.

The Rocketship is also designed to give you multiple opportunities to interact with members of Rocket Softwares Executive Team, an invaluable chance to showcase your skills and start building a network of professionals for future internships or post-graduate work.

Our program includes participation in a professional development program together with other interns with a goal to develop and deliver an executive level presentation to senior members of Rocket Softwares Executive Team at the end of the summer.

Rocket Software Inc. is committed to creating a diverse environment and is proud to be an equal opportunity employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, gender, gender identity or expression, sexual orientation, national origin, genetics, disability, age, or veteran status. Rocket Software Inc. is also committed to compliance with all fair employment practices regarding citizenship and immigration status.

Rocket is committed to working with and providing reasonable accommodation to individuals with physical and mental disabilities. If you need special assistance or an accommodation while seeking employment, please call: 781-577-4321 or send an email to hr@rocketsoftware.com. We will make a determination on your request for reasonable accommodation on a case-by-case basis.

Required Experience"
Software Engineer,ABILITY Network,"We have a unique opportunity for a Software Engineer to learn big data systems, scalable architecture and data security while working with a small team of extraordinary engineers developing a next generation enterprise data lake and analytic applications for the Healthcare Industry.

Responsibilities
You will work with smart people like you using proven technologies and tools to deliver data processing infrastructure that meets business requirements.
You will learn to use best design and development practices and introduce your own while working on ingesting data from multiple sources, cleaning and transforming the feed to meet specification for target systems.
You will participate in architecture review sessions and expend your area of expertise. You will improve your understanding of data and transport security and work with HIPAA certified services and protocols.
You will work on data integration projects, performing system configuration, creating designs according to specification, coding and testing the functionality that adds value to our customers.
You will have a lot of autonomy with full accountability for your work and have an environment where you could be successful at achieving your career aspirations.
Other duties as requested and/or determined

Qualifications
Bachelors Degree in Computer Science, Engineering etc.
2-3 years of commercial software engineering experience
Experience working on a system that involves processing large number of transactions originating from multiple sources. Ability to articulate challenges of the multifaceted data aggregation systems.
Ability to demonstrate understanding of complex enterprise-level systems, causal dependencies and relationship between components.
Hands-on experience with design patterns, data structures, algorithm with ability to demonstrate applicability and clearly articulate/defend choices.
Experience creating technical specifications, designs and architecture presentations.
Inspired by code quality and understand that testability is a function of engineering. Ability to demonstrate unit testing in action using dependency injection and mock frameworks.
Willingness to perform system administration tasks, dev ops and maintenance. Understanding how to proactively monitor and measure system stability and performance.
Several years working with relational databases, proficient at SQL execution analysis, data aggregation and transformation procedures. Familiar with database performance optimization.
Experience working with systems that support continues integration and can fully appreciate the development cycle.
The discipline to manage your workload, take accountability for your work, dig deeply and try harder to learn into the large dynamic technical landscape which comes with the territory.
Excellent verbal and written communications skills
Ability to provide extra effort when necessary to meet deadlines

Travel: 10%"
"Data Engineer, Business Intelligence",Factual,"At Factual we love all things data! Our mission is to organize and optimize the worlds location information. As a BI Data Engineer, you will have the opportunity to shape and influence the direction of our products and propel the growth of our business. You will collaborate with software engineers and business operations teams to design schemas, import (ETL) processes, establish proper indexing or data warehousing approaches to power insights and analytics. You will be designing metrics, aggregations, and sampling techniques from our massive data repositories to help answer critical business questions that will guide product and investment decisions.

About you:

You are passionate about using your programming, data wrangling, and analytical expertise to drive product and business decisions. You are an excellent communicator that will provide thought leadership on metrics, features, and products to teams across the company. You have strong engineering experience and enjoy working in a rapidly changing environment. And most of all: you ship.

What youll do:
Work closely with a range of stakeholders and team members to understand and finalize their business requirements for reporting and data aggregations
Design, develop and maintain analytical databases based on business requirements
Design, develop and test ETL, and ELT data workflows from a variety of big and small data sources.
Perform hands on implementation and maintenance of production ETL processes.
Write and tune SQL and other types of queries to support internal customer requests
Support ad-hoc data analysis requests
Audit quality and repair data from data sources for accuracy
What were looking for:
4+ years of industry experience maintaining production data warehouse and ETL pipelines and using data analytics to solve business problems
Understanding of database structure, design, theories and principles
Experience with Grafana, ELK, or Solr
Expertise with at least one of Python, Scala, or Java
Experience with Spark or MapReduce is a plus
Willingness and ability to wrangle messy data
Degree in Computer Science preferred
Experience working independently solving a variety of business problems.
Good communication skills.
Deep knowledge and hands-on experience with SQL is an absolute must."
Software Engineer,Docent Health,"You will:
 Be a founding member of an engineering team in a fast-growing startup
 Be actively involved in developing, testing, and deploying highly scalable, complex applications
 Work in a cross-functional, collaborative team with UX, product, and service team members to create an elegant, intuitive, and insight-rich application
 Help to architect a product with sound design for existing and new functionality with a mind towards future scalability
 Directly influence how we build software and create a foundation for a world-class engineering organization
 Join a first-mover organization focused on cracking the code on patient experience to ensure that all patients feel known and valued in the moments where they need it most
 Embody the Docent values by operating with thoughtful efficiency, leading with data, and interacting with empathy

You ideally have:
 A passion for solving some really difficult business challenges: for our end-users, architecture, team, and process to name just a few areas
 Bachelor's degree in Computer Science, Systems Engineering or similar; relevant experience is always considered in lieu of a specific degree path
 Strong computer science fundamentals; ability to analyze and solve problems, make difficult decisions, and take ownership of a problem from beginning to end
 3-7+ years of software development experience
 Some depth of experience in at least some areas of our current technologies: Ruby on Rails, Java, Python; MySQL and/or PostgreSQL; NoSQL (Redis, MongoDB and Cassandra); Elastic Search; HTML5, CSS3, JavaScript (React/Redux, Angular, Ember); REST-oriented architecture
 Depth of experience in either front or back end technologies but a desire to build your skills in a full stack engineering approach
 A passion to learn about great Agile software development processes and experience in or a desire for behavior-driven development as well as Continuous Integration/Continuous Deployment
 The ability to work in an environment at a fast pace and with changing priorities;
 Adaptability to new technologies and desire to contribute to consistent delivery of high quality software
 Strong verbal and written communication skills

Benefits:
Compensation for the role will include market-competitive cash compensation and a material equity opportunity. Consistent with our belief that our employees are our most valuable resource, Docent Health offers a competitive benefits package, including Blue Cross Blue Shield medical insurance, Guardian Dental and Vision, and a 401K for employee contribution, as well as unlimited snacks and company-wide catered lunch every Friday!

Docent Health is unable to offer H-1B Visa Sponsorship at this time."
SQL Database Engineer,McKinley,"SQL Database Engineer wanted in Ann Arbor.

Involves the setup, configuration and maintenance of our SQL Server Environments.

Job Requirements Requires a Bachelor's Degree in Engineering, Math, Computer Science or related field (foreign equivalent degree accepted) and 60 months in IT operations.

This position requires the following skills in the following areas: Database structures, principals and practices; Database development and knowledge of SQL programming using Microsoft SQL-Sever t-SQL, SSIS and SSRS in the 2005/2008/2012 environment; Ability to write t-SQL queries to pull, analyze and research data; Knowledge of reporting and query tools and practices; Ability to evaluate data integrity issues and troubleshoot; Scripting and automation; Web services and integrating 3rd party solutions.

Occasional domestic travel to other company offices in the United States. Trips are anticipated to be 2-3 days.

Applicants must send resumes and salary requirements and/or inquires for further details to:

McKinley Companies LLC.
320 North Main
Ann Arbor, MI 48104

No calls please."
"Data Engineer, Amazon Devices",Amazon.com,"The Amazon Devices team designs and engineers high-profile consumer electronics, including the best-selling Kindle family of products. We have also produced groundbreaking devices like Fire tablets, Fire TV, Amazon Dash, and Amazon Echo. Our team is serious about great scalable design and redefining best practices with a cloud-based approach to scalability and automation. What will you help us create?

Amazon's Devices Technology team is looking for a talented Data Engineer with a strong background in ETL, Data warehousing and interest in linking data to key business trends. You will work closely with the business and technical teams to analyze many non-standard and unique business problems and use creative-problem solving to deliver actionable output. Your work will have a direct impact on the day-to-day decision making in the Amazon Devices Sales & Operations Technology.

Basic Qualifications
Bachelors degree or higher in an engineering or technical area such as Computer Science, Physics, Mathematics, Statistics, Engineering or similar.
3+ years in with and detailed knowledge of data warehouse technical architectures, infrastructure components, ETL/ ELT and reporting/analytic tools and environments, data structures.
5+ years of demonstrated quantitative and qualitative data experience with impact to a business, a track record of problem solving using software systems, and the desire to create and maintain data warehouse systems.
Data management & data quality control experience

Preferred Qualifications
Advanced knowledge and expertise with Data modelling skills, Advanced SQL with Redshift, Oracle, MySQL, and Columnar Databases
Knowledge of OBIEE and Tableau
Experience with AWS
Wide data warehousing experience.
Proficient in the composition of Advanced SQL (analytical functions)
Proven track record of identifying metric variances and delivering solutions to address the changes
Ability to effectively communication with both business and technical teams
A self-starter who loves data and who enjoys spotting the trends in it!

Amazon.com is an Equal Opportunity-Affirmative Action Employer  Minority / Female / Disability / Veteran / Gender Identity / Sexual Orientation

#d2ctech tag"
Software Engineer,"I.D. Systems, Inc.","IDSY, the leading provider of end-to-end wireless asset management solutions, is offering a
great opportunity for a Software Engineer to be a key contributor to this growing technology
company.

IDSY is headquartered in New Jersey and is a publicly-traded (NASDAQ: IDSY) technology
company that provides complete solutions for wireless asset management for Fortune 500
companies, and now SMB opportunities after a recent acquisition. Our products provide
significant operational cost savings and safety benefits that are well documented, and are
integrated into the daily operations of the worlds most demanding customers.
systems.com.

For passenger Vehicle Management Systems, I.D. Systems HW/FW/SW solutions are deployed
by global rental fleet and car sharing solution providers as core systems to provide superior
customer experience, additional revenue not available with manual processes and as platforms
for consolidating and optimizing fleet resources across multiple business offerings.

For industrial Vehicle Management Systems, I.D. Systems designs and delivers products which
focus on facility and campus logistics operations via its vehicle management solutions. These
systems monitor and control vehicles in warehouses, manufacturing facilities, and
airport/seaports. Our SMB product line of similar systems is marketed by our Keytroller
Subsidiary out of their Tampa, Florida offices.

Asset Intelligence, a subsidiary of I.D. Systems, is based in Plano, Texas, and is a leading provider
of trailer and container tracking solutions for truck-load carriers, private fleets, leasing
companies, and freight transportation providers. We offer a full range of telematics solutions to
improve fleet productivity, utilization and profitability.

All employees are offered an excellent benefits package including medical, dental, vision,
voluntary life, short/long term disability, flexible spending, 401(k) plan, and paid time off. IDSY
stock grants are a part of our total compensation model.

If you thrive in a growing, high-energy, results-driven culture, we have a career opportunity for
you!

Position Responsibilities:
Create and modify SRDs (Software Requirements Documentation)
Design/Implement solutions from Requirements Documentation using best practices,
latest methodologies and technologies available.
Participate in Unit Testing, Code Review, and Integration Testing with our custom
hardware solutions
Provide System Test Support (Provide/explain features, troubleshooting, etc)
Ensure software sustainment (bug fixes, applying patches, documenting workarounds)
Provide excellent service to customers (Requirements gathering, GTM demonstrations)
Provide Tier 3 (Engineering) software support (debugging, patching, testing)
Manage software release packaging (Documentation, building, publishing, etc.)
Position Requirements:
Computer Science Degree (BS minimum, Masters preferred) plus 5 years of experience
Excellent problem solver with in-depth trouble shooting and analysis skills, utilizing all
resources available (Google, texts, etc.)
Exceptional math skills, utilizing binary/hexadecimal conversions and operations
(masking, modulus, shifting, mapping, etc.) for data/speed efficiency
Extensive experience with Microsoft Visual C# WCF and Windows Services
Experience with JSON web service design and consumption and REST web service design
and consumption
2+ years of experience with ASP, VBSCRIPT, and JAVASCRIPT, ADO.NET, with a strong
understanding of Angular JS with MVC 5.0 utilizing .NET Framework
Experience in UX Design (web development UI)
Experience with SQL database exposure to Microsoft SQL Server, including a substantial
understanding of Data Modeling, Stored Procedure Design, Performance Tuning, Index
Usage and Best Design Practices with Object Oriented SQL databases

Self-motivated, independent thinker with a strong desire to continually learn new things
Adaptable, energetic, and dedicated
Ability to work in a team environment, supporting group projects, and balancing
personal task assignments
Exposure to software development methodologies and documentation practices
Experience in telecommunications and wireless technologies (preferred)
Experience working with Agile development framework
Experience with MS C#
A technology expert with the ability to socialize and interact face to face with clients
Experience working with BI platform/embedded reporting preferred
Salary: DOE and full benefits package
EOE M/V/F/D"
Mapping Engineer (Data Scientist),"lvl5, Inc.","What We're Building

lvl5 builds HD maps for self-driving cars using computer vision. Our iPhone app, Payver, (
http://www.getpayver.com [http://www.getpayver.com/]) has thousands of drivers, sending us terabytes of driving data each day.

We receive video and other telemetry measurements from our fleet, and run advanced computer vision algorithms on the data to map the roads with high precision -- down to just a few centimeters. Unlike traditional maps, HD maps contain all the nitty-gritty details of the road, such as where the car should look for traffic lights, and how to turn through a complex intersection.

We work directly with automakers, and also with Tier 1 manufacturers to provide intelligent driver assist products as well as full localization capabilities for a ""Level 5"" self-driving vehicle.

Responsibilities:
Reduce terabytes of artifacts from our computer vision pipeline to find the accurate 3D pose of objects of interest.
Automate the detection of changes to our HD map
Manage tiling, building, and intelligent deployment of our maps to our customers
Work through noisy sensors such as GPS and IMU to retroactively localize vehicles in our fleet.
Requirements:
3+ years experience with Python, NumPy, Pandas, and SkLearn
Bachelor degree in Computer Science/Engineering/Data Science, or equivalent real-world experience
Passionate about self-driving technology and its potential impact on the world
Attention to detail and a passion for quality
Knows how to take a scientific approach to solving (near impossible) problems
Ability to measure performance comparing ground truth
Strong math & stats background
Bonus Points:
Masters/PhD in an discipline such as Computer Science, Computer Engineering, Physics, or Systems Engineering
You have designed and trained models for various aspects of autonomous driving such as perception
Self-starter, ability to operate without a manager
Robotics / Autonomy background
Running models at scale
Perks:
Health, Dental, and Vision Coverage
$5k budget to spend on your developer workstation
Free snacks, coffee, and drinks
Flexible Work Hours
Fun offsites"
Python Engineer,Odyssey Systems Consulting Group,"Job Details Description
Odyssey seeks a Python Engineer to support the research and development of large scale open architecture distributed / cloud-based data processing, analysis, and artificial intelligence (AI) projects utilizing Agile development methodology including web-based front and back end development.  Customer technical staff will design the software architecture with input from the subcontractors.
RESPONSIBILITIES AND DUTIES
The candidate will support the development distributed AI systems for large heterogeneous data sets. Projects involve system design and architecture and the development of algorithms for machine learning, natural language processing, and graph analytics implemented on enterprise big data architectures. The candidate must be experienced in software design, development, integration, testing, documentation, and maintenance. Work will be executed primarily in Python in a mixed Windows/Linux environment using open source and custom software stacks.

MINIMUM QUALIFICATIONS
 Must be a U.S. citizen with the ability to obtain and maintain an active Secret clearance
 PhD + 5 years experience OR MS + 8 years OR BS + 10 is required
 The candidate must have expertise with Python development, tools, common design patterns, and numerical packages.
 Significant experience with mathematical programming in Python
 Strong object-oriented development skills
 Experience with web application development using model-view-controller architecture (Django)
 Expertise in high performance multi-threaded applications
 Experience developing RESTful and/or SOAP based web services
 Experience with modern development tools such as git, GitHub, JIRA, Confluence (or other Wiki technology), etc.
 Experience with Python dependency and packaging tools such as Artifactory, pip, setuptools, distutils, pybuilder, etc.
 Solid understanding of Linux and Windows operating systems, basic shell scripting
 Work well within a fast moving, small team environment
PREFERRED QUALIFICATIONS
 Front end development using AJAX, jQuery, JavaScript, Angular, Bootstrap, HTML, CSS, CSS3
 Knowledge of full software lifecycle automation and testing tools in a DevOps environment
 Familiarity with standard relational (SQL) databases
 Familiarity with at least one non-relational database (Redis, Cassandra, MongoDB) or search engine (Elasticsearch)
 Familiarity with graph query languages such as Gremlin, Cypher, SPARQL
 Familiarity with big data technologies such as Hadoop, Cassandra, HBase, Storm, Kafka, Spark or Zookeeper
 Experience developing distributed and cloud-based systems and services
 Demonstrated knowledge in machine learning or statistical data inference
 Experience in developing machine learning, natural language processing or graph analytic applications
 Demonstrated ability to analyze, research, develop and rapidly prototype complex algorithms
 Existing DoD security clearance
WORKING LOCATIONS & ADDITIONAL INFO
 Location: Lexington, Ma
 Duration: 3 years and 40 hours a week
 Travel: Infrequent, approximately once every 4-6 months for a period of 1-2 days.
 Employment Status: Full-Time Employee with full benefits (Medical, Dental, Vision, STD, LTD, PTO, Retirement)
 Other: All candidates must also successfully pass a Commercial Background Investigation (CBI).

This job posting sets forth the authorities and responsibilities of this position, which may be changed from time to time as shall be determined.
Odyssey Systems Consulting Group, LTD. is an Equal Opportunity/Affirmative Action employer. All qualified applicants will receive consideration for employment without regard to race, color, religion, sex, pregnancy, national origin, disability, sexual orientation, gender identity or expression, marital status, genetic information, protected veteran status, or other factors protected by federal, state, and/or local law. This policy applies to all terms and conditions of employment, including: recruiting; hiring; placement; promotion; termination; layoff; recall; transfer; leaves of absence; compensation; and training.
#LI-POST
Equal Opportunity Employer/Protected Veterans/Individuals with Disabilities
The contractor will not discharge or in any other manner discriminate against employees or applicants because they have inquired about, discussed, or disclosed their own pay or the pay of another employee or applicant. However, employees who have access to the compensation information of other employees or applicants as a part of their essential job functions cannot disclose the pay of other employees or applicants to individuals who do not otherwise have access to compensation information, unless the disclosure is (a) in response to a formal complaint or charge, (b) in furtherance of an investigation, proceeding, hearing, or action, including an investigation conducted by the employer, or (c) consistent with the contractors legal duty to furnish information."
Software Engineer,Zayo Group,"Position Description:

Develop scripts and tools to automate network element configuration by performing the following duties:

Essential Duties/Responsibilities:
Develop Python scripts to access network elements with the purpose of audit and configuration update
Modify and expand existing scripts to build flexible support for multiple types of network elements
Gain knowledge of specific network element requirements on the job
Other Duties/Responsibilities:
Work directly with the Network Operations Center to test and support operation of scripts against production network elements
Work directly with vendors as required for special network knowledge
Other duties as assigned
Knowledge, Skills and Abilities Required:
1-3 years experience with Python development
Knowledge of SQL, relational databases, and other data storage solutions
Knowledge of communication protocols TCP/IP, Telnet, SSH, HTTP, SNMP
Knowledge of APIs used in SDN environment (REST, RESTCONF, NETCONF)
Unix/Linux skills, must be comfortable working in Linux command line
Flexible schedule to support the deployment of after-hours maintenance requests
Excellent verbal and written communication skills
Detail oriented
Flexible and adapts well to a constantly changing environment
Education and/or Experience:
Experience with programming and automation of network devices (L0-L3) a plus
Experience with TL-1 protocol or related command line interfaces a plus
Working Conditions

Most work is done in an area where normal office noise is present. Must be able to work remote to support escalation requests and after hour network augments. Flexible schedule is required.

Zayo is an Equal Opportunity Employer. Zayo does not discriminate on the basis of race, religion, color, sex, gender identity, sexual orientation, age, non-disqualifying physical or mental disability, national origin, veteran status or any other basis covered by appropriate law. All employment is decided on the basis of qualifications, merit, and business need."
Software Engineer,The Johns Hopkins Applied Physics Laboratory,"The Johns Hopkins Applied Physics Laboratory (JH APL), an independent, nonprofit organization that conducts essential research, development, and systems engineering to address national security challenges, located midway between Baltimore and Washington, DC is seeking a creative Software Engineer and Configuration Manager.
Job Summary: Participate in a team environment to develop and utilize tools and scripts to assess crucial naval system engineering issues associated with Integrated Air and Missile Defense (IAMD). Utilize these tools to (a) maintain existing modeling and simulation capabilities, (b) identify and resolve unintended model changes, and (c) develop new innovative ways to ensure software quality.

Duties (In order of importance with estimated percentage for each):
Participate in development of modeling and simulation tools written in C++, Python, and MATLAB. Work includes object oriented design, algorithm design, algorithm implementation, code refactoring, software testing, and associated documentation. (30%)
Develop and maintain testing suite of tools written in MATLAB, Python, and C++. Work includes identification of test cases, development of automated anomaly detection algorithms, and working with developers to ensure unintended changes are resolved in a timely manner. (30%)
Oversee configuration management of software applications in a variety of tools including Visual Studio Team Foundation Server, Subversion, GIT, and Mercurial. Work includes co-maintaining the development server, branching and merging code, and version control. (25%)
Provide subject matter expertise for military test events and real world scenarios by conducting research, studies, and analysis to provide supporting data and associated recommendations to advise our Sponsors. (15%)
Note: This job summary and listing of duties is for the purpose of describing the position and its essential functions at time of hire and may change over time.

Qualifications
: Required Qualifications: BS Degree in Computer Science, Computer Engineering, Mathematics, Physics, or equivalent. One to two years of experience with software development or testing. Experience with C++ or Java programming either in the workplace or an educational environment. Experience working successfully within a team environment. Excellent written and communication skills. Active SECRET clearance or ability to obtain INTERIM SECRET clearance.

Desired Qualifications: MS Degree in Computer Science or Computer Engineering. Two to four years of experience with configuration management and software testing. Experience with Team Foundation Server (TFS), Subversion, Mercurial, or GIT configuration management software. Experience with test automation and optimization. Proven success working in a multi-disciplinary team environment. Working knowledge of Ballistic Missile Defense and Aegis Command and Control. Active TOP SECRET clearance.

Special Working Conditions: Occasional travel to Sponsor, contractor, and peer organization facilities for meetings and reviews. Possible occasional support of test events involving extended hours, travel aboard ships, and travel to field site activities. International travel not envisioned at this time.

Security: Applicant selected will be subject to a government security clearance investigation and must meet the requirements for access to classified information. Eligibility requirements include U.S. citizenship.

Benefits: APL offers a comprehensive benefits package including a liberal vacation plan, a matching retirement program, significant educational assistance, a scholarship tuition program for staff with dependents, and competitive salaries commensurate with skills and experience."
Software Engineer,AssetWorks,"AssetWorks, a leader in enterprise asset management software, is on the market for an enthusiastic Mid-level Software Developer with 4+ years of experience in object oriented programming and components/Interface development utilizing Microsoft technologies like C# or vb.net, ASP.Net and web services.

The Software Developer will assist with our M5 Fleet Core product development. This is essentially a full-stack role since UI and backend experience are a must!
Job Description:
JOB RESPONSIBILITIES:
 Assist in the development of client sponsored functional enhancements to the web based application M5 FleetFocus.
 Maintenance of existing C#, ASPX, HTML5, XML and Java Script code.
 Construct SQL statements to select, insert and update data utilizing Oracle and SQL Server databases. Perform database table enhancements such as creation of views and the addition or modification of table columns.
 Assist in defining or modifying C# based system interfaces for 3rd party integrations.
 Investigate and implement solutions for trouble tickets as assigned.
 Consult with Project Managers, Quality Assurance and Support staff on the development of M5 FleetFocus products.
 Prepare and deliver technical correspondence to support both internal and external communications associated with the development/support of M5 FleetFocus.
 Utilize Microsoft Visual Studio 2015 and SVN to modify and check-in application code.
 Perform other duties as may be assigned by management.
Requirements include: Bachelors degree from an accredited college or university or a certificate from a technical training institute with at least three years of programming experience with related technologies; Creativity and proven track record in the use of and C# and ASP.Net are highly desired skills. XML, HTML/HTML5, Java Script and web technology as well as Oracle and SQL Server databases are desired competencies. Ability to interact positively and effectively in a team environment; excellent organizational, interpersonal, written, and verbal communication skills; ability to perform comfortably in a fast-paced, deadline-oriented work environment; ability to successfully execute many complex tasks simultaneously; and ability to work as a team member, as well as independently.
Worker Type:
Regular
Number of Openings Available:
1"
Software Engineer,Barbaricum,"Barbaricum is seeking driven software engineers with solid software development backgrounds who can solve big data problems with a cutting-edge data analysis platform. Candidates will have access to interesting technical problems and have the freedom to implement solutions on top of software while working with a team that maximizes efficiency and minimizes process friction. Youll have the ability to achieve operational impacts on a daily basis while supporting critical Government missions. Interested candidates should be willing to prepare for an interview process, which includes topics of coding, site reliability engineering (SRE), debugging, core Linux OS fundamentals, and technical problem solving.

QUALIFICATIONS
All clearance levels Secret and above are desired: Secret, TS/SCI, TS/SCI with CI Poly, TS/SCI with FS Poly
BS in Computer Science required
Proficiency in JVM languages (Java, Groovy, etc.)
Proficiency in Linux administration, networking, & tuning
Comfortable with Linux command line
Solid software development background with versatile knowledge base to solve a variety of technical challenges
Desire to solve big data problems

PREFERRED EXPERIENCE
DevOps (Puppet, Chef, Ansible)
Data storage (Oracle, Cassandra, Postgres)
SSL/PKI/Encryption
Recent Department of Defense experience"
Software Engineer,Honeywell,"Join a team recognized for leadership, innovation and diversity

Honeywell is a leading software industrial business, harnessing the power of cloud, mobile, data & analytics, IoT and design thinking. With the addition of our state-of-the-art software and innovation center in Midtown Atlanta, we will incubate, deploy and scale breakthrough offerings that will impact the lives of those around us and the world. There is no better time to join our team, showcase your skills and play a key part in advancing our offerings, market presence and culture.

The IoT Edge team will engineer contemporary architecture and services, constructing solutions that remain scalable, adaptable and replicable at the Edge across a range of businesses and applications. Our goal is to provide a foundation for our businesses to innovate faster and drive our competitive advantage. As a member of the Honeywell IOT Edge team, the candidate will be responsible for leading the design, development and implementation of our Edge Analytics IoT data solutions.

As a Lead Software Engineer with Honeywell, this is your opportunity to:
Lead developing strategic architecture and design patterns to process in real time data sets in constrained IoT Edge systems
Design and build a platform infrastructure to support Analytics functions at the IoT Edge, including tools to visualize, interpolate and correlate data streams in real time, aggregate and report results to the Cloud IoT Platform, deploy new rules on live systems.
Innovate by researching and deploying new tools, frameworks and patterns to build a sustainable Edge Analytics platform.
Triage, deep dive and troubleshoot complex issues in the analytics applications.
Identify opportunities to leverage and train 1st line support to prevent escalations in the future.
Develop, manage and follow operational policies and procedures including documentation and training.
Provide insight on new technology performance and recommendations of adoption opportunities.
This position is eligible for the Student Loan Paydown Benefit

35
Requirements and design

35
Software architecture and coding

30
Mentoring

YOU MUST HAVE:
Bachelors degree in Computer Science or related discipline.
2+ years of hands-on implementation experience developing Software in an embedded environment
2+ years of hands-on implementation experience working with at least one of the following technologies around Analytics: Commercial Tools, Python/Anaconda
2+ years of hands-on implementation experience working with real-time and in-memory database technology
WE VALUE:
Proficiency in embedded processing patterns
A solid background in designing secure, reliable, highly performant, real-time systems
Proficiency in managing the Requirements chain to influence both Hardware and Application design for optimal performance
Proficiency in languages such as C/C++, Java, Python, R, Linux, RTOS
Hands-on experience in data ingestion, filtering, identifying data patterns and predictive analytics techniques.
Working experience of designing & developing Edge solutions to securely connect to cloud platforms (Azure, AWS etc.)
Working experience with various tools for data ingestion and transformation like NiFi or MiFi
Experience in developing the full life-cycle: requirements analysis, design of the technical architecture, development, testing, and deployment of the proposed solution
Experience working in Agile development methodology
Working Experience with continuous integration and continuous delivery tools like Git, Bamboo
Experience working with and evaluating both commercial and open source technologies, and demonstrated ability to make objective choices
Understanding of the concepts around cloud based Big Data and Machine Learning
Excellent interpersonal and communications skills

Due to US export control laws, must be a US citizen, permanent resident or have protected status. Exempt How Honeywell is Connecting the World INCLUDES
Relocation Provided
Continued Professional Development
ADDITIONAL INFORMATION
Job ID: req117659
Category: Information Technology
Location: 115 Tabor Road, Morris Plains, NJ 07950 USA
Honeywell is an equal opportunity employer. Qualified applicants will be considered without regard to age, race, creed, color, national origin, ancestry, marital status, affectional or sexual orientation, gender identity or expression, disability, nationality, sex, or veteran status."
Data Engineer Intern (Sourcing Operations Engineering),Facebook,"(Menlo Park, CA) Facebook's mission is to give people the power to build community and bring the world closer together. Through our family of apps and services, we're building a different kind of company that connects billions of people around the world, gives them ways to share what matters most to them, and helps bring people closer together. Whether we're creating new products or helping a small business expand its reach, people at Facebook are builders at heart. Our global teams are constantly iterating, solving problems, and working together to empower people around the world to build community and connect in meaningful ways. Together, we can help people build stronger communities  we're just getting started. Facebook is seeking a Data Engineer intern to support and enable the continued growth critical to Facebooks Data Center organization. The intern will be responsible for performing data analysis in the Data Center engineering, design and construction sourcing space to better support internal planning and execute enhanced sourcing strategies. The intern will be expected to use data to provide meaningful recommendations and actionable strategies to key stakeholders, and develop best practices, including streamlining of data sources and related programmatic initiatives.

The ideal candidate will have a passion for working in white space and creating impact from the ground up in a fast-paced environment. Additionally, the ideal candidate will have a proven track record of thought leadership and impact in architecting, implementing and deploying data processes in production. This full-time internship position is part of the Infrastructure Data Center  Sourcing and Operations Engineering team, and is in our Menlo Park office.

What is working at Facebook like? If you like fast-paced decision making, acting on fact, creating your own future, decision making autonomy and building something new and different from scratch  then this is the role and company for you. At Facebook, your role is as big as you define it to be. The impact you have on the business is determined by your appetite for homeruns, and your skills and capability. Responsibilities
Architect, implement and deploy new data models and data processes in production.
Perform data analysis to generate business insights.
Interface with Sourcing Managers, Suppliers and Quality Engineers to understand data needs.
Build data expertise and own data quality for allocated areas of ownership.
Manage data warehouse plans for a workstream.
Support critical data processes running in production.
Minimum Qualifications
Pursuing a Masters or PhD degree in one of the following areas: Computer Science, Mathematics, Physics, or related technical field, preferred
Experience using data access tools and building meaningful visualizations using large datasets and multiple data sources
Programming expertise in a language of your choice
Knowledge of SQL
Knowledge of database systems
Curious, self-driven, analytical and excited to play with data
Ability to thrive in a fast-paced work environment
Technical knowledge of data center operations is a plus
Ability to obtain and maintain work authorization in the United States in 2018"
Data Engineer Intern - New Ventures Campus,McKinsey & Company,"Qualifications
Be currently enrolled in an academic program with expected graduation date of Dec. 2018 through Aug. 2019
Ability to structure and analyze data in appropriate framework (e.g., Excel, Access, VBA, SQL)
Passion for data tools and processes to drive business insights
Strong analytical thinking, process management and quality control
Ability to quickly understand and appreciate underlying business context, problems and objectives of analytical projects
Clear communication skills to run well defined analyses and produce reports
Excellent time management skills

Who You'll Work With Youll work with our New Ventures team in one of our North America offices over the summer for ~10 weeks.

McKinsey New Ventures fosters innovation driven by analytics, design thinking, mobile and social by developing new products/services and integrating them into our client work. It is helping to shift our model toward asset-based consulting and is a foundation for and expands our investment in our entrepreneurial culture. Through innovative software as a service solutions, strategic acquisitions, and a vibrant ecosystem of alliances, we are redefining what it means to work with McKinsey.

As one of the fastest-growing parts of our firm, New Ventures has more than 1,500 dedicated professionals (including more than 800 analysts and data scientists) and were hiring more mathematicians, data scientists, designers, software engineers, product managers, client development managers and general managers.Jr. Specialists have a base of expertise in a function or industry. They are staffed on engagements (largely in full time roles) and are expected to leverage expertise to solve some of the most pressing and complex issues at clients.
What You'll Do You'll conduct full life cycle analysis of data sets in consultation with senior colleagues, including implementing data acquisition, cleansing, transformation and upload activities.

You will provide analysis, analytical modeling, and/or visualization of data sets using relevant data tools. You'll identify possible trends and patterns in the data, and communicate relevant findings that help support problem solving, insight generation and decision making.

You'll apply your understanding of client data sets and intended use to effectively capture, validate, cleanse, transform and upload data. You'll perform quality checks to ensure consistency, integrity and robustness of data. You'll proactively identify potential issues (e.g., inconsistencies) with data quality, collection or reporting. You'll support solution delivery team, often remotely, in managing client data.

You'll leverage standard reporting to clearly articulate findings to solution delivery team or client team. You will be responsible for coding and scripting (SQL) for automating data transformation and loading specified by delivery team. You will perform impactful data enrichment, based on understanding of the sector/industry and types of data that are available in the space."
Senior Big Data Engineer,Fannie Mae,"Description:
THE COMPANYAre you interested in helping solve today's most critical housing challenges? In simplest terms, Fannie Mae serves the people who house America. We work at the heart of housing by providing reliable, affordable mortgage financing in all markets at all times, buying loans that banks and other lenders originate, so they can fund new loans. This gives more people the opportunity to buy, refinance, or rent homes and apartments. Creating these opportunities is what drives the people who work at Fannie Mae.For more information about Fannie Mae, visit http\: //www.fanniemae.com/progress

JOB INFORMATION
Join our Big Data Engineering team where were cutting-edge technologies to solve large-scale challenges. Our Big Data ecosystem consists of tools including, but not limited to\: Hadoop, HBase, Zookeeper, Kafka, Storm, and Sqoop. Youll be responsible for leading development teams' efforts to determine unit needs and business processes that are automated by the applications. Youll also participate in or review all of the steps in the software development life cycle to create and modify the software.

KEY JOB FUNCTIONS

Work with product owners and other development team members to determine new features and user stories needed in new/revised applications or large/complex development projects.
Create or Update documentation in support of development efforts. Documents may include detailed specifications, implementation guides, architecture diagrams or design documents.
Participate in code reviews with peers and managers to ensure that each increment adheres to original vision as described in the user story and all standard resource libraries and architecture patterns as appropriate.
Respond to trouble/support calls for applications in production in order to make quick repair to keep application in production.
Serve as a technical lead for an Agile team and actively participate in all Agile ceremonies. Participate in all team ceremonies including planning, grooming, product demonstration and team retrospectives.
Mentor less experienced technical staff; may use high end development tools to assist or facilitate development process.
Leverage Fannie Mae DevOps tool stack to build, inspect, deploy, test and promote new or updated features. Qualification:

EDUCATION
Bachelor's Degree or equivalent required

MINIMUM EXPERIENCE
6+ years of related experience

SPECIALIZED KNOWLEDGE & SKILLS

6+ years of hands on experience and strong and deep knowledge of Java application development
Experience processing large amounts of structured and unstructured data. MapReduce experience is a huge plus.
Experience building and coding applications leveraging Hadoop Components\: HDFS, HBase, Hive, Sqoop, Kafka, Storm etc
Experience coding in more than one of the following\: Java, MapReduce, Python, Pig Programming, Hadoop Streaming, HiveQL
Experience developing RESTful Web Services
Agile/scrum experience
Experience leading and managing large scale, complex applications with high performance needs
Vendor management experience leveraging staff augmentation and/or outcome based project delivery models; statement of work planning and incremental demand forecasting
Experience managing on-site and off-site staff and demonstrated ability to collaborate and influence others to ensure timely and effective completion of project tasks
Excellent written and verbal communication skills
EMPLOYMENT
As a condition of employment with Fannie Mae, any successful job applicant will be required to pass a pre-employment drug screen and to successfully complete a background investigation.

Fannie Mae is an Equal Opportunity Employer."
Sr. Software Engineer (Data Domain File Systems),DELL,"Why Work at Dell?
Endless challenges and rewards. Opportunities on six continents. A team of colleagues fueled by collaboration. All this, and a company deeply committed to integrity and responsibility."
Power BI Engineers,Capax Global LLC,"This is a full-time position for a consulting role to be a member of our customer-facing project delivery teams for POCs, Technical Assessments, and Production Project Delivery.

Location: Chicago, IL

Travel: Up to 30%

Required Skills:
Bachelors Degree
2 + years relevant experience in consulting on large scale Data Warehouse/Big Data projects in team environments.
Experience with Power BI, ( PerformancePoint, PowerView a plus) , SSRS, SSAS, Excel Services
Knowledge of Agile processes (Scrum preferred)
Experience with Power BI Dashboard/Report , Data Warehouse, Data Modeling and Data Integration
Proficiency in extracting and manipulating large data sets with SQL in MS SQL.
Some e xperience (not all mandatory) using mapping Software and Programming Languages: SQL, R, DAX , Microsoft Power BI, SSRS, Excel, and Tableau
Experience designing and building large-scale, distributed systems
Able to work with the customers stakeholders to understand business and technical requirements.
Experience with complex data modeling
Able to identify and communicate issues/risks applying technical and root cause analysis skills
Strong decision making, problem solving, and analytical skills
Excellent communication, presentation, influencing, and reasoning skills
Creativity and ability to think outside-the-box while defining sound and practical solutions

Preferred Skills ( not all mandatory) :
Experience with the following Technologies/Methodologies:
Azure and/or AWS
Tableau
SQL Server 2014/2016
Oracle, DB2, NoSQL
C#/.NET
Azure Data Services/Azure SQL Data Warehouse"
"Principal Engineer, Big Data Platform",Conviva,"Have you watched Game of Thrones on www.hbo.com or live broadcasting of FIFA World Cup on www.espn.com ? If the answer is yes, you have already used Conviva technologies.
With a view into over 4 billion streams per month from over 1.6 billion unique devices, Conviva provides a global map of the Internet with detailed intelligence, giving online video providers the power to navigate the pitfalls of delivering video over an infrastructure not built for video. By recording detailed, real-time information on each and every viewers experience during every viewing session, Convivas big data platform not only offers an unparalleled view into what is happening, but the ability to preemptively avoid the bottlenecks and outages that cause viewers to stop watching. We are looking for exceptional principal engineers and engineering leaders with a proven record who can help take the next major innovative steps in our path. This is a senior leadership role in the engineering team with a path toward either organizational leadership or technology leadership (or both).

As a Principal Engineer of Conviva big data platform team, you are a hands on individual contributor and technical expert in the big data platform area. You will be responsible for guiding and making the architectural decisions for the team, lead major initiatives of technology innovation and advancement, lead and contribute to the design and implementation of major new products and features with amazing systems in production environment that will have big business impacts for years to come. You will also work cross teams and organizations to ensure end to end product delivery with high quality, robustness, resilience and scalability. You will also help establishing technical standards and driving the overall technical architecture and best engineering practices through design reviews, architecture discussions and other technical leader activities.

What you get to do:
Serve as a technical lead on our most demanding, cross-functional projects. Exert technical influence over multiple teams, increasing their productivity and effectiveness by sharing your deep knowledge and experience
Ensure the quality of architecture and design of systems
Functionally decompose complex problems into simple, straight-forward solutions
Fully and completely understand system interdependencies and limitations
Leverage knowledge of internal and industry prior art in design decisions
Contribute intellectual property through patents
Assist in the career development of others, actively mentoring individuals and the community on advanced technical issues and helping managers guide the career growth of their team members What you bring to the role

BS in CS or equivalent
10+ years of software engineering experience, including Java, Scala, Python and/or C++
Deep hands-on technical expertise in at least one major technical area: big data processing platforms (e.g. Spark, Hadoop, Storm, Google Dataflow), big data storage and query technology (Hive/Phoenix, Redshift, Presto/Anthena, Snowflake, Impala, BigTable, BigQuery), messaing layer (Kafka, Kinesis), Cloud and container based deployments (Docker, Kubernetics)
Proven track records of leading the delivery or large scale big data projects successfully in production environments
Proven problem solving, software design and architecture skills
Comfortable with ambiquity
Possess expert knowledge in performance, scalability, big data technology and engineering best practices
Plus: Advanced degree in CS or related fields
Plus: Experience working in a complex, service oriented software development
2+ years experience as a Principal-level software engineer
Experience in Software as a Service Conviva powers every internet-connected screen with the most engaging viewing experiences imaginable by elevating the way OTT businesses use data-driven intelligence. For years, HBO, Sky, ESPN and the like have been using the Conviva Platform to enlighten, reveal and inform with important insights around consumer in-screen viewing experience allowing them to connect those metrics to important business outcomes. This allows customers to not only maximize subscriber retention and growth but also understand content and viewing trends so that they can deliver more personalized viewing experiences. We make engagement a data-driven outcome based on actionable quality of experience (QoE) analytics. Conviva is privately held and headquartered in Silicon Valley, California, with offices in New York and London. For more information, please visit us at www.conviva.com"
Intern - Energy Efficiency Analyst,Rexroth,"Job Description

This position is primarily to work on 4EE (energy efficiency) Retrofit projects, documentation and support processes

To learn and properly apply Bosch Rexroth products to customer machines within the 4EE area

Assist in the development of technical analysis of machine

Development of sales tools to promote the 4EE advantages

Assist in the Customer query process

Develop an automation method to capture, store and analyze data

Assist Applications Engineer in defining the application requirements and developing a quotation Qualifications

Strong communications skills, both written and verbal

High knowledge or Understanding of motion controls as related to electrical and/or mechanical machinery

Ability to travel to customer site as necessary

Ability to read / interpret Electrical and Mechanical prints

Good Organizational skills

Positive attitude / professionalism

Commitment to customer satisfaction

Ability to multi task

High level of computer knowledge (Microsoft office suite, excel, ppt)

Team Player - work well in teams

Open minded

Additional Information

Eligibility
We invite applications from enrolled students from the following BERN target universities only :
University of Illinois at Urbana-Champaign
Purdue
University of Michigan
Michigan State University
Ohio State University
Caltech
Stanford
MIT
UC Berkeley
Georgia Tech
University of South Carolina
Carnegie Mellon
Indiana University"
Machine Learning Engineering Manager  Creator,Spotify,"We are looking for a Machine Learning Engineering Manager (Chapter Lead) to drive the machine learning practice within the Creator Mission.

The Creator Mission strives to provide Artists with amazing insights and tools to help build and grow their careers and engage more deeply with their fans. In this role, you will be instrumental in growing the team, guiding the scientific and engineering approaches, and communicating strategy and results to the broader organization. You will be a partner to the Product Management team and help them define an effective ML-driven product strategy and roadmap.

Spotify has unique data, with rich information on the behavior of its 100+ million users across time. You will have the opportunity to have a large impact on how this data is used to build predictive models to power useful insights and features. The results of this effort will be used to improve the experience for users and to create products for artists, our marketing and content organizations and more.

Our organization of technologists, designers, and product managers is located in New York, Boston, London and Stockholm.

What youll do
You will be responsible for building upon Spotifys deep understanding of our content, users, and artists to develop rich analytics, engagement, and business applications
You will build and lead a team of engineers through hiring, coaching, mentoring, and hands-on career development
You will provide technical guidance in a number of aspects of data science, machine learning and engineering including hypothesis testing, analysis, modeling, and production deployment, especially in a JVM ecosystem
You will be hands-on occasionally as needed
You will work closely with counterparts in other disciplines as part of a cross-functional team, and nurture this culture in your team
You will be based in New York, but might travel occasionally
Who you are
You are either an experienced manager or a top-level individual contributor looking to make a move to team leadership
You are passionate about technology and what it can do for building business applications
You thrive when developing great people, not just great products
You are an experienced leader in using modern practices and tools to ingest, move, process, store, and expose consumer-scale datasets
You are well-versed in data-driven and data-informed product development
You are comfortable with the nuts and bolts of both data engineering and data science
You have experience in fostering a strong engineering culture in an agile environment
You want to make an impact
We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music."
Engineering Manager  Machine Learning,Spotify,"We are looking for a Senior Engineering Manager (Chapter Lead) with expertise in Machine Learning to join the User Engagement mission in New York. Our organization strives to make every user session amazing through personalization and discovery. In this role, youll work primarily with our squads based in New York who are focused on recommendations and personalized features (e.g. Discover Weekly, Release Radar, Daily Mix and Home), notifications and programming analytics.

What youll do
You will serve a group of machine learning engineers and research scientists through hiring, coaching, mentoring, career development, and, when needed, hands-on engineering
You will support those engineers and scientists in building upon Spotifys deep understanding of our content, users, and artists to facilitate development of rich and engaging experiences
You will provide technical mentorship in machine learning engineering and research topics including hypothesis testing, analysis, modeling, and production deployment, especially in a JVM ecosystem
You will work closely with other machine learning Chapter Leads to continue to mature the use of machine learning in Spotifys New York office and beyond
You will be based in New York, but travel occasionally to our Stockholm office
Who you are
Strong background in machine learning or a related field. Graduate education preferred.
Experience giving hands-on leadership, whether formally or informally (e.g., mentoring), to individuals implementing machine learning systems at scale in Java, Scala, Python or similar languages
You care about agile software processes, data-driven development, reliability, and disciplined experimentation
You preferably have experience with high-scale data processing and storage frameworks like Hadoop, Scalding, Spark, Storm, Cassandra, Kafka, etc.
You thrive when developing great people, not just great products
You want to make a global impact and believe music improves lives
We are proud to foster a workplace free from discrimination. We strongly believe that diversity of experience, perspectives, and background will lead to a better environment for our employees and a better product for our users and our creators. This is something we value deeply and we encourage everyone to come be a part of changing the way the world listens to music."
